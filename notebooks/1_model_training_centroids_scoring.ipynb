{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a39bd0b5-2ac1-499d-8938-2fdfe4c3d93d",
   "metadata": {},
   "source": [
    "# Development of Emotion and Reasoning in the General Speeches of the United Nations: A text-based machine learning approach\n",
    "## Script 1: Model Traning, Calculation Centroids & Speech Scoring\n",
    "\n",
    "Ensure that script 0_data_creation ran beforehand successfully.\n",
    "\n",
    "### Description:\n",
    "- Uses `clean_speeches_indexed` for sentence split (function repeats preprocessing but does not work on tokenized data; therefore repetition of the preprocessing)\n",
    "- Train Word2Vec model\n",
    "- Create centroids by multiplying weighted frequency with the vectors (one for affect dictionary list, the other for cognition)\n",
    "- Score speeches (repeat on doc level) and then compute cosine similarity for emotionality divided by cosine distance to rationality centroid\n",
    "- Store everything as `un_corpus_scored`\n",
    "ored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ca6f721-48ad-495c-8e5f-e60790225709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root set to: C:\\Users\\sarah\\Downloads\\TESTRUN\n"
     ]
    }
   ],
   "source": [
    "# == Import libraries ==\n",
    "\n",
    "import os\n",
    "import gensim\n",
    "from pathlib import Path\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from random import shuffle\n",
    "import nltk\n",
    "from gensim.utils import simple_preprocess\n",
    "import joblib\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from string import punctuation\n",
    "from scipy.spatial.distance import cosine\n",
    "import glob\n",
    "import spacy\n",
    "from multiprocessing import Pool, freeze_support\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# == Initialize NLP Tools ==\n",
    "\n",
    "translator = str.maketrans('', '', punctuation) \n",
    "tagger = nltk.perceptron.PerceptronTagger()\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# == Set Working Directory ==\n",
    "\n",
    "# --- Set base path to project root ---\n",
    "base_path = Path.cwd().parent  # project root\n",
    "print(f\"Project root set to: {base_path}\")\n",
    "\n",
    "# == Define Folder Paths ==\n",
    "\n",
    "# Folders were already created in the script 0_data_creation\n",
    "data_c = base_path / \"data\"\n",
    "data_temp = data_c / \"temp\"\n",
    "data_freq = data_c / \"freq\"\n",
    "data_dict = data_c / \"dictionaries\"\n",
    "data_sent = data_c / \"sentences\"\n",
    "data_preprocessed = data_c / \"preprocessed\"\n",
    "data_results = data_c / \"results\"\n",
    "data_models = data_c / \"models\" \n",
    "data_stopwords = data_c / \"stopwords\"\n",
    "\n",
    "# Load ressources\n",
    "stopwords = joblib.load(data_stopwords / \"stopwords_procedural_words.pkl\")         \n",
    "word_counts = joblib.load(data_freq / \"word_counts.pkl\")\n",
    "word_counts_weighted = joblib.load(data_freq / \"word_counts_weighted.pkl\")\n",
    "affect_dic = joblib.load(data_dict / 'dictionary_affect.pkl')\n",
    "cognition_dic = joblib.load(data_dict / 'dictionary_cognition.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "125227ca-a0b5-43d7-956d-7d49a66f81b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_temp)\n",
    "cleaned_files = [\n",
    "    str(data_temp / 'clean_speeches_indexed1.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed2.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed3.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed4.pkl')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855a7f37-bbf9-4259-8a07-7e2a589f1de4",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2645f36-e935-4b0e-b5e6-7461f398d887",
   "metadata": {},
   "source": [
    "### Sentence Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af0997e8-9da3-45c3-bdd4-7d38a708a474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of very short sentences being dropped: 86798\n",
      "C:\\Users\\sarah\\Downloads\\TESTRUN\\data\\temp\\clean_speeches_indexed1.pkl processed\n",
      "Unique tokens: 8157\n",
      "Example sentences (first 5):\n",
      "['choos', 'hostil']\n",
      "['outstand', 'alli', 'charter']\n",
      "['accumul', 'enorm', 'multilater']\n",
      "['aspect', 'scene', 'viet', 'nam']\n",
      "['colombia', 'pleas', 'expert', 'expert', 'colombia', 'likewis', 'colombian', 'connexion', 'invest']\n",
      "C:\\Users\\sarah\\Downloads\\TESTRUN\\data\\temp\\sentences_indexed1.pkl saved\n",
      "Number of very short sentences being dropped: 65694\n",
      "C:\\Users\\sarah\\Downloads\\TESTRUN\\data\\temp\\clean_speeches_indexed2.pkl processed\n",
      "Unique tokens: 8613\n",
      "Example sentences (first 5):\n",
      "['children', 'celebr', 'anniversari', 'demonstr', 'multilater', 'contribut']\n",
      "['forum', 'sprung', 'spontan', 'generat']\n",
      "['afghanistan', 'kampuchean', 'outright', 'interfer', 'kampuchea']\n",
      "['nigeria', 'chad']\n",
      "['cold', 'certainti', 'instabl', 'uneas', 'uncertainti', 'endeavour']\n",
      "C:\\Users\\sarah\\Downloads\\TESTRUN\\data\\temp\\sentences_indexed2.pkl saved\n",
      "Number of very short sentences being dropped: 48615\n",
      "C:\\Users\\sarah\\Downloads\\TESTRUN\\data\\temp\\clean_speeches_indexed3.pkl processed\n",
      "Unique tokens: 8228\n",
      "Example sentences (first 5):\n",
      "['farm', 'clean', 'drink', 'healthi', 'breath']\n",
      "['millennium', 'rica', 'dialogu', 'popular', 'strengthen', 'democraci']\n",
      "['emphas', 'multilater', 'promot', 'democraci', 'human', 'social', 'equiti', 'environment']\n",
      "['poor', 'afflict', 'degre', 'terribl', 'diseas']\n",
      "['barbado', 'share', 'vision', 'achiev', 'surpass', 'millennium', 'goal']\n",
      "C:\\Users\\sarah\\Downloads\\TESTRUN\\data\\temp\\sentences_indexed3.pkl saved\n",
      "Number of very short sentences being dropped: 56143\n",
      "C:\\Users\\sarah\\Downloads\\TESTRUN\\data\\temp\\clean_speeches_indexed4.pkl processed\n",
      "Unique tokens: 8179\n",
      "Example sentences (first 5):\n",
      "['testament', 'sovereignti', 'relationship']\n",
      "['gather', 'area']\n",
      "['strengthen', 'partnership', 'equit', 'african']\n",
      "['twostat', 'solut', 'vigour']\n",
      "['signific', 'environment', 'global', 'econom']\n",
      "C:\\Users\\sarah\\Downloads\\TESTRUN\\data\\temp\\sentences_indexed4.pkl saved\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_temp)\n",
    "\n",
    "# Function to split cleaned speeches (clean_speeches) into sentences, tokenize, clean, tag, stem, filter, and save them.\n",
    "\n",
    "def extract_sentences(dataname):\n",
    "    \"\"\"\n",
    "    Preprocess speeches into tokenized, cleaned sentences.\n",
    "\n",
    "    Steps:\n",
    "    - Load speeches and split into sentences\n",
    "    - Remove very short sentences and digits, lowercase, POS-tag, stem, remove stopwords\n",
    "    - Keep words with frequency >= 10\n",
    "    - Save to a new file\n",
    "    \"\"\"\n",
    "    \n",
    "    data = joblib.load(dataname)\n",
    "    data = [a[1] for a in data] \n",
    "\n",
    "    sentences = []\n",
    "    for doc in data:\n",
    "        sentences += sent_tokenize(doc) \n",
    "\n",
    "    sentences = [item for item in sentences if len(item.split()) > 1]\n",
    "    sentences = [gensim.utils.simple_preprocess(item) for item in sentences]\n",
    "\n",
    "    sentences = [[a for a in s if not a.isdigit()] for s in sentences]\n",
    "    sentences = [[a for a in s if len(a) > 2] for s in sentences]\n",
    "\n",
    "    sentences = [tagger.tag(s) for s in sentences]\n",
    "    sentences = [[i[0] for i in s if i[1].startswith(('N', 'V', 'J'))] for s in sentences]\n",
    "\n",
    "    sentences = [[stemmer.stem(i) for i in s] for s in sentences]\n",
    "    sentences = [[a for a in s if a not in stopwords] for s in sentences]\n",
    "    sentences = [[a for a in s if word_counts[a] >= 10] for s in sentences]\n",
    "\n",
    "    dropped_count = sum(1 for s in sentences if len(s) <= 1)\n",
    "    print(f\"Number of very short sentences being dropped: {dropped_count}\")\n",
    "\n",
    "    sentences = [s for s in sentences if len(s) > 1]\n",
    "    shuffle(sentences)\n",
    "\n",
    "    lab = dataname.replace('clean_speeches_', 'sentences_').replace('_.pkl', '.pkl')\n",
    "    print(f'{dataname} processed')\n",
    "    joblib.dump(sentences, lab)\n",
    "    \n",
    "    unique_tokens = set(token for s in sentences for token in s)\n",
    "    print(f\"Unique tokens: {len(unique_tokens)}\")\n",
    "\n",
    "    # Print preview of first 5 processed sentences\n",
    "    print(\"Example sentences (first 5):\")\n",
    "    for s in sentences[:5]:\n",
    "        print(s)\n",
    "\n",
    "\n",
    "    print(f'{lab} saved')\n",
    "\n",
    "for fname in cleaned_files:\n",
    "    extract_sentences(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9ae013e-bb3f-43d5-98e1-cd113108f0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentences (first 5):\n",
      "['choos', 'hostil']\n",
      "['outstand', 'alli', 'charter']\n",
      "['accumul', 'enorm', 'multilater']\n",
      "['aspect', 'scene', 'viet', 'nam']\n",
      "['colombia', 'pleas', 'expert', 'expert', 'colombia', 'likewis', 'colombian', 'connexion', 'invest']\n"
     ]
    }
   ],
   "source": [
    "# Pick the first file to see how the sentence split looks like\n",
    "file_path = os.path.join(data_temp, 'sentences_indexed1.pkl')\n",
    "\n",
    "sentences = joblib.load(file_path)\n",
    "\n",
    "print(\"Example sentences (first 5):\")\n",
    "for s in sentences[:5]:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fddaea83-7d1f-4b14-85b9-401b2e727c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_files = [\n",
    "    os.path.join(data_temp, 'sentences_indexed1.pkl'),\n",
    "    os.path.join(data_temp, 'sentences_indexed2.pkl'),\n",
    "    os.path.join(data_temp, 'sentences_indexed3.pkl'),\n",
    "    os.path.join(data_temp, 'sentences_indexed4.pkl')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c41580ba-2954-4d84-a574-e9c3d4c8ef1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens across all files: 9453\n",
      "Total number of tokens): 4286666\n"
     ]
    }
   ],
   "source": [
    "# == Get sum of unique tokens ==\n",
    "all_unique_tokens = set()\n",
    "total_token_count = 0\n",
    "\n",
    "for dataname in sentences_files:\n",
    "    data = joblib.load(dataname)\n",
    "    for sentence in data:\n",
    "        all_unique_tokens.update(sentence)\n",
    "        total_token_count += len(sentence)\n",
    "\n",
    "print(f\"Total unique tokens across all files: {len(all_unique_tokens)}\")\n",
    "print(f\"Total number of tokens: {total_token_count}\")\n",
    "\n",
    "sentences_files = [\n",
    "    'sentences_indexed1.pkl',\n",
    "    'sentences_indexed2.pkl',\n",
    "    'sentences_indexed3.pkl',\n",
    "    'sentences_indexed4.pkl']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28938326-25d1-43cc-b0a8-4400dc138584",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d8d0e2-3d81-41ea-90c4-6d36bc9fa11c",
   "metadata": {},
   "source": [
    "### Train Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "96249c10-a9dd-4bf4-b457-c5037f5a8299",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "19241543-24c5-4bfc-8d9e-20dc47825840",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for dataname in sentences_files:  \n",
    "    data = joblib.load(dataname)\n",
    "    dataset.extend(data) \n",
    "\n",
    "# === Model training ===\n",
    "w2v = Word2Vec(\n",
    "    sentences=dataset,    \n",
    "    vector_size=300,      # Dimension of the vector\n",
    "    window=8,             # Context window size\n",
    "    min_count=10,         # Minimum word count\n",
    "    workers=1,            # Number of threads\n",
    "    sample=1e-3,          # Downsample setting for frequent words\n",
    "    epochs=10,            # Number of iterations over the corpus\n",
    "    seed = 100\n",
    ")\n",
    "\n",
    "\n",
    "w2v.wv.fill_norms() \n",
    "\n",
    "# Save model\n",
    "data_models.mkdir(parents=True, exist_ok=True) \n",
    "w2v.save(str(data_models / 'w2v-vectors_8_300.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9d94142c-1e7f-4b0b-bfec-98604fc84304",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec.load(str(data_models / \"w2v-vectors_8_300.pkl\"))\n",
    "word_vectors = w2v.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aa347e-aad5-477c-a52a-40a84a7684c6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420620dc-4af4-4a55-a7b6-1827949d1b17",
   "metadata": {},
   "source": [
    "### Calculate Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5b75bd3b-2957-42d1-a89c-89e5b3ea44f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affect centroid vector:\n",
      " [ 0.13069108 -0.04834932 -0.15180138 -0.08613192 -0.08944192 -0.10178063\n",
      "  0.04531904 -0.03997415 -0.00702028  0.03948879 -0.12313504  0.1284162\n",
      " -0.09471979 -0.15128885 -0.01847997  0.02626847 -0.02801008 -0.015148\n",
      " -0.03062634  0.15429519 -0.08757771 -0.1254819  -0.00191349 -0.01670575\n",
      " -0.04033746  0.05174775 -0.07018315 -0.0884062  -0.11628897  0.27297094\n",
      " -0.02634064 -0.0077189  -0.02253625  0.0705168   0.1590486  -0.11155611\n",
      " -0.04112092  0.0123024  -0.12068146  0.15205382  0.02102724 -0.09592358\n",
      " -0.08436386  0.03344292 -0.09093704 -0.06677414  0.18301351  0.10604291\n",
      " -0.01274419 -0.02583208 -0.03017285  0.02446513 -0.16391745  0.00725619\n",
      "  0.1008782  -0.0144089   0.18961903 -0.1391073  -0.0154625   0.29413262\n",
      " -0.11091603  0.00592328  0.05586404 -0.1499145   0.04411521 -0.07590582\n",
      "  0.19943576 -0.23402527 -0.02421452 -0.13617899 -0.16322477  0.03454233\n",
      "  0.03855813  0.01439126  0.04541036 -0.00934297  0.10430469 -0.02472103\n",
      " -0.03293908  0.03978493  0.17454652 -0.02403194 -0.11619461  0.07396714\n",
      "  0.02058086  0.21999067 -0.07903649  0.12475405  0.07974017 -0.1265916\n",
      "  0.03671479  0.05863339  0.04715234 -0.09756701  0.02237379  0.01071883\n",
      "  0.0362838   0.08081559 -0.0955963   0.10809886 -0.18152677  0.0277741\n",
      "  0.05836419 -0.07241621 -0.02466171  0.04551981  0.15233138  0.04439453\n",
      " -0.01529398  0.00766053 -0.12404034  0.00475261 -0.02961398  0.02467526\n",
      " -0.01036466 -0.00542265 -0.05995197  0.15438154  0.1094843  -0.1320028\n",
      " -0.1103562  -0.06815085 -0.02877443  0.04576739 -0.0250789   0.0290018\n",
      " -0.05558107  0.09820887 -0.13018341 -0.03427212 -0.0048143   0.07304518\n",
      "  0.02240149 -0.0630976   0.02829809 -0.06460845 -0.01386584  0.17391735\n",
      "  0.11782444 -0.1618488   0.0335729  -0.00653213  0.06365746 -0.08353516\n",
      " -0.10460881 -0.2304684   0.11654823  0.07817386 -0.07597914 -0.08584198\n",
      "  0.15601052  0.09240181  0.02560363  0.067078   -0.03653667  0.17327797\n",
      " -0.10161476  0.10936226 -0.09203177  0.04627083  0.09061739  0.07906467\n",
      "  0.06970157 -0.01801894  0.07328778 -0.03415718 -0.0385333   0.2074719\n",
      " -0.13521284  0.05327459  0.04709513 -0.13193716 -0.19208296 -0.00724938\n",
      "  0.05688902 -0.01480741 -0.03346046  0.11582558 -0.01007907  0.01050916\n",
      " -0.06171915 -0.14700638 -0.08479916  0.03716897  0.02248292 -0.055182\n",
      "  0.18892016 -0.12778668  0.05629298 -0.00945733  0.17208838  0.20397167\n",
      "  0.09684256  0.00125733 -0.14635243 -0.09425101  0.09735832 -0.0201295\n",
      " -0.15726727 -0.09918624  0.00839271  0.00507364 -0.13076033  0.04351007\n",
      "  0.00229728 -0.12411962  0.17424831  0.04769347  0.09741209  0.04216978\n",
      "  0.00185518  0.09697992  0.17195906  0.07646284 -0.00176607 -0.16209882\n",
      " -0.02980587  0.04979812  0.16057216 -0.09106129 -0.05897952  0.1322996\n",
      "  0.14365438  0.15109381 -0.26023278  0.10458992  0.11045459 -0.01488741\n",
      "  0.02652153 -0.07227448 -0.00742346  0.13376342  0.04940968  0.24993207\n",
      " -0.10006169 -0.00082959 -0.00837121  0.04830951  0.22194947  0.09868689\n",
      "  0.03300807  0.15638039 -0.14510715 -0.07271524  0.03646572  0.01668492\n",
      "  0.15233018 -0.00483864 -0.0626777  -0.03453272  0.13783607 -0.11756274\n",
      " -0.06100126  0.0205969   0.05581841  0.11405911 -0.106117    0.09191112\n",
      "  0.02714293 -0.18224794  0.10502496  0.0107713   0.04520313  0.00209739\n",
      " -0.17224471 -0.18686354  0.04221249  0.10580756  0.12325889  0.21816312\n",
      " -0.0399365  -0.00916466  0.01713451  0.04463831 -0.02212407 -0.05450033\n",
      "  0.10457093  0.19114856 -0.01000534  0.08532368 -0.03871543 -0.1717893\n",
      "  0.06888861 -0.01615169 -0.2538562  -0.05827099  0.16804232 -0.01598581\n",
      "  0.00148381  0.22080001 -0.07450997  0.0941519   0.00691607 -0.1540039\n",
      " -0.04497302  0.09473547  0.10679756 -0.00586518 -0.16787842 -0.06608128]\n",
      "\n",
      "Cognition centroid vector:\n",
      " [ 0.10767796  0.00224065 -0.14986967  0.12444191  0.08737737 -0.06346916\n",
      "  0.06209296  0.0391826  -0.09720305 -0.00185494 -0.02767976  0.05527789\n",
      "  0.0215089   0.03904615  0.16515596  0.06635982 -0.04098734 -0.03932453\n",
      "  0.06463189 -0.01013502  0.03280047  0.00283701  0.1535236   0.01662424\n",
      " -0.13317105 -0.04126019  0.0920946  -0.07135395  0.06676798  0.03721828\n",
      "  0.01897861  0.06034892  0.01260655 -0.14242096 -0.03810303 -0.00577861\n",
      " -0.01989296  0.10106993 -0.09028802  0.02794634  0.04971607 -0.08575668\n",
      " -0.00255973 -0.03030886 -0.06781158  0.04085834  0.07841342 -0.05121641\n",
      "  0.01078741  0.05356739 -0.13878143  0.02720822  0.07830349 -0.01098105\n",
      "  0.06855522 -0.01513495  0.11399222 -0.03034867  0.00245837  0.08451398\n",
      " -0.03267809 -0.02985064  0.09508315 -0.02931702 -0.01165822 -0.16257921\n",
      "  0.19073908 -0.06880765  0.01999021 -0.00850811 -0.05339612 -0.06686823\n",
      "  0.12328755 -0.12475128 -0.08623529 -0.04278964  0.03133543 -0.01685807\n",
      " -0.01352329  0.026108   -0.04154879  0.04862965  0.00434575 -0.05244918\n",
      " -0.04843549  0.09761961 -0.15414685  0.07390115 -0.08520543 -0.01790803\n",
      " -0.16509095  0.14505866 -0.07302448 -0.00492459  0.0050477   0.0314688\n",
      "  0.12411917 -0.08088651 -0.17913008 -0.0571881  -0.08720596  0.02481268\n",
      "  0.01937061 -0.03892411 -0.04942717  0.02861676 -0.00679402  0.07363832\n",
      "  0.0408435  -0.02152358 -0.03638958  0.01982643 -0.0755059  -0.05702168\n",
      " -0.06482963 -0.14631924 -0.08833902  0.02340055  0.12824586 -0.10092803\n",
      " -0.08041467  0.05987702 -0.05125784 -0.04535938 -0.09721649 -0.11812314\n",
      "  0.06983384  0.04681724  0.03681379  0.04997252 -0.07665326  0.01496664\n",
      " -0.06778494 -0.11280306  0.0171679  -0.09482488  0.00947423  0.15726471\n",
      " -0.12599027  0.07343565 -0.11736742  0.07650884 -0.05866396  0.02527063\n",
      " -0.04356294 -0.02877104  0.09148879 -0.05187625 -0.08680177 -0.12742934\n",
      "  0.02782645  0.16151334 -0.04090402 -0.02599406 -0.01828927  0.03451017\n",
      "  0.00622727  0.09503272 -0.00175106 -0.03713952  0.03478052  0.03996632\n",
      "  0.1680461  -0.12788036 -0.00543379  0.04689253 -0.00234863  0.00897103\n",
      " -0.02649622 -0.05549725  0.14765733  0.03299316 -0.11789434 -0.06383576\n",
      "  0.00969349  0.00975237 -0.03053026 -0.06584723  0.02373895  0.1342342\n",
      "  0.01133926 -0.13007453 -0.14721131  0.03029124  0.04091199 -0.04381259\n",
      "  0.0611073   0.02278918 -0.03715921 -0.03749653 -0.0702209  -0.02466036\n",
      "  0.02146249 -0.05971066 -0.21969822 -0.06773955  0.0947556   0.04190043\n",
      " -0.0739468   0.01401092  0.05282013 -0.05232391  0.00910369  0.02999741\n",
      " -0.15929745  0.0088988   0.11330542  0.10174126  0.15944268  0.13449225\n",
      " -0.06931423  0.05151248  0.0442857  -0.02978444 -0.0743973   0.07803824\n",
      "  0.15604691  0.01026487 -0.04918944 -0.1694088  -0.09512532  0.08219532\n",
      "  0.04420402  0.08290792  0.03220557  0.04995188  0.05982247  0.0314655\n",
      "  0.05026177 -0.07919993 -0.03185225  0.1096677  -0.01140713  0.11313411\n",
      "  0.02613452 -0.02166252  0.06707475  0.04857498  0.02554518 -0.01787044\n",
      "  0.04053868  0.03764383 -0.09884371 -0.04469647  0.07554714 -0.03395213\n",
      "  0.16272941  0.20586601  0.12576458 -0.02304425  0.1713761  -0.0409503\n",
      " -0.10619394  0.09942169  0.04884027 -0.03450222 -0.02327288  0.05807352\n",
      "  0.00428819 -0.16052884 -0.03688563 -0.02228948 -0.09272652  0.02087906\n",
      " -0.06229018  0.02976764 -0.03736608  0.04734299  0.08778086  0.12801279\n",
      " -0.01980787  0.04327866 -0.03815107 -0.04179686 -0.14802925 -0.04667933\n",
      " -0.04708121 -0.02504993  0.01836215  0.00161142  0.07271884 -0.03487743\n",
      " -0.0530786   0.04221348 -0.05348019  0.12737782  0.0785934   0.01806032\n",
      "  0.12201879 -0.00127589 -0.09231128  0.10296874  0.02375428 -0.09995668\n",
      " -0.04349955  0.0406214   0.10380958  0.05758221 -0.05541785  0.03559504]\n",
      "\n",
      "Shape of affect centroid: (300,)\n",
      "Shape of cognition centroid: (300,)\n"
     ]
    }
   ],
   "source": [
    "# == Calculation ==\n",
    "def findcentroid(text, model):\n",
    "    \"\"\"\n",
    "    Compute the weighted average vector (centroid) of words in a list using Word2Vec.\n",
    "\n",
    "    - Each word vector is multiplied by its frequency weight and divided by the number of dictionary words (only the ones that appear in the text)\n",
    "    - Returns the mean vector for emotionality and rationality\n",
    "    \"\"\"\n",
    "    vecs = [model.wv[w] * word_counts_weighted[w] \n",
    "        for w in text \n",
    "        if w in model.wv and w in word_counts_weighted]\n",
    "    vecs = [v for v in vecs if len(v) > 0]\n",
    "    centroid = np.mean(vecs, axis=0)\n",
    "    #centroid = centroid.reshape(1, -1)\n",
    "    return centroid\n",
    "\n",
    "\n",
    "affect_centroid = findcentroid(affect_dic, w2v)\n",
    "cog_centroid = findcentroid(cognition_dic, w2v)\n",
    "\n",
    "os.chdir(data_c)\n",
    "joblib.dump(affect_centroid, 'centroids/affect_centroid.pkl')\n",
    "joblib.dump(cog_centroid, 'centroids/cog_centroid.pkl')\n",
    "\n",
    "# == Overview Vectors ==\n",
    "print(\"Affect centroid vector:\\n\", affect_centroid)\n",
    "print(\"\\nCognition centroid vector:\\n\", cog_centroid)\n",
    "\n",
    "# Shape and stats\n",
    "print(\"\\nShape of affect centroid:\", affect_centroid.shape)\n",
    "print(\"Shape of cognition centroid:\", cog_centroid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af347b1-4f5f-401a-8ca0-1c33eb7776b1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c8316f-f794-4bcd-87c9-578aa82f88b0",
   "metadata": {},
   "source": [
    "### Emotionality Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3376a4f6-a002-4179-8614-dfef96113180",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_preprocessed)\n",
    "\n",
    "# Load preprocessed speech data \n",
    "\n",
    "preprocessed_final_files = [\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed1_final.pkl')),\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed2_final.pkl')),\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed3_final.pkl')),\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed4_final.pkl'))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1cbd1f95-c7f4-41f1-a292-542ee34ae882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\sarah\\\\Downloads\\\\TESTRUN\\\\data\\\\results\\\\distances_10epochs.pkl']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to compute weighted document vectors and derive affective/cognitive distances and scores\n",
    "\n",
    "def documentvecweight(lista):\n",
    "     \"\"\"\n",
    "    Compute weighted document vectors\n",
    "    Compute their distances (cosine similarity) to affect and cognition centroids\n",
    "    Compute emotionality score \n",
    "    - Compute an emotionality score as (1 + 1 - affect_dist) / (1 + 1 - cognition_dist)\n",
    "    - Returns a list of [doc_id, affect_distance, cognition_distance, score].\n",
    "    \"\"\"\n",
    "\n",
    "    out = []\n",
    "    lista = [i for i in lista if len(i[1]) > 0]\n",
    "    for s in lista:\n",
    "        # Compute weighted word vectors for each token present in the Word2Vec model\n",
    "        vecs = [w2v.wv[w] * word_counts_weighted[w] for w in s[1] if w in w2v.wv]\n",
    "        if len(vecs) == 0:\n",
    "            a = np.nan\n",
    "            c = np.nan\n",
    "            score = np.nan\n",
    "        else:\n",
    "            # Compute mean vector for each speech\n",
    "            v = np.mean(vecs, axis=0)\n",
    "             # Cosine distance to affective centroid\n",
    "            a = cosine(v, affect_centroid)\n",
    "            # Cosine distance to cognitive centroid\n",
    "            c = cosine(v, cog_centroid)\n",
    "            score = (1 + 1 - a) / (1 + 1 - c)\n",
    "        out.append([s[0], a, c, score])\n",
    "    return out\n",
    "\n",
    "\n",
    "def main_function(file_path, idx):\n",
    "    dataset = joblib.load(file_path)\n",
    "    data = documentvecweight(dataset)\n",
    "    lab = os.path.join(data_results, f'distances_main_{idx}.pkl')\n",
    "    joblib.dump(data, lab)\n",
    "\n",
    "# Main loop: process all preprocessed speech files\n",
    "def main():\n",
    "    files = [\n",
    "        os.path.join(data_preprocessed, f'preprocessed_speeches_indexed{i+1}_final.pkl') #Changed!\n",
    "        for i in range(4)\n",
    "    ]\n",
    "    for i, f in enumerate(files, start=1):\n",
    "        main_function(f, i)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# Merge all distance files into one df\n",
    "DATA_temp = [os.path.join(data_results, f'distances_main_{i+1}.pkl') for i in range(4)]\n",
    "\n",
    "tot = []\n",
    "for dataname in DATA_temp:\n",
    "    d = joblib.load(dataname)\n",
    "    tot += d\n",
    "\n",
    "tot_df = pd.DataFrame(tot, columns=['filename', 'affect_d', 'cognition_d', 'score'])\n",
    "joblib.dump(tot_df, os.path.join(data_results, 'distances_10epochs.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fb955ab4-243d-470d-a967-e0af1e474c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          filename  affect_d  cognition_d     score\n",
      "0  ARG_01_1946.txt  0.959585     0.624323  0.756293\n",
      "1  AUS_01_1946.txt  1.265239     0.605132  0.526760\n",
      "2  BEL_01_1946.txt  1.170130     0.686274  0.631692\n",
      "3  BLR_01_1946.txt  0.798792     0.801597  1.002340\n",
      "4  BOL_01_1946.txt  0.516098     0.570015  1.037704\n"
     ]
    }
   ],
   "source": [
    "print(tot_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8c295cd4-4fa0-470f-b67c-4f584f7a3f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load un_corpus_cleaned_final and merge with tot_df by filename \n",
    "un_corpus_cleaned_final = pd.read_csv(os.path.join(data_preprocessed, \"un_corpus_cleaned_final.csv\"), sep=';', encoding='utf-8') \n",
    "un_corpus_scored = un_corpus_cleaned_final.merge(tot_df, on=\"filename\", how=\"left\")\n",
    "\n",
    "joblib.dump(un_corpus_scored, os.path.join(data_results, \"un_corpus_scored.pkl\"))\n",
    "\n",
    "un_corpus_scored.to_csv(\n",
    "    os.path.join(data_results, \"un_corpus_scored.csv\"),\n",
    "    sep=';', \n",
    "    index=False, \n",
    "    encoding='utf-8'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d8d13667-1eb3-482e-83ca-5969a229750a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              filename                                             speech  \\\n",
      "0      ARG_01_1946.txt  At the resumption of the first session of the ...   \n",
      "1      AUS_01_1946.txt  The General Assembly of the United Nations is ...   \n",
      "2      BEL_01_1946.txt  The principal organs of the United Nations hav...   \n",
      "3      BLR_01_1946.txt  As more than a year has elapsed since the Unit...   \n",
      "4      BOL_01_1946.txt  Coming to this platform where so many distingu...   \n",
      "...                ...                                                ...   \n",
      "10947  WSM_79_2024.txt  Excellencies, I extend my congratulations to H...   \n",
      "10948  YEM_79_2024.txt  Your Majesties, Excellencies, and Highnesses, ...   \n",
      "10949  ZAF_79_2024.txt  President of the 79th Session of the UN Genera...   \n",
      "10950  ZMB_79_2024.txt  YOUR EXCELLENCY PHILEMON YANG, PRESIDENT OF TH...   \n",
      "10951  ZWE_79_2024.txt  Your Excellency, Mr. Philemon Yang, President ...   \n",
      "\n",
      "      country_code  year  country_name  speech_length_words  \\\n",
      "0              ARG  1946     Argentina                 3364   \n",
      "1              AUS  1946     Australia                 4531   \n",
      "2              BEL  1946       Belgium                 2501   \n",
      "3              BLR  1946       Belarus                 3055   \n",
      "4              BOL  1946       Bolivia                 1501   \n",
      "...            ...   ...           ...                  ...   \n",
      "10947          WSM  2024         Samoa                 1426   \n",
      "10948          YEM  2024         Yemen                 1662   \n",
      "10949          ZAF  2024  South Africa                 1685   \n",
      "10950          ZMB  2024        Zambia                 2111   \n",
      "10951          ZWE  2024      Zimbabwe                 1652   \n",
      "\n",
      "       english_official_language  security_council_permanent  \\\n",
      "0                              0                           0   \n",
      "1                              0                           0   \n",
      "2                              1                           0   \n",
      "3                              0                           0   \n",
      "4                              0                           0   \n",
      "...                          ...                         ...   \n",
      "10947                          1                           0   \n",
      "10948                          0                           0   \n",
      "10949                          1                           0   \n",
      "10950                          1                           0   \n",
      "10951                          1                           0   \n",
      "\n",
      "                   speaker_name                               position  \\\n",
      "0                      Mr. Arce                                    NaN   \n",
      "1                     Mr. Makin                                    NaN   \n",
      "2            Mr. Van Langenhove                                    NaN   \n",
      "3                   Mr. Kiselev                                    NaN   \n",
      "4             Mr. Costa du Rels                                    NaN   \n",
      "...                         ...                                    ...   \n",
      "10947      Fiamē Naomi Mata'afa                (Deputy) Prime Minister   \n",
      "10948  Rashad Mohammed Al-Alimi                      (Vice-) President   \n",
      "10949  Matamela Cyril Ramaphosa                      (Vice-) President   \n",
      "10950   Mulambo Hamakuni Haimbe  (Deputy) Minister for Foreign Affairs   \n",
      "10951  Frederick Makamure Shava  (Deputy) Minister for Foreign Affairs   \n",
      "\n",
      "       gender_dummy         speech_label  \\\n",
      "0               0.0     Argentina (1946)   \n",
      "1               0.0     Australia (1946)   \n",
      "2               0.0       Belgium (1946)   \n",
      "3               0.0       Belarus (1946)   \n",
      "4               0.0       Bolivia (1946)   \n",
      "...             ...                  ...   \n",
      "10947           NaN         Samoa (2024)   \n",
      "10948           NaN         Yemen (2024)   \n",
      "10949           NaN  South Africa (2024)   \n",
      "10950           NaN        Zambia (2024)   \n",
      "10951           NaN      Zimbabwe (2024)   \n",
      "\n",
      "                                     speech_preprocessed  \\\n",
      "0      ['resumpt', 'argentin', 'renounc', 'decis', 's...   \n",
      "1      ['exercis', 'respons', 'charter', 'problem', '...   \n",
      "2      ['charter', 'respons', 'mainten', 'consider', ...   \n",
      "3      ['elaps', 'charter', 'san', 'stabl', 'charter'...   \n",
      "4      ['eloqu', 'preced', 'profit', 'tribut', 'ladi'...   \n",
      "...                                                  ...   \n",
      "10947  ['congratul', 'yang', 'cameroon', 'theme', 're...   \n",
      "10948  ['majesti', 'yang', 'antónio', 'guterr', 'ladi...   \n",
      "10949  ['yang', 'antónio', 'guterr', 'ladi', 'africa'...   \n",
      "10950  ['yang', 'sessi', 'cretari', 'guterr', 'honour...   \n",
      "10951  ['yang', 'guterr', 'majesti', 'heartfelt', 'co...   \n",
      "\n",
      "       speech_length_preprocessed  \\\n",
      "0                             354   \n",
      "1                             360   \n",
      "2                             270   \n",
      "3                             383   \n",
      "4                             146   \n",
      "...                           ...   \n",
      "10947                         281   \n",
      "10948                         360   \n",
      "10949                         297   \n",
      "10950                         387   \n",
      "10951                         343   \n",
      "\n",
      "                                            speech_final  speech_length_final  \\\n",
      "0      ['resumpt', 'argentin', 'renounc', 'decis', 's...                  339   \n",
      "1      ['exercis', 'respons', 'charter', 'problem', '...                  360   \n",
      "2      ['charter', 'respons', 'mainten', 'consider', ...                  269   \n",
      "3      ['elaps', 'charter', 'san', 'stabl', 'charter'...                  380   \n",
      "4      ['eloqu', 'preced', 'profit', 'tribut', 'ladi'...                  142   \n",
      "...                                                  ...                  ...   \n",
      "10947  ['congratul', 'yang', 'cameroon', 'theme', 're...                  276   \n",
      "10948  ['majesti', 'yang', 'antónio', 'guterr', 'ladi...                  355   \n",
      "10949  ['yang', 'antónio', 'guterr', 'ladi', 'africa'...                  294   \n",
      "10950  ['yang', 'guterr', 'honour', 'organis', 'ladi'...                  373   \n",
      "10951  ['yang', 'guterr', 'majesti', 'heartfelt', 'co...                  341   \n",
      "\n",
      "       affect_d  cognition_d     score  \n",
      "0      0.959585     0.624323  0.756293  \n",
      "1      1.265239     0.605132  0.526760  \n",
      "2      1.170130     0.686274  0.631692  \n",
      "3      0.798792     0.801597  1.002340  \n",
      "4      0.516098     0.570015  1.037704  \n",
      "...         ...          ...       ...  \n",
      "10947  1.275975     0.982730  0.711734  \n",
      "10948  0.831653     1.029185  1.203470  \n",
      "10949  0.936741     0.999957  1.063214  \n",
      "10950  1.393106     1.083387  0.662105  \n",
      "10951  1.321163     0.899727  0.616971  \n",
      "\n",
      "[10952 rows x 19 columns]\n"
     ]
    }
   ],
   "source": [
    "print(un_corpus_scored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1210b88f-cf13-4da0-807b-77764cf0d439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count where score is NaN: 0\n"
     ]
    }
   ],
   "source": [
    "# Count where score is NaN\n",
    "nan_count = un_corpus_scored['score'].isna().sum()\n",
    "\n",
    "print(\"Count where score is NaN:\", nan_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
