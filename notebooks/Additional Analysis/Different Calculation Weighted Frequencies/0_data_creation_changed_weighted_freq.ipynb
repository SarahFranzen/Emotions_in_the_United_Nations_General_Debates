{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b41cb22-e190-48d5-9926-7ac6fac8310d",
   "metadata": {},
   "source": [
    "# Development of Emotion and Reasoning in the General Speeches of the United Nations: A text-based machine learning approach\n",
    "## Additional Analysis: Different Calculation of the Weighted Frequencies - Data Creation\n",
    "\n",
    "### Description\r\n",
    "\n",
    "In the replication package, weighted frequencies are calculated on the full preprocessed corpus (35,009 unique words; 4,500,778 tokens), while the embedding corpus drops words occurring fewer than 10 times (9,453 unique words; 4,286,666 tokens). This script examines whether calculating weighted frequencies after removing these low-frequency words makes any differenceThe preprocessed files from the normal script are used as the different calculation of the weighted frequencies is based on those files and hence there are no changes before.ce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930ebd56-def9-4a16-91eb-78dbb7d037ae",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5588afc-105a-426e-9348-742a830a2732",
   "metadata": {},
   "source": [
    "## Setup, Installation of required Packages and Libraries & Folder Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32d7d499-0563-46e0-9730-914c9f106d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Import libraries for data processing and NLP ==\n",
    "\n",
    "import gensim\n",
    "import joblib\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import pycountry\n",
    "import random\n",
    "import re\n",
    "import spacy\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from multiprocessing import Pool, freeze_support\n",
    "from nltk.stem import SnowballStemmer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOPWORDS\n",
    "from string import punctuation\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "# === Initialize NLP Tools ===\n",
    "\n",
    "translator = str.maketrans('', '', punctuation)\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "tagger = nltk.perceptron.PerceptronTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d495156-bbb2-403c-86f2-83b47d8cb03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root set to: C:\\Users\\sarah\\Downloads\n"
     ]
    }
   ],
   "source": [
    "# === Set Working Directory ===\n",
    "\n",
    "# --- Set base path to project root ---\n",
    "base_path = Path.cwd().parents[2]  # project root\n",
    "print(f\"Project root set to: {base_path}\")\n",
    "\n",
    "# --- Paths ---\n",
    "data_c = base_path / \"data\"\n",
    "\n",
    "# === Define Folder Paths ===\n",
    "\n",
    "data_temp = data_c / \"temp\"\n",
    "data_freq = data_c / \"freq\"\n",
    "data_dict = data_c / \"dictionaries\"\n",
    "data_preprocessed = data_c / \"preprocessed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "296fc7d6-a2ac-406e-a322-6c9f2702ee1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\sarah\\Downloads\\TESTRUN\\data\n"
     ]
    }
   ],
   "source": [
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "68962929-b407-40d9-8d1c-e943061eeb5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\sarah\\\\Downloads\\\\data\\\\preprocessed\\\\preprocessed_speeches_indexed1.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m preprocessed_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fpath \u001b[38;5;129;01min\u001b[39;00m preprocessed_files:\n\u001b[1;32m----> 8\u001b[0m     data \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(fpath)\n\u001b[0;32m      9\u001b[0m     preprocessed_data\u001b[38;5;241m.\u001b[39mextend(data)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[0;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\sarah\\\\Downloads\\\\data\\\\preprocessed\\\\preprocessed_speeches_indexed1.pkl'"
     ]
    }
   ],
   "source": [
    "preprocessed_files = [\n",
    "    os.path.join(data_preprocessed, f\"preprocessed_speeches_indexed{i}.pkl\") \n",
    "    for i in range(1, 5) \n",
    "]\n",
    "\n",
    "preprocessed_data = []\n",
    "for fpath in preprocessed_files:\n",
    "    data = joblib.load(fpath)\n",
    "    preprocessed_data.extend(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bd410e-bc90-4e0b-aa08-a9bff13fab47",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d006e-6a23-453b-9c91-0e60bd3f8301",
   "metadata": {},
   "source": [
    "## Word-Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf2443e-4c76-41b2-bc16-e72d91c318a8",
   "metadata": {},
   "source": [
    "### Count frequencies of all tokens and display the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154d978d-cddd-431c-9d53-73c9faf8fdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#== Count token frequencies ==\n",
    "\n",
    "def count_frequencies(filenames):\n",
    "    total_freq = Counter()\n",
    "    for fname in tqdm(filenames):\n",
    "        data = joblib.load(fname)\n",
    "        tokens = chain.from_iterable(row[1] for row in data if isinstance(row[1], list))\n",
    "        total_freq.update(tokens)\n",
    "    return total_freq\n",
    "\n",
    "# Remove Words that appear less than 10x times\n",
    "removed_lowfreq_words_word_counts = count_frequencies(preprocessed_files)\n",
    "\n",
    "removed_lowfreq_words_word_counts  = Counter({w: c for w, c in removed_lowfreq_words_word_counts.items() if c >= 10})\n",
    "\n",
    "print(\"\\n[Stemmed] Top 50 most common words:\")\n",
    "for word, count in removed_lowfreq_words_word_counts.most_common(50):\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\n[Stemmed] Top 50 least common words:\")\n",
    "for word, count in removed_lowfreq_words_word_counts.most_common()[-50:]:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Save stemmed word counts\n",
    "save_path = os.path.join(data_freq, 'removed_lowfreq_words_word_counts.pkl')\n",
    "joblib.dump(removed_lowfreq_words_word_counts, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef220ae-b830-42a8-81e6-06a6f0b723a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_words = len(removed_lowfreq_words_word_counts)\n",
    "print(f\"Number of unique words: {num_unique_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932ce36d-d21c-4f1c-aff7-899c321dcfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_c)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39acefa1-c504-478e-98e5-9d465963dc82",
   "metadata": {},
   "source": [
    "### Count the frequency of the dictionary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e28f481-7e20-486c-a6b3-8729d6aeaad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "affect_path = os.path.join(data_dict, 'dictionary_affect.pkl')\n",
    "cognition_path = os.path.join(data_dict, 'dictionary_cognition.pkl')\n",
    "\n",
    "affect = joblib.load(affect_path)\n",
    "cognition = joblib.load(cognition_path)\n",
    "\n",
    "# == Count dictionary words\n",
    "\n",
    "print(\"Contents of affect dictionary:\")\n",
    "print(affect)\n",
    "print(\"Number of words in affect dictionary:\", len(affect))\n",
    "\n",
    "print(\"\\nContents of cognition dictionary:\")\n",
    "print(cognition)\n",
    "print(\"Number of words in cognition dictionary:\", len(cognition))\n",
    "\n",
    "a_list = [[i, removed_lowfreq_words_word_counts[i]] for i in affect if i in removed_lowfreq_words_word_counts]\n",
    "c_list = [[i, removed_lowfreq_words_word_counts[i]] for i in cognition if i in removed_lowfreq_words_word_counts]\n",
    "\n",
    "a_list = sorted(a_list, key=lambda x: x[1], reverse=True)\n",
    "c_list = sorted(c_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "a = [[i[0], f\"({i[1]}),\"] for i in a_list]\n",
    "c = [[i[0], f\"({i[1]}),\"] for i in c_list]\n",
    "\n",
    "a1 = ' '.join(str(r) for v in a for r in v)\n",
    "c1 = ' '.join(str(r) for v in c for r in v)\n",
    "\n",
    "affect_out_path = os.path.join(data_freq, \"removed_lowfreq_words_affect_words.txt\")\n",
    "cog_out_path = os.path.join(data_freq, \"removed_lowfreq_words_cog_words.txt\")\n",
    "\n",
    "with open(affect_out_path, \"w\") as output:\n",
    "    output.write(a1)\n",
    "\n",
    "with open(cog_out_path, \"w\") as output:\n",
    "    output.write(c1)\n",
    "\n",
    "# number of affect/cognitive words that appear in word_counts\n",
    "num_affect_words = len(a_list)\n",
    "num_cog_words = len(c_list)\n",
    "\n",
    "print(f\"Unique affect words in text: {num_affect_words}\")\n",
    "print(f\"Unique cognition words in text: {num_cog_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3313d3fd-fc3d-42eb-997a-d22a33e015db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Calculate weighted frequencies for all words\n",
    "\n",
    "l = sum(removed_lowfreq_words_word_counts.values())\n",
    "\n",
    "a = 0.001 # Method to downweight with a smoothing parameter: For frequent words (large v/1), weight approaches 0; for rare words (small v/1) closer to 1\n",
    "removed_lowfreq_words_word_counts_weighted = {k: a / (a + (v / l)) for k, v in removed_lowfreq_words_word_counts.items()}\n",
    "\n",
    "joblib.dump(removed_lowfreq_words_word_counts_weighted, os.path.join(data_freq, 'removed_lowfreq_words_word_counts_weighted.pkl'))\n",
    "\n",
    "# To print top 50 by weighted values, sort the dictionary by value descending:\n",
    "top_50_weighted = sorted(removed_lowfreq_words_word_counts_weighted.items(), key=lambda x: x[1], reverse=True)[:50]\n",
    "\n",
    "print(\"Top 50 words by weighted frequency:\")\n",
    "for word, weight in top_50_weighted:\n",
    "    print(f\"{word}: {weight}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
