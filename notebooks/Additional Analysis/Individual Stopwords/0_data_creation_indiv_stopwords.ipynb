{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b41cb22-e190-48d5-9926-7ac6fac8310d",
   "metadata": {},
   "source": [
    "# Development of Emotion and Reasoning in the General Speeches of the United Nations: A text-based machine learning approach\n",
    "## Additional Analysis: Different individualised Stopwords - Data Creation\n",
    "\n",
    "### Description: \n",
    "\n",
    "In the replication of Gennaro & Ash, their stopwords list was used. Since stopwords should be adapted for a different corpus, this script tests whether using a custom stopword list changes the results. As this does not affect the already cleaned corpus, the script starts with the clean corpus and adjusts the stopwords for preprocessing.\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930ebd56-def9-4a16-91eb-78dbb7d037ae",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32d7d499-0563-46e0-9730-914c9f106d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Import libraries for data processing and NLP ==\n",
    "\n",
    "import gensim\n",
    "import joblib\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import pycountry\n",
    "import random\n",
    "import re\n",
    "import spacy\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from multiprocessing import Pool, freeze_support\n",
    "from nltk.stem import SnowballStemmer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOPWORDS\n",
    "from string import punctuation\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "# === Initialize NLP Tools ===\n",
    "\n",
    "translator = str.maketrans('', '', punctuation)\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "tagger = nltk.perceptron.PerceptronTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d495156-bbb2-403c-86f2-83b47d8cb03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root set to: C:\\Users\\sarah\\Downloads\\TESTRUN\n"
     ]
    }
   ],
   "source": [
    "# === Set Working Directory and create folder structure ===\n",
    "\n",
    "# --- Set base path to project root ---\n",
    "base_path = Path.cwd().parents[2]  # project root\n",
    "print(f\"Project root set to: {base_path}\")\n",
    "\n",
    "# --- Paths ---\n",
    "data_c = base_path / \"data\"\n",
    "\n",
    "# Define paths\n",
    "data_dict = data_c / \"dictionaries\"\n",
    "data_stopwords = data_c / \"stopwords\"\n",
    "data_freq = data_c / \"freq\"\n",
    "data_sent = data_c / \"sentences\"\n",
    "data_preprocessed = data_c / \"preprocessed\"\n",
    "data_temp = data_c / \"temp\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "296fc7d6-a2ac-406e-a322-6c9f2702ee1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\sarah\\Downloads\\TESTRUN\\notebooks\\Additional Analysis\\Individual Stopwords\n"
     ]
    }
   ],
   "source": [
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bd410e-bc90-4e0b-aa08-a9bff13fab47",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff34b6-cbee-4f8c-8076-5f385f596b58",
   "metadata": {},
   "source": [
    "## Load Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a99b3df-550a-41a3-a751-2692c0f7ea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_files = [\n",
    "    os.path.join(data_temp, 'clean_speeches_indexed1.pkl'),\n",
    "    os.path.join(data_temp, 'clean_speeches_indexed2.pkl'),\n",
    "    os.path.join(data_temp, 'clean_speeches_indexed3.pkl'),\n",
    "    os.path.join(data_temp, 'clean_speeches_indexed4.pkl')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "372746d6-0a62-47aa-aef3-997581c1c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd.read_pickle(data_temp / \"df_clean.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea36f19a-789d-4c62-a470-3e73a6d848bf",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe0f4af-e939-4100-9618-5b522bdd72a1",
   "metadata": {},
   "source": [
    "### Text Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fd36a77-c5b3-4f7b-9c5e-04611ff909e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\Downloads\\TESTRUN\\notebooks\\Additional Analysis\\Individual Stopwords\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12f54a6d-84b5-4569-88f7-8f5707c6f77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 321 stemmed stopwords to C:\\Users\\sarah\\Downloads\\TESTRUN\\data\\stopwords\\ind_stopwords.pkl\n",
      "[\"'d\", \"'m\", \"'s\", 'a', 'about', 'abov', 'across', 'africa', 'after', 'afterward', 'again', 'against', 'all', 'almost', 'alon', 'along', 'alreadi', 'also', 'although', 'alway', 'am', 'america', 'among', 'amongst', 'amount', 'an', 'and', 'ani', 'anoth', 'anyon']\n"
     ]
    }
   ],
   "source": [
    "# Get SpaCy stopwords\n",
    "SPACY_STOPWORDS = list(nlp.Defaults.stop_words)\n",
    "\n",
    "# Stem SpaCy stopwords and convert to set\n",
    "stemmed_spacy = set(stemmer.stem(w) for w in SPACY_STOPWORDS)\n",
    "\n",
    "exclude_words = {\"please\", \"empti\", \"somehow\", \"anyhow\", \"somewher\"}  # can add more\n",
    "stemmed_spacy -= exclude_words\n",
    "\n",
    "# Already-stemmed custom stopwords\n",
    "my_stemmed_stopwords = {\"year\", \"time\", \"member\", \"session\", \"work\", \"oper\", \"united\", \"asia\", \"africa\", \"america\", \"europe\", \"task\",\n",
    "                        \"nation\", \"south\", \"east\", \"north\", \"west\", \"countri\", \"deleg\", \"project\",\n",
    "                        \"state\", \"peopl\", \"general\", \"organ\", \"assembl\",\n",
    "                        \"way\", \"role\", \"present\"}\n",
    "\n",
    "# Merge sets and sort to get a list\n",
    "\n",
    "STEMMED_STOPWORDS = sorted(stemmed_spacy.union(my_stemmed_stopwords))\n",
    "\n",
    "stopwords_path = os.path.join(data_stopwords, \"ind_stopwords.pkl\")\n",
    "joblib.dump(STEMMED_STOPWORDS, stopwords_path)\n",
    "\n",
    "stopwords = set(joblib.load(stopwords_path))\n",
    "\n",
    "print(f\"Saved {len(STEMMED_STOPWORDS)} stemmed stopwords to {stopwords_path}\")\n",
    "print(STEMMED_STOPWORDS[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0aa75944-4b75-4aa4-9430-9aa6bd9ac934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Functions to remove punctioation, tokenize, lowercase, pure digit tokens, words shorter than 2 letters, POS-Tagging, stemm, stopword removal ==\n",
    "\n",
    "def pro1(lista):\n",
    "    return [[row[0], row[1].translate(translator)] for row in lista]\n",
    "\n",
    "def pro2(lista):\n",
    "    return [[row[0], gensim.utils.simple_preprocess(row[1])] for row in lista]\n",
    "\n",
    "def pro3(lista):\n",
    "        a = [[row[0], [w for w in row[1] if not w.isdigit()]] for row in lista]\n",
    "        return a\n",
    "    \n",
    "def pro4(lista):\n",
    "    return [[row[0], [w for w in row[1] if len(w) > 2]] for row in lista]\n",
    "\n",
    "\n",
    "def tags(lista):\n",
    "    t = [[row[0], tagger.tag(row[1])] for row in lista]  # tag each tokenlist\n",
    "    t = [[row[0], [i[0] for i in row[1] if i[1].startswith(('N', 'V', 'J'))]] for row in t]\n",
    "    return t\n",
    "    \n",
    "def pro5(lista):\n",
    "    return [\n",
    "        [row[0], [stemmer.stem(token) for token in row[1]]]\n",
    "        for row in lista\n",
    "    ]\n",
    "    \n",
    "def pro6(lista):\n",
    "    return [[row[0], [w for w in row[1] if w not in stopwords]] for row in lista]\n",
    "\n",
    "def dropnull(lista):\n",
    "    return [row for row in lista if len(' '.join(row[1])) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "524a274a-004c-4d87-9bb3-5f789a42b90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing for C:\\Users\\sarah\\Downloads\\TESTRUN\\data\\temp\\clean_speeches_indexed1.pkl...\n",
      "[C:\\Users\\sarah\\Downloads\\TESTRUN\\data\\temp\\clean_speeches_indexed1.pkl] Before tagging: 28.82s\n",
      "[C:\\Users\\sarah\\Downloads\\TESTRUN\\data\\temp\\clean_speeches_indexed1.pkl] After tagging: 555.83s\n",
      "[C:\\Users\\sarah\\Downloads\\TESTRUN\\data\\temp\\clean_speeches_indexed1.pkl] Saved stemmed version: C:\\Users\\sarah\\Downloads\\TESTRUN\\data\\preprocessed\\ind_stopwords_preprocessed_speeches_indexed1.pkl\n",
      "[C:\\Users\\sarah\\Downloads\\TESTRUN\\data\\temp\\clean_speeches_indexed1.pkl] Done. Total time: 691.21s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\Downloads\\TESTRUN\\data\\temp\\clean_speeches_indexed2.pkl...\n",
      "[C:\\Users\\sarah\\Downloads\\TESTRUN\\data\\temp\\clean_speeches_indexed2.pkl] Before tagging: 24.20s\n",
      "[C:\\Users\\sarah\\Downloads\\TESTRUN\\data\\temp\\clean_speeches_indexed2.pkl] After tagging: 479.93s\n",
      "[C:\\Users\\sarah\\Downloads\\TESTRUN\\data\\temp\\clean_speeches_indexed2.pkl] Saved stemmed version: C:\\Users\\sarah\\Downloads\\TESTRUN\\data\\preprocessed\\ind_stopwords_preprocessed_speeches_indexed2.pkl\n",
      "[C:\\Users\\sarah\\Downloads\\TESTRUN\\data\\temp\\clean_speeches_indexed2.pkl] Done. Total time: 606.23s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\Downloads\\TESTRUN\\data\\temp\\clean_speeches_indexed3.pkl...\n",
      "[C:\\Users\\sarah\\Downloads\\TESTRUN\\data\\temp\\clean_speeches_indexed3.pkl] Before tagging: 15.98s\n",
      "[C:\\Users\\sarah\\Downloads\\TESTRUN\\data\\temp\\clean_speeches_indexed3.pkl] After tagging: 273.01s\n",
      "[C:\\Users\\sarah\\Downloads\\TESTRUN\\data\\temp\\clean_speeches_indexed3.pkl] Saved stemmed version: C:\\Users\\sarah\\Downloads\\TESTRUN\\data\\preprocessed\\ind_stopwords_preprocessed_speeches_indexed3.pkl\n",
      "[C:\\Users\\sarah\\Downloads\\TESTRUN\\data\\temp\\clean_speeches_indexed3.pkl] Done. Total time: 312.89s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\Downloads\\TESTRUN\\data\\temp\\clean_speeches_indexed4.pkl...\n",
      "[C:\\Users\\sarah\\Downloads\\TESTRUN\\data\\temp\\clean_speeches_indexed4.pkl] Before tagging: 8.03s\n",
      "[C:\\Users\\sarah\\Downloads\\TESTRUN\\data\\temp\\clean_speeches_indexed4.pkl] After tagging: 183.15s\n",
      "[C:\\Users\\sarah\\Downloads\\TESTRUN\\data\\temp\\clean_speeches_indexed4.pkl] Saved stemmed version: C:\\Users\\sarah\\Downloads\\TESTRUN\\data\\preprocessed\\ind_stopwords_preprocessed_speeches_indexed4.pkl\n",
      "[C:\\Users\\sarah\\Downloads\\TESTRUN\\data\\temp\\clean_speeches_indexed4.pkl] Done. Total time: 225.60s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# == Create full pre-processing function and run it\n",
    "\n",
    "def preprocessing(data_name):\n",
    "    t0 = time.time()\n",
    "    print(f\"Starting preprocessing for {data_name}...\")\n",
    "\n",
    "    data = joblib.load(data_name)\n",
    "    data = pro1(data)\n",
    "    data = pro2(data)\n",
    "    data = pro3(data)\n",
    "    data = pro4(data)\n",
    "\n",
    "    print(f\"[{data_name}] Before tagging: {time.time() - t0:.2f}s\")\n",
    "    data = tags(data)\n",
    "    print(f\"[{data_name}] After tagging: {time.time() - t0:.2f}s\")\n",
    "\n",
    "    data = pro5(data)\n",
    "    data = pro6(data)\n",
    "    \n",
    "    data = dropnull(data)\n",
    "\n",
    "    filename_preprocessed = data_name.replace('clean_speeches_', 'ind_stopwords_preprocessed_speeches_').replace('.pkl', '.pkl')\n",
    "    out_preprocessed = os.path.join(data_preprocessed, os.path.basename(filename_preprocessed))\n",
    "    joblib.dump(data, out_preprocessed\n",
    "               )\n",
    "    print(f\"[{data_name}] Saved stemmed version: {out_preprocessed}\")\n",
    "\n",
    "    print(f\"[{data_name}] Done. Total time: {time.time() - t0:.2f}s\\n\")\n",
    "\n",
    "def main():\n",
    "    for fname in clean_files:\n",
    "        preprocessing(fname)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3409987-485c-47c9-9430-01fa2c0e650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the pre-processed data\n",
    "preprocessed_files = [\n",
    "    os.path.join(data_preprocessed, 'ind_stopwords_preprocessed_speeches_indexed1.pkl'),\n",
    "    os.path.join(data_preprocessed, 'ind_stopwords_preprocessed_speeches_indexed2.pkl'),\n",
    "    os.path.join(data_preprocessed, 'ind_stopwords_preprocessed_speeches_indexed3.pkl'),\n",
    "    os.path.join(data_preprocessed, 'ind_stopwords_preprocessed_speeches_indexed4.pkl')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5782a5ba-2b38-4b49-9c3a-3e8e6310ae84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          filename                                             speech  \\\n",
      "0  ARG_01_1946.txt  At the resumption of the first session of the ...   \n",
      "1  AUS_01_1946.txt  The General Assembly of the United Nations is ...   \n",
      "2  BEL_01_1946.txt  The principal organs of the United Nations hav...   \n",
      "3  BLR_01_1946.txt  As more than a year has elapsed since the Unit...   \n",
      "4  BOL_01_1946.txt  Coming to this platform where so many distingu...   \n",
      "\n",
      "  country_code  year country_name  speech_length_words  \\\n",
      "0          ARG  1946    Argentina                 3364   \n",
      "1          AUS  1946    Australia                 4531   \n",
      "2          BEL  1946      Belgium                 2501   \n",
      "3          BLR  1946      Belarus                 3055   \n",
      "4          BOL  1946      Bolivia                 1501   \n",
      "\n",
      "   english_official_language  security_council_permanent        speaker_name  \\\n",
      "0                          0                           0            Mr. Arce   \n",
      "1                          0                           0           Mr. Makin   \n",
      "2                          1                           0  Mr. Van Langenhove   \n",
      "3                          0                           0         Mr. Kiselev   \n",
      "4                          0                           0   Mr. Costa du Rels   \n",
      "\n",
      "  position  gender_dummy      speech_label  \\\n",
      "0      NaN           0.0  Argentina (1946)   \n",
      "1      NaN           0.0  Australia (1946)   \n",
      "2      NaN           0.0    Belgium (1946)   \n",
      "3      NaN           0.0    Belarus (1946)   \n",
      "4      NaN           0.0    Bolivia (1946)   \n",
      "\n",
      "                                 speech_preprocessed  \n",
      "0  [resumpt, argentin, wish, view, number, questi...  \n",
      "1  [unit, meet, hospit, citi, new, york, exercis,...  \n",
      "2  [princip, unit, function, month, report, form,...  \n",
      "3  [elaps, unit, charter, sign, san, francisco, c...  \n",
      "4  [come, platform, distinguish, eloqu, speaker, ...  \n"
     ]
    }
   ],
   "source": [
    "# Load all preprocessed pickle files\n",
    "preprocessed_data = []\n",
    "for f in preprocessed_files:\n",
    "    preprocessed_data.extend(joblib.load(f))\n",
    "\n",
    "# Turn into DataFrame\n",
    "df_preprocessed = pd.DataFrame(preprocessed_data, columns=[\"filename\", \"speech_preprocessed\"])\n",
    "\n",
    "# Merge into df_clean\n",
    "ind_stopwords_df_clean = df_clean.merge(df_preprocessed, on=\"filename\", how=\"left\")\n",
    "\n",
    "print(ind_stopwords_df_clean.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62beb4e4-e324-4398-bb0e-637eabe6af25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          filename  speech_length_preprocessed\n",
      "0  ARG_01_1946.txt                        1207\n",
      "1  AUS_01_1946.txt                        1656\n",
      "2  BEL_01_1946.txt                         964\n",
      "3  BLR_01_1946.txt                        1232\n",
      "4  BOL_01_1946.txt                         517\n",
      "Total unique tokens: 39996\n",
      "Average number of tokens per speech: 1174.44\n"
     ]
    }
   ],
   "source": [
    "# == New variable: Speech length of the preprocessed corpus ==\n",
    "\n",
    "# Count tokens in preprocessed speech\n",
    "ind_stopwords_df_clean[\"speech_length_preprocessed\"] = ind_stopwords_df_clean[\"speech_preprocessed\"].apply(\n",
    "    lambda x: len(x) if isinstance(x, list) else 0\n",
    ")\n",
    "\n",
    "print(ind_stopwords_df_clean[[\"filename\", \"speech_length_preprocessed\"]].head())\n",
    "all_tokens = [token for speech in ind_stopwords_df_clean[\"speech_preprocessed\"].dropna() for token in speech]\n",
    "unique_tokens = set(all_tokens)\n",
    "print(\"Total unique tokens:\", len(unique_tokens))\n",
    "\n",
    "# Average length of preprocessed speeches\n",
    "average_length = ind_stopwords_df_clean[\"speech_length_preprocessed\"].mean()\n",
    "\n",
    "print(f\"Average number of tokens per speech: {average_length:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "746e374b-d840-4c4e-88be-ec29c1d08587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          filename                                             speech  \\\n",
      "0  ARG_01_1946.txt  At the resumption of the first session of the ...   \n",
      "1  AUS_01_1946.txt  The General Assembly of the United Nations is ...   \n",
      "2  BEL_01_1946.txt  The principal organs of the United Nations hav...   \n",
      "3  BLR_01_1946.txt  As more than a year has elapsed since the Unit...   \n",
      "4  BOL_01_1946.txt  Coming to this platform where so many distingu...   \n",
      "\n",
      "  country_code  year country_name  speech_length_words  \\\n",
      "0          ARG  1946    Argentina                 3364   \n",
      "1          AUS  1946    Australia                 4531   \n",
      "2          BEL  1946      Belgium                 2501   \n",
      "3          BLR  1946      Belarus                 3055   \n",
      "4          BOL  1946      Bolivia                 1501   \n",
      "\n",
      "   english_official_language  security_council_permanent        speaker_name  \\\n",
      "0                          0                           0            Mr. Arce   \n",
      "1                          0                           0           Mr. Makin   \n",
      "2                          1                           0  Mr. Van Langenhove   \n",
      "3                          0                           0         Mr. Kiselev   \n",
      "4                          0                           0   Mr. Costa du Rels   \n",
      "\n",
      "  position  gender_dummy      speech_label  \\\n",
      "0      NaN           0.0  Argentina (1946)   \n",
      "1      NaN           0.0  Australia (1946)   \n",
      "2      NaN           0.0    Belgium (1946)   \n",
      "3      NaN           0.0    Belarus (1946)   \n",
      "4      NaN           0.0    Bolivia (1946)   \n",
      "\n",
      "                                 speech_preprocessed  \\\n",
      "0  [resumpt, argentin, wish, view, number, questi...   \n",
      "1  [unit, meet, hospit, citi, new, york, exercis,...   \n",
      "2  [princip, unit, function, month, report, form,...   \n",
      "3  [elaps, unit, charter, sign, san, francisco, c...   \n",
      "4  [come, platform, distinguish, eloqu, speaker, ...   \n",
      "\n",
      "   speech_length_preprocessed  \n",
      "0                        1207  \n",
      "1                        1656  \n",
      "2                         964  \n",
      "3                        1232  \n",
      "4                         517  \n"
     ]
    }
   ],
   "source": [
    "print(ind_stopwords_df_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "134ddbf7-9b95-4697-9d9c-f9c55b52ec80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\Downloads\\TESTRUN\\notebooks\\Additional Analysis\\Individual Stopwords\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73db5f7-566d-4276-a631-c1f33fab494d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d006e-6a23-453b-9c91-0e60bd3f8301",
   "metadata": {},
   "source": [
    "## Word-Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf2443e-4c76-41b2-bc16-e72d91c318a8",
   "metadata": {},
   "source": [
    "### Count frequencies of all tokens and display the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "154d978d-cddd-431c-9d53-73c9faf8fdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:23<00:00,  5.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Stemmed] Top 50 most common words:\n",
      "unit: 187107\n",
      "intern: 161305\n",
      "develop: 147090\n",
      "peac: 134933\n",
      "world: 133091\n",
      "secur: 86024\n",
      "govern: 75551\n",
      "econom: 73475\n",
      "right: 67482\n",
      "new: 59536\n",
      "effort: 57570\n",
      "human: 57178\n",
      "problem: 56809\n",
      "support: 55692\n",
      "continu: 53696\n",
      "communiti: 49388\n",
      "region: 48963\n",
      "polit: 48469\n",
      "war: 41867\n",
      "need: 41509\n",
      "council: 41193\n",
      "import: 40881\n",
      "achiev: 39487\n",
      "power: 38401\n",
      "hope: 38377\n",
      "conflict: 37632\n",
      "presid: 37232\n",
      "situat: 36254\n",
      "principl: 36246\n",
      "global: 36179\n",
      "resolut: 35258\n",
      "republ: 34661\n",
      "forc: 34483\n",
      "great: 34066\n",
      "relat: 33847\n",
      "order: 33512\n",
      "concern: 33294\n",
      "action: 32668\n",
      "nuclear: 32246\n",
      "solut: 32199\n",
      "establish: 31713\n",
      "confer: 31362\n",
      "polici: 30730\n",
      "commit: 30720\n",
      "social: 30684\n",
      "respect: 30371\n",
      "effect: 30316\n",
      "independ: 29254\n",
      "chang: 28912\n",
      "interest: 28268\n",
      "\n",
      "[Stemmed] Top 50 least common words:\n",
      "salway: 1\n",
      "shshout: 1\n",
      "montremontreux: 1\n",
      "navinavig: 1\n",
      "wbecam: 1\n",
      "mmadam: 1\n",
      "aattack: 1\n",
      "paa: 1\n",
      "desexu: 1\n",
      "nhuman: 1\n",
      "inshallah: 1\n",
      "ffnpt: 1\n",
      "fakafetai: 1\n",
      "lasi: 1\n",
      "bbt: 1\n",
      "highemiss: 1\n",
      "nabbanja: 1\n",
      "robinah: 1\n",
      "llife: 1\n",
      "necconnext: 1\n",
      "justesjustic: 1\n",
      "unundertak: 1\n",
      "hasnt: 1\n",
      "hadnt: 1\n",
      "despi: 1\n",
      "avort: 1\n",
      "arri: 1\n",
      "bioweapon: 1\n",
      "ebgf: 1\n",
      "transafghan: 1\n",
      "monadolog: 1\n",
      "serendip: 1\n",
      "unvarnish: 1\n",
      "bonair: 1\n",
      "ineff: 1\n",
      "milei: 1\n",
      "cni: 1\n",
      "faafetai: 1\n",
      "hadramout: 1\n",
      "yemenia: 1\n",
      "richmond: 1\n",
      "seventyeight: 1\n",
      "sessi: 1\n",
      "cretari: 1\n",
      "samim: 1\n",
      "jodo: 1\n",
      "lourenqo: 1\n",
      "futl: 1\n",
      "jre: 1\n",
      "elnino: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\sarah\\\\Downloads\\\\TESTRUN\\\\data\\\\freq\\\\ind_stopwords_word_counts.pkl']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#== Count token frequencies ==\n",
    "\n",
    "def count_frequencies(filenames):\n",
    "    total_freq = Counter()\n",
    "    for fname in tqdm(filenames):\n",
    "        data = joblib.load(fname)\n",
    "        tokens = chain.from_iterable(row[1] for row in data if isinstance(row[1], list))\n",
    "        total_freq.update(tokens)\n",
    "    return total_freq\n",
    "\n",
    "#def remove_rare_words(filenames, freqs, min_count=10):\n",
    "   # for fname in filenames:\n",
    "       # data = joblib.load(fname)\n",
    "       # filtered_data = []\n",
    "        #for doc_id, tokens in data:\n",
    "          #  filtered_tokens = [w for w in tokens if freqs.get(w, 0) >= min_count]\n",
    "          #  filtered_data.append([doc_id, filtered_tokens])\n",
    "       # joblib.dump(filtered_data, fname)  # overwrite or save as new file\n",
    "       # print(f\"Processed {fname}: removed words with freq < {min_count}\")\n",
    "\n",
    "# === Count for preprocessed (stemmed) speeches ===\n",
    "ind_stopwords_word_counts = count_frequencies(preprocessed_files)\n",
    "\n",
    "#remove_rare_words(preprocessed_files, word_counts, min_count=10)\n",
    "\n",
    "print(\"\\n[Stemmed] Top 50 most common words:\")\n",
    "for word, count in ind_stopwords_word_counts.most_common(50):\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\n[Stemmed] Top 50 least common words:\")\n",
    "for word, count in ind_stopwords_word_counts.most_common()[-50:]:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Save stemmed word counts\n",
    "save_path = os.path.join(data_freq, 'ind_stopwords_word_counts.pkl')\n",
    "joblib.dump(ind_stopwords_word_counts, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ef220ae-b830-42a8-81e6-06a6f0b723a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 39996\n"
     ]
    }
   ],
   "source": [
    "num_unique_words = len(ind_stopwords_word_counts)\n",
    "print(f\"Number of unique words: {num_unique_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "932ce36d-d21c-4f1c-aff7-899c321dcfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\Downloads\\TESTRUN\\data\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_c)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39acefa1-c504-478e-98e5-9d465963dc82",
   "metadata": {},
   "source": [
    "### Count the frequency of the dictionary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e28f481-7e20-486c-a6b3-8729d6aeaad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of affect dictionary:\n",
      "['forbid', 'unattract', 'cruelti', 'crappi', 'apathi', 'scari', 'unimpress', 'sin', 'dumbest', 'eas', 'agit', 'sob', 'shocker', 'tragedi', 'fabul', 'strongest', 'giver', 'sigh', 'aw', 'witch', 'hurtl', 'fucktard', 'cruel', 'glamor', 'funni', 'smarter', 'brillianc', 'irrate', 'alright', 'honest', 'profit', 'fearless', 'grievous', 'relax', 'isolationist', 'hah', 'shyness', 'poorest', 'cruelest', 'troublemak', 'disagre', 'agon', 'terror', 'fight', 'pleas', 'poor', 'crazi', 'hostil', 'stupid', 'damnat', 'vain', 'jade', 'heartless', 'nag', 'gloomi', 'damn', 'dishearten', 'pleaser', 'credit', 'warmth', 'greatest', 'whine', 'shame', 'angriest', 'envious', 'grin', 'blameless', 'sweeter', 'laidback', 'stupidest', 'unprotect', 'whiner', 'unlov', 'shake', 'boredom', 'fairer', 'weaker', 'wellb', 'bold', 'sucki', 'unsuccess', 'mourner', 'liken', 'defens', 'invigor', 'tedious', 'paranoid', 'cynic', 'dignifi', 'paranoia', 'sweetest', 'contented', 'humili', 'crush', 'terrif', 'alarm', 'asham', 'impati', 'fake', 'flirti', 'laziest', 'libertarian', 'empti', 'panicki', 'rape', 'contemptu', 'incent', 'reward', 'brutal', 'ignoramus', 'bereav', 'wimp', 'laugh', 'nicer', 'warm', 'resent', 'threat', 'hell', 'tortur', 'mad', 'worship', 'uncomfort', 'depress', 'unsaf', 'faultless', 'numb', 'humorless', 'murder', 'prettiest', 'jealous', 'tragic', 'discomfort', 'loveli', 'ungrat', 'divin', 'lamest', 'weaken', 'horribl', 'fuck', 'agoni', 'grievanc', 'ach', 'isolation', 'masoch', 'uglier', 'amaz', 'devilish', 'bitter', 'devil', 'smartest', 'glori', 'trite', 'vicious', 'unsavori', 'grossest', 'sinist', 'cheeri', 'dismay', 'easier', 'flatter', 'virtuous', 'egot', 'generos', 'cherish', 'heartwarm', 'advers', 'tough', 'adorn', 'overwhelm', 'devast', 'egotist', 'sociabl', 'lazier', 'affection', 'sarcasm', 'humor', 'funniest', 'startl', 'innoc', 'vomit', 'shyli', 'kiss', 'burdensom', 'melancholi', 'weakest', 'worthless', 'gentlest', 'proud', 'alarmist', 'pervert', 'demeanor', 'loneli', 'reassur', 'weird', 'passionless', 'loyalti', 'frustrat', 'weak', 'immor', 'hater', 'libertin', 'flirt', 'terribl', 'threaten', 'fatalist', 'melancholia', 'avers', 'prick', 'sucker', 'meanest', 'virtuos', 'benefici', 'neurot', 'disappoint', 'restless', 'romant', 'bastard', 'angri', 'flatteri', 'stun', 'bolder', 'amor', 'forbidden', 'hug', 'demean', 'disgust', 'truer', 'benefic', 'teas', 'stinki', 'condemnatori', 'turmoil', 'poison', 'cunt', 'gorgeous', 'harm', 'lousi', 'glorifi', 'piti', 'wimpish', 'impression', 'freakish', 'happiest', 'apathet', 'pitiabl', 'unpleas', 'suffer', 'miseri', 'toler', 'mess', 'casual', 'anxieti', 'stale', 'fieri', 'grudg', 'arrog', 'outrag', 'thrill', 'faithless', 'popular', 'vaniti', 'palat', 'fantasi', 'decay', 'domin', 'mood', 'proudest', 'scream', 'stress', 'calm', 'fatigu', 'dumber', 'benign', 'rude', 'steal', 'tension', 'enthusiast', 'intimid', 'sweetheart', 'prettier', 'perv', 'pathet', 'flawless', 'contradictori', 'distress', 'moron', 'gratifi', 'frighten', 'anguish', 'snob', 'defenseless', 'fantast', 'sad', 'thief', 'worsen', 'tear', 'handsom', 'attract', 'messi', 'smother', 'scarier', 'afraid', 'insult', 'disreput', 'wept', 'victim', 'traumat', 'prejudic', 'amus', 'decent', 'joyous', 'masochist', 'abandon', 'foe', 'pleasantri', 'shitless', 'strengthen', 'antagonist', 'liar', 'romantic', 'violenc', 'sentimentalist', 'gratif', 'decept', 'witchcraft', 'liveli', 'impressionist', 'guilt', 'griev', 'rotten', 'disgrac', 'insincer', 'pushi', 'enjoy', 'freaki', 'ador', 'grate', 'homesick', 'braver', 'battlefield', 'niceti', 'hooray', 'incompet', 'thiev', 'timid', 'adventur', 'needi', 'joyless', 'grimac', 'gloom', 'dump', 'horrid', 'magnific', 'unlucki', 'killer', 'uncontrol', 'funnier', 'ruin', 'weepi', 'play', 'encourag', 'fantas', 'irrit', 'sincer', 'rage', 'despis', 'unhappi', 'hate', 'bravest', 'harmless', 'hero', 'vigor', 'pervers', 'dumb', 'degrad', 'shameless', 'villaini', 'envi', 'aggress', 'fucker', 'courag', 'craziest', 'delici', 'scare', 'finer', 'bittersweet', 'trivial', 'wealthi', 'heroic', 'ecstat', 'irrat', 'prejudici', 'smug', 'stammer', 'feroci', 'phobic', 'villain', 'anger', 'fond', 'obsession', 'awesom', 'wors', 'compassion', 'weakl', 'calmer', 'inhibit', 'bother', 'rancid', 'hatr', 'beaten', 'opportunist', 'offens', 'haunt', 'appal', 'painless', 'pessimist', 'molest', 'lol', 'wrongdo', 'frantic', 'heroin', 'rejoic', 'woe', 'miser', 'meaner', 'saddest', 'mooch', 'attack', 'enthusiasm', 'joke', 'meritocraci', 'angrier', 'emot', 'weirdest', 'silliest', 'kisser', 'hurrah', 'crueler', 'nervous', 'dishonor', 'gracious', 'sicker', 'hopeless', 'delect', 'compliment', 'petrifi', 'cuter', 'reek', 'loveless', 'daze', 'weirder', 'triumphant', 'impolit', 'offend', 'richer', 'violent', 'crazier', 'comedown', 'heroism', 'stubborn', 'maniac', 'strain', 'broke', 'jerk', 'unkind', 'asshol', 'reveng', 'jealousi', 'damnabl', 'unimport', 'careless', 'gentler', 'bitch', 'shi', 'terrorist', 'rigid', 'harsh', 'passion', 'annoy', 'compass', 'geek', 'weirdo', 'agre', 'whore', 'splendor', 'suck', 'fun', 'chuckl', 'snobberi', 'mock', 'doom', 'reluct', 'mourn', 'boldest', 'helpless', 'trembl', 'pestil', 'curs', 'desper', 'harass', 'complain', 'joker', 'obsess', 'uneasi', 'nasti', 'greedi', 'belov', 'grim', 'upset', 'greed', 'honesti', 'phoni', 'feudal', 'loveliest', 'trauma', 'dwell', 'ecstasi', 'blame', 'legit', 'calmest', 'sickest', 'scariest', 'sarcast', 'cheat', 'fatal', 'stronger', 'energet', 'feroc', 'healthi', 'goddam', 'disast', 'flirtati', 'defeatist', 'glorious', 'terrifi', 'gossip', 'forbad', 'sicken', 'poorer', 'strangest', 'cute', 'struggl', 'woebegon', 'worri', 'gratitud', 'cheer', 'distraught', 'ok', 'vulner', 'blessed', 'fairest', 'lame', 'optim', 'grouch', 'fright', 'pessim', 'sadder', 'worst', 'despair', 'bash', 'intellectu', 'promis', 'prais', 'heartfelt', 'horror', 'crap', 'repress', 'prouder', 'fought', 'discourag', 'heal', 'lazi', 'precious', 'melanchol', 'isol', 'aggressor', 'giggl', 'idiot', 'twitchi', 'glorif', 'kid', 'selfish', 'smile', 'grief', 'hellish', 'shit', 'fume', 'bitchi', 'fool', 'trustworthi', 'optimist', 'dumpi', 'unwelcom', 'neat', 'nicest', 'furi', 'unfriend', 'ridicul', 'arguabl', 'shamefac', 'pervi', 'upbeat', 'wow', 'easygo', 'blameworthi', 'charmer', 'stink', 'phobia', 'adversari', 'sillier', 'distrust', 'dread', 'happier', 'vital', 'dissatisfact', 'insecur', 'assault', 'ineffect', 'panic', 'powerless', 'nightmar', 'wealth', 'enrag', 'splendid', 'foolish', 'ugliest', 'contradict', 'worrier', 'freak']\n",
      "Number of words in affect dictionary: 629\n",
      "\n",
      "Contents of cognition dictionary:\n",
      "['analyt', 'guess', 'suspect', 'coz', 'mysteri', 'unawar', 'proverbi', 'dissimilar', 'consequenti', 'stimul', 'occasion', 'rethink', 'purest', 'entireti', 'abnorm', 'referenti', 'differenti', 'diagnos', 'ambigu', 'concentr', 'undon', 'undecid', 'disclosur', 'fundamentalist', 'blur', 'reaction', 'variabl', 'recogniz', 'meant', 'reconstruct', 'supposit', 'acknowledg', 'choosi', 'accuraci', 'mislead', 'perceiv', 'hypothesi', 'unknow', 'cuz', 'reveal', 'potenti', 'puriti', 'persuad', 'indirect', 'generat', 'presumpt', 'clue', 'justif', 'evidenti', 'realize', 'deduct', 'causal', 'inevit', 'thinker', 'evidentiari', 'factor', 'undeni', 'unclear', 'pick', 'induc', 'accur', 'absolut', 'everytim', 'reconsider', 'diagnost', 'solut', 'diagnosi', 'discoveri', 'ration', 'unambigu', 'explicit', 'decis', 'insight', 'confess', 'fuzzi', 'led', 'lesson', 'theoriz', 'analyz', 'learner', 'split', 'enlighten', 'cos', 'respons', 'dubious', 'react', 'guarante', 'someday', 'blatant', 'undo', 'perceptu', 'infal', 'afterthought', 'inequ', 'feasibl', 'knowabl', 'approxim', 'logic', 'meaning', 'unknown', 'categor', 'trigger', 'incomplet', 'curious', 'attribut', 'pretend', 'reactionari', 'prove', 'meaningless', 'reactiv', 'proof', 'bet', 'purpos', 'suppos', 'discern', 'imagin', 'kinda', 'comprehens', 'sens', 'percept', 'everyday', 'rational', 'unquestion', 'random', 'implicit', 'clarif', 'theori', 'dunno', 'analysi', 'obedi', 'memor', 'genuin', 'evalu', 'explor', 'linkag', 'grasp', 'curios', 'methink', 'somewher', 'deduc', 'unwant', 'induct', 'unrel', 'outstand', 'discov', 'unresolv', 'exploratori', 'choos', 'perspect', 'inferenti', 'disclos', 'factual', 'arbitrari', 'conting', 'vagu', 'persuas', 'manipul', 'impli', 'somehow', 'sorta', 'borderlin', 'reconcil', 'anyhow', 'conscious', 'probabilist', 'precis', 'rearrang', 'referenc', 'obscur']\n",
      "Number of words in cognition dictionary: 169\n",
      "Unique affect words in text: 541\n",
      "Unique cognition words in text: 154\n",
      "Affect words with count < 10: 118\n",
      "Cognition words with count < 10: 7\n",
      "Top 50 words by weighted frequency:\n",
      "roca: 0.9999222607506579\n",
      "errazurriz: 0.9999222607506579\n",
      "mariano: 0.9999222607506579\n",
      "varela: 0.9999222607506579\n",
      "verdi: 0.9999222607506579\n",
      "degrell: 0.9999222607506579\n",
      "werth: 0.9999222607506579\n",
      "réformat: 0.9999222607506579\n",
      "threshhold: 0.9999222607506579\n",
      "ofconcret: 0.9999222607506579\n",
      "bristol: 0.9999222607506579\n",
      "irredentum: 0.9999222607506579\n",
      "goodand: 0.9999222607506579\n",
      "ijnrra: 0.9999222607506579\n",
      "andco: 0.9999222607506579\n",
      "profert: 0.9999222607506579\n",
      "heydrich: 0.9999222607506579\n",
      "streicher: 0.9999222607506579\n",
      "baiter: 0.9999222607506579\n",
      "cavel: 0.9999222607506579\n",
      "zhukov: 0.9999222607506579\n",
      "arnhem: 0.9999222607506579\n",
      "willki: 0.9999222607506579\n",
      "vetj: 0.9999222607506579\n",
      "upet: 0.9999222607506579\n",
      "submargin: 0.9999222607506579\n",
      "establis: 0.9999222607506579\n",
      "jester: 0.9999222607506579\n",
      "bondman: 0.9999222607506579\n",
      "necesari: 0.9999222607506579\n",
      "noelbak: 0.9999222607506579\n",
      "greekalbanian: 0.9999222607506579\n",
      "murmer: 0.9999222607506579\n",
      "graf: 0.9999222607506579\n",
      "spee: 0.9999222607506579\n",
      "boreali: 0.9999222607506579\n",
      "winant: 0.9999222607506579\n",
      "chanter: 0.9999222607506579\n",
      "responsibil: 0.9999222607506579\n",
      "galeazzo: 0.9999222607506579\n",
      "ciano: 0.9999222607506579\n",
      "gladston: 0.9999222607506579\n",
      "ustashi: 0.9999222607506579\n",
      "nedic: 0.9999222607506579\n",
      "chetnik: 0.9999222607506579\n",
      "burntdown: 0.9999222607506579\n",
      "inspit: 0.9999222607506579\n",
      "sedan: 0.9999222607506579\n",
      "concilatori: 0.9999222607506579\n",
      "permissionwa: 0.9999222607506579\n"
     ]
    }
   ],
   "source": [
    "affect_path = os.path.join(data_dict, 'dictionary_affect.pkl')\n",
    "cognition_path = os.path.join(data_dict, 'dictionary_cognition.pkl')\n",
    "\n",
    "affect = joblib.load(affect_path)\n",
    "cognition = joblib.load(cognition_path)\n",
    "\n",
    "# == Count dictionary words\n",
    "\n",
    "print(\"Contents of affect dictionary:\")\n",
    "print(affect)\n",
    "print(\"Number of words in affect dictionary:\", len(affect))\n",
    "\n",
    "print(\"\\nContents of cognition dictionary:\")\n",
    "print(cognition)\n",
    "print(\"Number of words in cognition dictionary:\", len(cognition))\n",
    "\n",
    "a_list = [[i, ind_stopwords_word_counts[i]] for i in affect if i in ind_stopwords_word_counts]\n",
    "c_list = [[i, ind_stopwords_word_counts[i]] for i in cognition if i in ind_stopwords_word_counts]\n",
    "\n",
    "a_list = sorted(a_list, key=lambda x: x[1], reverse=True)\n",
    "c_list = sorted(c_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "a = [[i[0], f\"({i[1]}),\"] for i in a_list]\n",
    "c = [[i[0], f\"({i[1]}),\"] for i in c_list]\n",
    "\n",
    "a1 = ' '.join(str(r) for v in a for r in v)\n",
    "c1 = ' '.join(str(r) for v in c for r in v)\n",
    "\n",
    "affect_out_path = os.path.join(data_freq, \"ind_stopwords_affect_words.txt\")\n",
    "cog_out_path = os.path.join(data_freq, \"ind_stopwords_cog_words.txt\")\n",
    "\n",
    "with open(affect_out_path, \"w\") as output:\n",
    "    output.write(a1)\n",
    "\n",
    "with open(cog_out_path, \"w\") as output:\n",
    "    output.write(c1)\n",
    "\n",
    "# number of affect/cognitive words that appear in word_counts\n",
    "num_affect_words = len(a_list)\n",
    "num_cog_words = len(c_list)\n",
    "\n",
    "# Dictionary words that appear less than 10 times\n",
    "num_affect_lt10 = sum(1 for _, count in a_list if count < 10)\n",
    "num_cog_lt10 = sum(1 for _, count in c_list if count < 10)\n",
    "\n",
    "print(f\"Unique affect words in text: {num_affect_words}\")\n",
    "print(f\"Unique cognition words in text: {num_cog_words}\")\n",
    "print(f\"Affect words with count < 10: {num_affect_lt10}\")\n",
    "print(f\"Cognition words with count < 10: {num_cog_lt10}\")\n",
    "\n",
    "\n",
    "# == Calculate weighted frequencies for all words\n",
    "\n",
    "l = sum(ind_stopwords_word_counts.values())\n",
    "\n",
    "a = 0.001 # Method to downweight with a smoothing parameter: For frequent words (large v/1), weight approaches 0; for rare words (small v/1) closer to 1\n",
    "ind_stopwords_word_counts_weighted = {k: a / (a + (v / l)) for k, v in ind_stopwords_word_counts.items()}\n",
    "\n",
    "joblib.dump(ind_stopwords_word_counts_weighted, os.path.join(data_freq, 'ind_stopwords_word_counts_weighted.pkl'))\n",
    "\n",
    "# To print top 50 by weighted values, sort the dictionary by value descending:\n",
    "top_50_weighted = sorted(ind_stopwords_word_counts_weighted.items(), key=lambda x: x[1], reverse=True)[:50]\n",
    "\n",
    "print(\"Top 50 words by weighted frequency:\")\n",
    "for word, weight in top_50_weighted:\n",
    "    print(f\"{word}: {weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a7ec60-2d73-4c68-948c-592970c4ad93",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df626ded-5e49-4a34-a3d1-315ba370226f",
   "metadata": {},
   "source": [
    "## Final Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d0856a2-44f1-4304-b221-09a9ab72ad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_freq)\n",
    "\n",
    "ind_stopwords_word_counts = joblib.load('ind_stopwords_word_counts.pkl')  # load stemmed counts\n",
    "# For each speech only keep tokens that appear at least 10x\n",
    "\n",
    "def select(lista):\n",
    "    for i in range(len(lista)):\n",
    "        x = lista[i][0]\n",
    "        y = lista[i][1]\n",
    "        y = [w for w in y if ind_stopwords_word_counts.get(w, 0) >= 10]\n",
    "        lista[i] = [x, y]\n",
    "    return lista\n",
    "\n",
    "for data_path in preprocessed_files:\n",
    "    data = joblib.load(data_path)\n",
    "    data = select(data)\n",
    "    cleaned_path = data_path.replace('.pkl', '_final.pkl')\n",
    "    joblib.dump(data, cleaned_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "605ff6af-7346-4c8e-9729-f7fcbda0f927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          filename  speech_length_final\n",
      "0  ARG_01_1946.txt                 1188\n",
      "1  AUS_01_1946.txt                 1656\n",
      "2  BEL_01_1946.txt                  963\n",
      "3  BLR_01_1946.txt                 1229\n",
      "4  BOL_01_1946.txt                  511\n",
      "Total unique tokens across all final speeches: 12495\n",
      "Average tokens per final speech: 1168.859203798393\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_preprocessed)\n",
    "\n",
    "final_files = [\n",
    "    os.path.join(data_preprocessed, 'ind_stopwords_preprocessed_speeches_indexed1_final.pkl'),\n",
    "    os.path.join(data_preprocessed, 'ind_stopwords_preprocessed_speeches_indexed2_final.pkl'),\n",
    "    os.path.join(data_preprocessed, 'ind_stopwords_preprocessed_speeches_indexed3_final.pkl'),\n",
    "    os.path.join(data_preprocessed, 'ind_stopwords_preprocessed_speeches_indexed4_final.pkl')\n",
    "]\n",
    "\n",
    "final_data = []\n",
    "for fname in final_files:\n",
    "    final_data.extend(joblib.load(fname))\n",
    "\n",
    "# Merge with df_merged\n",
    "ind_stopwords_df_final = pd.DataFrame(final_data, columns=[\"filename\", \"speech_final\"])\n",
    "ind_stopwords_df_final = df_clean.merge(ind_stopwords_df_final, on=\"filename\", how=\"left\")\n",
    "\n",
    "# Create speech_length_final column\n",
    "ind_stopwords_df_final[\"speech_length_final\"] = ind_stopwords_df_final[\"speech_final\"].apply(\n",
    "    lambda x: len(x) if isinstance(x, list) else 0\n",
    ")\n",
    "\n",
    "print(ind_stopwords_df_final[[\"filename\", \"speech_length_final\"]].head())\n",
    "\n",
    "all_tokens_final = [token for speech in ind_stopwords_df_final[\"speech_final\"].dropna() for token in speech]\n",
    "unique_tokens_final = set(all_tokens_final)\n",
    "print(\"Total unique tokens across all final speeches:\", len(unique_tokens_final))\n",
    "\n",
    "print(\"Average tokens per final speech:\", ind_stopwords_df_final[\"speech_length_final\"].mean())\n",
    "\n",
    "# Save as pickle\n",
    "joblib.dump(ind_stopwords_df_final, os.path.join(data_preprocessed, \"un_corpus_cleaned_final_ind_stopwords.pkl\"))\n",
    "\n",
    "# Save as CSV\n",
    "ind_stopwords_df_final.to_csv(\n",
    "    os.path.join(data_preprocessed, \"un_corpus_cleaned_final_ind_stopwords.csv\"),\n",
    "    sep=';',\n",
    "    index=False,\n",
    "    encoding='utf-8'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20631f47-da72-48d5-9a9a-1e38f359b0d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
