{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37f0994f-b5e3-45b5-a495-5c67b083d3d1",
   "metadata": {},
   "source": [
    "# Development of Emotion and Reasoning in the General Speeches of the United Nations: A text-based machine learning approach\n",
    "## MAIN - File\n",
    "\n",
    "### Description: \n",
    "It automatically runs all notebooks in the correct order, handles optional additional analyses, and can install required Python packages and download necessary resources. All outputs, including figures, tables, and results, are saved to their respective folders.\n",
    "\n",
    "It runs all notebooks in the correct order. The script installs required packages and downloads necessary resources. If you do not want this, set InstallPackages = False. It does not overwrite existing packages; it only installs packages that are missing. The script can also download necessary resources for NLTK and spaCy. This includes tokenizers, taggers, and the en_core_web_lg spaCy model. Existing resources are not overwritten.\r\n",
    "\n",
    "By default, it will also run the Additional Analysis. If you do not want the Additional Analysis to run, set RUN_ADDITIONAL_ANALYSIS = False. \r",
    "t\n",
    "All figures, tables, and results are saved automatically in the corresponding folders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0fb740-4d1c-401e-bda0-7e71e42213ca",
   "metadata": {},
   "source": [
    "##  Installation of required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e6f260-fcf6-44de-9e45-9371d5014c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to True to install the following packages \n",
    "# The function will only install packages that are not installed yet\n",
    "\n",
    "InstallPackages = False \n",
    "\n",
    "if InstallPackages:\n",
    "    import sys\n",
    "\n",
    "    packages = [\n",
    "         \"gensim\",\n",
    "        \"joblib\",\n",
    "        \"matplotlib\",\n",
    "        \"nbconvert\",\n",
    "        \"nltk\",\n",
    "        \"numpy\",\n",
    "        \"pandas\",\n",
    "        \"pycountry\",\n",
    "        \"rapidfuzz\",\n",
    "        \"scipy\",\n",
    "        \"seaborn\",\n",
    "        \"spacy\",\n",
    "        \"tableone\",\n",
    "        \"tabulate\",\n",
    "        \"tqdm\"\n",
    "        \n",
    "    ]\n",
    "\n",
    "    for package in packages:\n",
    "        if importlib.util.find_spec(package) is None:\n",
    "            print(f\"Installing package: {package}\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        else:\n",
    "            print(f\"Package already installed: {package}\")\n",
    "\n",
    "# Set to True to download resources; it will only install resources that are missing\n",
    "DownloadAdditions = False  \n",
    "\n",
    "if DownloadAdditions:\n",
    "    import nltk\n",
    "    import spacy\n",
    "    import subprocess\n",
    "    import sys\n",
    "\n",
    "    # --- NLTK resources ---\n",
    "    nltk_packages = [\"punkt\", \"averaged_perceptron_tagger\"]\n",
    "    for pkg in nltk_packages:\n",
    "        try:\n",
    "            nltk.data.find(f\"tokenizers/{pkg}\" if pkg == \"punkt\" else f\"taggers/{pkg}\")\n",
    "            print(f\"NLTK resource already exists: {pkg}\")\n",
    "        except LookupError:\n",
    "            print(f\"Downloading NLTK resource: {pkg}\")\n",
    "            nltk.download(pkg)\n",
    "\n",
    "    # --- spaCy model ---\n",
    "    spacy_model = \"en_core_web_lg\"\n",
    "    try:\n",
    "        spacy.load(spacy_model)\n",
    "        print(f\"spaCy model already exists: {spacy_model}\")\n",
    "    except OSError:\n",
    "        print(f\"Downloading spaCy model: {spacy_model}\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", spacy_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce23377-f5c6-48e3-abd9-eec2995721f8",
   "metadata": {},
   "source": [
    "## Run Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dc991c-abb3-4aa4-80f9-6cdbb9fe004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from nbconvert.preprocessors import ExecutePreprocessor\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36b3fce2-3eba-4639-b305-eff974803d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running notebook: 0_data_creation.ipynb ...\n"
     ]
    },
    {
     "ename": "CellExecutionError",
     "evalue": "An error occurred while executing the following cell:\n------------------\n# == New variable: Speech length of the preprocessed corpus ==\n\n# Count tokens in preprocessed speech\ndf_clean[\"speech_length_preprocessed\"] = dfclean[\"speech_preprocessed\"].apply(\n    lambda x: len(x) if isinstance(x, list) else 0\n)\n\nprint(df_clean[[\"filename\", \"speech_length_preprocessed\"]].head())\nall_tokens = [token for speech in df_clean[\"speech_preprocessed\"].dropna() for token in speech]\nunique_tokens = set(all_tokens)\nprint(\"Total unique tokens:\", len(unique_tokens))\n\n# Average length of preprocessed speeches\naverage_length = df_clean[\"speech_length_preprocessed\"].mean()\n\nprint(f\"Average number of tokens per speech: {average_length:.2f}\")\n------------------\n\n\n\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\nCell \u001b[1;32mIn[28], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# == New variable: Speech length of the preprocessed corpus ==\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Count tokens in preprocessed speech\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m df_clean[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeech_length_preprocessed\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dfclean[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeech_preprocessed\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_clean[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeech_length_preprocessed\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m      9\u001b[0m all_tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m speech \u001b[38;5;129;01min\u001b[39;00m df_clean[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeech_preprocessed\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdropna() \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m speech]\n\n\u001b[1;31mNameError\u001b[0m: name 'dfclean' is not defined\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCellExecutionError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 33\u001b[0m\n\u001b[0;32m     25\u001b[0m notebooks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnotebooks/0_data_creation.ipynb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnotebooks/1_model_training_centroids_scoring.ipynb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnotebooks/2_figures.ipynb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnotebooks/3_tables.ipynb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     30\u001b[0m ]\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nb \u001b[38;5;129;01min\u001b[39;00m notebooks:\n\u001b[1;32m---> 33\u001b[0m     run_notebook(nb)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll notebooks executed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 19\u001b[0m, in \u001b[0;36mrun_notebook\u001b[1;34m(notebook_path, timeout)\u001b[0m\n\u001b[0;32m     16\u001b[0m     nb \u001b[38;5;241m=\u001b[39m nbformat\u001b[38;5;241m.\u001b[39mread(f, as_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m     18\u001b[0m ep \u001b[38;5;241m=\u001b[39m ExecutePreprocessor(timeout\u001b[38;5;241m=\u001b[39mtimeout, kernel_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m ep\u001b[38;5;241m.\u001b[39mpreprocess(nb, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m: notebook_path\u001b[38;5;241m.\u001b[39mparent}})\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished notebook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnotebook_path\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nbconvert\\preprocessors\\execute.py:102\u001b[0m, in \u001b[0;36mExecutePreprocessor.preprocess\u001b[1;34m(self, nb, resources, km)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage_info\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m info_msg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage_info\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, cell \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb\u001b[38;5;241m.\u001b[39mcells):\n\u001b[1;32m--> 102\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_cell(cell, resources, index)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_widgets_metadata()\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresources\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nbconvert\\preprocessors\\execute.py:123\u001b[0m, in \u001b[0;36mExecutePreprocessor.preprocess_cell\u001b[1;34m(self, cell, resources, index)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;124;03mOverride if you want to apply some preprocessing to each cell.\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;124;03mMust return modified cell and resource dictionary.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m    Index of the cell being processed\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_assign_resources(resources)\n\u001b[1;32m--> 123\u001b[0m cell \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute_cell(cell, index, store_history\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cell, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresources\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\jupyter_core\\utils\\__init__.py:158\u001b[0m, in \u001b[0;36mrun_sync.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _runner_map:\n\u001b[0;32m    157\u001b[0m         _runner_map[name] \u001b[38;5;241m=\u001b[39m _TaskRunner()\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _runner_map[name]\u001b[38;5;241m.\u001b[39mrun(inner)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\jupyter_core\\utils\\__init__.py:126\u001b[0m, in \u001b[0;36m_TaskRunner.run\u001b[1;34m(self, coro)\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__runner_thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m    125\u001b[0m fut \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun_coroutine_threadsafe(coro, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__io_loop)\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fut\u001b[38;5;241m.\u001b[39mresult(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nbclient\\client.py:1058\u001b[0m, in \u001b[0;36mNotebookClient.async_execute_cell\u001b[1;34m(self, cell, cell_index, execution_count, store_history)\u001b[0m\n\u001b[0;32m   1054\u001b[0m                 new_outputs\u001b[38;5;241m.\u001b[39minsert(i, stdout)\n\u001b[0;32m   1056\u001b[0m     cell\u001b[38;5;241m.\u001b[39moutputs \u001b[38;5;241m=\u001b[39m new_outputs\n\u001b[1;32m-> 1058\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_raise_for_error(cell, cell_index, exec_reply)\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcells\u001b[39m\u001b[38;5;124m'\u001b[39m][cell_index] \u001b[38;5;241m=\u001b[39m cell\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cell\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nbclient\\client.py:914\u001b[0m, in \u001b[0;36mNotebookClient._check_raise_for_error\u001b[1;34m(self, cell, cell_index, exec_reply)\u001b[0m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m run_hook(\n\u001b[0;32m    911\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_cell_error, cell\u001b[38;5;241m=\u001b[39mcell, cell_index\u001b[38;5;241m=\u001b[39mcell_index, execute_reply\u001b[38;5;241m=\u001b[39mexec_reply\n\u001b[0;32m    912\u001b[0m )\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cell_allows_errors:\n\u001b[1;32m--> 914\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CellExecutionError\u001b[38;5;241m.\u001b[39mfrom_cell_and_msg(cell, exec_reply_content)\n",
      "\u001b[1;31mCellExecutionError\u001b[0m: An error occurred while executing the following cell:\n------------------\n# == New variable: Speech length of the preprocessed corpus ==\n\n# Count tokens in preprocessed speech\ndf_clean[\"speech_length_preprocessed\"] = dfclean[\"speech_preprocessed\"].apply(\n    lambda x: len(x) if isinstance(x, list) else 0\n)\n\nprint(df_clean[[\"filename\", \"speech_length_preprocessed\"]].head())\nall_tokens = [token for speech in df_clean[\"speech_preprocessed\"].dropna() for token in speech]\nunique_tokens = set(all_tokens)\nprint(\"Total unique tokens:\", len(unique_tokens))\n\n# Average length of preprocessed speeches\naverage_length = df_clean[\"speech_length_preprocessed\"].mean()\n\nprint(f\"Average number of tokens per speech: {average_length:.2f}\")\n------------------\n\n\n\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\nCell \u001b[1;32mIn[28], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# == New variable: Speech length of the preprocessed corpus ==\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Count tokens in preprocessed speech\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m df_clean[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeech_length_preprocessed\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dfclean[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeech_preprocessed\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_clean[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeech_length_preprocessed\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m      9\u001b[0m all_tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m speech \u001b[38;5;129;01min\u001b[39;00m df_clean[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeech_preprocessed\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdropna() \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m speech]\n\n\u001b[1;31mNameError\u001b[0m: name 'dfclean' is not defined\n"
     ]
    }
   ],
   "source": [
    "def run_notebook(notebook_path, timeout=20000):\n",
    "    \"\"\"\n",
    "    Executes the Jupyter notebooks automatically.\n",
    "\n",
    "    The notebook is loaded and run cell-by-cell.\n",
    "    \"\"\"\n",
    "    notebook_path = Path(notebook_path)\n",
    "    if not notebook_path.exists():\n",
    "        raise FileNotFoundError(f\"Notebook {notebook_path} not found.\")\n",
    "\n",
    "    print(f\"Running notebook: {notebook_path.name} ...\")\n",
    "    \n",
    "    with open(notebook_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        nb = nbformat.read(f, as_version=4)\n",
    "    \n",
    "    ep = ExecutePreprocessor(timeout=timeout, kernel_name=\"python3\")\n",
    "    ep.preprocess(nb, {'metadata': {'path': notebook_path.parent}})\n",
    "    \n",
    "    print(f\"Finished notebook: {notebook_path.name}\\n\")\n",
    "\n",
    "\n",
    "notebooks = [\n",
    "    \"notebooks/0_data_creation.ipynb\",\n",
    "    \"notebooks/1_model_training_centroids_scoring.ipynb\",\n",
    "    \"notebooks/2_figures.ipynb\",\n",
    "    \"notebooks/3_tables.ipynb\"\n",
    "]\n",
    "\n",
    "for nb in notebooks:\n",
    "    run_notebook(nb)\n",
    "\n",
    "print(\"All notebooks executed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c57273-07a6-405b-871c-4437f5e25516",
   "metadata": {},
   "source": [
    "## Additional Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088e3348-0b37-41e1-b255-c3c68ca87d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Optional: Run Additional Analysis ===\n",
    "# If you want to run also the additional analysis, then set the function here to TRUE\n",
    "RUN_ADDITIONAL_ANALYSIS = False  # Set to True to execute additional analysis notebooks\n",
    "\n",
    "if RUN_ADDITIONAL_ANALYSIS:\n",
    "    \"\"\"\n",
    "    Executes the additional analysis notebooks automatically.\n",
    "\n",
    "    Each notebook is loaded and run cell-by-cell.\n",
    "    \"\"\"\n",
    "    additional_notebooks = [\n",
    "        # Different Calculation Weighted Frequencies\n",
    "        \"notebooks/Additional_Analysis/Different_Calculation_Weighted_Frequencies/0_data_creation_changed_weighted_freq.ipynb\",\n",
    "        \"notebooks/Additional_Analysis/Different_Calculation_Weighted_Frequencies/1_model_training_centroids_scoring_changed_weighted_freq.ipynb\",\n",
    "        \"notebooks/Additional_Analysis/Different_Calculation_Weighted_Frequencies/2_figures_changed_weighted_freq.ipynb\",\n",
    "\n",
    "        # Individual Stopwords\n",
    "        \"notebooks/Additional_Analysis/Individual_Stopwords/0_data_creation_ind_stopwords.ipynb\",\n",
    "        \"notebooks/Additional_Analysis/Individual_Stopwords/1_model_training_centroids_scoring_ind_stopwords.ipynb\",\n",
    "        \"notebooks/Additional_Analysis/Individual_Stopwords/2_figures_ind_stopwords.ipynb\",\n",
    "\n",
    "        # Figure Comparison Emotionality Score for the different calculations\n",
    "        \"notebooks/Additional_Analysis/2_figure_comparison_emotionality_score.ipynb\",\n",
    "    ]\n",
    "\n",
    "    print(\"Running Additional Analysis Notebooks...\")\n",
    "    for nb in additional_notebooks:\n",
    "        run_notebook(nb)\n",
    "\n",
    "    print(\"All Notebooks for Additional analysis executed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
