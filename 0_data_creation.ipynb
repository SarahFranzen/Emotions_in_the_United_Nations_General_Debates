{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b41cb22-e190-48d5-9926-7ac6fac8310d",
   "metadata": {},
   "source": [
    "# Emotion and Reason in Political Language: Examining the UN General speeches\n",
    "## Script 1: Preprocessing & Token Frequencies\n",
    "## Author: Sarah Franzen\n",
    "\n",
    "### Instructions BEFORE running this script:\n",
    "- Ensure all required packages are installed. If not, set `InstallPackages = TRUE` (see code cells below).  \n",
    "- Set your working directory appropriately.  \n",
    "- The script will automatically create the required folder structure.  \n",
    "- Later, you will be asked to download the data from:  \n",
    "  https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/0TJX8Y  \n",
    "  and store it **unzipped** inside the created folder *data_original*\n",
    "\n",
    "\n",
    "### Description: \n",
    "- Extract documents from their original txt documents and store them as one csv\n",
    "- Data Cleaning and Pre-Processing\n",
    "- Count word frequencies and weight themhemnal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5588afc-105a-426e-9348-742a830a2732",
   "metadata": {},
   "source": [
    "## Setup, Installation and Verification of required Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebb8987a-a0da-453e-97e2-aafa209d0e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "InstallPackages = False # Set this to True to install the following packages \n",
    "\n",
    "if InstallPackages:\n",
    "    import sys\n",
    "\n",
    "    packages = [\n",
    "        \"pandas\",\n",
    "        \"nltk\",\n",
    "        \"spacy\",\n",
    "        \"numpy\",\n",
    "        \"gensim\",\n",
    "        \"pycountry\",\n",
    "        \"wordcloud\",\n",
    "        \"matplotlib\",\n",
    "        \"tqdm\"\n",
    "    ]\n",
    "\n",
    "    for package in packages:\n",
    "        !{sys.executable} -m pip install {package}\n",
    "\n",
    "\n",
    "DownloadAdditions = False # Set this to True to download these additional resources\n",
    "if DownloadAdditions:\n",
    "    nltk.download(\"punkt\")\n",
    "    nltk.download(\"averaged_perceptron_tagger\")\n",
    "    spacy.cli.download('en_core_web_lg')         # Download spaCy English model (large)\n",
    "\n",
    "#########################\n",
    "# Check if all packages are included\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32d7d499-0563-46e0-9730-914c9f106d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Import standard and third-party libraries for data processing, NLP, and visualization ==\n",
    "\n",
    "import gensim\n",
    "import joblib\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import pycountry\n",
    "import random\n",
    "import re\n",
    "import spacy\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "#from matplotlib.colors import ListedColormap\n",
    "from multiprocessing import Pool, freeze_support\n",
    "# from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOPWORDS\n",
    "from string import punctuation\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# === Initialize NLP Tools ===\n",
    "\n",
    "# Translator to remove punctuation\n",
    "translator = str.maketrans('', '', punctuation)\n",
    "\n",
    "# POS tagger (not used by SpaCy, but optionally available via NLTK)\n",
    "tagger = nltk.perceptron.PerceptronTagger()\n",
    "\n",
    "# Load SpaCy English model with unnecessary components disabled\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d495156-bbb2-403c-86f2-83b47d8cb03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Folder structure created:\n",
      "- C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\n",
      "  - data_original\n",
      "  - dictionaries\n",
      "  - freq\n",
      "  - preprocessed\n",
      "  - temp\n",
      "  - tokenized\n"
     ]
    }
   ],
   "source": [
    "# === Set Working Directory and create folder structure ===\n",
    "\n",
    "# Prompt user to enter working directory path\n",
    "#wd = input(\"Please enter your working directory path (e.g., C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit): \").strip()\n",
    "\n",
    "# Change to the entered working directory\n",
    "#try:\n",
    "   # os.chdir(wd)\n",
    "    #print(f\"Working directory set to: {os.getcwd()}\")\n",
    "#except FileNotFoundError:\n",
    "   # print(\"ERROR: The directory you entered does not exist. Please restart and enter a valid path.\")\n",
    "    #exit(1)\n",
    "\n",
    "# Set your working directory (adjust this as needed)\n",
    "wd = r\"C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\"\n",
    "os.chdir(wd)\n",
    "\n",
    "# Set base path to current working directory\n",
    "base_path = Path.cwd()\n",
    "data_path = base_path / \"data\"\n",
    "\n",
    "# List of subfolders to create inside 'data'\n",
    "subfolders = [\"data_original\", \"dictionaries\", \"freq\", \"preprocessed\", \"temp\", \"tokenized\"]\n",
    "\n",
    "# Create 'data' folder if it doesn't exist\n",
    "data_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Create subfolders\n",
    "for name in subfolders:\n",
    "    (data_path / name).mkdir(exist_ok=True)\n",
    "\n",
    "print(\"\\nFolder structure created:\")\n",
    "print(f\"- {data_path}\")\n",
    "for name in subfolders:\n",
    "    print(f\"  - {name}\")\n",
    "\n",
    "# Prompt user to place raw data files\n",
    "#print(f\"\\nPlease place your raw data files (unzipped) into the folder:\\n  {data_path / 'data_original'}\")\n",
    "#input(\"Press Enter after you have placed the files to continue...\")\n",
    "\n",
    "#print(\"Continuing with the script...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56fa9a21-650f-4d8e-8a21-3d9629655c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define Folder Paths ===\n",
    "\n",
    "# Make sure that you have these folders in your working directory\n",
    "data_c = os.path.join(wd, 'data')\n",
    "data_temp = os.path.join(data_c, 'temp')\n",
    "data_freq = os.path.join(data_c, 'freq')\n",
    "data_dict = os.path.join(data_c, 'dictionaries')\n",
    "data_preprocessed = os.path.join(data_c, 'preprocessed')\n",
    "data_tokenized = os.path.join(data_c, 'tokenized')\n",
    "fig = os.path.join(wd, 'fig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "296fc7d6-a2ac-406e-a322-6c9f2702ee1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\n"
     ]
    }
   ],
   "source": [
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff34b6-cbee-4f8c-8076-5f385f596b58",
   "metadata": {},
   "source": [
    "## Load and Prepare Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18067b9-6337-4fb3-a570-603083a7deae",
   "metadata": {},
   "source": [
    "### This chunk can be skipped at the moment\n",
    "### Think of proper header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93d247a0-5ff2-4062-add9-f3267ae1fe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¾ Total speeches found: 10761\n",
      "\n",
      "âœ… Saved raw data with 600 speeches to '.\\data\\un_corpus_raw.csv'\n"
     ]
    }
   ],
   "source": [
    "# == Load and Save Sample from UN General Debate Corpus ==\n",
    "\n",
    "# Set Folder path containing the original TXT files    \n",
    "base_folder = r\".\\data\\data_original\\UN General Debate Corpus\\UNGDC_1946-2023\\TXT\"\n",
    "\n",
    "#  Gather all relevant txt-files\n",
    "all_txt_files = []\n",
    "for root, dirs, files in os.walk(base_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.txt') and not file.startswith('._'):\n",
    "            all_txt_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"ðŸ§¾ Total speeches found: {len(all_txt_files)}\")\n",
    "\n",
    "# Randomly pick 800 files from the full collection   ###################################################### REMOVE AT LATER POINT\n",
    "sampled_files = random.sample(all_txt_files,600)\n",
    "\n",
    "# Read the selected files into a list\n",
    "raw_data = []\n",
    "for filepath in sampled_files:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        raw_data.append({'filename': os.path.basename(filepath), 'speech': content})\n",
    "\n",
    "# Create DataFrame from the collected speeches\n",
    "df_raw = pd.DataFrame(raw_data)\n",
    "\n",
    "df_raw = df_raw[df_raw['filename'] != '.DS_Store-to-UTF-8.txt'].copy()\n",
    "\n",
    "# Save df_raw as a pickle file for quick future loading\n",
    "raw_pickle_path = r\".\\data\\un_corpus_raw.pkl\"\n",
    "df_raw.to_pickle(raw_pickle_path)\n",
    "\n",
    "# Export df as CSV \n",
    "raw_output_path = r\".\\data\\un_corpus_raw.csv\"\n",
    "df_raw.to_csv(raw_output_path, index=False, sep=';', encoding='utf-8')\n",
    "\n",
    "print(f\"\\nâœ… Saved raw data with {len(df_raw)} speeches to '{raw_output_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6405ff20-92fa-4c97-becb-ac5fff70f84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GIN_77_2022.txt</td>\n",
       "      <td>At the outset of my remarks, I wish to convey ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GHA_28_1973.txt</td>\n",
       "      <td>ï»¿29.\\t Permit me, Sir, on behalf of the Govern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GRC_23_1968.txt</td>\n",
       "      <td>40. Mr. President, on behalf of the Greek dele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BGR_21_1966.txt</td>\n",
       "      <td>106.     On behalf of the delegation of the Pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ESP_27_1972.txt</td>\n",
       "      <td>Mr. President, before addressing this twenty- ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech\n",
       "0  GIN_77_2022.txt  At the outset of my remarks, I wish to convey ...\n",
       "1  GHA_28_1973.txt  ï»¿29.\\t Permit me, Sir, on behalf of the Govern...\n",
       "2  GRC_23_1968.txt  40. Mr. President, on behalf of the Greek dele...\n",
       "3  BGR_21_1966.txt  106.     On behalf of the delegation of the Pe...\n",
       "4  ESP_27_1972.txt  Mr. President, before addressing this twenty- ..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# == Check if everything worked & drop empty speeches ==\n",
    "\n",
    "# Load df_raw\n",
    "df_raw = pd.read_pickle(r\".\\data\\un_corpus_raw.pkl\")\n",
    "\n",
    "# Drop empty speeches\n",
    "df_raw['speech'] = df_raw['speech'].astype(str)\n",
    "df_raw = df_raw[df_raw['speech'].str.strip() != ''].copy()\n",
    "\n",
    "# View df to check structure\n",
    "df_raw.head()         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7d3162-ebbd-449b-a2be-1148fceda725",
   "metadata": {},
   "source": [
    "## Create new variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25710e79-5068-4d39-8023-f4f97513f165",
   "metadata": {},
   "source": [
    "### Year, country_code and country_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "162da07c-1112-43e8-a434-2bef0ed32de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min year: 1946\n",
      "Max year: 2023\n",
      "Missing codes: []\n",
      "df_raw saved to C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\un_corpus_raw.pkl\n"
     ]
    }
   ],
   "source": [
    "# Extract country code (first 3 letters) and year (last 4 digits before .txt)\n",
    "df_raw['country_code'] = df_raw['filename'].str.extract(r'^([A-Z]{2,3})')\n",
    "df_raw['year'] = df_raw['filename'].str.extract(r'_(\\d{4})\\.txt$').astype(int)\n",
    "\n",
    "print(\"Min year:\", df_raw['year'].min())\n",
    "print(\"Max year:\", df_raw['year'].max())\n",
    "\n",
    "# Match country codes to country names\n",
    "code_to_name = {country.alpha_3: country.name for country in pycountry.countries}\n",
    "\n",
    "# Add custom short names and legacy codes\n",
    "custom_names = {\n",
    "    \"BOL\": \"Bolivia\",\n",
    "    \"COD\": \"The Democratic Republic of the Congo\",\n",
    "    \"IRN\": \"Iran\",\n",
    "    \"LAO\": \"Laos\",\n",
    "    \"MDA\": \"Moldova\",\n",
    "    \"PRK\": \"North Korea\",\n",
    "    \"PSE\": \"Palestine\",\n",
    "    \"RUS\": \"Russia\",\n",
    "    \"SYR\": \"Syria\",\n",
    "    \"TZA\": \"Tanzania\",\n",
    "    \"VAT\": \"Vatican City State\",\n",
    "    \"VEN\": \"Venezuela\",\n",
    "    \"VNM\": \"Vietnam\",\n",
    "    \"YMD\": \"South Yemen\",\n",
    "    \"YUG\": \"Yugoslavia\",\n",
    "    \"DDR\": \"East Germany\",\n",
    "    # \"SUN\": \"Soviet Union\",\n",
    "    \"EU\": \"European Union\",\n",
    "    \"CSK\": \"Czechoslovakia\",\n",
    "    \"FSM\": \"Micronesia\",\n",
    "    \"KOR\": \"South Korea\"\n",
    "    \n",
    "}\n",
    "\n",
    "# Update the main mapping with custom names\n",
    "code_to_name.update(custom_names)\n",
    "\n",
    "# Map with updated dictionary\n",
    "df_raw['country_name'] = df_raw['country_code'].map(code_to_name)\n",
    "\n",
    "# Check missing mappings\n",
    "missing = df_raw.loc[df_raw['country_name'].isna(), 'country_code'].unique()\n",
    "print(\"Missing codes:\", missing)\n",
    "\n",
    "# Check structure of the df\n",
    "df_raw.head() \n",
    "\n",
    "save_path = os.path.join(data_c, 'un_corpus_raw.pkl')\n",
    "df_raw.to_pickle(save_path)\n",
    "print(f\"df_raw saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "383d2904-f015-4c9d-9aeb-51a94f03f950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    country_code                          country_name\n",
      "0            AFG                           Afghanistan\n",
      "1            AGO                                Angola\n",
      "2            ALB                               Albania\n",
      "3            ARE                  United Arab Emirates\n",
      "4            ARG                             Argentina\n",
      "5            ARM                               Armenia\n",
      "6            ATG                   Antigua and Barbuda\n",
      "7            AUS                             Australia\n",
      "8            AUT                               Austria\n",
      "9            AZE                            Azerbaijan\n",
      "10           BDI                               Burundi\n",
      "11           BEL                               Belgium\n",
      "12           BEN                                 Benin\n",
      "13           BFA                          Burkina Faso\n",
      "14           BGD                            Bangladesh\n",
      "15           BGR                              Bulgaria\n",
      "16           BHR                               Bahrain\n",
      "17           BHS                               Bahamas\n",
      "18           BIH                Bosnia and Herzegovina\n",
      "19           BLR                               Belarus\n",
      "20           BLZ                                Belize\n",
      "21           BOL                               Bolivia\n",
      "22           BRA                                Brazil\n",
      "23           BRB                              Barbados\n",
      "24           BTN                                Bhutan\n",
      "25           BWA                              Botswana\n",
      "26           CAF              Central African Republic\n",
      "27           CAN                                Canada\n",
      "28           CHL                                 Chile\n",
      "29           CHN                                 China\n",
      "30           CIV                         CÃ´te d'Ivoire\n",
      "31           COD  The Democratic Republic of the Congo\n",
      "32           COG                                 Congo\n",
      "33           COL                              Colombia\n",
      "34           COM                               Comoros\n",
      "35           CPV                            Cabo Verde\n",
      "36           CRI                            Costa Rica\n",
      "37           CSK                        Czechoslovakia\n",
      "38           CUB                                  Cuba\n",
      "39           CYP                                Cyprus\n",
      "40           DDR                          East Germany\n",
      "41           DEU                               Germany\n",
      "42           DJI                              Djibouti\n",
      "43           DNK                               Denmark\n",
      "44           DOM                    Dominican Republic\n",
      "45           DZA                               Algeria\n",
      "46           ECU                               Ecuador\n",
      "47           EGY                                 Egypt\n",
      "48           ERI                               Eritrea\n",
      "49           ESP                                 Spain\n",
      "50           EST                               Estonia\n",
      "51           ETH                              Ethiopia\n",
      "52           FIN                               Finland\n",
      "53           FJI                                  Fiji\n",
      "54           FRA                                France\n",
      "55           GAB                                 Gabon\n",
      "56           GBR                        United Kingdom\n",
      "57           GEO                               Georgia\n",
      "58           GHA                                 Ghana\n",
      "59           GIN                                Guinea\n",
      "60           GMB                                Gambia\n",
      "61           GNB                         Guinea-Bissau\n",
      "62           GNQ                     Equatorial Guinea\n",
      "63           GRC                                Greece\n",
      "64           GRD                               Grenada\n",
      "65           GTM                             Guatemala\n",
      "66           GUY                                Guyana\n",
      "67           HND                              Honduras\n",
      "68           HRV                               Croatia\n",
      "69           HTI                                 Haiti\n",
      "70           HUN                               Hungary\n",
      "71           IDN                             Indonesia\n",
      "72           IND                                 India\n",
      "73           IRL                               Ireland\n",
      "74           IRN                                  Iran\n",
      "75           IRQ                                  Iraq\n",
      "76           ISL                               Iceland\n",
      "77           ISR                                Israel\n",
      "78           ITA                                 Italy\n",
      "79           JAM                               Jamaica\n",
      "80           JOR                                Jordan\n",
      "81           JPN                                 Japan\n",
      "82           KEN                                 Kenya\n",
      "83           KGZ                            Kyrgyzstan\n",
      "84           KHM                              Cambodia\n",
      "85           KIR                              Kiribati\n",
      "86           KNA                 Saint Kitts and Nevis\n",
      "87           KOR                           South Korea\n",
      "88           LAO                                  Laos\n",
      "89           LBN                               Lebanon\n",
      "90           LBR                               Liberia\n",
      "91           LBY                                 Libya\n",
      "92           LCA                           Saint Lucia\n",
      "93           LIE                         Liechtenstein\n",
      "94           LKA                             Sri Lanka\n",
      "95           LSO                               Lesotho\n",
      "96           LTU                             Lithuania\n",
      "97           LUX                            Luxembourg\n",
      "98           LVA                                Latvia\n",
      "99           MAR                               Morocco\n",
      "100          MCO                                Monaco\n",
      "101          MDA                               Moldova\n",
      "102          MDG                            Madagascar\n",
      "103          MDV                              Maldives\n",
      "104          MEX                                Mexico\n",
      "105          MHL                      Marshall Islands\n",
      "106          MKD                       North Macedonia\n",
      "107          MLI                                  Mali\n",
      "108          MMR                               Myanmar\n",
      "109          MNE                            Montenegro\n",
      "110          MNG                              Mongolia\n",
      "111          MOZ                            Mozambique\n",
      "112          MRT                            Mauritania\n",
      "113          MUS                             Mauritius\n",
      "114          MWI                                Malawi\n",
      "115          MYS                              Malaysia\n",
      "116          NAM                               Namibia\n",
      "117          NER                                 Niger\n",
      "118          NGA                               Nigeria\n",
      "119          NIC                             Nicaragua\n",
      "120          NLD                           Netherlands\n",
      "121          NOR                                Norway\n",
      "122          NPL                                 Nepal\n",
      "123          NZL                           New Zealand\n",
      "124          OMN                                  Oman\n",
      "125          PAK                              Pakistan\n",
      "126          PAN                                Panama\n",
      "127          PER                                  Peru\n",
      "128          PHL                           Philippines\n",
      "129          PNG                      Papua New Guinea\n",
      "130          POL                                Poland\n",
      "131          PRK                           North Korea\n",
      "132          PRT                              Portugal\n",
      "133          PRY                              Paraguay\n",
      "134          QAT                                 Qatar\n",
      "135          ROU                               Romania\n",
      "136          RUS                                Russia\n",
      "137          RWA                                Rwanda\n",
      "138          SAU                          Saudi Arabia\n",
      "139          SDN                                 Sudan\n",
      "140          SEN                               Senegal\n",
      "141          SGP                             Singapore\n",
      "142          SLE                          Sierra Leone\n",
      "143          SLV                           El Salvador\n",
      "144          SMR                            San Marino\n",
      "145          SOM                               Somalia\n",
      "146          SRB                                Serbia\n",
      "147          STP                 Sao Tome and Principe\n",
      "148          SUR                              Suriname\n",
      "149          SVK                              Slovakia\n",
      "150          SVN                              Slovenia\n",
      "151          SWE                                Sweden\n",
      "152          SWZ                              Eswatini\n",
      "153          SYC                            Seychelles\n",
      "154          SYR                                 Syria\n",
      "155          TCD                                  Chad\n",
      "156          TGO                                  Togo\n",
      "157          THA                              Thailand\n",
      "158          TJK                            Tajikistan\n",
      "159          TKM                          Turkmenistan\n",
      "160          TLS                           Timor-Leste\n",
      "161          TTO                   Trinidad and Tobago\n",
      "162          TUN                               Tunisia\n",
      "163          TUR                               TÃ¼rkiye\n",
      "164          TUV                                Tuvalu\n",
      "165          TZA                              Tanzania\n",
      "166          UGA                                Uganda\n",
      "167          UKR                               Ukraine\n",
      "168          URY                               Uruguay\n",
      "169          USA                         United States\n",
      "170          VAT                    Vatican City State\n",
      "171          VCT      Saint Vincent and the Grenadines\n",
      "172          VEN                             Venezuela\n",
      "173          VNM                               Vietnam\n",
      "174          VUT                               Vanuatu\n",
      "175          WSM                                 Samoa\n",
      "176          YEM                                 Yemen\n",
      "177          YMD                           South Yemen\n",
      "178          YUG                            Yugoslavia\n",
      "179          ZAF                          South Africa\n",
      "180          ZMB                                Zambia\n",
      "181          ZWE                              Zimbabwe\n"
     ]
    }
   ],
   "source": [
    "# == Check the country names \n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(df_raw[['country_code', 'country_name']].drop_duplicates().sort_values('country_code').reset_index(drop=True))\n",
    "# Reset to default afterward\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5928538b-8519-40e2-bdee-ac72024f9f48",
   "metadata": {},
   "source": [
    "### Length of raw speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9841ec29-b451-419b-957e-7803fbe9401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column: speech length in words\n",
    "df_raw['speech_length_words'] = df_raw['speech'].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "911e2229-337f-4479-984c-3a6ab9dff5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average speech length (words): 3030.47\n",
      "20 shortest speeches:\n",
      "            filename        country_name  year  speech_length_words\n",
      "146  SYC_52_1997.txt          Seychelles  1997                  597\n",
      "147  IRN_03_1948.txt                Iran  1948                  813\n",
      "145  OMN_66_2011.txt                Oman  2011                  892\n",
      "12   GNB_78_2023.txt       Guinea-Bissau  2023                  904\n",
      "584  UGA_72_2017.txt              Uganda  2017                  910\n",
      "562  FIN_69_2014.txt             Finland  2014                  991\n",
      "144  ERI_65_2010.txt             Eritrea  2010                  997\n",
      "67   JOR_74_2019.txt              Jordan  2019                 1010\n",
      "104  BOL_60_2005.txt             Bolivia  2005                 1060\n",
      "83   FRA_61_2006.txt              France  2006                 1069\n",
      "468  BGR_57_2002.txt            Bulgaria  2002                 1097\n",
      "48   COG_71_2016.txt               Congo  2016                 1122\n",
      "10   MLI_64_2009.txt                Mali  2009                 1156\n",
      "377  DOM_04_1949.txt  Dominican Republic  1949                 1174\n",
      "326  AFG_64_2009.txt         Afghanistan  2009                 1180\n",
      "557  ITA_63_2008.txt               Italy  2008                 1200\n",
      "139  JOR_78_2023.txt              Jordan  2023                 1202\n",
      "413  LBY_77_2022.txt               Libya  2022                 1268\n",
      "301  SWZ_66_2011.txt            Eswatini  2011                 1271\n",
      "509  EGY_03_1948.txt               Egypt  1948                 1273\n",
      "\n",
      "20 longest speeches:\n",
      "            filename    country_name  year  speech_length_words\n",
      "399  CUB_15_1960.txt            Cuba  1960                21777\n",
      "422  BGR_15_1960.txt        Bulgaria  1960                11116\n",
      "123  NGA_18_1963.txt         Nigeria  1963                 9804\n",
      "311  IRL_39_1984.txt         Ireland  1984                 9612\n",
      "254  ROU_15_1960.txt         Romania  1960                 9208\n",
      "189  GHA_15_1960.txt           Ghana  1960                 7995\n",
      "503  GBR_09_1954.txt  United Kingdom  1954                 7728\n",
      "446  SDN_24_1969.txt           Sudan  1969                 7681\n",
      "272  PAN_36_1981.txt          Panama  1981                 7585\n",
      "251  RUS_21_1966.txt          Russia  1966                 7402\n",
      "138  RUS_22_1967.txt          Russia  1967                 7181\n",
      "78   MYS_18_1963.txt        Malaysia  1963                 7133\n",
      "597  FJI_27_1972.txt            Fiji  1972                 6823\n",
      "275  BEN_29_1974.txt           Benin  1974                 6654\n",
      "194  CSK_21_1966.txt  Czechoslovakia  1966                 6606\n",
      "21   LUX_40_1985.txt      Luxembourg  1985                 6577\n",
      "550  EGY_32_1977.txt           Egypt  1977                 6570\n",
      "156  MUS_40_1985.txt       Mauritius  1985                 6530\n",
      "499  GRC_38_1983.txt          Greece  1983                 6495\n",
      "363  MEX_12_1957.txt          Mexico  1957                 6402\n"
     ]
    }
   ],
   "source": [
    "# Calculate average length\n",
    "avg_length = df_raw['speech_length_words'].mean()\n",
    "\n",
    "# Print it\n",
    "print(\"Average speech length (words):\", round(avg_length, 2))\n",
    "\n",
    "# 20 shortest speeches\n",
    "print(\"20 shortest speeches:\")\n",
    "print(df_raw.nsmallest(20, 'speech_length_words')[['filename', 'country_name', 'year', 'speech_length_words']])\n",
    "\n",
    "# 20 longest speeches\n",
    "print(\"\\n20 longest speeches:\")\n",
    "print(df_raw.nlargest(20, 'speech_length_words')[['filename', 'country_name', 'year', 'speech_length_words']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424f152-b786-45bf-bf86-e169a900f628",
   "metadata": {},
   "source": [
    "### New variable: English as Official Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1d2c2a6-23d6-4044-84cc-8785167e6b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries not matched in df_raw['country_name']:\n",
      "Anguilla\n",
      "Bermuda\n",
      "British Virgin Islands\n",
      "Cameroon\n",
      "Cayman Islands\n",
      "Christmas Island\n",
      "Cook Islands\n",
      "Dominica\n",
      "Hong Kong\n",
      "Jersey\n",
      "Malta\n",
      "Micronesia\n",
      "Niue\n",
      "Norfolk Island\n",
      "Northern Mariana Islands\n",
      "Palau\n",
      "Pitcairn Islands\n",
      "Sint Maarten\n",
      "Solomon Islands\n",
      "South Sudan\n",
      "Tonga\n",
      "Turks and Caicos Islands\n"
     ]
    }
   ],
   "source": [
    "# Source for english as official language : https://gradschool.utk.edu/future-students/office-of-graduate-admissions/applying-to-graduate-school/admission-requirements/testing-requirements/countries-with-english-as-official-language/\n",
    "# They are quoting: https://www.cia.gov/the-world-factbook/field/languages/\n",
    "\n",
    "english_countries = [\n",
    "    \"Anguilla\", \"Antigua and Barbuda\", \"Bahamas\", \"Barbados\", \"Belize\", \"Belgium\",\n",
    "    \"Bermuda\", \"Botswana\", \"British Virgin Islands\", \"Burundi\", \"Cameroon\", \"Canada\",\n",
    "    \"Cayman Islands\", \"Christmas Island\", \"Cook Islands\", \"Dominica\", \"Fiji\", \"Gambia\",\n",
    "    \"Ghana\", \"Grenada\", \"Guyana\", \"Hong Kong\", \"India\", \"Ireland\", \"Jersey\", \"Kenya\",\n",
    "    \"Liberia\", \"Malawi\", \"Malta\", \"Marshall Islands\", \"Micronesia\",\n",
    "    \"Namibia\", \"New Zealand\", \"Nigeria\", \"Niue\", \"Norfolk Island\", \"Northern Mariana Islands\",\n",
    "    \"Pakistan\", \"Palau\", \"Papua New Guinea\", \"Philippines\", \"Pitcairn Islands\", \"Rwanda\",\n",
    "    \"Saint Kitts and Nevis\", \"Saint Lucia\", \"Samoa\", \"Seychelles\", \"Sierra Leone\", \"Singapore\",\n",
    "    \"Sint Maarten\", \"Solomon Islands\", \"Somalia\", \"South Africa\", \"South Sudan\", \"Sudan\",\n",
    "    \"Eswatini\", \"Tanzania\", \"Tonga\", \"Trinidad and Tobago\", \"Turks and Caicos Islands\",\n",
    "    \"Tuvalu\", \"Uganda\", \"Zambia\", \"Zimbabwe\"\n",
    "]\n",
    "\n",
    "# 2. Create dummy column\n",
    "df_raw['english_official_language'] = df_raw['country_name'].apply(\n",
    "    lambda x: 1 if x in english_countries else 0\n",
    ")\n",
    "\n",
    "# 3. Find countries in the list that did not match any entry in df_raw\n",
    "matched = set(df_raw['country_name'])\n",
    "unmatched = [country for country in english_countries if country not in matched]\n",
    "\n",
    "# 4. Print unmatched country names\n",
    "print(\"Countries not matched in df_raw['country_name']:\")\n",
    "for country in unmatched:\n",
    "    print(country)\n",
    "\n",
    "# All of these countries are either British Overseas Territories, Australian Territories, self-governing island territories or Special Administrative Regions\n",
    "    # None of the unmatched regions are UN Members\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f094c95f-834e-4454-9442-0d28d4a66090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "      <th>country_code</th>\n",
       "      <th>year</th>\n",
       "      <th>country_name</th>\n",
       "      <th>speech_length_words</th>\n",
       "      <th>english_official_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GIN_77_2022.txt</td>\n",
       "      <td>At the outset of my remarks, I wish to convey ...</td>\n",
       "      <td>GIN</td>\n",
       "      <td>2022</td>\n",
       "      <td>Guinea</td>\n",
       "      <td>1794</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GHA_28_1973.txt</td>\n",
       "      <td>ï»¿29.\\t Permit me, Sir, on behalf of the Govern...</td>\n",
       "      <td>GHA</td>\n",
       "      <td>1973</td>\n",
       "      <td>Ghana</td>\n",
       "      <td>6227</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GRC_23_1968.txt</td>\n",
       "      <td>40. Mr. President, on behalf of the Greek dele...</td>\n",
       "      <td>GRC</td>\n",
       "      <td>1968</td>\n",
       "      <td>Greece</td>\n",
       "      <td>2878</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BGR_21_1966.txt</td>\n",
       "      <td>106.     On behalf of the delegation of the Pe...</td>\n",
       "      <td>BGR</td>\n",
       "      <td>1966</td>\n",
       "      <td>Bulgaria</td>\n",
       "      <td>4173</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ESP_27_1972.txt</td>\n",
       "      <td>Mr. President, before addressing this twenty- ...</td>\n",
       "      <td>ESP</td>\n",
       "      <td>1972</td>\n",
       "      <td>Spain</td>\n",
       "      <td>3013</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech  \\\n",
       "0  GIN_77_2022.txt  At the outset of my remarks, I wish to convey ...   \n",
       "1  GHA_28_1973.txt  ï»¿29.\\t Permit me, Sir, on behalf of the Govern...   \n",
       "2  GRC_23_1968.txt  40. Mr. President, on behalf of the Greek dele...   \n",
       "3  BGR_21_1966.txt  106.     On behalf of the delegation of the Pe...   \n",
       "4  ESP_27_1972.txt  Mr. President, before addressing this twenty- ...   \n",
       "\n",
       "  country_code  year country_name  speech_length_words  \\\n",
       "0          GIN  2022       Guinea                 1794   \n",
       "1          GHA  1973        Ghana                 6227   \n",
       "2          GRC  1968       Greece                 2878   \n",
       "3          BGR  1966     Bulgaria                 4173   \n",
       "4          ESP  1972        Spain                 3013   \n",
       "\n",
       "   english_official_language  \n",
       "0                          0  \n",
       "1                          1  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82540d8a-b656-45fb-aeb0-2fd5b4d7108a",
   "metadata": {},
   "source": [
    "### New variable: permanent member security council"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fb92fe4-5c22-43fa-8b48-62ec2240d716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define permanent members of the UN Security Council\n",
    "permanent_members = ['RUS', 'USA', 'FRA', 'GBR', 'CHN']\n",
    "\n",
    "# Create dummy variable\n",
    "df_raw['security_council_permanent'] = df_raw['country_code'].isin(permanent_members).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b7ed94c-7369-4b52-a84e-21bbdccd51ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    country_code    country_name  security_council_permanent  year\n",
      "83           FRA          France                           1  2006\n",
      "91           USA   United States                           1  2011\n",
      "106          USA   United States                           1  1960\n",
      "112          USA   United States                           1  2007\n",
      "138          RUS          Russia                           1  1967\n",
      "166          FRA          France                           1  2000\n",
      "181          FRA          France                           1  1946\n",
      "209          RUS          Russia                           1  2021\n",
      "219          CHN           China                           1  2009\n",
      "251          RUS          Russia                           1  1966\n",
      "258          CHN           China                           1  1996\n",
      "262          RUS          Russia                           1  1986\n",
      "440          RUS          Russia                           1  2003\n",
      "447          USA   United States                           1  1955\n",
      "449          CHN           China                           1  1958\n",
      "471          RUS          Russia                           1  1949\n",
      "500          CHN           China                           1  1951\n",
      "503          GBR  United Kingdom                           1  1954\n",
      "543          FRA          France                           1  2023\n"
     ]
    }
   ],
   "source": [
    "print(df_raw[df_raw['country_code'].isin(permanent_members)][\n",
    "    ['country_code', 'country_name', 'security_council_permanent', 'year']\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523547d9-9b10-473c-8c85-d1e2fc4fd3e1",
   "metadata": {},
   "source": [
    "### New variables: speaker & position & gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "213e961b-816c-43f1-8ad6-a17bae6cca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_c)\n",
    "df_speakers = pd.read_excel(r\"data_original\\UN General Debate Corpus\\Speakers_by_session.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd158957-391b-4b86-9160-7c606336bacc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Session</th>\n",
       "      <th>ISO Code</th>\n",
       "      <th>Country</th>\n",
       "      <th>Name of Person Speaking</th>\n",
       "      <th>Post</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>78</td>\n",
       "      <td>BRA</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Luiz Inacio Lula da Silva</td>\n",
       "      <td>President</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023</td>\n",
       "      <td>78</td>\n",
       "      <td>USA</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>Joseph R. Biden</td>\n",
       "      <td>President</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023</td>\n",
       "      <td>78</td>\n",
       "      <td>COL</td>\n",
       "      <td>Colombia</td>\n",
       "      <td>Gustavo Petro Urrego</td>\n",
       "      <td>President</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023</td>\n",
       "      <td>78</td>\n",
       "      <td>JOR</td>\n",
       "      <td>Jordan</td>\n",
       "      <td>Abdullah II ibn Al Hussein</td>\n",
       "      <td>King</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023</td>\n",
       "      <td>78</td>\n",
       "      <td>POL</td>\n",
       "      <td>Poland</td>\n",
       "      <td>Andrzej Duda</td>\n",
       "      <td>President</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Session ISO Code                   Country  \\\n",
       "0  2023       78      BRA                   Brazil    \n",
       "1  2023       78      USA  United States of America   \n",
       "2  2023       78      COL                  Colombia   \n",
       "3  2023       78      JOR                    Jordan   \n",
       "4  2023       78      POL                    Poland   \n",
       "\n",
       "      Name of Person Speaking       Post Unnamed: 6  \n",
       "0   Luiz Inacio Lula da Silva  President        NaN  \n",
       "1             Joseph R. Biden  President        NaN  \n",
       "2        Gustavo Petro Urrego  President        NaN  \n",
       "3  Abdullah II ibn Al Hussein       King        NaN  \n",
       "4                Andrzej Duda  President        NaN  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_speakers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18be2d52-b8d1-4cb1-a74d-491a523f26c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "      <th>country_code</th>\n",
       "      <th>year</th>\n",
       "      <th>country_name</th>\n",
       "      <th>speech_length_words</th>\n",
       "      <th>english_official_language</th>\n",
       "      <th>security_council_permanent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GIN_77_2022.txt</td>\n",
       "      <td>At the outset of my remarks, I wish to convey ...</td>\n",
       "      <td>GIN</td>\n",
       "      <td>2022</td>\n",
       "      <td>Guinea</td>\n",
       "      <td>1794</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GHA_28_1973.txt</td>\n",
       "      <td>ï»¿29.\\t Permit me, Sir, on behalf of the Govern...</td>\n",
       "      <td>GHA</td>\n",
       "      <td>1973</td>\n",
       "      <td>Ghana</td>\n",
       "      <td>6227</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GRC_23_1968.txt</td>\n",
       "      <td>40. Mr. President, on behalf of the Greek dele...</td>\n",
       "      <td>GRC</td>\n",
       "      <td>1968</td>\n",
       "      <td>Greece</td>\n",
       "      <td>2878</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BGR_21_1966.txt</td>\n",
       "      <td>106.     On behalf of the delegation of the Pe...</td>\n",
       "      <td>BGR</td>\n",
       "      <td>1966</td>\n",
       "      <td>Bulgaria</td>\n",
       "      <td>4173</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ESP_27_1972.txt</td>\n",
       "      <td>Mr. President, before addressing this twenty- ...</td>\n",
       "      <td>ESP</td>\n",
       "      <td>1972</td>\n",
       "      <td>Spain</td>\n",
       "      <td>3013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech  \\\n",
       "0  GIN_77_2022.txt  At the outset of my remarks, I wish to convey ...   \n",
       "1  GHA_28_1973.txt  ï»¿29.\\t Permit me, Sir, on behalf of the Govern...   \n",
       "2  GRC_23_1968.txt  40. Mr. President, on behalf of the Greek dele...   \n",
       "3  BGR_21_1966.txt  106.     On behalf of the delegation of the Pe...   \n",
       "4  ESP_27_1972.txt  Mr. President, before addressing this twenty- ...   \n",
       "\n",
       "  country_code  year country_name  speech_length_words  \\\n",
       "0          GIN  2022       Guinea                 1794   \n",
       "1          GHA  1973        Ghana                 6227   \n",
       "2          GRC  1968       Greece                 2878   \n",
       "3          BGR  1966     Bulgaria                 4173   \n",
       "4          ESP  1972        Spain                 3013   \n",
       "\n",
       "   english_official_language  security_council_permanent  \n",
       "0                          0                           0  \n",
       "1                          1                           0  \n",
       "2                          0                           0  \n",
       "3                          0                           0  \n",
       "4                          0                           0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "170a6cc9-fa93-454c-9f81-f489a84afe1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [filename, speech, country_code, year, country_name, speech_length_words, english_official_language, security_council_permanent]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(df_raw[(df_raw['country_code'] == 'MEX') & (df_raw['year'] == 1982)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "672de39e-93ba-4808-82fd-f92b122f0843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "      <th>country_code</th>\n",
       "      <th>year</th>\n",
       "      <th>country_name</th>\n",
       "      <th>speech_length_words</th>\n",
       "      <th>english_official_language</th>\n",
       "      <th>security_council_permanent</th>\n",
       "      <th>Year</th>\n",
       "      <th>ISO Code</th>\n",
       "      <th>Name of Person Speaking</th>\n",
       "      <th>Post</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GIN_77_2022.txt</td>\n",
       "      <td>At the outset of my remarks, I wish to convey ...</td>\n",
       "      <td>GIN</td>\n",
       "      <td>2022</td>\n",
       "      <td>Guinea</td>\n",
       "      <td>1794</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>GIN</td>\n",
       "      <td>Bernard Gomou</td>\n",
       "      <td>Prime Minister</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GHA_28_1973.txt</td>\n",
       "      <td>ï»¿29.\\t Permit me, Sir, on behalf of the Govern...</td>\n",
       "      <td>GHA</td>\n",
       "      <td>1973</td>\n",
       "      <td>Ghana</td>\n",
       "      <td>6227</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1973.0</td>\n",
       "      <td>GHA</td>\n",
       "      <td>Baah</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GRC_23_1968.txt</td>\n",
       "      <td>40. Mr. President, on behalf of the Greek dele...</td>\n",
       "      <td>GRC</td>\n",
       "      <td>1968</td>\n",
       "      <td>Greece</td>\n",
       "      <td>2878</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1968.0</td>\n",
       "      <td>GRC</td>\n",
       "      <td>Mr. PIPINELIS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BGR_21_1966.txt</td>\n",
       "      <td>106.     On behalf of the delegation of the Pe...</td>\n",
       "      <td>BGR</td>\n",
       "      <td>1966</td>\n",
       "      <td>Bulgaria</td>\n",
       "      <td>4173</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1966.0</td>\n",
       "      <td>BGR</td>\n",
       "      <td>Mr. BASHEV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ESP_27_1972.txt</td>\n",
       "      <td>Mr. President, before addressing this twenty- ...</td>\n",
       "      <td>ESP</td>\n",
       "      <td>1972</td>\n",
       "      <td>Spain</td>\n",
       "      <td>3013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1972.0</td>\n",
       "      <td>ESP</td>\n",
       "      <td>Mr. Lopez-Bravo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech  \\\n",
       "0  GIN_77_2022.txt  At the outset of my remarks, I wish to convey ...   \n",
       "1  GHA_28_1973.txt  ï»¿29.\\t Permit me, Sir, on behalf of the Govern...   \n",
       "2  GRC_23_1968.txt  40. Mr. President, on behalf of the Greek dele...   \n",
       "3  BGR_21_1966.txt  106.     On behalf of the delegation of the Pe...   \n",
       "4  ESP_27_1972.txt  Mr. President, before addressing this twenty- ...   \n",
       "\n",
       "  country_code  year country_name  speech_length_words  \\\n",
       "0          GIN  2022       Guinea                 1794   \n",
       "1          GHA  1973        Ghana                 6227   \n",
       "2          GRC  1968       Greece                 2878   \n",
       "3          BGR  1966     Bulgaria                 4173   \n",
       "4          ESP  1972        Spain                 3013   \n",
       "\n",
       "   english_official_language  security_council_permanent    Year ISO Code  \\\n",
       "0                          0                           0  2022.0      GIN   \n",
       "1                          1                           0  1973.0      GHA   \n",
       "2                          0                           0  1968.0      GRC   \n",
       "3                          0                           0  1966.0      BGR   \n",
       "4                          0                           0  1972.0      ESP   \n",
       "\n",
       "  Name of Person Speaking            Post _merge  \n",
       "0           Bernard Gomou  Prime Minister   both  \n",
       "1                    Baah             NaN   both  \n",
       "2          Mr. PIPINELIS              NaN   both  \n",
       "3              Mr. BASHEV             NaN   both  \n",
       "4         Mr. Lopez-Bravo             NaN   both  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged = df_raw.merge(\n",
    "    df_speakers[['Year', 'ISO Code', 'Name of Person Speaking', 'Post']],\n",
    "    left_on=['year', 'country_code'],\n",
    "    right_on=['Year', 'ISO Code'],\n",
    "    how='left',\n",
    "    indicator=True)\n",
    "\n",
    "df_merged.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "604ceb1f-a9fc-49ec-aca8-fdc28ad7e73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            filename  year country_code country_name\n",
      "62   HTI_18_1963.txt  1963          HTI        Haiti\n",
      "78   MYS_18_1963.txt  1963          MYS     Malaysia\n",
      "97   EGY_17_1962.txt  1962          EGY        Egypt\n",
      "299  YMD_44_1989.txt  1989          YMD  South Yemen\n",
      "455  YMD_35_1980.txt  1980          YMD  South Yemen\n",
      "540  SDN_18_1963.txt  1963          SDN        Sudan\n"
     ]
    }
   ],
   "source": [
    "# Merge with indicator and set unmatched rows to NA\n",
    "df_merged = df_raw.merge(\n",
    "    df_speakers[['Year', 'ISO Code', 'Name of Person Speaking', 'Post']],\n",
    "    left_on=['year', 'country_code'],\n",
    "    right_on=['Year', 'ISO Code'],\n",
    "    how='left',\n",
    "    indicator=True)\n",
    "\n",
    "# Get rows with no match in df_speakers\n",
    "unmatched = df_merged[df_merged['_merge'] == 'left_only']\n",
    "\n",
    "# Print unmatched rows with selected columns (panda sets them to NA by default)\n",
    "print(unmatched[['filename', 'year', 'country_code', 'country_name']])\n",
    "\n",
    "\n",
    "# Drop the '_merge' column from merged df\n",
    "df_merged = df_merged.drop(columns=['Year', 'ISO Code', '_merge'])\n",
    "\n",
    "# Rename columns\n",
    "df_merged = df_merged.rename(columns={\n",
    "    'Name of Person Speaking': 'speaker_name',\n",
    "    'Post': 'position'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c247f434-ab90-47ce-8c29-f40c65753755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gender_dummy \n",
    "df_merged['gender_dummy'] = df_merged['speaker_name'].apply(\n",
    "    lambda name: 0 if pd.notnull(name) and re.search(r'^(?:Mr|Sir)\\b', name, re.IGNORECASE)\n",
    "    else 1 if pd.notnull(name) and re.search(r'^(?:Mrs|Ms)\\b', name, re.IGNORECASE)\n",
    "    else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "452d80fd-3f71-41d5-9373-cc30ccbe5992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    gender_dummy  count\n",
      "0       0 (male)    256\n",
      "1     1 (female)      9\n",
      "2  NaN (unknown)    335\n"
     ]
    }
   ],
   "source": [
    "# Count all values including NaN\n",
    "counts = df_merged['gender_dummy'].value_counts(dropna=False)\n",
    "\n",
    "# Build summary using .get() to handle missing keys\n",
    "gender_summary = pd.DataFrame({\n",
    "    'gender_dummy': ['0 (male)', '1 (female)', 'NaN (unknown)'],\n",
    "    'count': [\n",
    "        counts.get(0, 0),\n",
    "        counts.get(1, 0),\n",
    "        counts.get(np.nan, 0)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(gender_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f9ba91d-cf5c-43e7-9fae-e35c868fbdf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "      <th>country_code</th>\n",
       "      <th>year</th>\n",
       "      <th>country_name</th>\n",
       "      <th>speech_length_words</th>\n",
       "      <th>english_official_language</th>\n",
       "      <th>security_council_permanent</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>position</th>\n",
       "      <th>gender_dummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GIN_77_2022.txt</td>\n",
       "      <td>At the outset of my remarks, I wish to convey ...</td>\n",
       "      <td>GIN</td>\n",
       "      <td>2022</td>\n",
       "      <td>Guinea</td>\n",
       "      <td>1794</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Bernard Gomou</td>\n",
       "      <td>Prime Minister</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GHA_28_1973.txt</td>\n",
       "      <td>ï»¿29.\\t Permit me, Sir, on behalf of the Govern...</td>\n",
       "      <td>GHA</td>\n",
       "      <td>1973</td>\n",
       "      <td>Ghana</td>\n",
       "      <td>6227</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Baah</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GRC_23_1968.txt</td>\n",
       "      <td>40. Mr. President, on behalf of the Greek dele...</td>\n",
       "      <td>GRC</td>\n",
       "      <td>1968</td>\n",
       "      <td>Greece</td>\n",
       "      <td>2878</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mr. PIPINELIS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BGR_21_1966.txt</td>\n",
       "      <td>106.     On behalf of the delegation of the Pe...</td>\n",
       "      <td>BGR</td>\n",
       "      <td>1966</td>\n",
       "      <td>Bulgaria</td>\n",
       "      <td>4173</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mr. BASHEV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ESP_27_1972.txt</td>\n",
       "      <td>Mr. President, before addressing this twenty- ...</td>\n",
       "      <td>ESP</td>\n",
       "      <td>1972</td>\n",
       "      <td>Spain</td>\n",
       "      <td>3013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mr. Lopez-Bravo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech  \\\n",
       "0  GIN_77_2022.txt  At the outset of my remarks, I wish to convey ...   \n",
       "1  GHA_28_1973.txt  ï»¿29.\\t Permit me, Sir, on behalf of the Govern...   \n",
       "2  GRC_23_1968.txt  40. Mr. President, on behalf of the Greek dele...   \n",
       "3  BGR_21_1966.txt  106.     On behalf of the delegation of the Pe...   \n",
       "4  ESP_27_1972.txt  Mr. President, before addressing this twenty- ...   \n",
       "\n",
       "  country_code  year country_name  speech_length_words  \\\n",
       "0          GIN  2022       Guinea                 1794   \n",
       "1          GHA  1973        Ghana                 6227   \n",
       "2          GRC  1968       Greece                 2878   \n",
       "3          BGR  1966     Bulgaria                 4173   \n",
       "4          ESP  1972        Spain                 3013   \n",
       "\n",
       "   english_official_language  security_council_permanent     speaker_name  \\\n",
       "0                          0                           0    Bernard Gomou   \n",
       "1                          1                           0             Baah   \n",
       "2                          0                           0   Mr. PIPINELIS    \n",
       "3                          0                           0       Mr. BASHEV   \n",
       "4                          0                           0  Mr. Lopez-Bravo   \n",
       "\n",
       "         position  gender_dummy  \n",
       "0  Prime Minister           NaN  \n",
       "1             NaN           NaN  \n",
       "2             NaN           0.0  \n",
       "3             NaN           0.0  \n",
       "4             NaN           0.0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "94a901c8-3fc4-4b33-a1ff-cede22bd196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store df_merged\n",
    "\n",
    "os.chdir(wd)\n",
    "\n",
    "# Save df_merged as a pickle file for quick future loading\n",
    "merged_pickle_path = r\".\\data\\un_corpus_merged.pkl\"\n",
    "df_merged.to_pickle(merged_pickle_path)\n",
    "\n",
    "# Export df as CSV \n",
    "merged_output_path = r\".\\data\\un_corpus_merged.csv\"\n",
    "df_merged.to_csv(merged_output_path, index=False, sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe0f4af-e939-4100-9618-5b522bdd72a1",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2661f431-a194-47d1-9a1c-89162bf85a91",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd37e08d-b4f2-459e-823c-3816a825ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Clean text by removing empty spaces, line breaks, hyphenation, stray characters, and escape quote ==\n",
    "\n",
    "# Define cleaning function\n",
    "def clean_text(content):\n",
    "    if pd.isna(content):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove line breaks and carriage returns\n",
    "    content = content.replace('\\n', ' ').replace('\\r', ' ')\n",
    "\n",
    "    # Collapse multiple spaces\n",
    "    content = ' '.join(content.split())\n",
    "\n",
    "    # Ensure spacing after punctuation\n",
    "    content = re.sub(r'(?<=[.,])(?=[^\\s])', r' ', content)\n",
    "\n",
    "    # Remove hyphenation at line breaks (e.g. \"inter- national\" â†’ \"international\")\n",
    "    content = re.sub(r'-\\s', '', content)\n",
    "\n",
    "     # Replace hyphen between letters with a space to prevent merging words (e.g. \"russian-and\" â†’ \"russian and\")\n",
    "    content = re.sub(r'(?<=\\w)-(?=\\w)', ' ', content)\n",
    "\n",
    "    # Remove stray backslashes\n",
    "    content = content.replace(\"\\\\\", \"\")\n",
    "\n",
    "    return content\n",
    "\n",
    "# Apply cleaning to each speech\n",
    "df_raw['speech'] = df_raw['speech'].astype(str)  # Ensure column is string type\n",
    "df_clean = df_raw.copy()\n",
    "df_clean['speech'] = df_clean['speech'].apply(clean_text)\n",
    "\n",
    "# Drop rows with empty speeches after cleaning\n",
    "df_clean = df_clean[df_clean['speech'].str.strip().astype(bool)].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7c6bd148-d5cf-4eb3-a66d-fc706e99c13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved clean speeches chunks in 'C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp'\n"
     ]
    }
   ],
   "source": [
    "# == Split cleaned data into chunks and save as separate files ==\n",
    "\n",
    "# Convert cleaned DataFrame to list of lists\n",
    "clean_data = df_clean[['filename', 'speech']].values.tolist()\n",
    "\n",
    "# Split cleaned data into 4 equal chunks\n",
    "data_id1 = clean_data[:int(len(clean_data)/4)]\n",
    "data_id2 = clean_data[int(len(clean_data)/4): int(2*len(clean_data)/4)]\n",
    "data_id3 = clean_data[int(2*len(clean_data)/4): int(3*len(clean_data)/4)]\n",
    "data_id4 = clean_data[int(3*len(clean_data)/4):]\n",
    "\n",
    "# Change directory to the temp folder\n",
    "os.chdir(data_temp)  # make sure `data_temp` exists and is defined\n",
    "\n",
    "# Save each chunk with joblib\n",
    "joblib.dump(data_id1, 'clean_speeches_indexed1.pkl')\n",
    "joblib.dump(data_id2, 'clean_speeches_indexed2.pkl')\n",
    "joblib.dump(data_id3, 'clean_speeches_indexed3.pkl')\n",
    "joblib.dump(data_id4, 'clean_speeches_indexed4.pkl')\n",
    "\n",
    "# Store list of cleaned data chunk paths to feed into preprocessing function later\n",
    "clean_files = [\n",
    "    os.path.join(data_temp, 'clean_speeches_indexed1.pkl'),\n",
    "    os.path.join(data_temp, 'clean_speeches_indexed2.pkl'),\n",
    "    os.path.join(data_temp, 'clean_speeches_indexed3.pkl'),\n",
    "    os.path.join(data_temp, 'clean_speeches_indexed4.pkl')\n",
    "]\n",
    "\n",
    "print(f\"âœ… Saved clean speeches chunks in '{data_temp}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6e5109-8cf7-41e3-8fb9-8e7dde29b4e0",
   "metadata": {},
   "source": [
    "### Advanced Text Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfca8dd9-c6d7-48c7-96b0-f5a449814336",
   "metadata": {},
   "source": [
    "#### Extend stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "11d056b5-6729-45f8-a233-3e8e6de1f69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 326 stopwords to C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\stopwords.pkl\n"
     ]
    }
   ],
   "source": [
    "# Get SpaCy stopwords\n",
    "SPACY_STOPWORDS = list(nlp.Defaults.stop_words)\n",
    "\n",
    "# Path to save\n",
    "stopwords_path = os.path.join(data_c, \"stopwords.pkl\")\n",
    "\n",
    "# Save stopwords\n",
    "joblib.dump(SPACY_STOPWORDS, stopwords_path)\n",
    "\n",
    "print(f\"Saved {len(SPACY_STOPWORDS)} stopwords to {stopwords_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0aa75944-4b75-4aa4-9430-9aa6bd9ac934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Define function to Tokenize, eliminate digits, remove stopwords, lemmatize, POS-Tagging ==\n",
    "\n",
    "def pro1(lista):\n",
    "    # Remove punctuation\n",
    "    return [[row[0], row[1].translate(translator)] for row in lista]\n",
    "\n",
    "def pro2(lista):\n",
    "    # Tokenize and lowercase with gensim\n",
    "    return [[row[0], gensim.utils.simple_preprocess(row[1])] for row in lista]\n",
    "\n",
    "def pro3(lista):\n",
    "    # Remove tokens that are only digits\n",
    "        a = [[row[0], [w for w in row[1] if not w.isdigit()]] for row in lista]\n",
    "        return a\n",
    "    \n",
    "def pro4(lista):\n",
    "    # Drop short words\n",
    "    return [[row[0], [w for w in row[1] if len(w) > 2]] for row in lista]\n",
    "\n",
    "def tags_spacy(lista):\n",
    "    texts = [' '.join(row[1]) for row in lista]\n",
    "    docs = list(nlp.pipe(texts, batch_size=20, n_process=1))\n",
    "    result = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        filtered_tokens = [token.text for token in doc if token.tag_.startswith(('N', 'V', 'J'))]\n",
    "        result.append([lista[i][0], filtered_tokens])\n",
    "    return result\n",
    "\n",
    "\n",
    "def pro5(lista):\n",
    "    # Remove stopwords using SpaCy stopword list\n",
    "    return [[row[0], [w for w in row[1] if w not in SPACY_STOPWORDS]] for row in lista]\n",
    "\n",
    "def pro6(lista):\n",
    "      return [\n",
    "        [row[0], [stemmer.stem(token) for token in row[1]]]\n",
    "        for row in lista\n",
    "    ]\n",
    "########################## Question for Max: They removed procedural words in the paper\n",
    "\n",
    "def dropnull(lista):\n",
    "    # Drop empty speeches\n",
    "    return [row for row in lista if len(' '.join(row[1])) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "524a274a-004c-4d87-9bb3-5f789a42b90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl] Before tagging: 1.10s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl] After tagging: 31.77s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed1.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl] Done. Total time: 37.41s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl] Before tagging: 1.04s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl] After tagging: 30.06s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed2.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl] Done. Total time: 35.52s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl] Before tagging: 0.88s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl] After tagging: 30.68s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed3.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl] Done. Total time: 35.81s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl] Before tagging: 0.92s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl] After tagging: 27.80s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed4.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl] Done. Total time: 32.34s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# == Create full pre-processing function and call it\n",
    "\n",
    "def preprocessing(data_name):\n",
    "    t0 = time.time()\n",
    "    print(f\"Starting preprocessing for {data_name}...\")\n",
    "\n",
    "    data = joblib.load(data_name)\n",
    "    data = pro1(data)\n",
    "    data = pro2(data)\n",
    "    data = pro3(data)\n",
    "    data = pro4(data)\n",
    "\n",
    "    print(f\"[{data_name}] Before tagging: {time.time() - t0:.2f}s\")\n",
    "    data = tags_spacy(data)\n",
    "    print(f\"[{data_name}] After tagging: {time.time() - t0:.2f}s\")\n",
    "\n",
    "    data = pro5(data)\n",
    "   # data = pro6(data)\n",
    "    data = dropnull(data)\n",
    "\n",
    "    # out_name = data_name.replace('cleanspeeches_', 'preprocessed_speeches_').replace('.pkl', '_temp.pkl')\n",
    "\n",
    "    # Store preprocessed corupus (before stemming) for wordcloud\n",
    "    filename_wordcloud = data_name.replace('clean_speeches_', 'wordcloud_speeches_').replace('.pkl', '.pkl')\n",
    "    out_name_wordcloud = os.path.join(data_preprocessed, os.path.basename(filename_wordcloud))\n",
    "    joblib.dump(data, out_name_wordcloud)\n",
    "\n",
    "    # Apply stemming\n",
    "    data_stemmed = pro6(data)\n",
    "\n",
    "    filename_preprocessed = data_name.replace('clean_speeches_', 'preprocessed_speeches_').replace('.pkl', '.pkl')\n",
    "    out_preprocessed = os.path.join(data_preprocessed, os.path.basename(filename_preprocessed))\n",
    "    joblib.dump(data_stemmed, out_preprocessed\n",
    "               )\n",
    "    print(f\"[{data_name}] Saved stemmed version: {out_preprocessed}\")\n",
    "\n",
    "    print(f\"[{data_name}] Done. Total time: {time.time() - t0:.2f}s\\n\")\n",
    "\n",
    "def main():\n",
    "    for fname in clean_files:\n",
    "        preprocessing(fname)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d3409987-485c-47c9-9430-01fa2c0e650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the pre-processed data\n",
    "preprocessed_files = [\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed1.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed2.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed3.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed4.pkl')\n",
    "]\n",
    "\n",
    "wordcloud_files = [\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed1.pkl'),\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed2.pkl'),\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed3.pkl'),\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed4.pkl')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "134ddbf7-9b95-4697-9d9c-f9c55b52ec80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d006e-6a23-453b-9c91-0e60bd3f8301",
   "metadata": {},
   "source": [
    "## Word-Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf2443e-4c76-41b2-bc16-e72d91c318a8",
   "metadata": {},
   "source": [
    "### Count frequencies of all tokens and display the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "154d978d-cddd-431c-9d53-73c9faf8fdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed1.pkl: removed words with freq < 10\n",
      "Processed C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed2.pkl: removed words with freq < 10\n",
      "Processed C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed3.pkl: removed words with freq < 10\n",
      "Processed C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed4.pkl: removed words with freq < 10\n",
      "\n",
      "[Stemmed] Top 50 most common words:\n",
      "nation: 12923\n",
      "unit: 10584\n",
      "countri: 10339\n",
      "intern: 9114\n",
      "develop: 8329\n",
      "state: 7886\n",
      "world: 7620\n",
      "peac: 7445\n",
      "peopl: 7201\n",
      "secur: 4740\n",
      "general: 4582\n",
      "govern: 4426\n",
      "econom: 4345\n",
      "organ: 3835\n",
      "assembl: 3768\n",
      "right: 3725\n",
      "year: 3677\n",
      "problem: 3446\n",
      "new: 3404\n",
      "effort: 3276\n",
      "continu: 3183\n",
      "human: 3091\n",
      "support: 3077\n",
      "polit: 2861\n",
      "communiti: 2847\n",
      "time: 2685\n",
      "region: 2632\n",
      "member: 2513\n",
      "session: 2478\n",
      "africa: 2466\n",
      "import: 2327\n",
      "war: 2314\n",
      "need: 2273\n",
      "council: 2266\n",
      "achiev: 2251\n",
      "work: 2228\n",
      "conflict: 2174\n",
      "hope: 2152\n",
      "situat: 2130\n",
      "principl: 2124\n",
      "power: 2082\n",
      "relat: 2081\n",
      "forc: 2074\n",
      "south: 2035\n",
      "presid: 1994\n",
      "republ: 1981\n",
      "oper: 1962\n",
      "global: 1956\n",
      "concern: 1950\n",
      "order: 1924\n",
      "\n",
      "[Stemmed] Top 50 least common words:\n",
      "simul: 1\n",
      "impenit: 1\n",
      "cession: 1\n",
      "shill: 1\n",
      "compagni: 1\n",
      "triptych: 1\n",
      "banker: 1\n",
      "lÃ©opold: 1\n",
      "sÃ©dar: 1\n",
      "roubl: 1\n",
      "bagaza: 1\n",
      "signer: 1\n",
      "meaber: 1\n",
      "botha: 1\n",
      "placatori: 1\n",
      "ah: 1\n",
      "ifad: 1\n",
      "cuter: 1\n",
      "mankinj: 1\n",
      "hardwon: 1\n",
      "praia: 1\n",
      "dizzi: 1\n",
      "lang: 1\n",
      "belfast: 1\n",
      "prorogu: 1\n",
      "irishmen: 1\n",
      "cakobau: 1\n",
      "nought: 1\n",
      "copra: 1\n",
      "perfection: 1\n",
      "canberra: 1\n",
      "transient: 1\n",
      "depolar: 1\n",
      "subjectiv: 1\n",
      "endanger: 1\n",
      "dovetail: 1\n",
      "refashion: 1\n",
      "warp: 1\n",
      "recast: 1\n",
      "obloquy: 1\n",
      "plc: 1\n",
      "uncomplet: 1\n",
      "constanc: 1\n",
      "bandaranaik: 1\n",
      "guyer: 1\n",
      "endu: 1\n",
      "sinaimor: 1\n",
      "theafrican: 1\n",
      "arabeuropean: 1\n",
      "vij: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Wordcloud] Top 50 most common words:\n",
      "united: 10469\n",
      "nations: 10232\n",
      "international: 8746\n",
      "world: 7458\n",
      "countries: 6812\n",
      "peace: 6071\n",
      "states: 5709\n",
      "development: 4755\n",
      "people: 4585\n",
      "general: 4468\n",
      "security: 4398\n",
      "economic: 4318\n",
      "assembly: 3700\n",
      "new: 3404\n",
      "country: 3392\n",
      "government: 3355\n",
      "organization: 2874\n",
      "political: 2748\n",
      "community: 2664\n",
      "human: 2663\n",
      "efforts: 2649\n",
      "peoples: 2611\n",
      "africa: 2389\n",
      "rights: 2367\n",
      "session: 2363\n",
      "support: 2272\n",
      "council: 2234\n",
      "time: 2204\n",
      "war: 2041\n",
      "south: 2033\n",
      "problems: 1997\n",
      "state: 1952\n",
      "republic: 1914\n",
      "situation: 1887\n",
      "great: 1869\n",
      "national: 1864\n",
      "developing: 1858\n",
      "years: 1854\n",
      "nuclear: 1843\n",
      "order: 1839\n",
      "year: 1817\n",
      "conference: 1748\n",
      "global: 1744\n",
      "social: 1717\n",
      "work: 1675\n",
      "relations: 1637\n",
      "charter: 1569\n",
      "hope: 1560\n",
      "continue: 1548\n",
      "african: 1547\n",
      "\n",
      "[Wordcloud] Top 50 least common words:\n",
      "irishmen: 1\n",
      "absolutes: 1\n",
      "cakobau: 1\n",
      "wings: 1\n",
      "venturing: 1\n",
      "nought: 1\n",
      "copra: 1\n",
      "perfectionism: 1\n",
      "scribed: 1\n",
      "dissimilarities: 1\n",
      "islanders: 1\n",
      "canberra: 1\n",
      "expired: 1\n",
      "exhilarating: 1\n",
      "transient: 1\n",
      "oligarchical: 1\n",
      "depolarization: 1\n",
      "subjectivism: 1\n",
      "endangerment: 1\n",
      "harbingers: 1\n",
      "dovetailing: 1\n",
      "wrench: 1\n",
      "refashioning: 1\n",
      "warped: 1\n",
      "recast: 1\n",
      "pervert: 1\n",
      "tri: 1\n",
      "obloquy: 1\n",
      "obviousness: 1\n",
      "plc: 1\n",
      "pointlessness: 1\n",
      "stigmas: 1\n",
      "scrupulousness: 1\n",
      "uncompleted: 1\n",
      "constancy: 1\n",
      "bandaranaike: 1\n",
      "supersede: 1\n",
      "stipulating: 1\n",
      "touchstones: 1\n",
      "guyer: 1\n",
      "enduing: 1\n",
      "sinaimore: 1\n",
      "theafrican: 1\n",
      "procrastinating: 1\n",
      "arabism: 1\n",
      "arabeuropean: 1\n",
      "voyages: 1\n",
      "supplanting: 1\n",
      "vij: 1\n",
      "accedes: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit\\\\data\\\\freq\\\\word_counts_wordcloud.pkl']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#== Count token frequencies ==\n",
    "\n",
    "def count_frequencies(filenames):\n",
    "    total_freq = Counter()\n",
    "    for fname in tqdm(filenames):\n",
    "        data = joblib.load(fname)\n",
    "        tokens = chain.from_iterable(row[1] for row in data if isinstance(row[1], list))\n",
    "        total_freq.update(tokens)\n",
    "    return total_freq\n",
    "\n",
    "def remove_rare_words(filenames, freqs, min_count=10):\n",
    "    for fname in filenames:\n",
    "        data = joblib.load(fname)\n",
    "        filtered_data = []\n",
    "        for doc_id, tokens in data:\n",
    "            filtered_tokens = [w for w in tokens if freqs.get(w, 0) >= min_count]\n",
    "            filtered_data.append([doc_id, filtered_tokens])\n",
    "        joblib.dump(filtered_data, fname)  # overwrite or save as new file\n",
    "        print(f\"Processed {fname}: removed words with freq < {min_count}\")\n",
    "\n",
    "# === Count for preprocessed (stemmed) speeches ===\n",
    "word_counts_stemmed = count_frequencies(preprocessed_files)\n",
    "\n",
    "remove_rare_words(preprocessed_files, word_counts_stemmed, min_count=10)\n",
    "\n",
    "print(\"\\n[Stemmed] Top 50 most common words:\")\n",
    "for word, count in word_counts_stemmed.most_common(50):\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\n[Stemmed] Top 50 least common words:\")\n",
    "for word, count in word_counts_stemmed.most_common()[-50:]:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Save stemmed word counts\n",
    "save_path_stemmed = os.path.join(data_freq, 'word_counts_stemmed.pkl')\n",
    "joblib.dump(word_counts_stemmed, save_path_stemmed)\n",
    "\n",
    "# === Count for wordcloud (unstemmed) speeches ===\n",
    "word_counts_wordcloud = count_frequencies(wordcloud_files)\n",
    "\n",
    "print(\"\\n[Wordcloud] Top 50 most common words:\")\n",
    "for word, count in word_counts_wordcloud.most_common(50):\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\n[Wordcloud] Top 50 least common words:\")\n",
    "for word, count in word_counts_wordcloud.most_common()[-50:]:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Save unstemmed word counts\n",
    "save_path_wordcloud = os.path.join(data_freq, 'word_counts_wordcloud.pkl')\n",
    "joblib.dump(word_counts_wordcloud, save_path_wordcloud)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "932ce36d-d21c-4f1c-aff7-899c321dcfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_c)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39acefa1-c504-478e-98e5-9d465963dc82",
   "metadata": {},
   "source": [
    "### Count the frequency of the dictionary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9e28f481-7e20-486c-a6b3-8729d6aeaad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of affect dictionary:\n",
      "['forbid', 'unattract', 'cruelti', 'crappi', 'apathi', 'scari', 'unimpress', 'sin', 'dumbest', 'eas', 'agit', 'sob', 'shocker', 'tragedi', 'fabul', 'strongest', 'giver', 'sigh', 'aw', 'witch', 'hurtl', 'fucktard', 'cruel', 'glamor', 'funni', 'smarter', 'brillianc', 'irrate', 'alright', 'honest', 'profit', 'fearless', 'grievous', 'relax', 'isolationist', 'hah', 'shyness', 'poorest', 'cruelest', 'troublemak', 'disagre', 'agon', 'terror', 'fight', 'pleas', 'poor', 'crazi', 'hostil', 'stupid', 'damnat', 'vain', 'jade', 'heartless', 'nag', 'gloomi', 'damn', 'dishearten', 'pleaser', 'credit', 'warmth', 'greatest', 'whine', 'shame', 'angriest', 'envious', 'grin', 'blameless', 'sweeter', 'laidback', 'stupidest', 'unprotect', 'whiner', 'unlov', 'shake', 'boredom', 'fairer', 'weaker', 'wellb', 'bold', 'sucki', 'unsuccess', 'mourner', 'liken', 'defens', 'invigor', 'tedious', 'paranoid', 'cynic', 'dignifi', 'paranoia', 'sweetest', 'contented', 'humili', 'crush', 'terrif', 'alarm', 'asham', 'impati', 'fake', 'flirti', 'laziest', 'libertarian', 'empti', 'panicki', 'rape', 'contemptu', 'incent', 'reward', 'brutal', 'ignoramus', 'bereav', 'wimp', 'laugh', 'nicer', 'warm', 'resent', 'threat', 'hell', 'tortur', 'mad', 'worship', 'uncomfort', 'depress', 'unsaf', 'faultless', 'numb', 'humorless', 'murder', 'prettiest', 'jealous', 'tragic', 'discomfort', 'loveli', 'ungrat', 'divin', 'lamest', 'weaken', 'horribl', 'fuck', 'agoni', 'grievanc', 'ach', 'isolation', 'masoch', 'uglier', 'amaz', 'devilish', 'bitter', 'devil', 'smartest', 'glori', 'trite', 'vicious', 'unsavori', 'grossest', 'sinist', 'cheeri', 'dismay', 'easier', 'flatter', 'virtuous', 'egot', 'generos', 'cherish', 'heartwarm', 'advers', 'tough', 'adorn', 'overwhelm', 'devast', 'egotist', 'sociabl', 'lazier', 'affection', 'sarcasm', 'humor', 'funniest', 'startl', 'innoc', 'vomit', 'shyli', 'kiss', 'burdensom', 'melancholi', 'weakest', 'worthless', 'gentlest', 'proud', 'alarmist', 'pervert', 'demeanor', 'loneli', 'reassur', 'weird', 'passionless', 'loyalti', 'frustrat', 'weak', 'immor', 'hater', 'libertin', 'flirt', 'terribl', 'threaten', 'fatalist', 'melancholia', 'avers', 'prick', 'sucker', 'meanest', 'virtuos', 'benefici', 'neurot', 'disappoint', 'restless', 'romant', 'bastard', 'angri', 'flatteri', 'stun', 'bolder', 'amor', 'forbidden', 'hug', 'demean', 'disgust', 'truer', 'benefic', 'teas', 'stinki', 'condemnatori', 'turmoil', 'poison', 'cunt', 'gorgeous', 'harm', 'lousi', 'glorifi', 'piti', 'wimpish', 'impression', 'freakish', 'happiest', 'apathet', 'pitiabl', 'unpleas', 'suffer', 'miseri', 'toler', 'mess', 'casual', 'anxieti', 'stale', 'fieri', 'grudg', 'arrog', 'outrag', 'thrill', 'faithless', 'popular', 'vaniti', 'palat', 'fantasi', 'decay', 'domin', 'mood', 'proudest', 'scream', 'stress', 'calm', 'fatigu', 'dumber', 'benign', 'rude', 'steal', 'tension', 'enthusiast', 'intimid', 'sweetheart', 'prettier', 'perv', 'pathet', 'flawless', 'contradictori', 'distress', 'moron', 'gratifi', 'frighten', 'anguish', 'snob', 'defenseless', 'fantast', 'sad', 'thief', 'worsen', 'tear', 'handsom', 'attract', 'messi', 'smother', 'scarier', 'afraid', 'insult', 'disreput', 'wept', 'victim', 'traumat', 'prejudic', 'amus', 'decent', 'joyous', 'masochist', 'abandon', 'foe', 'pleasantri', 'shitless', 'strengthen', 'antagonist', 'liar', 'romantic', 'violenc', 'sentimentalist', 'gratif', 'decept', 'witchcraft', 'liveli', 'impressionist', 'guilt', 'griev', 'rotten', 'disgrac', 'insincer', 'pushi', 'enjoy', 'freaki', 'ador', 'grate', 'homesick', 'braver', 'battlefield', 'niceti', 'hooray', 'incompet', 'thiev', 'timid', 'adventur', 'needi', 'joyless', 'grimac', 'gloom', 'dump', 'horrid', 'magnific', 'unlucki', 'killer', 'uncontrol', 'funnier', 'ruin', 'weepi', 'play', 'encourag', 'fantas', 'irrit', 'sincer', 'rage', 'despis', 'unhappi', 'hate', 'bravest', 'harmless', 'hero', 'vigor', 'pervers', 'dumb', 'degrad', 'shameless', 'villaini', 'envi', 'aggress', 'fucker', 'courag', 'craziest', 'delici', 'scare', 'finer', 'bittersweet', 'trivial', 'wealthi', 'heroic', 'ecstat', 'irrat', 'prejudici', 'smug', 'stammer', 'feroci', 'phobic', 'villain', 'anger', 'fond', 'obsession', 'awesom', 'wors', 'compassion', 'weakl', 'calmer', 'inhibit', 'bother', 'rancid', 'hatr', 'beaten', 'opportunist', 'offens', 'haunt', 'appal', 'painless', 'pessimist', 'molest', 'lol', 'wrongdo', 'frantic', 'heroin', 'rejoic', 'woe', 'miser', 'meaner', 'saddest', 'mooch', 'attack', 'enthusiasm', 'joke', 'meritocraci', 'angrier', 'emot', 'weirdest', 'silliest', 'kisser', 'hurrah', 'crueler', 'nervous', 'dishonor', 'gracious', 'sicker', 'hopeless', 'delect', 'compliment', 'petrifi', 'cuter', 'reek', 'loveless', 'daze', 'weirder', 'triumphant', 'impolit', 'offend', 'richer', 'violent', 'crazier', 'comedown', 'heroism', 'stubborn', 'maniac', 'strain', 'broke', 'jerk', 'unkind', 'asshol', 'reveng', 'jealousi', 'damnabl', 'unimport', 'careless', 'gentler', 'bitch', 'shi', 'terrorist', 'rigid', 'harsh', 'passion', 'annoy', 'compass', 'geek', 'weirdo', 'agre', 'whore', 'splendor', 'suck', 'fun', 'chuckl', 'snobberi', 'mock', 'doom', 'reluct', 'mourn', 'boldest', 'helpless', 'trembl', 'pestil', 'curs', 'desper', 'harass', 'complain', 'joker', 'obsess', 'uneasi', 'nasti', 'greedi', 'belov', 'grim', 'upset', 'greed', 'honesti', 'phoni', 'feudal', 'loveliest', 'trauma', 'dwell', 'ecstasi', 'blame', 'legit', 'calmest', 'sickest', 'scariest', 'sarcast', 'cheat', 'fatal', 'stronger', 'energet', 'feroc', 'healthi', 'goddam', 'disast', 'flirtati', 'defeatist', 'glorious', 'terrifi', 'gossip', 'forbad', 'sicken', 'poorer', 'strangest', 'cute', 'struggl', 'woebegon', 'worri', 'gratitud', 'cheer', 'distraught', 'ok', 'vulner', 'blessed', 'fairest', 'lame', 'optim', 'grouch', 'fright', 'pessim', 'sadder', 'worst', 'despair', 'bash', 'intellectu', 'promis', 'prais', 'heartfelt', 'horror', 'crap', 'repress', 'prouder', 'fought', 'discourag', 'heal', 'lazi', 'precious', 'melanchol', 'isol', 'aggressor', 'giggl', 'idiot', 'twitchi', 'glorif', 'kid', 'selfish', 'smile', 'grief', 'hellish', 'shit', 'fume', 'bitchi', 'fool', 'trustworthi', 'optimist', 'dumpi', 'unwelcom', 'neat', 'nicest', 'furi', 'unfriend', 'ridicul', 'arguabl', 'shamefac', 'pervi', 'upbeat', 'wow', 'easygo', 'blameworthi', 'charmer', 'stink', 'phobia', 'adversari', 'sillier', 'distrust', 'dread', 'happier', 'vital', 'dissatisfact', 'insecur', 'assault', 'ineffect', 'panic', 'powerless', 'nightmar', 'wealth', 'enrag', 'splendid', 'foolish', 'ugliest', 'contradict', 'worrier', 'freak']\n",
      "Number of words in affect dictionary: 629\n",
      "Contents of cognition dictionary:\n",
      "['analyt', 'guess', 'suspect', 'coz', 'mysteri', 'unawar', 'proverbi', 'dissimilar', 'consequenti', 'stimul', 'occasion', 'rethink', 'purest', 'entireti', 'abnorm', 'referenti', 'differenti', 'diagnos', 'ambigu', 'concentr', 'undon', 'undecid', 'disclosur', 'fundamentalist', 'blur', 'reaction', 'variabl', 'recogniz', 'meant', 'reconstruct', 'supposit', 'acknowledg', 'choosi', 'accuraci', 'mislead', 'perceiv', 'hypothesi', 'unknow', 'cuz', 'reveal', 'potenti', 'puriti', 'persuad', 'indirect', 'generat', 'presumpt', 'clue', 'justif', 'evidenti', 'realize', 'deduct', 'causal', 'inevit', 'thinker', 'evidentiari', 'factor', 'undeni', 'unclear', 'pick', 'induc', 'accur', 'absolut', 'everytim', 'reconsider', 'diagnost', 'solut', 'diagnosi', 'discoveri', 'ration', 'unambigu', 'explicit', 'decis', 'insight', 'confess', 'fuzzi', 'led', 'lesson', 'theoriz', 'analyz', 'learner', 'split', 'enlighten', 'cos', 'respons', 'dubious', 'react', 'guarante', 'someday', 'blatant', 'undo', 'perceptu', 'infal', 'afterthought', 'inequ', 'feasibl', 'knowabl', 'approxim', 'logic', 'meaning', 'unknown', 'categor', 'trigger', 'incomplet', 'curious', 'attribut', 'pretend', 'reactionari', 'prove', 'meaningless', 'reactiv', 'proof', 'bet', 'purpos', 'suppos', 'discern', 'imagin', 'kinda', 'comprehens', 'sens', 'percept', 'everyday', 'rational', 'unquestion', 'random', 'implicit', 'clarif', 'theori', 'dunno', 'analysi', 'obedi', 'memor', 'genuin', 'evalu', 'explor', 'linkag', 'grasp', 'curios', 'methink', 'somewher', 'deduc', 'unwant', 'induct', 'unrel', 'outstand', 'discov', 'unresolv', 'exploratori', 'choos', 'perspect', 'inferenti', 'disclos', 'factual', 'arbitrari', 'conting', 'vagu', 'persuas', 'manipul', 'impli', 'somehow', 'sorta', 'borderlin', 'reconcil', 'anyhow', 'conscious', 'probabilist', 'precis', 'rearrang', 'referenc', 'obscur']\n",
      "Number of words in cognition dictionary: 169\n"
     ]
    }
   ],
   "source": [
    "# == Count dictionary words\n",
    "\n",
    "# Load dictionaries            ##### How did they come up with this dictionary? Why did they exclude words?\n",
    "\n",
    "affect_path = os.path.join(data_dict, 'dictionary_affect.pkl')\n",
    "cognition_path = os.path.join(data_dict, 'dictionary_cognition.pkl')\n",
    "\n",
    "with open(affect_path, 'rb') as f:\n",
    "    affect_dict = pickle.load(f)\n",
    "print(\"Contents of affect dictionary:\")\n",
    "print(affect_dict)\n",
    "print(\"Number of words in affect dictionary:\", len(affect_dict))\n",
    "\n",
    "with open(cognition_path, 'rb') as f:\n",
    "    cognition_dict = pickle.load(f)\n",
    "print(\"Contents of cognition dictionary:\")\n",
    "print(cognition_dict)\n",
    "print(\"Number of words in cognition dictionary:\", len(cognition_dict))\n",
    "\n",
    "affect = joblib.load(affect_path)\n",
    "cognition = joblib.load(cognition_path)\n",
    "\n",
    "a = [[i, word_counts_stemmed[i]] for i in affect if i in word_counts_stemmed]\n",
    "c = [[i, word_counts_stemmed[i]] for i in cognition if i in word_counts_stemmed]\n",
    "\n",
    "a = sorted(a, key=lambda x: x[1], reverse=True)\n",
    "c = sorted(c, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "a = [[i[0], f\"({i[1]}),\"] for i in a]\n",
    "c = [[i[0], f\"({i[1]}),\"] for i in c]\n",
    "\n",
    "a1 = ' '.join(str(r) for v in a for r in v)\n",
    "c1 = ' '.join(str(r) for v in c for r in v)\n",
    "\n",
    "affect_out_path = os.path.join(data_freq, \"affect_words.txt\")\n",
    "cog_out_path = os.path.join(data_freq, \"cog_words.txt\")\n",
    "\n",
    "os.makedirs(data_freq, exist_ok=True)  # ensure directory exists\n",
    "\n",
    "with open(affect_out_path, \"w\") as output:\n",
    "    output.write(a1)\n",
    "\n",
    "with open(cog_out_path, \"w\") as output:\n",
    "    output.write(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0dffe87f-d712-4d71-9305-fb288cf06402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 words by weighted frequency:\n",
      "francophon: 0.9987871304841413\n",
      "kidal: 0.9987871304841413\n",
      "macki: 0.9987871304841413\n",
      "complexion: 0.9987871304841413\n",
      "everwiden: 0.9987871304841413\n",
      "pointer: 0.9987871304841413\n",
      "anteced: 0.9987871304841413\n",
      "buoyanc: 0.9987871304841413\n",
      "unachiev: 0.9987871304841413\n",
      "highwat: 0.9987871304841413\n",
      "wiriyamu: 0.9987871304841413\n",
      "chawola: 0.9987871304841413\n",
      "enthron: 0.9987871304841413\n",
      "allembrac: 0.9987871304841413\n",
      "ymca: 0.9987871304841413\n",
      "footbal: 0.9987871304841413\n",
      "schoolboy: 0.9987871304841413\n",
      "newcastl: 0.9987871304841413\n",
      "knipe: 0.9987871304841413\n",
      "porter: 0.9987871304841413\n",
      "phoney: 0.9987871304841413\n",
      "minc: 0.9987871304841413\n",
      "bloodier: 0.9987871304841413\n",
      "quit: 0.9987871304841413\n",
      "talleyrand: 0.9987871304841413\n",
      "metaphys: 0.9987871304841413\n",
      "blameworthi: 0.9987871304841413\n",
      "infract: 0.9987871304841413\n",
      "reprob: 0.9987871304841413\n",
      "thiam: 0.9987871304841413\n",
      "organizet: 0.9987871304841413\n",
      "savior: 0.9987871304841413\n",
      "seaman: 0.9987871304841413\n",
      "impressionthat: 0.9987871304841413\n",
      "toolkit: 0.9987871304841413\n",
      "evergreat: 0.9987871304841413\n",
      "diabet: 0.9987871304841413\n",
      "pueril: 0.9987871304841413\n",
      "untruth: 0.9987871304841413\n",
      "aquina: 0.9987871304841413\n",
      "skirmish: 0.9987871304841413\n",
      "interlud: 0.9987871304841413\n",
      "xii: 0.9987871304841413\n",
      "somat: 0.9987871304841413\n",
      "rapacki: 0.9987871304841413\n",
      "inferno: 0.9987871304841413\n",
      "dah: 0.9987871304841413\n",
      "longoverdu: 0.9987871304841413\n",
      "icj: 0.9987871304841413\n",
      "shab: 0.9987871304841413\n",
      "slipperi: 0.9987871304841413\n",
      "riidig: 0.9987871304841413\n",
      "habomai: 0.9987871304841413\n",
      "shikotan: 0.9987871304841413\n",
      "kunashiri: 0.9987871304841413\n",
      "etorofu: 0.9987871304841413\n",
      "kuril: 0.9987871304841413\n",
      "straiten: 0.9987871304841413\n",
      "amadou: 0.9987871304841413\n",
      "toumani: 0.9987871304841413\n",
      "almati: 0.9987871304841413\n",
      "fieldin: 0.9987871304841413\n",
      "axiomat: 0.9987871304841413\n",
      "aryamehr: 0.9987871304841413\n",
      "harvard: 0.9987871304841413\n",
      "shelterless: 0.9987871304841413\n",
      "siberia: 0.9987871304841413\n",
      "latinamerican: 0.9987871304841413\n",
      "martinez: 0.9987871304841413\n",
      "saenz: 0.9987871304841413\n",
      "hipoteca: 0.9987871304841413\n",
      "asegurada: 0.9987871304841413\n",
      "instituto: 0.9987871304841413\n",
      "investigacion: 0.9987871304841413\n",
      "tecnologica: 0.9987871304841413\n",
      "electricidad: 0.9987871304841413\n",
      "comercio: 0.9987871304841413\n",
      "desarrollo: 0.9987871304841413\n",
      "economico: 0.9987871304841413\n",
      "fondo: 0.9987871304841413\n",
      "seguro: 0.9987871304841413\n",
      "deposito: 0.9987871304841413\n",
      "comis: 0.9987871304841413\n",
      "ejecutiva: 0.9987871304841413\n",
      "cooperacion: 0.9987871304841413\n",
      "agricola: 0.9987871304841413\n",
      "minera: 0.9987871304841413\n",
      "aviacion: 0.9987871304841413\n",
      "viscount: 0.9987871304841413\n",
      "miami: 0.9987871304841413\n",
      "ferrocarril: 0.9987871304841413\n",
      "habana: 0.9987871304841413\n",
      "britishown: 0.9987871304841413\n",
      "rca: 0.9987871304841413\n",
      "newsprint: 0.9987871304841413\n",
      "bagass: 0.9987871304841413\n",
      "hanabanilla: 0.9987871304841413\n",
      "toa: 0.9987871304841413\n",
      "gallon: 0.9987871304841413\n",
      "esso: 0.9987871304841413\n"
     ]
    }
   ],
   "source": [
    "# == Calculate weighted frequencies for all words\n",
    "\n",
    "# - downweights very common words by giving more importance to rare ones\n",
    "word_counts_stemmed = joblib.load(os.path.join(data_freq, 'word_counts_stemmed.pkl'))\n",
    "\n",
    "l = sum(word_counts_stemmed.values())\n",
    "\n",
    "a = 0.001\n",
    "word_counts_weighted = {k: a / (a + (v / l)) for k, v in word_counts_stemmed.items()}\n",
    "#for key in word_counts.keys():\n",
    " #   word_counts[key] = a / (a + (word_counts[key] / l))\n",
    "\n",
    "joblib.dump(word_counts_weighted, os.path.join(data_freq, 'word_counts_weighted.pkl'))\n",
    "\n",
    "################################################################################ ISSUE##################\n",
    "# To print top 100 by weighted values, sort the dictionary by value descending:\n",
    "top_100_weighted = sorted(word_counts_weighted.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "\n",
    "print(\"Top 100 words by weighted frequency:\")\n",
    "for word, weight in top_100_weighted:\n",
    "    print(f\"{word}: {weight}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df626ded-5e49-4a34-a3d1-315ba370226f",
   "metadata": {},
   "source": [
    "## Final Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0d0856a2-44f1-4304-b221-09a9ab72ad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_c)\n",
    "os.chdir(data_freq)\n",
    "\n",
    "count = joblib.load('word_counts_stemmed.pkl')  # load stemmed counts\n",
    "\n",
    "# For each speech only keep tokens that appear at least 10x\n",
    "\n",
    "def select(lista):\n",
    "    for i in range(len(lista)):\n",
    "        x = lista[i][0]\n",
    "        y = lista[i][1]\n",
    "        y = [w for w in y if count.get(w, 0) >= 10]\n",
    "        lista[i] = [x, y]\n",
    "    return lista\n",
    "\n",
    "for data_path in preprocessed_files:\n",
    "    data = joblib.load(data_path)\n",
    "    data = select(data)\n",
    "    cleaned_path = data_path.replace('.pkl', '_final.pkl')\n",
    "    joblib.dump(data, cleaned_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5ba7c0-33c7-4420-9cb0-fe54a94ee649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf1db25-1369-45bc-b6c4-ddddb77a3c52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
