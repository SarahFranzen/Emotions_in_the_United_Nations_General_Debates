{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b41cb22-e190-48d5-9926-7ac6fac8310d",
   "metadata": {},
   "source": [
    "# Emotion and Reason in Political Language: Examining the UN General speeches\n",
    "## Script 1: Preprocessing & Token Frequencies\n",
    "## Author: Sarah Franzen\n",
    "\n",
    "### Instructions BEFORE running this script:\n",
    "- Ensure all required packages are installed. If not, set `InstallPackages = TRUE` (see code cells below).  \n",
    "- Set your working directory appropriately.  \n",
    "- The script will automatically create the required folder structure.  \n",
    "- Later, you will be asked to download the data from:  \n",
    "  https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/0TJX8Y  \n",
    "  and store it **unzipped** inside the created folder *data_original*\n",
    "\n",
    "\n",
    "### Description: \n",
    "- Extract documents from their original txt documents and store them as one csv\n",
    "- Data Cleaning and Pre-Processing\n",
    "- Count word frequencies and weight themh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930ebd56-def9-4a16-91eb-78dbb7d037ae",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5588afc-105a-426e-9348-742a830a2732",
   "metadata": {},
   "source": [
    "## Setup, Installation and Verification of required Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ebb8987a-a0da-453e-97e2-aafa209d0e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "InstallPackages = False # Set this to True to install the following packages \n",
    "\n",
    "if InstallPackages:\n",
    "    import sys\n",
    "\n",
    "    packages = [\n",
    "        \"pandas\",\n",
    "        \"nltk\",\n",
    "        \"spacy\",\n",
    "        \"numpy\",\n",
    "        \"gensim\",\n",
    "        \"pycountry\",\n",
    "        \"wordcloud\",\n",
    "        \"matplotlib\",\n",
    "        \"tqdm\"\n",
    "    ]\n",
    "\n",
    "    for package in packages:\n",
    "        !{sys.executable} -m pip install {package}\n",
    "\n",
    "\n",
    "DownloadAdditions = False # Set this to True to download these additional resources\n",
    "if DownloadAdditions:\n",
    "    nltk.download(\"punkt\")\n",
    "    nltk.download(\"averaged_perceptron_tagger\")\n",
    "    spacy.cli.download('en_core_web_lg')         # Download spaCy English model (large)\n",
    "\n",
    "#########################\n",
    "# Check if all packages are included\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "32d7d499-0563-46e0-9730-914c9f106d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Import standard and third-party libraries for data processing, NLP, and visualization ==\n",
    "\n",
    "import gensim\n",
    "import joblib\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import pycountry\n",
    "import random\n",
    "import re\n",
    "import spacy\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from multiprocessing import Pool, freeze_support\n",
    "from nltk.stem import SnowballStemmer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOPWORDS\n",
    "from string import punctuation\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# === Initialize NLP Tools ===\n",
    "\n",
    "translator = str.maketrans('', '', punctuation)\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4d495156-bbb2-403c-86f2-83b47d8cb03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Folder structure created:\n",
      "- C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\n",
      "  - data_original\n",
      "  - dictionaries\n",
      "  - freq\n",
      "  - preprocessed\n",
      "  - temp\n",
      "  - tokenized\n"
     ]
    }
   ],
   "source": [
    "# === Set Working Directory and create folder structure ===\n",
    "\n",
    "# Prompt user to enter working directory path\n",
    "#wd = input(\"Please enter your working directory path (e.g., C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit): \").strip()\n",
    "\n",
    "# Change to the entered working directory\n",
    "#try:\n",
    "   # os.chdir(wd)\n",
    "    #print(f\"Working directory set to: {os.getcwd()}\")\n",
    "#except FileNotFoundError:\n",
    "   # print(\"ERROR: The directory you entered does not exist. Please restart and enter a valid path.\")\n",
    "    #exit(1)\n",
    "\n",
    "# Set your working directory (adjust this as needed)\n",
    "wd = r\"C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\"\n",
    "os.chdir(wd)\n",
    "\n",
    "# Set base path to current working directory\n",
    "base_path = Path.cwd()\n",
    "data_path = base_path / \"data\"\n",
    "\n",
    "# List of subfolders to create inside 'data'\n",
    "subfolders = [\"data_original\", \"dictionaries\", \"freq\", \"preprocessed\", \"temp\", \"tokenized\"]\n",
    "\n",
    "# Create 'data' folder if it doesn't exist\n",
    "data_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Create subfolders\n",
    "for name in subfolders:\n",
    "    (data_path / name).mkdir(exist_ok=True)\n",
    "\n",
    "print(\"\\nFolder structure created:\")\n",
    "print(f\"- {data_path}\")\n",
    "for name in subfolders:\n",
    "    print(f\"  - {name}\")\n",
    "\n",
    "# Prompt user to place raw data files\n",
    "#print(f\"\\nPlease place your raw data files (unzipped) into the folder:\\n  {data_path / 'data_original'}\")\n",
    "#input(\"Press Enter after you have placed the files to continue...\")\n",
    "\n",
    "#print(\"Continuing with the script...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "56fa9a21-650f-4d8e-8a21-3d9629655c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define Folder Paths ===\n",
    "\n",
    "# If an error occurs, make sure that you actually have these folders in your working directory\n",
    "data_c = os.path.join(wd, 'data')\n",
    "data_temp = os.path.join(data_c, 'temp')\n",
    "data_freq = os.path.join(data_c, 'freq')\n",
    "data_dict = os.path.join(data_c, 'dictionaries')\n",
    "data_preprocessed = os.path.join(data_c, 'preprocessed')\n",
    "data_tokenized = os.path.join(data_c, 'tokenized')\n",
    "fig = os.path.join(wd, 'fig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "296fc7d6-a2ac-406e-a322-6c9f2702ee1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\n"
     ]
    }
   ],
   "source": [
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bd410e-bc90-4e0b-aa08-a9bff13fab47",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff34b6-cbee-4f8c-8076-5f385f596b58",
   "metadata": {},
   "source": [
    "## Load and Prepare Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18067b9-6337-4fb3-a570-603083a7deae",
   "metadata": {},
   "source": [
    "### This chunk can be skipped at the moment\n",
    "### Think of proper header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "93d247a0-5ff2-4062-add9-f3267ae1fe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total speeches found: 10761\n",
      "\n",
      " Saved raw data with 10760 speeches to '.\\data\\un_corpus_raw.csv'\n"
     ]
    }
   ],
   "source": [
    "# == Load Sample from UN General Debate Corpus ==\n",
    "\n",
    "# Set Folder path containing the original TXT files    \n",
    "base_folder = r\".\\data\\data_original\\UN General Debate Corpus\\UNGDC_1946-2023\\TXT\"\n",
    "\n",
    "# Collect txt-files\n",
    "all_txt_files = []\n",
    "for root, dirs, files in os.walk(base_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.txt') and not file.startswith('._'):\n",
    "            all_txt_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Total speeches found: {len(all_txt_files)}\")\n",
    "\n",
    "# Randomly pick 800 files from the full collection   ###################################################### REMOVE AT LATER POINT\n",
    "sampled_files = random.sample(all_txt_files,10761)\n",
    "\n",
    "# Read the selected files into a list\n",
    "raw_data = []\n",
    "for filepath in sampled_files:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        raw_data.append({'filename': os.path.basename(filepath), 'speech': content})\n",
    "\n",
    "df_raw = pd.DataFrame(raw_data)\n",
    "\n",
    "# == Store as csv and pkl ==\n",
    "\n",
    "df_raw = df_raw[df_raw['filename'] != '.DS_Store-to-UTF-8.txt'].copy()\n",
    "\n",
    "raw_pickle_path = r\".\\data\\un_corpus_raw.pkl\"\n",
    "df_raw.to_pickle(raw_pickle_path)\n",
    "\n",
    "raw_output_path = r\".\\data\\un_corpus_raw.csv\"\n",
    "df_raw.to_csv(raw_output_path, index=False, sep=';', encoding='utf-8')\n",
    "\n",
    "print(f\"\\n Saved raw data with {len(df_raw)} speeches to '{raw_output_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6405ff20-92fa-4c97-becb-ac5fff70f84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SEN_63_2008.txt</td>\n",
       "      <td>Mr. Miguel \\nd’Escoto, in electing you to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KWT_73_2018.txt</td>\n",
       "      <td>At the outset, it is my pleasure, on behalf of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFG_53_1998.txt</td>\n",
       "      <td>The urgency of the\\nalarming situation in Afgh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ARG_12_1957.txt</td>\n",
       "      <td>First of all, I should like to join my congrat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SYR_23_1968.txt</td>\n",
       "      <td>1. When I, as the then Chairman of the Asian G...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech\n",
       "0  SEN_63_2008.txt  Mr. Miguel \\nd’Escoto, in electing you to the ...\n",
       "1  KWT_73_2018.txt  At the outset, it is my pleasure, on behalf of...\n",
       "2  AFG_53_1998.txt  The urgency of the\\nalarming situation in Afgh...\n",
       "3  ARG_12_1957.txt  First of all, I should like to join my congrat...\n",
       "4  SYR_23_1968.txt  1. When I, as the then Chairman of the Asian G..."
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# == Load data & drop empty speeches ==\n",
    "\n",
    "df_raw = pd.read_pickle(r\".\\data\\un_corpus_raw.pkl\")\n",
    "\n",
    "# Drop empty speeches\n",
    "df_raw['speech'] = df_raw['speech'].astype(str)\n",
    "df_raw = df_raw[df_raw['speech'].str.strip() != ''].copy()\n",
    "\n",
    "df_raw.head()         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7fb6b142-5356-4173-9aba-b34d08400bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [filename, speech]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "dupe_labels = df_raw[df_raw['filename'].duplicated(keep=False)]\n",
    "print(dupe_labels[['filename', 'speech']].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7980e945-9d56-4df3-99e5-acf491617568",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7d3162-ebbd-449b-a2be-1148fceda725",
   "metadata": {},
   "source": [
    "## Create new variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25710e79-5068-4d39-8023-f4f97513f165",
   "metadata": {},
   "source": [
    "#### New Variables: Year, Country Code and Country Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "162da07c-1112-43e8-a434-2bef0ed32de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min year: 1946\n",
      "Max year: 2023\n",
      "Missing codes: []\n"
     ]
    }
   ],
   "source": [
    "# == Create variable: country code & year\n",
    "\n",
    "# Create contry_code and year variable\n",
    "df_raw['country_code'] = df_raw['filename'].str.extract(r'^([A-Z]{2,3})')\n",
    "df_raw['year'] = df_raw['filename'].str.extract(r'_(\\d{4})\\.txt$').astype(int)\n",
    "\n",
    "print(\"Min year:\", df_raw['year'].min())\n",
    "print(\"Max year:\", df_raw['year'].max())\n",
    "# Speeches range from 1946 to 2023\n",
    "\n",
    "# == Create variable: country_name by matching ISO country code \n",
    "code_to_name = {country.alpha_3: country.name for country in pycountry.countries}\n",
    "\n",
    "# Add custom short names and legacy codes\n",
    "custom_names = {\n",
    "    \"BOL\": \"Bolivia\",\n",
    "    \"COD\": \"Democratic Republic of Congo\",\n",
    "    \"IRN\": \"Iran\",\n",
    "    \"LAO\": \"Laos\",\n",
    "    \"MDA\": \"Moldova\",\n",
    "    \"PRK\": \"North Korea\",\n",
    "    \"PSE\": \"Palestine\",\n",
    "    \"RUS\": \"Russia\",\n",
    "    \"SYR\": \"Syria\",\n",
    "    \"TZA\": \"Tanzania\",\n",
    "    \"VAT\": \"Vatican City State\",\n",
    "    \"VEN\": \"Venezuela\",\n",
    "    \"VNM\": \"Vietnam\",\n",
    "    \"YMD\": \"South Yemen\",\n",
    "    \"YUG\": \"Yugoslavia\",\n",
    "    \"DDR\": \"East Germany\",\n",
    "    \"EU\": \"European Union\",\n",
    "    \"CSK\": \"Czechoslovakia\",\n",
    "    \"FSM\": \"Micronesia\",\n",
    "    \"KOR\": \"South Korea\"\n",
    "}\n",
    "\n",
    "code_to_name.update(custom_names)\n",
    "df_raw['country_name'] = df_raw['country_code'].map(code_to_name)\n",
    "\n",
    "# Check missing mappings\n",
    "missing = df_raw.loc[df_raw['country_name'].isna(), 'country_code'].unique()\n",
    "print(\"Missing codes:\", missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "383d2904-f015-4c9d-9aeb-51a94f03f950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    country_code                      country_name\n",
      "0            AFG                       Afghanistan\n",
      "1            AGO                            Angola\n",
      "2            ALB                           Albania\n",
      "3            AND                           Andorra\n",
      "4            ARE              United Arab Emirates\n",
      "5            ARG                         Argentina\n",
      "6            ARM                           Armenia\n",
      "7            ATG               Antigua and Barbuda\n",
      "8            AUS                         Australia\n",
      "9            AUT                           Austria\n",
      "10           AZE                        Azerbaijan\n",
      "11           BDI                           Burundi\n",
      "12           BEL                           Belgium\n",
      "13           BEN                             Benin\n",
      "14           BFA                      Burkina Faso\n",
      "15           BGD                        Bangladesh\n",
      "16           BGR                          Bulgaria\n",
      "17           BHR                           Bahrain\n",
      "18           BHS                           Bahamas\n",
      "19           BIH            Bosnia and Herzegovina\n",
      "20           BLR                           Belarus\n",
      "21           BLZ                            Belize\n",
      "22           BOL                           Bolivia\n",
      "23           BRA                            Brazil\n",
      "24           BRB                          Barbados\n",
      "25           BRN                 Brunei Darussalam\n",
      "26           BTN                            Bhutan\n",
      "27           BWA                          Botswana\n",
      "28           CAF          Central African Republic\n",
      "29           CAN                            Canada\n",
      "30           CHE                       Switzerland\n",
      "31           CHL                             Chile\n",
      "32           CHN                             China\n",
      "33           CIV                     Côte d'Ivoire\n",
      "34           CMR                          Cameroon\n",
      "35           COD      Democratic Republic of Congo\n",
      "36           COG                             Congo\n",
      "37           COL                          Colombia\n",
      "38           COM                           Comoros\n",
      "39           CPV                        Cabo Verde\n",
      "40           CRI                        Costa Rica\n",
      "41           CSK                    Czechoslovakia\n",
      "42           CUB                              Cuba\n",
      "43           CYP                            Cyprus\n",
      "44           CZE                           Czechia\n",
      "45           DDR                      East Germany\n",
      "46           DEU                           Germany\n",
      "47           DJI                          Djibouti\n",
      "48           DMA                          Dominica\n",
      "49           DNK                           Denmark\n",
      "50           DOM                Dominican Republic\n",
      "51           DZA                           Algeria\n",
      "52           ECU                           Ecuador\n",
      "53           EGY                             Egypt\n",
      "54           ERI                           Eritrea\n",
      "55           ESP                             Spain\n",
      "56           EST                           Estonia\n",
      "57           ETH                          Ethiopia\n",
      "58            EU                    European Union\n",
      "59           FIN                           Finland\n",
      "60           FJI                              Fiji\n",
      "61           FRA                            France\n",
      "62           FSM                        Micronesia\n",
      "63           GAB                             Gabon\n",
      "64           GBR                    United Kingdom\n",
      "65           GEO                           Georgia\n",
      "66           GHA                             Ghana\n",
      "67           GIN                            Guinea\n",
      "68           GMB                            Gambia\n",
      "69           GNB                     Guinea-Bissau\n",
      "70           GNQ                 Equatorial Guinea\n",
      "71           GRC                            Greece\n",
      "72           GRD                           Grenada\n",
      "73           GTM                         Guatemala\n",
      "74           GUY                            Guyana\n",
      "75           HND                          Honduras\n",
      "76           HRV                           Croatia\n",
      "77           HTI                             Haiti\n",
      "78           HUN                           Hungary\n",
      "79           IDN                         Indonesia\n",
      "80           IND                             India\n",
      "81           IRL                           Ireland\n",
      "82           IRN                              Iran\n",
      "83           IRQ                              Iraq\n",
      "84           ISL                           Iceland\n",
      "85           ISR                            Israel\n",
      "86           ITA                             Italy\n",
      "87           JAM                           Jamaica\n",
      "88           JOR                            Jordan\n",
      "89           JPN                             Japan\n",
      "90           KAZ                        Kazakhstan\n",
      "91           KEN                             Kenya\n",
      "92           KGZ                        Kyrgyzstan\n",
      "93           KHM                          Cambodia\n",
      "94           KIR                          Kiribati\n",
      "95           KNA             Saint Kitts and Nevis\n",
      "96           KOR                       South Korea\n",
      "97           KWT                            Kuwait\n",
      "98           LAO                              Laos\n",
      "99           LBN                           Lebanon\n",
      "100          LBR                           Liberia\n",
      "101          LBY                             Libya\n",
      "102          LCA                       Saint Lucia\n",
      "103          LIE                     Liechtenstein\n",
      "104          LKA                         Sri Lanka\n",
      "105          LSO                           Lesotho\n",
      "106          LTU                         Lithuania\n",
      "107          LUX                        Luxembourg\n",
      "108          LVA                            Latvia\n",
      "109          MAR                           Morocco\n",
      "110          MCO                            Monaco\n",
      "111          MDA                           Moldova\n",
      "112          MDG                        Madagascar\n",
      "113          MDV                          Maldives\n",
      "114          MEX                            Mexico\n",
      "115          MHL                  Marshall Islands\n",
      "116          MKD                   North Macedonia\n",
      "117          MLI                              Mali\n",
      "118          MLT                             Malta\n",
      "119          MMR                           Myanmar\n",
      "120          MNE                        Montenegro\n",
      "121          MNG                          Mongolia\n",
      "122          MOZ                        Mozambique\n",
      "123          MRT                        Mauritania\n",
      "124          MUS                         Mauritius\n",
      "125          MWI                            Malawi\n",
      "126          MYS                          Malaysia\n",
      "127          NAM                           Namibia\n",
      "128          NER                             Niger\n",
      "129          NGA                           Nigeria\n",
      "130          NIC                         Nicaragua\n",
      "131          NLD                       Netherlands\n",
      "132          NOR                            Norway\n",
      "133          NPL                             Nepal\n",
      "134          NRU                             Nauru\n",
      "135          NZL                       New Zealand\n",
      "136          OMN                              Oman\n",
      "137          PAK                          Pakistan\n",
      "138          PAN                            Panama\n",
      "139          PER                              Peru\n",
      "140          PHL                       Philippines\n",
      "141          PLW                             Palau\n",
      "142          PNG                  Papua New Guinea\n",
      "143          POL                            Poland\n",
      "144          PRK                       North Korea\n",
      "145          PRT                          Portugal\n",
      "146          PRY                          Paraguay\n",
      "147          PSE                         Palestine\n",
      "148          QAT                             Qatar\n",
      "149          ROU                           Romania\n",
      "150          RUS                            Russia\n",
      "151          RWA                            Rwanda\n",
      "152          SAU                      Saudi Arabia\n",
      "153          SDN                             Sudan\n",
      "154          SEN                           Senegal\n",
      "155          SGP                         Singapore\n",
      "156          SLB                   Solomon Islands\n",
      "157          SLE                      Sierra Leone\n",
      "158          SLV                       El Salvador\n",
      "159          SMR                        San Marino\n",
      "160          SOM                           Somalia\n",
      "161          SRB                            Serbia\n",
      "162          SSD                       South Sudan\n",
      "163          STP             Sao Tome and Principe\n",
      "164          SUR                          Suriname\n",
      "165          SVK                          Slovakia\n",
      "166          SVN                          Slovenia\n",
      "167          SWE                            Sweden\n",
      "168          SWZ                          Eswatini\n",
      "169          SYC                        Seychelles\n",
      "170          SYR                             Syria\n",
      "171          TCD                              Chad\n",
      "172          TGO                              Togo\n",
      "173          THA                          Thailand\n",
      "174          TJK                        Tajikistan\n",
      "175          TKM                      Turkmenistan\n",
      "176          TLS                       Timor-Leste\n",
      "177          TON                             Tonga\n",
      "178          TTO               Trinidad and Tobago\n",
      "179          TUN                           Tunisia\n",
      "180          TUR                           Türkiye\n",
      "181          TUV                            Tuvalu\n",
      "182          TZA                          Tanzania\n",
      "183          UGA                            Uganda\n",
      "184          UKR                           Ukraine\n",
      "185          URY                           Uruguay\n",
      "186          USA                     United States\n",
      "187          UZB                        Uzbekistan\n",
      "188          VAT                Vatican City State\n",
      "189          VCT  Saint Vincent and the Grenadines\n",
      "190          VEN                         Venezuela\n",
      "191          VNM                           Vietnam\n",
      "192          VUT                           Vanuatu\n",
      "193          WSM                             Samoa\n",
      "194          YEM                             Yemen\n",
      "195          YMD                       South Yemen\n",
      "196          YUG                        Yugoslavia\n",
      "197          ZAF                      South Africa\n",
      "198          ZMB                            Zambia\n",
      "199          ZWE                          Zimbabwe\n"
     ]
    }
   ],
   "source": [
    "# == Check country names and structure\n",
    "\n",
    "df_raw.head() \n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(df_raw[['country_code', 'country_name']].drop_duplicates().sort_values('country_code').reset_index(drop=True))\n",
    "# Reset to default afterward\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5928538b-8519-40e2-bdee-ac72024f9f48",
   "metadata": {},
   "source": [
    "#### New Variable: Length of speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "911e2229-337f-4479-984c-3a6ab9dff5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average speech length (words): 2931.35\n",
      "20 shortest speeches:\n",
      "             filename                      country_name  year  \\\n",
      "8537  EGY_28_1973.txt                             Egypt  1973   \n",
      "8591  VCT_41_1986.txt  Saint Vincent and the Grenadines  1986   \n",
      "9521  BEN_71_2016.txt                             Benin  2016   \n",
      "4964  GNB_76_2021.txt                     Guinea-Bissau  2021   \n",
      "8783  HTI_01_1946.txt                             Haiti  1946   \n",
      "559   RWA_76_2021.txt                            Rwanda  2021   \n",
      "8792  DDR_45_1990.txt                      East Germany  1990   \n",
      "3721  LTU_73_2018.txt                         Lithuania  2018   \n",
      "6769  RWA_69_2014.txt                            Rwanda  2014   \n",
      "7804  IRN_01_1946.txt                              Iran  1946   \n",
      "6329  LTU_72_2017.txt                         Lithuania  2017   \n",
      "1197  RWA_70_2015.txt                            Rwanda  2015   \n",
      "1457  RWA_72_2017.txt                            Rwanda  2017   \n",
      "3403  ERI_75_2020.txt                           Eritrea  2020   \n",
      "2131  URY_02_1947.txt                           Uruguay  1947   \n",
      "5478  SAU_01_1946.txt                      Saudi Arabia  1946   \n",
      "9849  JOR_65_2010.txt                            Jordan  2010   \n",
      "8419  QAT_63_2008.txt                             Qatar  2008   \n",
      "7221  JOR_61_2006.txt                            Jordan  2006   \n",
      "2540  CUB_01_1946.txt                              Cuba  1946   \n",
      "\n",
      "      speech_length_words  \n",
      "8537                  423  \n",
      "8591                  462  \n",
      "9521                  463  \n",
      "4964                  480  \n",
      "8783                  485  \n",
      "559                   492  \n",
      "8792                  493  \n",
      "3721                  508  \n",
      "6769                  523  \n",
      "7804                  531  \n",
      "6329                  537  \n",
      "1197                  539  \n",
      "1457                  540  \n",
      "3403                  540  \n",
      "2131                  544  \n",
      "5478                  555  \n",
      "9849                  563  \n",
      "8419                  567  \n",
      "7221                  571  \n",
      "2540                  590  \n",
      "\n",
      "20 longest speeches:\n",
      "             filename country_name  year  speech_length_words\n",
      "7414  IND_15_1960.txt        India  1960                22003\n",
      "1975  CUB_15_1960.txt         Cuba  1960                21777\n",
      "4165  RUS_06_1951.txt       Russia  1951                19045\n",
      "3746  GIN_17_1962.txt       Guinea  1962                18952\n",
      "4398  IND_10_1955.txt        India  1955                18053\n",
      "1400  RUS_15_1960.txt       Russia  1960                17943\n",
      "341   IND_14_1959.txt        India  1959                17526\n",
      "4675  IND_09_1954.txt        India  1954                17178\n",
      "7762  RUS_13_1958.txt       Russia  1958                16872\n",
      "273   IND_08_1953.txt        India  1953                16080\n",
      "32    IND_12_1957.txt        India  1957                14927\n",
      "2984  IND_16_1961.txt        India  1961                13951\n",
      "3365  RUS_16_1961.txt       Russia  1961                13941\n",
      "6510  GIN_15_1960.txt       Guinea  1960                13916\n",
      "2926  IND_11_1956.txt        India  1956                13482\n",
      "6328  IDN_15_1960.txt    Indonesia  1960                13463\n",
      "9756  IND_13_1958.txt        India  1958                12599\n",
      "3496  RUS_17_1962.txt       Russia  1962                12255\n",
      "576   RUS_09_1954.txt       Russia  1954                12055\n",
      "6108  RUS_14_1959.txt       Russia  1959                11959\n"
     ]
    }
   ],
   "source": [
    "# Add a new column: speech length in words\n",
    "df_raw['speech_length_words'] = df_raw['speech'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Calculate average length\n",
    "avg_length = df_raw['speech_length_words'].mean()\n",
    "print(\"Average speech length (words):\", round(avg_length, 2))\n",
    "\n",
    "# 20 shortest & longest speeches\n",
    "print(\"20 shortest speeches:\")\n",
    "print(df_raw.nsmallest(20, 'speech_length_words')[['filename', 'country_name', 'year', 'speech_length_words']])\n",
    "\n",
    "print(\"\\n20 longest speeches:\")\n",
    "print(df_raw.nlargest(20, 'speech_length_words')[['filename', 'country_name', 'year', 'speech_length_words']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424f152-b786-45bf-bf86-e169a900f628",
   "metadata": {},
   "source": [
    "#### New variable: English as Official Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d1d2c2a6-23d6-4044-84cc-8785167e6b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries not matched in df_raw['country_name']:\n",
      "Anguilla\n",
      "Bermuda\n",
      "British Virgin Islands\n",
      "Cayman Islands\n",
      "Christmas Island\n",
      "Cook Islands\n",
      "Hong Kong\n",
      "Jersey\n",
      "Niue\n",
      "Norfolk Island\n",
      "Northern Mariana Islands\n",
      "Pitcairn Islands\n",
      "Sint Maarten\n",
      "Turks and Caicos Islands\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "      <th>country_code</th>\n",
       "      <th>year</th>\n",
       "      <th>country_name</th>\n",
       "      <th>speech_length_words</th>\n",
       "      <th>english_official_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SEN_63_2008.txt</td>\n",
       "      <td>Mr. Miguel \\nd’Escoto, in electing you to the ...</td>\n",
       "      <td>SEN</td>\n",
       "      <td>2008</td>\n",
       "      <td>Senegal</td>\n",
       "      <td>2269</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KWT_73_2018.txt</td>\n",
       "      <td>At the outset, it is my pleasure, on behalf of...</td>\n",
       "      <td>KWT</td>\n",
       "      <td>2018</td>\n",
       "      <td>Kuwait</td>\n",
       "      <td>2997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFG_53_1998.txt</td>\n",
       "      <td>The urgency of the\\nalarming situation in Afgh...</td>\n",
       "      <td>AFG</td>\n",
       "      <td>1998</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2385</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ARG_12_1957.txt</td>\n",
       "      <td>First of all, I should like to join my congrat...</td>\n",
       "      <td>ARG</td>\n",
       "      <td>1957</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>2081</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SYR_23_1968.txt</td>\n",
       "      <td>1. When I, as the then Chairman of the Asian G...</td>\n",
       "      <td>SYR</td>\n",
       "      <td>1968</td>\n",
       "      <td>Syria</td>\n",
       "      <td>6902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech  \\\n",
       "0  SEN_63_2008.txt  Mr. Miguel \\nd’Escoto, in electing you to the ...   \n",
       "1  KWT_73_2018.txt  At the outset, it is my pleasure, on behalf of...   \n",
       "2  AFG_53_1998.txt  The urgency of the\\nalarming situation in Afgh...   \n",
       "3  ARG_12_1957.txt  First of all, I should like to join my congrat...   \n",
       "4  SYR_23_1968.txt  1. When I, as the then Chairman of the Asian G...   \n",
       "\n",
       "  country_code  year country_name  speech_length_words  \\\n",
       "0          SEN  2008      Senegal                 2269   \n",
       "1          KWT  2018       Kuwait                 2997   \n",
       "2          AFG  1998  Afghanistan                 2385   \n",
       "3          ARG  1957    Argentina                 2081   \n",
       "4          SYR  1968        Syria                 6902   \n",
       "\n",
       "   english_official_language  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          0  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source for english as official language : https://gradschool.utk.edu/future-students/office-of-graduate-admissions/applying-to-graduate-school/admission-requirements/testing-requirements/countries-with-english-as-official-language/\n",
    "# They are quoting: https://www.cia.gov/the-world-factbook/field/languages/\n",
    "\n",
    "english_countries = [\n",
    "    \"Anguilla\", \"Antigua and Barbuda\", \"Bahamas\", \"Barbados\", \"Belize\", \"Belgium\",\n",
    "    \"Bermuda\", \"Botswana\", \"British Virgin Islands\", \"Burundi\", \"Cameroon\", \"Canada\",\n",
    "    \"Cayman Islands\", \"Christmas Island\", \"Cook Islands\", \"Dominica\", \"Fiji\", \"Gambia\",\n",
    "    \"Ghana\", \"Grenada\", \"Guyana\", \"Hong Kong\", \"India\", \"Ireland\", \"Jersey\", \"Kenya\",\n",
    "    \"Liberia\", \"Malawi\", \"Malta\", \"Marshall Islands\", \"Micronesia\",\n",
    "    \"Namibia\", \"New Zealand\", \"Nigeria\", \"Niue\", \"Norfolk Island\", \"Northern Mariana Islands\",\n",
    "    \"Pakistan\", \"Palau\", \"Papua New Guinea\", \"Philippines\", \"Pitcairn Islands\", \"Rwanda\",\n",
    "    \"Saint Kitts and Nevis\", \"Saint Lucia\", \"Samoa\", \"Seychelles\", \"Sierra Leone\", \"Singapore\",\n",
    "    \"Sint Maarten\", \"Solomon Islands\", \"Somalia\", \"South Africa\", \"South Sudan\", \"Sudan\",\n",
    "    \"Eswatini\", \"Tanzania\", \"Tonga\", \"Trinidad and Tobago\", \"Turks and Caicos Islands\",\n",
    "    \"Tuvalu\", \"Uganda\", \"Zambia\", \"Zimbabwe\"\n",
    "]\n",
    "\n",
    "# Create dummy column\n",
    "df_raw['english_official_language'] = df_raw['country_name'].apply(\n",
    "    lambda x: 1 if x in english_countries else 0\n",
    ")\n",
    "\n",
    "# Detect unmatched countries \n",
    "matched = set(df_raw['country_name'])\n",
    "unmatched = [country for country in english_countries if country not in matched]\n",
    "\n",
    "print(\"Countries not matched in df_raw['country_name']:\")\n",
    "for country in unmatched:\n",
    "    print(country)\n",
    "\n",
    "# All of these countries are either British Overseas Territories, Australian Territories, self-governing island territories or Special Administrative Regions\n",
    "    # None of the unmatched regions are UN Members\n",
    "\n",
    "# Check df with new variable english_official_language\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82540d8a-b656-45fb-aeb0-2fd5b4d7108a",
   "metadata": {},
   "source": [
    "#### New variable: Permanent member security council"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4fb92fe4-5c22-43fa-8b48-62ec2240d716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      country_code    country_name  security_council_permanent  year\n",
      "13             GBR  United Kingdom                           1  2004\n",
      "27             FRA          France                           1  1966\n",
      "29             GBR  United Kingdom                           1  1958\n",
      "91             FRA          France                           1  2012\n",
      "150            CHN           China                           1  1965\n",
      "...            ...             ...                         ...   ...\n",
      "10647          RUS          Russia                           1  2023\n",
      "10662          CHN           China                           1  1977\n",
      "10670          FRA          France                           1  1976\n",
      "10713          RUS          Russia                           1  2006\n",
      "10738          USA   United States                           1  1969\n",
      "\n",
      "[383 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define permanent members of the UN Security Council and create dummy\n",
    "permanent_members = ['RUS', 'USA', 'FRA', 'GBR', 'CHN']\n",
    "\n",
    "df_raw['security_council_permanent'] = df_raw['country_code'].isin(permanent_members).astype(int)\n",
    "\n",
    "print(df_raw[df_raw['country_code'].isin(permanent_members)][\n",
    "    ['country_code', 'country_name', 'security_council_permanent', 'year']\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e3990c63-e221-457f-ab3f-b408b147cd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              filename                                             speech  \\\n",
      "0      USA_65_2010.txt  It is a great honour to address \\nthis Assembl...   \n",
      "1      CMR_68_2013.txt  Allow me at the outset to convey to all presen...   \n",
      "2      BGR_21_1966.txt  106.     On behalf of the delegation of the Pe...   \n",
      "3      PER_25_1970.txt  1.\\t  Mr. President, may I start my statement ...   \n",
      "4      BHR_59_2004.txt  Mr. President, at the\\noutset, I have the plea...   \n",
      "...                ...                                                ...   \n",
      "10755  VCT_54_1999.txt  I take this opportunity to congratulate Mr. Th...   \n",
      "10756  PNG_51_1996.txt  ﻿On behalf of\\nthe people and the Government o...   \n",
      "10757  SYR_33_1978.txt  ﻿\\n\\n\\n\\n140.\\tMr. President, I wish to extend...   \n",
      "10758  NIC_46_1991.txt  ﻿Allow me to congratulate Ambassador Shihabi o...   \n",
      "10759  CHN_13_1958.txt  118.\\tMr. President, allow me first of all to ...   \n",
      "\n",
      "      country_code  year                      country_name  \\\n",
      "0              USA  2010                     United States   \n",
      "1              CMR  2013                          Cameroon   \n",
      "2              BGR  1966                          Bulgaria   \n",
      "3              PER  1970                              Peru   \n",
      "4              BHR  2004                           Bahrain   \n",
      "...            ...   ...                               ...   \n",
      "10755          VCT  1999  Saint Vincent and the Grenadines   \n",
      "10756          PNG  1996                  Papua New Guinea   \n",
      "10757          SYR  1978                             Syria   \n",
      "10758          NIC  1991                         Nicaragua   \n",
      "10759          CHN  1958                             China   \n",
      "\n",
      "       speech_length_words  english_official_language  \\\n",
      "0                     4100                          0   \n",
      "1                     1997                          1   \n",
      "2                     4173                          0   \n",
      "3                     4547                          0   \n",
      "4                     1914                          0   \n",
      "...                    ...                        ...   \n",
      "10755                 1976                          0   \n",
      "10756                 2982                          1   \n",
      "10757                 3349                          0   \n",
      "10758                 3485                          0   \n",
      "10759                 3312                          0   \n",
      "\n",
      "       security_council_permanent                            speaker_name  \\\n",
      "0                               1                            Barack Obama   \n",
      "1                               0                          Moukoko Mbonjo   \n",
      "2                               0                              Mr. BASHEV   \n",
      "3                               0                      Mr. MERCADO JARRIN   \n",
      "4                               0  Shaikh Mohammed bin Mubarak AL-KHALIFA   \n",
      "...                           ...                                     ...   \n",
      "10755                           0                       Allan Cruickshank   \n",
      "10756                           0                             Julius Chan   \n",
      "10757                           0                                 Kaddour   \n",
      "10758                           0                        DREYFUS MORALES    \n",
      "10759                           1                              Mr. Tsiang   \n",
      "\n",
      "                           position  gender_dummy  \n",
      "0                         President           NaN  \n",
      "1      Minister for Foreign Affairs           NaN  \n",
      "2                               NaN           0.0  \n",
      "3                               NaN           0.0  \n",
      "4            Deputy Prime Minister            NaN  \n",
      "...                             ...           ...  \n",
      "10755  Minister for Foreign Affairs           NaN  \n",
      "10756                Prime Minister           NaN  \n",
      "10757                           NaN           NaN  \n",
      "10758                           NaN           NaN  \n",
      "10759                           NaN           0.0  \n",
      "\n",
      "[10760 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523547d9-9b10-473c-8c85-d1e2fc4fd3e1",
   "metadata": {},
   "source": [
    "#### New variables: Speaker, Position & Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "7efb21ee-d51a-4f5a-a162-4d1ff6ceabfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "       Year  Session ISO Code                              Country  \\\n",
      "10506  1951        6      RUS  Union of Soviet Socialist Republics   \n",
      "10552  1951        6      RUS  Union of Soviet Socialist Republics   \n",
      "10381  1954        9      PHL                          Philippines   \n",
      "10415  1954        9      PHL                          Philippines   \n",
      "10261  1956       11      IRQ                                 Iraq   \n",
      "10324  1956       11      IRQ                                 Iraq   \n",
      "10323  1956       11      SYR                                Syria   \n",
      "10326  1956       11      SYR                                Syria   \n",
      "10205  1957       12      CSK                       Czechoslovakia   \n",
      "10243  1957       12      CSK                       Czechoslovakia   \n",
      "10159  1958       13      BGR                             Bulgaria   \n",
      "10181  1958       13      BGR                             Bulgaria   \n",
      "10127  1958       13      CSK                       Czechoslovakia   \n",
      "10183  1958       13      CSK                       Czechoslovakia   \n",
      "10140  1958       13      IRQ                                 Iraq   \n",
      "10186  1958       13      IRQ                                 Iraq   \n",
      "10116  1958       13      RUS  Union of Soviet Socialist Republics   \n",
      "10171  1958       13      RUS  Union of Soviet Socialist Republics   \n",
      "10039  1959       14      RUS  Union of Soviet Socialist Republics   \n",
      "10109  1959       14      RUS  Union of Soviet Socialist Republics   \n",
      "\n",
      "        Name of Person Speaking                                  Post  \\\n",
      "10506            Mr. VYSHINSKY                                    NaN   \n",
      "10552            Mr. VYSHINSKY                                    NaN   \n",
      "10381                Mr. ROMULO                                   NaN   \n",
      "10415               Mr. SERRANO                                   NaN   \n",
      "10261                Mr. JAMALI                                   NaN   \n",
      "10324                Mr. JAMALI                                   NaN   \n",
      "10323            Mr. ZEINEDDINE                                   NaN   \n",
      "10326           Mr. ZEINEDDINE                                    NaN   \n",
      "10205                Mr. DAVID                                    NaN   \n",
      "10243                 Mr. DAVID                                   NaN   \n",
      "10159               Mr. Lukanov                                   NaN   \n",
      "10181               Mr. Lukanov                                   NaN   \n",
      "10127                 Mr. David                                   NaN   \n",
      "10183                 Mr. David                                   NaN   \n",
      "10140                 Mr. Jawad                                   NaN   \n",
      "10186                Mr. Jomard                                   NaN   \n",
      "10116               Mr. Gromyko                                   NaN   \n",
      "10171              Mr. GROMYKO                                    NaN   \n",
      "10039  Mr. Nikita S. KHRUSHCHEV  Chairman of the Council of Ministers   \n",
      "10109             Mr. Kuznetsov                                   NaN   \n",
      "\n",
      "      Unnamed: 6  \n",
      "10506        NaN  \n",
      "10552        NaN  \n",
      "10381        NaN  \n",
      "10415        NaN  \n",
      "10261        NaN  \n",
      "10324        NaN  \n",
      "10323        NaN  \n",
      "10326        NaN  \n",
      "10205        NaN  \n",
      "10243        NaN  \n",
      "10159        NaN  \n",
      "10181        NaN  \n",
      "10127        NaN  \n",
      "10183        NaN  \n",
      "10140        NaN  \n",
      "10186        NaN  \n",
      "10116        NaN  \n",
      "10171        NaN  \n",
      "10039        NaN  \n",
      "10109        NaN  \n",
      "0\n"
     ]
    }
   ],
   "source": [
    "df_speakers = pd.read_excel(os.path.join(data_c, \"data_original\", \"UN General Debate Corpus\", \"Speakers_by_session.xlsx\"))\n",
    "\n",
    "# Check uniqueness of keys in df_speakers\n",
    "print(df_speakers.duplicated(subset=['Year', 'ISO Code']).sum())\n",
    "\n",
    "# If you want to see actual duplicates in df_speakers\n",
    "dupes_speakers = df_speakers[df_speakers.duplicated(subset=['Year', 'ISO Code'], keep=False)]\n",
    "print(dupes_speakers.sort_values(['Year', 'ISO Code']).head(20))\n",
    "\n",
    "# for 1958 Iraq Mr. Jomard see https://digitallibrary.un.org/record/380721\n",
    "# for 1954 Phillipines Mr. Romulo see https://digitallibrary.un.org/record/380429\n",
    "\n",
    "df_speakers_cleaned = (\n",
    "    df_speakers[~(\n",
    "        ((df_speakers['ISO Code'] == \"IRQ\") & (df_speakers['Year'] == 1958) & (df_speakers['Name of Person Speaking'] == \"Mr. Jawad\")) |\n",
    "        ((df_speakers['ISO Code'] == \"PHL\") & (df_speakers['Year'] == 1954) & (df_speakers['Name of Person Speaking'] == \"Mr. SERRANO\"))\n",
    "    )]\n",
    "    .drop_duplicates(subset=['Year', 'ISO Code'], keep='first')\n",
    ")\n",
    "print(df_speakers_unique.duplicated(subset=['Year', 'ISO Code']).sum())  # should be 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "de783a81-efbf-49a6-96f6-ae6a93fa1573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              filename  year country_code              country_name\n",
      "16     KWT_49_1994.txt  1994          KWT                    Kuwait\n",
      "812    YMD_24_1969.txt  1969          YMD               South Yemen\n",
      "829    SYR_18_1963.txt  1963          SYR                     Syria\n",
      "834    IND_18_1963.txt  1963          IND                     India\n",
      "877    CAF_18_1963.txt  1963          CAF  Central African Republic\n",
      "...                ...   ...          ...                       ...\n",
      "10117  BEL_18_1963.txt  1963          BEL                   Belgium\n",
      "10166  TZA_18_1963.txt  1963          TZA                  Tanzania\n",
      "10304  YMD_39_1984.txt  1984          YMD               South Yemen\n",
      "10461  NOR_18_1963.txt  1963          NOR                    Norway\n",
      "10701  YMD_37_1982.txt  1982          YMD               South Yemen\n",
      "\n",
      "[84 rows x 4 columns]\n",
      "84 rows could not be matched\n",
      "    gender_dummy  count\n",
      "0       0 (male)   4521\n",
      "1     1 (female)    183\n",
      "2  NaN (unknown)   6056\n"
     ]
    }
   ],
   "source": [
    "# Supplmentary xlsx-file from the UN Dataset provides information on the speaker and their position\n",
    "\n",
    "####### CHECK IF THIS WOULD WORK IN A REPLICATION #####################################\n",
    "\n",
    "# == Create variable speaker_name and position ==\n",
    "\n",
    "# Merge new infrormation to dataframe\n",
    "df_merged = df_raw.merge(\n",
    "    df_speakers_unique[['Year', 'ISO Code', 'Name of Person Speaking', 'Post']],\n",
    "    left_on=['year', 'country_code'],\n",
    "    right_on=['Year', 'ISO Code'],\n",
    "    how='left',\n",
    "    indicator=True)\n",
    "\n",
    "# Detect unmatched rows\n",
    "unmatched = df_merged[df_merged['_merge'] == 'left_only']\n",
    "unmatched_count = (df_merged['_merge'] == 'left_only').sum()\n",
    "\n",
    "print(unmatched[['filename', 'year', 'country_code', 'country_name']])\n",
    "print(f\"{unmatched_count} rows could not be matched\")\n",
    "\n",
    "# Clean up \n",
    "df_merged = df_merged.drop(columns=['Year', 'ISO Code', '_merge']).rename(columns={\n",
    "    'Name of Person Speaking': 'speaker_name',\n",
    "    'Post': 'position'\n",
    "})\n",
    "\n",
    "# == Create gender dummy ==\n",
    "df_merged['gender_dummy'] = df_merged['speaker_name'].apply(\n",
    "    lambda name: 0 if pd.notnull(name) and re.search(r'^(?:Mr|Sir)\\b', name, re.IGNORECASE)\n",
    "    else 1 if pd.notnull(name) and re.search(r'^(?:Mrs|Ms)\\b', name, re.IGNORECASE)\n",
    "    else None\n",
    ")\n",
    "\n",
    "# Count all values including NaN\n",
    "counts = df_merged['gender_dummy'].value_counts(dropna=False)\n",
    "\n",
    "# Build summary using .get() to handle missing keys\n",
    "gender_summary = pd.DataFrame({\n",
    "    'gender_dummy': ['0 (male)', '1 (female)', 'NaN (unknown)'],\n",
    "    'count': [\n",
    "        counts.get(0, 0),\n",
    "        counts.get(1, 0),\n",
    "        counts.get(np.nan, 0)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(gender_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3cfdc9-d972-454b-9719-3285b1e5d3e2",
   "metadata": {},
   "source": [
    "Looking at the structure, highest position always seems to be mentioned first --> drop everything else if speaker has more than one position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "462ad8f6-1fd1-4c56-9d3f-3f8e90b5c454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [filename, speech, country_code, year, country_name, speech_length_words, english_official_language, security_council_permanent, speaker_name, position, gender_dummy]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "dupes_speakers = df_merged[df_merged.duplicated(subset=['year', 'country_code'], keep=False)]\n",
    "print(dupes_speakers.sort_values(['year', 'country_code']).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "6a1ce3d9-a03d-46f6-90aa-923660bc4474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmatched position: Cabinet Secretary for Foreign Affairs and International Trade\n",
      "Unmatched position: Secretary of State\n",
      "Unmatched position: Chief Executive\n",
      "Unmatched position: Secretary of Relations with States\n",
      "Unmatched position: Minister of State\n",
      "Unmatched position: Secretary for Foreign Affairs\n",
      "Unmatched position: Chairman of the Council of Ministers of the Union of Soviet Socialist Republics\n",
      "Unmatched position: Chairman of the military council and coucil of ministers\n",
      "Unmatched position: Sheikh\n",
      "Unmatched position: Chairman of the military council\n",
      "Unmatched position: Head of the Federal Military Government\n",
      "Unmatched position: Spanish\n",
      "Unmatched position: Secretary for Relations with States\n",
      "Unmatched position: Chairman of the Transitional Military Council\n",
      "Unmatched position: Chairman\n",
      "Unmatched position: Chairman of the Presidential Council\n",
      "Unmatched position: Chairman\n",
      "Unmatched position: Member of the Presidency\n",
      "Unmatched position: Minister of Youth\n",
      "Unmatched position: Union Minister of the Office of the State Counsellor\n",
      "Unmatched position: Chairman of the Presidency\n",
      "Unmatched position: Deputy Minister Foreign Affairs\n",
      "Unmatched position: Minister for Development Cooperation\n",
      "Unmatched position: Chairman of the Presidency\n",
      "Unmatched position: Minister for Development Cooperation\n",
      "Unmatched position: Cairman of the Presidency\n",
      "Unmatched position: Secretary of State\n",
      "Unmatched position: PREMIER OF THE ADMINISTRATIVE COUNCIL\n",
      "Unmatched position: Chairman of the Presidency\n",
      "Unmatched position: Chairman of the Presidency\n",
      "Unmatched position: Chairman of the Council of Ministers\n",
      "Unmatched position: Special Envoy of the Prime Minister and Minister for Public Service\n",
      "Unmatched position: Union Minister for the Office of the State Counsellor\n",
      "Unmatched position: Chief Executive Officer\n",
      "Unmatched position: Chairman of the Assembly Presidium\n",
      "Unmatched position: Emperor\n",
      "Unmatched position: Minister for Development Cooperation\n",
      "Unmatched position: Union Minister for the Office of the State Counsellor\n",
      "Unmatched position: Not indicated\n",
      "Unmatched position: Secretary for Relations with States\n",
      "Unmatched position: \n",
      "Unmatched position: Secretary for Foreign Affairs\n",
      "Unmatched position: Premier of the State Council\n",
      "Unmatched position: Pope\n",
      "Unmatched position: Secretary for Foreign Affairs\n",
      "Unmatched position: Chairman of the Council of Ministers of the People's Republic of Albania\n",
      "Unmatched position: Chancellor\n",
      "Unmatched position: Coordinator of the Junta of the Government\n",
      "Unmatched position: Coordinator of the Junta of the Govermnent of National Reconstruction\n",
      "Unmatched position: Member of the Junta of the Government of National Reconstruction\n",
      "Unmatched position: Chairman of the Presidency\n",
      "Unmatched position: Coordinator of the Junta of the Government of National Reconstruction\n",
      "Unmatched position: Secretary for Foreign Affairs\n",
      "Unmatched position: Chairman\n",
      "Unmatched position: Secretary of State\n",
      "Unmatched position: First Secretary of the Central Committee of the Polish United Workers' Party\n",
      "Unmatched position: Head of the Church\n",
      "Unmatched position: Chairman of the Council of Ministers\n",
      "Unmatched position: Chairman of the United National Front of Kampuchea French\n",
      "Unmatched position: Chairman of the Presidency\n",
      "Unmatched position: CHAIRMAN OF THE SUPREME COUNCIL\n"
     ]
    }
   ],
   "source": [
    "# == Adjust position variable\n",
    "def normalize_position(pos):\n",
    "    if pd.isna(pos):\n",
    "        return pos\n",
    "\n",
    "    pos = pos.strip()\n",
    "\n",
    "    # --- Fix common typos and extra spaces ---\n",
    "    pos = re.sub(r'\\s+', ' ', pos)  # collapse multiple spaces\n",
    "    pos_lower = pos.lower()\n",
    "\n",
    "     # Turn all ministers that deal with foreign affairs and international relations to \"Minister for Foreign Affairs\n",
    "    foreign_affairs_variants = [\n",
    "        'minister for foregn affairs',\n",
    "        'minister responsible for foreign affairs',\n",
    "        'minsiter for foreign and caricom affairs',\n",
    "        'minister for external affairs',\n",
    "        'minister of external relations',  # <-- added\n",
    "        'foreign minister',\n",
    "        'minister for international affairs and cooperation',\n",
    "        'minister for external relations',\n",
    "        'federal minister for european and international affairs',\n",
    "        'international cooperation',\n",
    "        'federal minister for foreign affairs',\n",
    "        'minister for foreign and caricom affairs',\n",
    "        'minister of foreign affairs and cooperation',\n",
    "        'minister for international relations and cooperation',\n",
    "        'ministry of external relations',\n",
    "        'acting minister for foreign affairs and international cooperation',\n",
    "        'ministry of foreign affairs',\n",
    "        'minister for foreign and political affairs',\n",
    "        'federal minister for europe, integration, and foreign affairs',\n",
    "        'federal minister for europe, integration and foreign affairs',\n",
    "        'minister of foreign and european affaris',\n",
    "        'minister of foreign affairs',\n",
    "        'minister for foreign',\n",
    "        'minister of foreign and european affairs and minister of immigration and asylum',\n",
    "        'minister for foreign affairs and senegalese living abroad',\n",
    "        'minister for foreign affairs with responsibility for brexit',\n",
    "        'minister for foreign affairs and investment promotion'\n",
    "       \n",
    "    ]\n",
    "    if any(variant in pos_lower for variant in foreign_affairs_variants):\n",
    "        return \"Minister for Foreign Affairs\"\n",
    "\n",
    "    # --- Fix \"rime minister\" typo ---\n",
    "    pos = re.sub(r'(?i)\\brime[- ]?minister\\b', 'Prime Minister', pos)\n",
    "\n",
    "    # Normalize different versions of Head of Government, President, Prime Minsiter and Vice-President-\n",
    "    exact_matches = {\n",
    "        r'(?i)^president of (the )?government$': 'Head of Government',\n",
    "        r'(?i)^acting president$': 'President',\n",
    "        r'(?i)^interim president$': 'President',\n",
    "        r'(?i)^constitutional president$': 'President',\n",
    "        r'(?i)^first executive president$': 'President',\n",
    "        r'(?i)^first prime[- ]?minister$': 'Prime Minister',\n",
    "        r'(?i)^head of the goverment$': 'Head of Government',  # <-- catch typo + spaces\n",
    "        r'(?i)^head\\s+of\\s+govern?ment$': 'Head of Government',\n",
    "        r'(?i)^first vice[- ]?president$': 'Vice-President'\n",
    "    }\n",
    "    for pattern, replacement in exact_matches.items():\n",
    "        if re.fullmatch(pattern, pos):\n",
    "            return replacement\n",
    "\n",
    "    # --- Normalize prefixes ---\n",
    "    pos = re.sub(r'(?i)^first vice[- ]?president\\b', 'Vice-President', pos)\n",
    "    pos = re.sub(r'(?i)\\bprime[- ]?minister\\b', 'Prime Minister', pos)\n",
    "    pos = re.sub(r'(?i)\\bpresident\\b', 'President', pos)\n",
    "    pos = re.sub(r'(?i)\\bvice[- ]?president\\b', 'Vice-President', pos)\n",
    "\n",
    "    # --- Collapse primary roles if they appear at start ---\n",
    "    primary_roles = [\n",
    "        (r'(?i)^prime[- ]?minister\\b', 'Prime Minister'),\n",
    "        (r'(?i)^deputy prime[- ]?minister\\b', 'Deputy Prime Minister'),\n",
    "        (r'(?i)^president\\b', 'President'),\n",
    "        (r'(?i)^vice[- ]?president\\b', 'Vice-President'),\n",
    "        (r'(?i)^head of state\\b', 'Head of State'),\n",
    "        (r'(?i)^(crown prince|prince|king|emir|amir)\\b', 'Monarch'),\n",
    "        (r'(?i)^(un representative|permanent representative|delegation|chair of (the )?delegation|chair of diplomatic representative)\\b', 'Diplomatic Representative')\n",
    "    ]\n",
    "    for pattern, replacement in primary_roles:\n",
    "        if re.match(pattern, pos):\n",
    "            return replacement\n",
    "\n",
    "    # --- Monarchs ---\n",
    "    if re.search(r'(?i)\\b(crown prince|prince|king|emir|amir)\\b', pos):\n",
    "        return \"Monarch\"\n",
    "\n",
    "    # --- Head of State ---\n",
    "    if re.search(r'(?i)head of state', pos):\n",
    "        return \"Head of State\"\n",
    "        \n",
    "    # --- Diplomatic Representatives ---\n",
    "    if re.search(r'(?i)(un representative|permanent representative|delegation|chair of (the )?delegation|chair of diplomatic representative)', pos):\n",
    "        return \"Diplomatic Representative\"\n",
    "\n",
    "    # --- Everything else ---\n",
    "    print(\"Unmatched position:\", pos)  # print before assigning Others\n",
    "    return \"Others\"\n",
    "\n",
    "# Apply\n",
    "df_merged[\"position\"] = df_merged[\"position\"].apply(normalize_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "7a99b3df-550a-41a3-a751-2692c0f7ea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_positions(pos):\n",
    "    if pd.isna(pos):\n",
    "        return pos  # keep NaN\n",
    "    \n",
    "    if pos in [\"Prime Minister\", \"Deputy Prime Minister\"]:\n",
    "        return \"(Deputy) Prime Minister\"\n",
    "    \n",
    "    if pos in [\"President\", \"Vice-President\"]:\n",
    "        return \"(Vice-) President\"\n",
    "        \n",
    "    if pos in [\"Minister for Foreign Affairs\", \"Deputy Minister for Foreign Affairs\",\n",
    "        \"Deputy Minister Foreign Affairs\",\n",
    "        \"Second Minister for Foreign Affairs\",\n",
    "        \"Second Minister for Foreign Affairs and Trade\",\n",
    "        \"Vice Minister for Foreign Affairs\"]:\n",
    "        return \"(Deputy) Minister for Foreign Affairs\"\n",
    "    \n",
    "    return pos\n",
    "\n",
    "df_merged[\"position\"] = df_merged[\"position\"].apply(merge_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "14ececb8-262d-4270-8d52-5742a864cf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN                                      4678\n",
      "(Deputy) Minister for Foreign Affairs    2332\n",
      "(Vice-) President                        1988\n",
      "(Deputy) Prime Minister                  1190\n",
      "Diplomatic Representative                 333\n",
      "Head of State                              74\n",
      "Others                                     61\n",
      "Monarch                                    56\n",
      "Head of Government                         48\n",
      "Name: position, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Pandas so einstellen, dass es alles ausgibt\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "# Alle Positionen mit Häufigkeit\n",
    "position_counts = df_merged['position'].value_counts(dropna=False)\n",
    "\n",
    "print(position_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "df4b9986-4599-46cc-8d2b-c49667f86e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      total_rows  missing  not_missing\n",
      "year                                  \n",
      "1946          39       39            0\n",
      "1947          39       39            0\n",
      "1948          39       39            0\n",
      "1949          35       35            0\n",
      "1950          44       44            0\n",
      "1951          51       51            0\n",
      "1952          43       43            0\n",
      "1953          44       44            0\n",
      "1954          42       41            1\n",
      "1955          45       45            0\n",
      "1956          66       66            0\n",
      "1957          71       71            0\n",
      "1958          72       72            0\n",
      "1959          79       78            1\n",
      "1960          79       60           19\n",
      "1961          81       81            0\n",
      "1962          92       90            2\n",
      "1963          97       95            2\n",
      "1964          95       93            2\n",
      "1965         101      101            0\n",
      "1966         108      108            0\n",
      "1967         110      109            1\n",
      "1968         112      110            2\n",
      "1969         116      113            3\n",
      "1970          70       67            3\n",
      "1971         116      115            1\n",
      "1972         125      125            0\n",
      "1973         120      115            5\n",
      "1974         129      121            8\n",
      "1975         126      121            5\n",
      "1976         134      125            9\n",
      "1977         140      129           11\n",
      "1978         141      131           10\n",
      "1979         144      128           16\n",
      "1980         149      139           10\n",
      "1981         145      132           13\n",
      "1982         147      136           11\n",
      "1983         149      125           24\n",
      "1984         150      140           10\n",
      "1985         137      119           18\n",
      "1986         149      128           21\n",
      "1987         152      129           23\n",
      "1988         154      132           22\n",
      "1989         153      130           23\n",
      "1990         156      117           39\n",
      "1991         162      128           34\n",
      "1992         167      127           40\n",
      "1993         175      133           42\n",
      "1994         178        8          170\n",
      "1995         172        0          172\n",
      "1996         181        0          181\n",
      "1997         176        0          176\n",
      "1998         181        0          181\n",
      "1999         181        0          181\n",
      "2000         178        0          178\n",
      "2001         189        2          187\n",
      "2002         188        0          188\n",
      "2003         189        0          189\n",
      "2004         192        0          192\n",
      "2005         185        0          185\n",
      "2006         193        0          193\n",
      "2007         191        0          191\n",
      "2008         192        0          192\n",
      "2009         193        0          193\n",
      "2010         189        0          189\n",
      "2011         194        1          193\n",
      "2012         195        0          195\n",
      "2013         193        1          192\n",
      "2014         194        1          193\n",
      "2015         193        1          192\n",
      "2016         194        2          192\n",
      "2017         196        2          194\n",
      "2018         196        0          196\n",
      "2019         195        0          195\n",
      "2020         193        1          192\n",
      "2021         194        0          194\n",
      "2022         193        0          193\n",
      "2023         192        0          192\n"
     ]
    }
   ],
   "source": [
    "# Started to document positions properly from 1986 on, before yearly sample size per year mostly less than 20 samples\n",
    "\n",
    "yearly_counts = df_merged.groupby('year')['position'].agg(\n",
    "    total_rows='size',\n",
    "    missing=lambda x: x.isna().sum()\n",
    ")\n",
    "\n",
    "# Add not_missing column\n",
    "yearly_counts['not_missing'] = yearly_counts['total_rows'] - yearly_counts['missing']\n",
    "\n",
    "\n",
    "# Print the entire table\n",
    "pd.set_option('display.max_rows', None)  # show all rows\n",
    "print(yearly_counts)\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41d97e2-840b-40eb-b032-197040a0246c",
   "metadata": {},
   "source": [
    "#### New Variable: Country (Year)\n",
    "\n",
    "This variable is later needed to create clean description plots and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d647e412-7bd4-4190-9c34-39874b4620a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_merged.copy()\n",
    "df_merged['speech_label'] = df_merged['country_name'] + \" (\" + df_merged['year'].astype(str) + \")\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc05d65d-be72-42ae-b753-0940de5df17c",
   "metadata": {},
   "source": [
    "#### Save dataframe with all new variables as un_corpus_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "94a901c8-3fc4-4b33-a1ff-cede22bd196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(wd)\n",
    "\n",
    "# Save df_merged as a pickle file for quick future loading\n",
    "merged_pickle_path = r\".\\data\\un_corpus_merged.pkl\"\n",
    "df_merged.to_pickle(merged_pickle_path)\n",
    "\n",
    "# Export df as CSV \n",
    "merged_output_path = r\".\\data\\un_corpus_merged.csv\"\n",
    "df_merged.to_csv(merged_output_path, index=False, sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea36f19a-789d-4c62-a470-3e73a6d848bf",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe0f4af-e939-4100-9618-5b522bdd72a1",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2661f431-a194-47d1-9a1c-89162bf85a91",
   "metadata": {},
   "source": [
    "#### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "cd37e08d-b4f2-459e-823c-3816a825ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Clean text by removing empty spaces, line breaks, hyphenation, stray characters, and escape quote ==\n",
    "\n",
    "def cleaning(content):\n",
    "    if pd.isna(content):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove line breaks and carriage returns\n",
    "    content = content.replace('\\n', ' ').replace('\\r', ' ')\n",
    "\n",
    "    # Collapse multiple spaces\n",
    "    content = ' '.join(content.split())\n",
    "\n",
    "    # Ensure spacing after punctuation\n",
    "    content = re.sub(r'(?<=[.,])(?=[^\\s])', r' ', content)\n",
    "\n",
    "    # Remove hyphenation at line breaks (e.g. \"inter- national\" → \"international\")\n",
    "    content = re.sub(r'-\\s', '', content)\n",
    "\n",
    "     # Replace hyphen between letters with a space to prevent merging words (e.g. \"russian-and\" → \"russian and\")\n",
    "    content = re.sub(r'(?<=\\w)-(?=\\w)', ' ', content)\n",
    "\n",
    "    # Remove stray backslashes\n",
    "    content = content.replace(\"\\\\\", \"\")\n",
    "\n",
    "    return content\n",
    "\n",
    "# Apply cleaning to each speech\n",
    "df_merged['speech'] = df_merged['speech'].astype(str)  # Ensure column is string type\n",
    "df_clean = df_merged.copy()\n",
    "df_clean['speech'] = df_clean['speech'].apply(cleaning)\n",
    "\n",
    "# Drop rows with empty speeches after cleaning\n",
    "df_clean = df_clean[df_clean['speech'].str.strip().astype(bool)].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "7c6bd148-d5cf-4eb3-a66d-fc706e99c13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved clean speeches chunks in 'C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp'\n"
     ]
    }
   ],
   "source": [
    "# == Split cleaned data into chunks and save as separate files ==\n",
    "\n",
    "# Convert cleaned DataFrame to list of lists\n",
    "clean_data = df_clean[['filename', 'speech']].values.tolist()\n",
    "\n",
    "# Split cleaned data into 4 equal chunks\n",
    "data_id1 = clean_data[:int(len(clean_data)/4)]\n",
    "data_id2 = clean_data[int(len(clean_data)/4): int(2*len(clean_data)/4)]\n",
    "data_id3 = clean_data[int(2*len(clean_data)/4): int(3*len(clean_data)/4)]\n",
    "data_id4 = clean_data[int(3*len(clean_data)/4):]\n",
    "\n",
    "# Change directory to the temp folder\n",
    "os.chdir(data_temp)  # make sure `data_temp` exists and is defined\n",
    "\n",
    "# Save each chunk with joblib\n",
    "joblib.dump(data_id1, 'clean_speeches_indexed1.pkl')\n",
    "joblib.dump(data_id2, 'clean_speeches_indexed2.pkl')\n",
    "joblib.dump(data_id3, 'clean_speeches_indexed3.pkl')\n",
    "joblib.dump(data_id4, 'clean_speeches_indexed4.pkl')\n",
    "\n",
    "# Store list of cleaned data chunk paths to feed into preprocessing function later\n",
    "clean_files = [\n",
    "    os.path.join(data_temp, 'clean_speeches_indexed1.pkl'),\n",
    "    os.path.join(data_temp, 'clean_speeches_indexed2.pkl'),\n",
    "    os.path.join(data_temp, 'clean_speeches_indexed3.pkl'),\n",
    "    os.path.join(data_temp, 'clean_speeches_indexed4.pkl')\n",
    "]\n",
    "\n",
    "print(f\"Saved clean speeches chunks in '{data_temp}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6e5109-8cf7-41e3-8fb9-8e7dde29b4e0",
   "metadata": {},
   "source": [
    "### Advanced Text Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfca8dd9-c6d7-48c7-96b0-f5a449814336",
   "metadata": {},
   "source": [
    "#### Extend stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "11d056b5-6729-45f8-a233-3e8e6de1f69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 326 stopwords to C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\stopwords.pkl\n"
     ]
    }
   ],
   "source": [
    "# Get SpaCy stopwords\n",
    "SPACY_STOPWORDS = list(nlp.Defaults.stop_words)\n",
    "\n",
    "# Path to save\n",
    "stopwords_path = os.path.join(data_c, \"stopwords.pkl\")\n",
    "\n",
    "# Save stopwords\n",
    "joblib.dump(SPACY_STOPWORDS, stopwords_path)\n",
    "\n",
    "print(f\"Saved {len(SPACY_STOPWORDS)} stopwords to {stopwords_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0aa75944-4b75-4aa4-9430-9aa6bd9ac934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Define function to Tokenize, eliminate digits, remove stopwords, lemmatize, POS-Tagging ==\n",
    "\n",
    "def pro1(lista):\n",
    "    # Remove punctuation\n",
    "    return [[row[0], row[1].translate(translator)] for row in lista]\n",
    "\n",
    "def pro2(lista):\n",
    "    # Tokenize and lowercase with gensim\n",
    "    return [[row[0], gensim.utils.simple_preprocess(row[1])] for row in lista]\n",
    "\n",
    "def pro3(lista):\n",
    "    # Remove tokens that are only digits\n",
    "        a = [[row[0], [w for w in row[1] if not w.isdigit()]] for row in lista]\n",
    "        return a\n",
    "    \n",
    "def pro4(lista):\n",
    "    # Drop short words\n",
    "    return [[row[0], [w for w in row[1] if len(w) > 2]] for row in lista]\n",
    "\n",
    "def tags_spacy(lista):\n",
    "    texts = [' '.join(row[1]) for row in lista]\n",
    "    docs = list(nlp.pipe(texts, batch_size=20, n_process=1))\n",
    "    result = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        filtered_tokens = [token.text for token in doc if token.tag_.startswith(('N', 'V', 'J'))]\n",
    "        result.append([lista[i][0], filtered_tokens])\n",
    "    return result\n",
    "\n",
    "\n",
    "def pro5(lista):\n",
    "    # Remove stopwords using SpaCy stopword list\n",
    "    return [[row[0], [w for w in row[1] if w not in SPACY_STOPWORDS]] for row in lista]\n",
    "\n",
    "def pro6(lista):\n",
    "      return [\n",
    "        [row[0], [stemmer.stem(token) for token in row[1]]]\n",
    "        for row in lista\n",
    "    ]\n",
    "########################## Question for Max: They removed procedural words in the paper\n",
    "\n",
    "def dropnull(lista):\n",
    "    # Drop empty speeches\n",
    "    return [row for row in lista if len(' '.join(row[1])) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "524a274a-004c-4d87-9bb3-5f789a42b90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl] Before tagging: 24.45s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl] After tagging: 792.12s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed1.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl] Done. Total time: 952.78s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl] Before tagging: 25.21s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl] After tagging: 714.22s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed2.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl] Done. Total time: 886.01s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl] Before tagging: 24.49s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl] After tagging: 746.06s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed3.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl] Done. Total time: 904.50s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl] Before tagging: 24.42s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl] After tagging: 713.96s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed4.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl] Done. Total time: 874.34s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# == Create full pre-processing function and call it\n",
    "\n",
    "def preprocessing(data_name):\n",
    "    t0 = time.time()\n",
    "    print(f\"Starting preprocessing for {data_name}...\")\n",
    "\n",
    "    data = joblib.load(data_name)\n",
    "    data = pro1(data)\n",
    "    data = pro2(data)\n",
    "    data = pro3(data)\n",
    "    data = pro4(data)\n",
    "\n",
    "    print(f\"[{data_name}] Before tagging: {time.time() - t0:.2f}s\")\n",
    "    data = tags_spacy(data)\n",
    "    print(f\"[{data_name}] After tagging: {time.time() - t0:.2f}s\")\n",
    "\n",
    "    data = pro5(data)\n",
    "   # data = pro6(data)\n",
    "    data = dropnull(data)\n",
    "\n",
    "    # out_name = data_name.replace('cleanspeeches_', 'preprocessed_speeches_').replace('.pkl', '_temp.pkl')\n",
    "\n",
    "    # Store preprocessed corupus (before stemming) for wordcloud\n",
    "    filename_wordcloud = data_name.replace('clean_speeches_', 'wordcloud_speeches_').replace('.pkl', '.pkl')\n",
    "    out_name_wordcloud = os.path.join(data_preprocessed, os.path.basename(filename_wordcloud))\n",
    "    joblib.dump(data, out_name_wordcloud)\n",
    "\n",
    "    # Apply stemming\n",
    "    data_stemmed = pro6(data)\n",
    "\n",
    "    filename_preprocessed = data_name.replace('clean_speeches_', 'preprocessed_speeches_').replace('.pkl', '.pkl')\n",
    "    out_preprocessed = os.path.join(data_preprocessed, os.path.basename(filename_preprocessed))\n",
    "    joblib.dump(data_stemmed, out_preprocessed\n",
    "               )\n",
    "    print(f\"[{data_name}] Saved stemmed version: {out_preprocessed}\")\n",
    "\n",
    "    print(f\"[{data_name}] Done. Total time: {time.time() - t0:.2f}s\\n\")\n",
    "\n",
    "def main():\n",
    "    for fname in clean_files:\n",
    "        preprocessing(fname)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d3409987-485c-47c9-9430-01fa2c0e650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the pre-processed data\n",
    "preprocessed_files = [\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed1.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed2.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed3.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed4.pkl')\n",
    "]\n",
    "\n",
    "wordcloud_files = [\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed1.pkl'),\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed2.pkl'),\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed3.pkl'),\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed4.pkl')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "134ddbf7-9b95-4697-9d9c-f9c55b52ec80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73db5f7-566d-4276-a631-c1f33fab494d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d006e-6a23-453b-9c91-0e60bd3f8301",
   "metadata": {},
   "source": [
    "## Word-Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf2443e-4c76-41b2-bc16-e72d91c318a8",
   "metadata": {},
   "source": [
    "### Count frequencies of all tokens and display the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "154d978d-cddd-431c-9d53-73c9faf8fdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:50<00:00, 12.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Stemmed] Top 50 most common words:\n",
      "nation: 232332\n",
      "unit: 186036\n",
      "countri: 176251\n",
      "intern: 159425\n",
      "develop: 145364\n",
      "peac: 133382\n",
      "world: 131630\n",
      "state: 128681\n",
      "peopl: 126148\n",
      "secur: 84579\n",
      "general: 76542\n",
      "govern: 74954\n",
      "econom: 72977\n",
      "organ: 68141\n",
      "right: 65874\n",
      "year: 65829\n",
      "assembl: 63469\n",
      "new: 58990\n",
      "effort: 56971\n",
      "problem: 56698\n",
      "human: 56072\n",
      "support: 54788\n",
      "continu: 53001\n",
      "communiti: 48791\n",
      "region: 48258\n",
      "polit: 48075\n",
      "time: 47283\n",
      "member: 42574\n",
      "africa: 42224\n",
      "session: 41637\n",
      "need: 41400\n",
      "war: 41154\n",
      "import: 40624\n",
      "council: 40598\n",
      "work: 40082\n",
      "achiev: 39915\n",
      "hope: 38156\n",
      "power: 38070\n",
      "conflict: 36935\n",
      "situat: 36044\n",
      "presid: 35955\n",
      "principl: 35891\n",
      "resolut: 34895\n",
      "global: 34852\n",
      "republ: 34447\n",
      "forc: 34210\n",
      "south: 34176\n",
      "great: 33856\n",
      "relat: 33671\n",
      "order: 33397\n",
      "\n",
      "[Stemmed] Top 50 least common words:\n",
      "thatar: 1\n",
      "aslong: 1\n",
      "bibinagar: 1\n",
      "wain: 1\n",
      "restag: 1\n",
      "interstic: 1\n",
      "apercus: 1\n",
      "thebattlefield: 1\n",
      "roi: 1\n",
      "shona: 1\n",
      "nbele: 1\n",
      "quaff: 1\n",
      "mallorca: 1\n",
      "standardis: 1\n",
      "toearth: 1\n",
      "osceoffic: 1\n",
      "unmikí: 1\n",
      "skapo: 1\n",
      "amcng: 1\n",
      "anag: 1\n",
      "neurosurgeri: 1\n",
      "sera: 1\n",
      "kair: 1\n",
      "aba: 1\n",
      "ecdwa: 1\n",
      "reappli: 1\n",
      "recognr: 1\n",
      "shat: 1\n",
      "moitari: 1\n",
      "yinh: 1\n",
      "bariton: 1\n",
      "countertenor: 1\n",
      "notr: 1\n",
      "vasyl: 1\n",
      "slipak: 1\n",
      "eyelash: 1\n",
      "yearsa: 1\n",
      "ingwenyama: 1\n",
      "indlovukati: 1\n",
      "healthinsur: 1\n",
      "ludzidzini: 1\n",
      "oclock: 1\n",
      "encyclopaedist: 1\n",
      "galdo: 1\n",
      "curi: 1\n",
      "marsha: 1\n",
      "domitila: 1\n",
      "misogynist: 1\n",
      "zocalo: 1\n",
      "ennui: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit\\\\data\\\\freq\\\\word_counts.pkl']"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#== Count token frequencies ==\n",
    "\n",
    "def count_frequencies(filenames):\n",
    "    total_freq = Counter()\n",
    "    for fname in tqdm(filenames):\n",
    "        data = joblib.load(fname)\n",
    "        tokens = chain.from_iterable(row[1] for row in data if isinstance(row[1], list))\n",
    "        total_freq.update(tokens)\n",
    "    return total_freq\n",
    "\n",
    "#def remove_rare_words(filenames, freqs, min_count=10):\n",
    "   # for fname in filenames:\n",
    "       # data = joblib.load(fname)\n",
    "       # filtered_data = []\n",
    "        #for doc_id, tokens in data:\n",
    "          #  filtered_tokens = [w for w in tokens if freqs.get(w, 0) >= min_count]\n",
    "          #  filtered_data.append([doc_id, filtered_tokens])\n",
    "       # joblib.dump(filtered_data, fname)  # overwrite or save as new file\n",
    "       # print(f\"Processed {fname}: removed words with freq < {min_count}\")\n",
    "\n",
    "# === Count for preprocessed (stemmed) speeches ===\n",
    "word_counts = count_frequencies(preprocessed_files)\n",
    "\n",
    "#remove_rare_words(preprocessed_files, word_counts, min_count=10)\n",
    "\n",
    "print(\"\\n[Stemmed] Top 50 most common words:\")\n",
    "for word, count in word_counts.most_common(50):\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\n[Stemmed] Top 50 least common words:\")\n",
    "for word, count in word_counts.most_common()[-50:]:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Save stemmed word counts\n",
    "save_path = os.path.join(data_freq, 'word_counts.pkl')\n",
    "joblib.dump(word_counts, save_path)\n",
    "\n",
    "# === Count for wordcloud (unstemmed) speeches ===\n",
    "#word_counts_wordcloud = count_frequencies(wordcloud_files)\n",
    "\n",
    "#print(\"\\n[Wordcloud] Top 50 most common words:\")\n",
    "#for word, count in word_counts_wordcloud.most_common(50):\n",
    "    #print(f\"{word}: {count}\")\n",
    "\n",
    "#print(\"\\n[Wordcloud] Top 50 least common words:\")\n",
    "#for word, count in word_counts_wordcloud.most_common()[-50:]:\n",
    "    #print(f\"{word}: {count}\")\n",
    "\n",
    "# Save unstemmed word counts\n",
    "#save_path_wordcloud = os.path.join(data_freq, 'word_counts_wordcloud.pkl')\n",
    "#joblib.dump(word_counts_wordcloud, save_path_wordcloud)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "932ce36d-d21c-4f1c-aff7-899c321dcfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_c)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39acefa1-c504-478e-98e5-9d465963dc82",
   "metadata": {},
   "source": [
    "### Count the frequency of the dictionary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "9e28f481-7e20-486c-a6b3-8729d6aeaad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of affect dictionary:\n",
      "['forbid', 'unattract', 'cruelti', 'crappi', 'apathi', 'scari', 'unimpress', 'sin', 'dumbest', 'eas', 'agit', 'sob', 'shocker', 'tragedi', 'fabul', 'strongest', 'giver', 'sigh', 'aw', 'witch', 'hurtl', 'fucktard', 'cruel', 'glamor', 'funni', 'smarter', 'brillianc', 'irrate', 'alright', 'honest', 'profit', 'fearless', 'grievous', 'relax', 'isolationist', 'hah', 'shyness', 'poorest', 'cruelest', 'troublemak', 'disagre', 'agon', 'terror', 'fight', 'pleas', 'poor', 'crazi', 'hostil', 'stupid', 'damnat', 'vain', 'jade', 'heartless', 'nag', 'gloomi', 'damn', 'dishearten', 'pleaser', 'credit', 'warmth', 'greatest', 'whine', 'shame', 'angriest', 'envious', 'grin', 'blameless', 'sweeter', 'laidback', 'stupidest', 'unprotect', 'whiner', 'unlov', 'shake', 'boredom', 'fairer', 'weaker', 'wellb', 'bold', 'sucki', 'unsuccess', 'mourner', 'liken', 'defens', 'invigor', 'tedious', 'paranoid', 'cynic', 'dignifi', 'paranoia', 'sweetest', 'contented', 'humili', 'crush', 'terrif', 'alarm', 'asham', 'impati', 'fake', 'flirti', 'laziest', 'libertarian', 'empti', 'panicki', 'rape', 'contemptu', 'incent', 'reward', 'brutal', 'ignoramus', 'bereav', 'wimp', 'laugh', 'nicer', 'warm', 'resent', 'threat', 'hell', 'tortur', 'mad', 'worship', 'uncomfort', 'depress', 'unsaf', 'faultless', 'numb', 'humorless', 'murder', 'prettiest', 'jealous', 'tragic', 'discomfort', 'loveli', 'ungrat', 'divin', 'lamest', 'weaken', 'horribl', 'fuck', 'agoni', 'grievanc', 'ach', 'isolation', 'masoch', 'uglier', 'amaz', 'devilish', 'bitter', 'devil', 'smartest', 'glori', 'trite', 'vicious', 'unsavori', 'grossest', 'sinist', 'cheeri', 'dismay', 'easier', 'flatter', 'virtuous', 'egot', 'generos', 'cherish', 'heartwarm', 'advers', 'tough', 'adorn', 'overwhelm', 'devast', 'egotist', 'sociabl', 'lazier', 'affection', 'sarcasm', 'humor', 'funniest', 'startl', 'innoc', 'vomit', 'shyli', 'kiss', 'burdensom', 'melancholi', 'weakest', 'worthless', 'gentlest', 'proud', 'alarmist', 'pervert', 'demeanor', 'loneli', 'reassur', 'weird', 'passionless', 'loyalti', 'frustrat', 'weak', 'immor', 'hater', 'libertin', 'flirt', 'terribl', 'threaten', 'fatalist', 'melancholia', 'avers', 'prick', 'sucker', 'meanest', 'virtuos', 'benefici', 'neurot', 'disappoint', 'restless', 'romant', 'bastard', 'angri', 'flatteri', 'stun', 'bolder', 'amor', 'forbidden', 'hug', 'demean', 'disgust', 'truer', 'benefic', 'teas', 'stinki', 'condemnatori', 'turmoil', 'poison', 'cunt', 'gorgeous', 'harm', 'lousi', 'glorifi', 'piti', 'wimpish', 'impression', 'freakish', 'happiest', 'apathet', 'pitiabl', 'unpleas', 'suffer', 'miseri', 'toler', 'mess', 'casual', 'anxieti', 'stale', 'fieri', 'grudg', 'arrog', 'outrag', 'thrill', 'faithless', 'popular', 'vaniti', 'palat', 'fantasi', 'decay', 'domin', 'mood', 'proudest', 'scream', 'stress', 'calm', 'fatigu', 'dumber', 'benign', 'rude', 'steal', 'tension', 'enthusiast', 'intimid', 'sweetheart', 'prettier', 'perv', 'pathet', 'flawless', 'contradictori', 'distress', 'moron', 'gratifi', 'frighten', 'anguish', 'snob', 'defenseless', 'fantast', 'sad', 'thief', 'worsen', 'tear', 'handsom', 'attract', 'messi', 'smother', 'scarier', 'afraid', 'insult', 'disreput', 'wept', 'victim', 'traumat', 'prejudic', 'amus', 'decent', 'joyous', 'masochist', 'abandon', 'foe', 'pleasantri', 'shitless', 'strengthen', 'antagonist', 'liar', 'romantic', 'violenc', 'sentimentalist', 'gratif', 'decept', 'witchcraft', 'liveli', 'impressionist', 'guilt', 'griev', 'rotten', 'disgrac', 'insincer', 'pushi', 'enjoy', 'freaki', 'ador', 'grate', 'homesick', 'braver', 'battlefield', 'niceti', 'hooray', 'incompet', 'thiev', 'timid', 'adventur', 'needi', 'joyless', 'grimac', 'gloom', 'dump', 'horrid', 'magnific', 'unlucki', 'killer', 'uncontrol', 'funnier', 'ruin', 'weepi', 'play', 'encourag', 'fantas', 'irrit', 'sincer', 'rage', 'despis', 'unhappi', 'hate', 'bravest', 'harmless', 'hero', 'vigor', 'pervers', 'dumb', 'degrad', 'shameless', 'villaini', 'envi', 'aggress', 'fucker', 'courag', 'craziest', 'delici', 'scare', 'finer', 'bittersweet', 'trivial', 'wealthi', 'heroic', 'ecstat', 'irrat', 'prejudici', 'smug', 'stammer', 'feroci', 'phobic', 'villain', 'anger', 'fond', 'obsession', 'awesom', 'wors', 'compassion', 'weakl', 'calmer', 'inhibit', 'bother', 'rancid', 'hatr', 'beaten', 'opportunist', 'offens', 'haunt', 'appal', 'painless', 'pessimist', 'molest', 'lol', 'wrongdo', 'frantic', 'heroin', 'rejoic', 'woe', 'miser', 'meaner', 'saddest', 'mooch', 'attack', 'enthusiasm', 'joke', 'meritocraci', 'angrier', 'emot', 'weirdest', 'silliest', 'kisser', 'hurrah', 'crueler', 'nervous', 'dishonor', 'gracious', 'sicker', 'hopeless', 'delect', 'compliment', 'petrifi', 'cuter', 'reek', 'loveless', 'daze', 'weirder', 'triumphant', 'impolit', 'offend', 'richer', 'violent', 'crazier', 'comedown', 'heroism', 'stubborn', 'maniac', 'strain', 'broke', 'jerk', 'unkind', 'asshol', 'reveng', 'jealousi', 'damnabl', 'unimport', 'careless', 'gentler', 'bitch', 'shi', 'terrorist', 'rigid', 'harsh', 'passion', 'annoy', 'compass', 'geek', 'weirdo', 'agre', 'whore', 'splendor', 'suck', 'fun', 'chuckl', 'snobberi', 'mock', 'doom', 'reluct', 'mourn', 'boldest', 'helpless', 'trembl', 'pestil', 'curs', 'desper', 'harass', 'complain', 'joker', 'obsess', 'uneasi', 'nasti', 'greedi', 'belov', 'grim', 'upset', 'greed', 'honesti', 'phoni', 'feudal', 'loveliest', 'trauma', 'dwell', 'ecstasi', 'blame', 'legit', 'calmest', 'sickest', 'scariest', 'sarcast', 'cheat', 'fatal', 'stronger', 'energet', 'feroc', 'healthi', 'goddam', 'disast', 'flirtati', 'defeatist', 'glorious', 'terrifi', 'gossip', 'forbad', 'sicken', 'poorer', 'strangest', 'cute', 'struggl', 'woebegon', 'worri', 'gratitud', 'cheer', 'distraught', 'ok', 'vulner', 'blessed', 'fairest', 'lame', 'optim', 'grouch', 'fright', 'pessim', 'sadder', 'worst', 'despair', 'bash', 'intellectu', 'promis', 'prais', 'heartfelt', 'horror', 'crap', 'repress', 'prouder', 'fought', 'discourag', 'heal', 'lazi', 'precious', 'melanchol', 'isol', 'aggressor', 'giggl', 'idiot', 'twitchi', 'glorif', 'kid', 'selfish', 'smile', 'grief', 'hellish', 'shit', 'fume', 'bitchi', 'fool', 'trustworthi', 'optimist', 'dumpi', 'unwelcom', 'neat', 'nicest', 'furi', 'unfriend', 'ridicul', 'arguabl', 'shamefac', 'pervi', 'upbeat', 'wow', 'easygo', 'blameworthi', 'charmer', 'stink', 'phobia', 'adversari', 'sillier', 'distrust', 'dread', 'happier', 'vital', 'dissatisfact', 'insecur', 'assault', 'ineffect', 'panic', 'powerless', 'nightmar', 'wealth', 'enrag', 'splendid', 'foolish', 'ugliest', 'contradict', 'worrier', 'freak']\n",
      "Number of words in affect dictionary: 629\n",
      "\n",
      "Contents of cognition dictionary:\n",
      "['analyt', 'guess', 'suspect', 'coz', 'mysteri', 'unawar', 'proverbi', 'dissimilar', 'consequenti', 'stimul', 'occasion', 'rethink', 'purest', 'entireti', 'abnorm', 'referenti', 'differenti', 'diagnos', 'ambigu', 'concentr', 'undon', 'undecid', 'disclosur', 'fundamentalist', 'blur', 'reaction', 'variabl', 'recogniz', 'meant', 'reconstruct', 'supposit', 'acknowledg', 'choosi', 'accuraci', 'mislead', 'perceiv', 'hypothesi', 'unknow', 'cuz', 'reveal', 'potenti', 'puriti', 'persuad', 'indirect', 'generat', 'presumpt', 'clue', 'justif', 'evidenti', 'realize', 'deduct', 'causal', 'inevit', 'thinker', 'evidentiari', 'factor', 'undeni', 'unclear', 'pick', 'induc', 'accur', 'absolut', 'everytim', 'reconsider', 'diagnost', 'solut', 'diagnosi', 'discoveri', 'ration', 'unambigu', 'explicit', 'decis', 'insight', 'confess', 'fuzzi', 'led', 'lesson', 'theoriz', 'analyz', 'learner', 'split', 'enlighten', 'cos', 'respons', 'dubious', 'react', 'guarante', 'someday', 'blatant', 'undo', 'perceptu', 'infal', 'afterthought', 'inequ', 'feasibl', 'knowabl', 'approxim', 'logic', 'meaning', 'unknown', 'categor', 'trigger', 'incomplet', 'curious', 'attribut', 'pretend', 'reactionari', 'prove', 'meaningless', 'reactiv', 'proof', 'bet', 'purpos', 'suppos', 'discern', 'imagin', 'kinda', 'comprehens', 'sens', 'percept', 'everyday', 'rational', 'unquestion', 'random', 'implicit', 'clarif', 'theori', 'dunno', 'analysi', 'obedi', 'memor', 'genuin', 'evalu', 'explor', 'linkag', 'grasp', 'curios', 'methink', 'somewher', 'deduc', 'unwant', 'induct', 'unrel', 'outstand', 'discov', 'unresolv', 'exploratori', 'choos', 'perspect', 'inferenti', 'disclos', 'factual', 'arbitrari', 'conting', 'vagu', 'persuas', 'manipul', 'impli', 'somehow', 'sorta', 'borderlin', 'reconcil', 'anyhow', 'conscious', 'probabilist', 'precis', 'rearrang', 'referenc', 'obscur']\n",
      "Number of words in cognition dictionary: 169\n",
      "Top 100 words by weighted frequency:\n",
      "fooddepend: 0.999930102776877\n",
      "carmin: 0.999930102776877\n",
      "calo: 0.999930102776877\n",
      "unislam: 0.999930102776877\n",
      "louvain: 0.999930102776877\n",
      "talman: 0.999930102776877\n",
      "unblush: 0.999930102776877\n",
      "eschkol: 0.999930102776877\n",
      "herut: 0.999930102776877\n",
      "quol: 0.999930102776877\n",
      "ahmediy: 0.999930102776877\n",
      "hauran: 0.999930102776877\n",
      "maan: 0.999930102776877\n",
      "yasseen: 0.999930102776877\n",
      "wallenberg: 0.999930102776877\n",
      "fras: 0.999930102776877\n",
      "overcloud: 0.999930102776877\n",
      "nonrefund: 0.999930102776877\n",
      "mizzi: 0.999930102776877\n",
      "solidlybas: 0.999930102776877\n",
      "lastresort: 0.999930102776877\n",
      "nationí: 0.999930102776877\n",
      "terroristsí: 0.999930102776877\n",
      "bigley: 0.999930102776877\n",
      "bigleyí: 0.999930102776877\n",
      "efimovich: 0.999930102776877\n",
      "ho: 0.999930102776877\n",
      "paia: 0.999930102776877\n",
      "wingti: 0.999930102776877\n",
      "shockeffect: 0.999930102776877\n",
      "edwardo: 0.999930102776877\n",
      "meansprovid: 0.999930102776877\n",
      "skype: 0.999930102776877\n",
      "negotiationsto: 0.999930102776877\n",
      "shay: 0.999930102776877\n",
      "shamrock: 0.999930102776877\n",
      "disanchor: 0.999930102776877\n",
      "swaraj: 0.999930102776877\n",
      "libertiesi: 0.999930102776877\n",
      "sta: 0.999930102776877\n",
      "geneticist: 0.999930102776877\n",
      "plasm: 0.999930102776877\n",
      "alamogordo: 0.999930102776877\n",
      "interspac: 0.999930102776877\n",
      "nonindiffer: 0.999930102776877\n",
      "cariccm: 0.999930102776877\n",
      "climatefuel: 0.999930102776877\n",
      "highvalu: 0.999930102776877\n",
      "vakalevu: 0.999930102776877\n",
      "moralweight: 0.999930102776877\n",
      "yamoussoukrou: 0.999930102776877\n",
      "igreat: 0.999930102776877\n",
      "parrilla: 0.999930102776877\n",
      "vjnite: 0.999930102776877\n",
      "nebi: 0.999930102776877\n",
      "sumwail: 0.999930102776877\n",
      "kofr: 0.999930102776877\n",
      "qaddum: 0.999930102776877\n",
      "citiesand: 0.999930102776877\n",
      "verdeanship: 0.999930102776877\n",
      "orograph: 0.999930102776877\n",
      "macaronesia: 0.999930102776877\n",
      "cesaria: 0.999930102776877\n",
      "evora: 0.999930102776877\n",
      "ribeira: 0.999930102776877\n",
      "cidad: 0.999930102776877\n",
      "velha: 0.999930102776877\n",
      "tarraf: 0.999930102776877\n",
      "leong: 0.999930102776877\n",
      "jime: 0.999930102776877\n",
      "bekaart: 0.999930102776877\n",
      "perti: 0.999930102776877\n",
      "blather: 0.999930102776877\n",
      "galla: 0.999930102776877\n",
      "angloethiopian: 0.999930102776877\n",
      "ointens: 0.999930102776877\n",
      "compulsorili: 0.999930102776877\n",
      "slab: 0.999930102776877\n",
      "blackmarket: 0.999930102776877\n",
      "kuczynski: 0.999930102776877\n",
      "retrogad: 0.999930102776877\n",
      "typewritten: 0.999930102776877\n",
      "jamahirya: 0.999930102776877\n",
      "tambroni: 0.999930102776877\n",
      "baab: 0.999930102776877\n",
      "crossspectrum: 0.999930102776877\n",
      "donorsí: 0.999930102776877\n",
      "inkind: 0.999930102776877\n",
      "yashil: 0.999930102776877\n",
      "makon: 0.999930102776877\n",
      "afound: 0.999930102776877\n",
      "mardi: 0.999930102776877\n",
      "liberationist: 0.999930102776877\n",
      "maslira: 0.999930102776877\n",
      "leastfund: 0.999930102776877\n",
      "maux: 0.999930102776877\n",
      "viennent: 0.999930102776877\n",
      "ces: 0.999930102776877\n",
      "jauteuil: 0.999930102776877\n",
      "inoccupé: 0.999930102776877\n"
     ]
    }
   ],
   "source": [
    "# == Count dictionary words\n",
    "\n",
    "# Load dictionaries            ##### How did they come up with this dictionary? Why did they exclude words?\n",
    "\n",
    "affect_path = os.path.join(data_dict, 'dictionary_affect.pkl')\n",
    "cognition_path = os.path.join(data_dict, 'dictionary_cognition.pkl')\n",
    "\n",
    "affect = joblib.load(affect_path)\n",
    "cognition = joblib.load(cognition_path)\n",
    "\n",
    "print(\"Contents of affect dictionary:\")\n",
    "print(affect)\n",
    "print(\"Number of words in affect dictionary:\", len(affect))\n",
    "\n",
    "print(\"\\nContents of cognition dictionary:\")\n",
    "print(cognition)\n",
    "print(\"Number of words in cognition dictionary:\", len(cognition))\n",
    "\n",
    "#with open(affect_path, 'rb') as f:\n",
    "  #  affect_dict = pickle.load(f)\n",
    "#print(\"Contents of affect dictionary:\")\n",
    "#print(affect_dict)\n",
    "#print(\"Number of words in affect dictionary:\", len(affect_dict))\n",
    "\n",
    "#with open(cognition_path, 'rb') as f:\n",
    "  #  cognition_dict = pickle.load(f)\n",
    "#print(\"Contents of cognition dictionary:\")\n",
    "#print(cognition_dict)\n",
    "#print(\"Number of words in cognition dictionary:\", len(cognition_dict))\n",
    "\n",
    "a = [[i, word_counts[i]] for i in affect if i in word_counts]\n",
    "c = [[i, word_counts[i]] for i in cognition if i in word_counts]\n",
    "\n",
    "a = sorted(a, key=lambda x: x[1], reverse=True)\n",
    "c = sorted(c, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "a = [[i[0], f\"({i[1]}),\"] for i in a]\n",
    "c = [[i[0], f\"({i[1]}),\"] for i in c]\n",
    "\n",
    "a1 = ' '.join(str(r) for v in a for r in v)\n",
    "c1 = ' '.join(str(r) for v in c for r in v)\n",
    "\n",
    "affect_out_path = os.path.join(data_freq, \"affect_words.txt\")\n",
    "cog_out_path = os.path.join(data_freq, \"cog_words.txt\")\n",
    "\n",
    "os.makedirs(data_freq, exist_ok=True)  # ensure directory exists\n",
    "\n",
    "with open(affect_out_path, \"w\") as output:\n",
    "    output.write(a1)\n",
    "\n",
    "with open(cog_out_path, \"w\") as output:\n",
    "    output.write(c1)\n",
    "\n",
    "\n",
    "# == Calculate weighted frequencies for all words\n",
    "\n",
    "# - downweights very common words by giving more importance to rare ones\n",
    "#word_counts = joblib.load(os.path.join(data_freq, 'word_counts.pkl'))\n",
    "\n",
    "l = sum(word_counts.values())\n",
    "\n",
    "a = 0.001\n",
    "word_counts_weighted = {k: a / (a + (v / l)) for k, v in word_counts.items()}\n",
    "\n",
    "joblib.dump(word_counts_weighted, os.path.join(data_freq, 'word_counts_weighted.pkl'))\n",
    "\n",
    "################################################################################ ISSUE##################\n",
    "# To print top 100 by weighted values, sort the dictionary by value descending:\n",
    "top_100_weighted = sorted(word_counts_weighted.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "\n",
    "print(\"Top 100 words by weighted frequency:\")\n",
    "for word, weight in top_100_weighted:\n",
    "    print(f\"{word}: {weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a7ec60-2d73-4c68-948c-592970c4ad93",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df626ded-5e49-4a34-a3d1-315ba370226f",
   "metadata": {},
   "source": [
    "## Final Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "0d0856a2-44f1-4304-b221-09a9ab72ad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_c)\n",
    "os.chdir(data_freq)\n",
    "\n",
    "count = joblib.load('word_counts.pkl')  # load stemmed counts\n",
    "\n",
    "# For each speech only keep tokens that appear at least 10x\n",
    "\n",
    "def select(lista):\n",
    "    for i in range(len(lista)):\n",
    "        x = lista[i][0]\n",
    "        y = lista[i][1]\n",
    "        y = [w for w in y if count.get(w, 0) >= 10]\n",
    "        lista[i] = [x, y]\n",
    "    return lista\n",
    "\n",
    "for data_path in preprocessed_files:\n",
    "    data = joblib.load(data_path)\n",
    "    data = select(data)\n",
    "    cleaned_path = data_path.replace('.pkl', '_final.pkl')\n",
    "    joblib.dump(data, cleaned_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86873a31-3f1e-4d31-aa16-0f2813161647",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
