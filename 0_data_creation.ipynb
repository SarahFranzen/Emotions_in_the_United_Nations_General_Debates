{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b41cb22-e190-48d5-9926-7ac6fac8310d",
   "metadata": {},
   "source": [
    "# Emotion and Reason in Political Language: Examining the UN General speeches\n",
    "## Script 1: Preprocessing & Token Frequencies\n",
    "## Author: Sarah Franzen\n",
    "\n",
    "### Instructions BEFORE running this script:\n",
    "- Ensure all required packages are installed. If not, set `InstallPackages = TRUE` (see code cells below).  \n",
    "- Set your working directory appropriately.  \n",
    "- The script will automatically create the required folder structure.  \n",
    "- Later, you will be asked to download the data from:  \n",
    "  https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/0TJX8Y  \n",
    "  and store it **unzipped** inside the created folder *data_original*\n",
    "\n",
    "\n",
    "### Description: \n",
    "- Extract documents from their original txt documents and store them as one csv\n",
    "- Data Cleaning and Pre-Processing\n",
    "- Count word frequencies and weight themhemnal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5588afc-105a-426e-9348-742a830a2732",
   "metadata": {},
   "source": [
    "## Setup, Installation and Verification of required Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ebb8987a-a0da-453e-97e2-aafa209d0e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "InstallPackages = False # Set this to True to install the following packages \n",
    "\n",
    "if InstallPackages:\n",
    "    import sys\n",
    "\n",
    "    packages = [\n",
    "        \"pandas\",\n",
    "        \"nltk\",\n",
    "        \"spacy\",\n",
    "        \"numpy\",\n",
    "        \"gensim\",\n",
    "        \"pycountry\",\n",
    "        \"wordcloud\",\n",
    "        \"matplotlib\",\n",
    "        \"tqdm\"\n",
    "    ]\n",
    "\n",
    "    for package in packages:\n",
    "        !{sys.executable} -m pip install {package}\n",
    "\n",
    "\n",
    "DownloadAdditions = False # Set this to True to download these additional resources\n",
    "if DownloadAdditions:\n",
    "    nltk.download(\"punkt\")\n",
    "    nltk.download(\"averaged_perceptron_tagger\")\n",
    "    spacy.cli.download('en_core_web_lg')         # Download spaCy English model (large)\n",
    "\n",
    "#########################\n",
    "# Check if all packages are included\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "32d7d499-0563-46e0-9730-914c9f106d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Import standard and third-party libraries for data processing, NLP, and visualization ==\n",
    "\n",
    "import gensim\n",
    "import joblib\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import pycountry\n",
    "import random\n",
    "import re\n",
    "import spacy\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "#from matplotlib.colors import ListedColormap\n",
    "from multiprocessing import Pool, freeze_support\n",
    "# from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOPWORDS\n",
    "from string import punctuation\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# === Initialize NLP Tools ===\n",
    "\n",
    "# Translator to remove punctuation\n",
    "translator = str.maketrans('', '', punctuation)\n",
    "\n",
    "# POS tagger (not used by SpaCy, but optionally available via NLTK)\n",
    "tagger = nltk.perceptron.PerceptronTagger()\n",
    "\n",
    "# Load SpaCy English model with unnecessary components disabled\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4d495156-bbb2-403c-86f2-83b47d8cb03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Folder structure created:\n",
      "- C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\n",
      "  - data_original\n",
      "  - dictionaries\n",
      "  - freq\n",
      "  - preprocessed\n",
      "  - temp\n",
      "  - tokenized\n"
     ]
    }
   ],
   "source": [
    "# === Set Working Directory and create folder structure ===\n",
    "\n",
    "# Prompt user to enter working directory path\n",
    "#wd = input(\"Please enter your working directory path (e.g., C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit): \").strip()\n",
    "\n",
    "# Change to the entered working directory\n",
    "#try:\n",
    "   # os.chdir(wd)\n",
    "    #print(f\"Working directory set to: {os.getcwd()}\")\n",
    "#except FileNotFoundError:\n",
    "   # print(\"ERROR: The directory you entered does not exist. Please restart and enter a valid path.\")\n",
    "    #exit(1)\n",
    "\n",
    "# Set your working directory (adjust this as needed)\n",
    "wd = r\"C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\"\n",
    "os.chdir(wd)\n",
    "\n",
    "# Set base path to current working directory\n",
    "base_path = Path.cwd()\n",
    "data_path = base_path / \"data\"\n",
    "\n",
    "# List of subfolders to create inside 'data'\n",
    "subfolders = [\"data_original\", \"dictionaries\", \"freq\", \"preprocessed\", \"temp\", \"tokenized\"]\n",
    "\n",
    "# Create 'data' folder if it doesn't exist\n",
    "data_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Create subfolders\n",
    "for name in subfolders:\n",
    "    (data_path / name).mkdir(exist_ok=True)\n",
    "\n",
    "print(\"\\nFolder structure created:\")\n",
    "print(f\"- {data_path}\")\n",
    "for name in subfolders:\n",
    "    print(f\"  - {name}\")\n",
    "\n",
    "# Prompt user to place raw data files\n",
    "#print(f\"\\nPlease place your raw data files (unzipped) into the folder:\\n  {data_path / 'data_original'}\")\n",
    "#input(\"Press Enter after you have placed the files to continue...\")\n",
    "\n",
    "#print(\"Continuing with the script...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "56fa9a21-650f-4d8e-8a21-3d9629655c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define Folder Paths ===\n",
    "\n",
    "# Make sure that you have these folders in your working directory\n",
    "data_c = os.path.join(wd, 'data')\n",
    "data_temp = os.path.join(data_c, 'temp')\n",
    "data_freq = os.path.join(data_c, 'freq')\n",
    "data_dict = os.path.join(data_c, 'dictionaries')\n",
    "data_preprocessed = os.path.join(data_c, 'preprocessed')\n",
    "data_tokenized = os.path.join(data_c, 'tokenized')\n",
    "fig = os.path.join(wd, 'fig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "296fc7d6-a2ac-406e-a322-6c9f2702ee1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\n"
     ]
    }
   ],
   "source": [
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff34b6-cbee-4f8c-8076-5f385f596b58",
   "metadata": {},
   "source": [
    "## Load and Prepare Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18067b9-6337-4fb3-a570-603083a7deae",
   "metadata": {},
   "source": [
    "### This chunk can be skipped at the moment\n",
    "### Think of proper header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "93d247a0-5ff2-4062-add9-f3267ae1fe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¾ Total speeches found: 10761\n",
      "\n",
      "âœ… Saved raw data with 800 speeches to '.\\data\\un_corpus_raw.csv'\n"
     ]
    }
   ],
   "source": [
    "# == Load and Save Sample from UN General Debate Corpus ==\n",
    "\n",
    "# Set Folder path containing the original TXT files    \n",
    "base_folder = r\".\\data\\data_original\\UN General Debate Corpus\\UNGDC_1946-2023\\TXT\"\n",
    "\n",
    "#  Gather all relevant txt-files\n",
    "all_txt_files = []\n",
    "for root, dirs, files in os.walk(base_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.txt') and not file.startswith('._'):\n",
    "            all_txt_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"ðŸ§¾ Total speeches found: {len(all_txt_files)}\")\n",
    "\n",
    "# Randomly pick 800 files from the full collection   ###################################################### REMOVE AT LATER POINT\n",
    "sampled_files = random.sample(all_txt_files,800)\n",
    "\n",
    "# Read the selected files into a list\n",
    "raw_data = []\n",
    "for filepath in sampled_files:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        raw_data.append({'filename': os.path.basename(filepath), 'speech': content})\n",
    "\n",
    "# Create DataFrame from the collected speeches\n",
    "df_raw = pd.DataFrame(raw_data)\n",
    "\n",
    "df_raw = df_raw[df_raw['filename'] != '.DS_Store-to-UTF-8.txt'].copy()\n",
    "\n",
    "# Save df_raw as a pickle file for quick future loading\n",
    "raw_pickle_path = r\".\\data\\un_corpus_raw.pkl\"\n",
    "df_raw.to_pickle(raw_pickle_path)\n",
    "\n",
    "# Export df as CSV \n",
    "raw_output_path = r\".\\data\\un_corpus_raw.csv\"\n",
    "df_raw.to_csv(raw_output_path, index=False, sep=';', encoding='utf-8')\n",
    "\n",
    "print(f\"\\nâœ… Saved raw data with {len(df_raw)} speeches to '{raw_output_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6405ff20-92fa-4c97-becb-ac5fff70f84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WSM_72_2017.txt</td>\n",
       "      <td>Samoa warmly welcomes the assumption of Mr. Mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KGZ_76_2021.txt</td>\n",
       "      <td>Mr. President, Mr. Secretary-General, ladies a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MCO_70_2015.txt</td>\n",
       "      <td>The successful outcome of the United Nations S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BGD_53_1998.txt</td>\n",
       "      <td>May I convey to you, Sir,\\non behalf of my del...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISL_28_1973.txt</td>\n",
       "      <td>ï»¿111.\\t Mr. President, at the outset permit me...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech\n",
       "0  WSM_72_2017.txt  Samoa warmly welcomes the assumption of Mr. Mi...\n",
       "1  KGZ_76_2021.txt  Mr. President, Mr. Secretary-General, ladies a...\n",
       "2  MCO_70_2015.txt  The successful outcome of the United Nations S...\n",
       "3  BGD_53_1998.txt  May I convey to you, Sir,\\non behalf of my del...\n",
       "4  ISL_28_1973.txt  ï»¿111.\\t Mr. President, at the outset permit me..."
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# == Check if everything worked & drop empty speeches ==\n",
    "\n",
    "# Load df_raw\n",
    "df_raw = pd.read_pickle(r\".\\data\\un_corpus_raw.pkl\")\n",
    "\n",
    "# Drop empty speeches\n",
    "df_raw['speech'] = df_raw['speech'].astype(str)\n",
    "df_raw = df_raw[df_raw['speech'].str.strip() != ''].copy()\n",
    "\n",
    "# View df to check structure\n",
    "df_raw.head()         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5928538b-8519-40e2-bdee-ac72024f9f48",
   "metadata": {},
   "source": [
    "### Length of raw speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "9841ec29-b451-419b-957e-7803fbe9401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column: speech length in words\n",
    "df_raw['speech_length_words'] = df_raw['speech'].apply(lambda x: len(str(x).split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "1099cb66-71f8-4061-8b82-2e9102acca75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 shortest speeches:\n",
      "            filename                          country_name  year  \\\n",
      "731  GNB_76_2021.txt                         Guinea-Bissau  2021   \n",
      "13   SYC_52_1997.txt                            Seychelles  1997   \n",
      "391  UGA_69_2014.txt                                Uganda  2014   \n",
      "382  GHA_60_2005.txt                                 Ghana  2005   \n",
      "379  BRN_62_2007.txt                     Brunei Darussalam  2007   \n",
      "738  RWA_78_2023.txt                                Rwanda  2023   \n",
      "270  CZE_71_2016.txt                               Czechia  2016   \n",
      "477  UZB_71_2016.txt                            Uzbekistan  2016   \n",
      "93   LVA_55_2000.txt                                Latvia  2000   \n",
      "130  YEM_28_1973.txt                                 Yemen  1973   \n",
      "468  COD_67_2012.txt  The Democratic Republic of the Congo  2012   \n",
      "634  LVA_68_2013.txt                                Latvia  2013   \n",
      "542  UZB_69_2014.txt                            Uzbekistan  2014   \n",
      "153  USA_75_2020.txt                         United States  2020   \n",
      "392  EST_55_2000.txt                               Estonia  2000   \n",
      "441  SOM_62_2007.txt                               Somalia  2007   \n",
      "261  BTN_59_2004.txt                                Bhutan  2004   \n",
      "217  NAM_67_2012.txt                               Namibia  2012   \n",
      "154  BFA_60_2005.txt                          Burkina Faso  2005   \n",
      "283  PRT_57_2002.txt                              Portugal  2002   \n",
      "\n",
      "     speech_length_words  \n",
      "731                  480  \n",
      "13                   597  \n",
      "391                  720  \n",
      "382                  738  \n",
      "379                  767  \n",
      "738                  805  \n",
      "270                  895  \n",
      "477                  915  \n",
      "93                   930  \n",
      "130                  948  \n",
      "468                  950  \n",
      "634                  960  \n",
      "542                  966  \n",
      "153                  981  \n",
      "392                  997  \n",
      "441                  998  \n",
      "261                 1034  \n",
      "217                 1046  \n",
      "154                 1051  \n",
      "283                 1054  \n",
      "\n",
      "20 longest speeches:\n",
      "            filename    country_name  year  speech_length_words\n",
      "607  IND_10_1955.txt           India  1955                18053\n",
      "647  IND_08_1953.txt           India  1953                16080\n",
      "587  IDN_16_1961.txt       Indonesia  1961                11784\n",
      "249  RUS_08_1953.txt          Russia  1953                11568\n",
      "327  NGA_16_1961.txt         Nigeria  1961                10056\n",
      "155  POL_07_1952.txt          Poland  1952                 9773\n",
      "161  LBN_03_1948.txt         Lebanon  1948                 9565\n",
      "648  EGY_15_1960.txt           Egypt  1960                 8425\n",
      "387  ALB_26_1971.txt         Albania  1971                 7945\n",
      "86   FRA_07_1952.txt          France  1952                 7921\n",
      "595  DEU_33_1978.txt         Germany  1978                 7248\n",
      "333  SYR_24_1969.txt           Syria  1969                 7206\n",
      "115  PHL_23_1968.txt     Philippines  1968                 7055\n",
      "677  IRL_32_1977.txt         Ireland  1977                 7041\n",
      "576  RUS_03_1948.txt          Russia  1948                 6888\n",
      "656  SLV_25_1970.txt     El Salvador  1970                 6853\n",
      "529  IRN_39_1984.txt            Iran  1984                 6755\n",
      "381  VEN_29_1974.txt       Venezuela  1974                 6405\n",
      "523  GBR_08_1953.txt  United Kingdom  1953                 6297\n",
      "355  RUS_34_1979.txt          Russia  1979                 6266\n"
     ]
    }
   ],
   "source": [
    "# Count words in each speech\n",
    "df_raw['speech_length_words'] = df_raw['speech'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Calculate average length\n",
    "avg_length = df_raw['speech_length_words'].mean()\n",
    "\n",
    "# Print it\n",
    "print(\"Average speech length (words):\", round(avg_length, 2))\n",
    "\n",
    "# 20 shortest speeches\n",
    "print(\"20 shortest speeches:\")\n",
    "print(df_raw.nsmallest(20, 'speech_length_words')[['filename', 'country_name', 'year', 'speech_length_words']])\n",
    "\n",
    "# 20 longest speeches\n",
    "print(\"\\n20 longest speeches:\")\n",
    "print(df_raw.nlargest(20, 'speech_length_words')[['filename', 'country_name', 'year', 'speech_length_words']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "73e015c9-9288-4297-8007-0ea295473afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average speech length (words): 2922.5\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee7d3162-ebbd-449b-a2be-1148fceda725",
   "metadata": {},
   "source": [
    "### Create new variables: year, country_code and country_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "162da07c-1112-43e8-a434-2bef0ed32de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min year: 1948\n",
      "Max year: 2023\n",
      "Missing codes: []\n",
      "df_raw saved to C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\un_corpus_raw.pkl\n"
     ]
    }
   ],
   "source": [
    "# Extract country code (first 3 letters) and year (last 4 digits before .txt)\n",
    "df_raw['country_code'] = df_raw['filename'].str.extract(r'^([A-Z]{2,3})')\n",
    "df_raw['year'] = df_raw['filename'].str.extract(r'_(\\d{4})\\.txt$').astype(int)\n",
    "\n",
    "print(\"Min year:\", df_raw['year'].min())\n",
    "print(\"Max year:\", df_raw['year'].max())\n",
    "\n",
    "# Match country codes to country names\n",
    "code_to_name = {country.alpha_3: country.name for country in pycountry.countries}\n",
    "\n",
    "# Add custom short names and legacy codes\n",
    "custom_names = {\n",
    "    \"BOL\": \"Bolivia\",\n",
    "    \"COD\": \"The Democratic Republic of the Congo\",\n",
    "    \"IRN\": \"Iran\",\n",
    "    \"LAO\": \"Laos\",\n",
    "    \"MDA\": \"Moldova\",\n",
    "    \"PRK\": \"North Korea\",\n",
    "    \"PSE\": \"Palestine\",\n",
    "    \"RUS\": \"Russia\",\n",
    "    \"SYR\": \"Syria\",\n",
    "    \"TZA\": \"Tanzania\",\n",
    "    \"VAT\": \"Vatican City State\",\n",
    "    \"VEN\": \"Venezuela\",\n",
    "    \"VNM\": \"Vietnam\",\n",
    "    \"YMD\": \"South Yemen\",\n",
    "    \"YUG\": \"Yugoslavia\",\n",
    "    \"DDR\": \"East Germany\",\n",
    "    # \"SUN\": \"Soviet Union\",\n",
    "    \"EU\": \"European Union\",\n",
    "    \"CSK\": \"Czechoslovakia\",\n",
    "    \"FSM\": \"Micronesia\",\n",
    "    \"KOR\": \"South Korea\"\n",
    "    \n",
    "}\n",
    "\n",
    "# Update the main mapping with custom names\n",
    "code_to_name.update(custom_names)\n",
    "\n",
    "# Map with updated dictionary\n",
    "df_raw['country_name'] = df_raw['country_code'].map(code_to_name)\n",
    "\n",
    "# Check missing mappings\n",
    "missing = df_raw.loc[df_raw['country_name'].isna(), 'country_code'].unique()\n",
    "print(\"Missing codes:\", missing)\n",
    "\n",
    "# Check structure of the df\n",
    "df_raw.head() \n",
    "\n",
    "save_path = os.path.join(data_c, 'un_corpus_raw.pkl')\n",
    "df_raw.to_pickle(save_path)\n",
    "print(f\"df_raw saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "383d2904-f015-4c9d-9aeb-51a94f03f950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    country_code                          country_name\n",
      "0            AFG                           Afghanistan\n",
      "1            AGO                                Angola\n",
      "2            ALB                               Albania\n",
      "3            AND                               Andorra\n",
      "4            ARE                  United Arab Emirates\n",
      "5            ARG                             Argentina\n",
      "6            ARM                               Armenia\n",
      "7            ATG                   Antigua and Barbuda\n",
      "8            AUS                             Australia\n",
      "9            AUT                               Austria\n",
      "10           AZE                            Azerbaijan\n",
      "11           BDI                               Burundi\n",
      "12           BEL                               Belgium\n",
      "13           BEN                                 Benin\n",
      "14           BFA                          Burkina Faso\n",
      "15           BGD                            Bangladesh\n",
      "16           BGR                              Bulgaria\n",
      "17           BHR                               Bahrain\n",
      "18           BHS                               Bahamas\n",
      "19           BIH                Bosnia and Herzegovina\n",
      "20           BLR                               Belarus\n",
      "21           BLZ                                Belize\n",
      "22           BOL                               Bolivia\n",
      "23           BRA                                Brazil\n",
      "24           BRB                              Barbados\n",
      "25           BRN                     Brunei Darussalam\n",
      "26           BTN                                Bhutan\n",
      "27           BWA                              Botswana\n",
      "28           CAF              Central African Republic\n",
      "29           CAN                                Canada\n",
      "30           CHL                                 Chile\n",
      "31           CHN                                 China\n",
      "32           CIV                         CÃ´te d'Ivoire\n",
      "33           CMR                              Cameroon\n",
      "34           COD  The Democratic Republic of the Congo\n",
      "35           COG                                 Congo\n",
      "36           COL                              Colombia\n",
      "37           COM                               Comoros\n",
      "38           CPV                            Cabo Verde\n",
      "39           CRI                            Costa Rica\n",
      "40           CSK                        Czechoslovakia\n",
      "41           CUB                                  Cuba\n",
      "42           CYP                                Cyprus\n",
      "43           CZE                               Czechia\n",
      "44           DDR                          East Germany\n",
      "45           DEU                               Germany\n",
      "46           DJI                              Djibouti\n",
      "47           DNK                               Denmark\n",
      "48           DOM                    Dominican Republic\n",
      "49           DZA                               Algeria\n",
      "50           ECU                               Ecuador\n",
      "51           EGY                                 Egypt\n",
      "52           ERI                               Eritrea\n",
      "53           ESP                                 Spain\n",
      "54           EST                               Estonia\n",
      "55           ETH                              Ethiopia\n",
      "56           FIN                               Finland\n",
      "57           FJI                                  Fiji\n",
      "58           FRA                                France\n",
      "59           FSM                            Micronesia\n",
      "60           GAB                                 Gabon\n",
      "61           GBR                        United Kingdom\n",
      "62           GEO                               Georgia\n",
      "63           GHA                                 Ghana\n",
      "64           GIN                                Guinea\n",
      "65           GMB                                Gambia\n",
      "66           GNB                         Guinea-Bissau\n",
      "67           GNQ                     Equatorial Guinea\n",
      "68           GRC                                Greece\n",
      "69           GRD                               Grenada\n",
      "70           GTM                             Guatemala\n",
      "71           GUY                                Guyana\n",
      "72           HND                              Honduras\n",
      "73           HRV                               Croatia\n",
      "74           HTI                                 Haiti\n",
      "75           HUN                               Hungary\n",
      "76           IDN                             Indonesia\n",
      "77           IND                                 India\n",
      "78           IRL                               Ireland\n",
      "79           IRN                                  Iran\n",
      "80           IRQ                                  Iraq\n",
      "81           ISL                               Iceland\n",
      "82           ISR                                Israel\n",
      "83           JAM                               Jamaica\n",
      "84           JOR                                Jordan\n",
      "85           JPN                                 Japan\n",
      "86           KAZ                            Kazakhstan\n",
      "87           KEN                                 Kenya\n",
      "88           KGZ                            Kyrgyzstan\n",
      "89           KHM                              Cambodia\n",
      "90           KIR                              Kiribati\n",
      "91           KNA                 Saint Kitts and Nevis\n",
      "92           KOR                           South Korea\n",
      "93           KWT                                Kuwait\n",
      "94           LAO                                  Laos\n",
      "95           LBN                               Lebanon\n",
      "96           LBR                               Liberia\n",
      "97           LBY                                 Libya\n",
      "98           LCA                           Saint Lucia\n",
      "99           LIE                         Liechtenstein\n",
      "100          LKA                             Sri Lanka\n",
      "101          LSO                               Lesotho\n",
      "102          LTU                             Lithuania\n",
      "103          LUX                            Luxembourg\n",
      "104          LVA                                Latvia\n",
      "105          MAR                               Morocco\n",
      "106          MCO                                Monaco\n",
      "107          MDA                               Moldova\n",
      "108          MDV                              Maldives\n",
      "109          MEX                                Mexico\n",
      "110          MHL                      Marshall Islands\n",
      "111          MKD                       North Macedonia\n",
      "112          MLI                                  Mali\n",
      "113          MLT                                 Malta\n",
      "114          MMR                               Myanmar\n",
      "115          MNE                            Montenegro\n",
      "116          MNG                              Mongolia\n",
      "117          MOZ                            Mozambique\n",
      "118          MRT                            Mauritania\n",
      "119          MUS                             Mauritius\n",
      "120          MWI                                Malawi\n",
      "121          MYS                              Malaysia\n",
      "122          NAM                               Namibia\n",
      "123          NER                                 Niger\n",
      "124          NGA                               Nigeria\n",
      "125          NIC                             Nicaragua\n",
      "126          NLD                           Netherlands\n",
      "127          NOR                                Norway\n",
      "128          NPL                                 Nepal\n",
      "129          NRU                                 Nauru\n",
      "130          NZL                           New Zealand\n",
      "131          OMN                                  Oman\n",
      "132          PAK                              Pakistan\n",
      "133          PAN                                Panama\n",
      "134          PER                                  Peru\n",
      "135          PHL                           Philippines\n",
      "136          PLW                                 Palau\n",
      "137          PNG                      Papua New Guinea\n",
      "138          POL                                Poland\n",
      "139          PRK                           North Korea\n",
      "140          PRT                              Portugal\n",
      "141          PRY                              Paraguay\n",
      "142          PSE                             Palestine\n",
      "143          ROU                               Romania\n",
      "144          RUS                                Russia\n",
      "145          RWA                                Rwanda\n",
      "146          SAU                          Saudi Arabia\n",
      "147          SDN                                 Sudan\n",
      "148          SEN                               Senegal\n",
      "149          SGP                             Singapore\n",
      "150          SLB                       Solomon Islands\n",
      "151          SLE                          Sierra Leone\n",
      "152          SLV                           El Salvador\n",
      "153          SMR                            San Marino\n",
      "154          SOM                               Somalia\n",
      "155          SUR                              Suriname\n",
      "156          SVK                              Slovakia\n",
      "157          SVN                              Slovenia\n",
      "158          SWE                                Sweden\n",
      "159          SWZ                              Eswatini\n",
      "160          SYC                            Seychelles\n",
      "161          SYR                                 Syria\n",
      "162          TCD                                  Chad\n",
      "163          TGO                                  Togo\n",
      "164          THA                              Thailand\n",
      "165          TJK                            Tajikistan\n",
      "166          TLS                           Timor-Leste\n",
      "167          TON                                 Tonga\n",
      "168          TTO                   Trinidad and Tobago\n",
      "169          TUN                               Tunisia\n",
      "170          TUR                               TÃ¼rkiye\n",
      "171          TUV                                Tuvalu\n",
      "172          TZA                              Tanzania\n",
      "173          UGA                                Uganda\n",
      "174          UKR                               Ukraine\n",
      "175          URY                               Uruguay\n",
      "176          USA                         United States\n",
      "177          UZB                            Uzbekistan\n",
      "178          VAT                    Vatican City State\n",
      "179          VCT      Saint Vincent and the Grenadines\n",
      "180          VEN                             Venezuela\n",
      "181          VNM                               Vietnam\n",
      "182          VUT                               Vanuatu\n",
      "183          WSM                                 Samoa\n",
      "184          YEM                                 Yemen\n",
      "185          YMD                            Soth Yemen\n",
      "186          YUG                            Yugoslavia\n",
      "187          ZAF                          South Africa\n",
      "188          ZMB                                Zambia\n",
      "189          ZWE                              Zimbabwe\n"
     ]
    }
   ],
   "source": [
    "# == Check the country names \n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(df_raw[['country_code', 'country_name']].drop_duplicates().sort_values('country_code').reset_index(drop=True))\n",
    "# Reset to default afterward\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523547d9-9b10-473c-8c85-d1e2fc4fd3e1",
   "metadata": {},
   "source": [
    "### Create variable speaker & position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "213e961b-816c-43f1-8ad6-a17bae6cca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_c)\n",
    "df_speakers = pd.read_excel(r\"data_original\\UN General Debate Corpus\\Speakers_by_session.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "cd158957-391b-4b86-9160-7c606336bacc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Session</th>\n",
       "      <th>ISO Code</th>\n",
       "      <th>Country</th>\n",
       "      <th>Name of Person Speaking</th>\n",
       "      <th>Post</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>78</td>\n",
       "      <td>BRA</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Luiz Inacio Lula da Silva</td>\n",
       "      <td>President</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023</td>\n",
       "      <td>78</td>\n",
       "      <td>USA</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>Joseph R. Biden</td>\n",
       "      <td>President</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023</td>\n",
       "      <td>78</td>\n",
       "      <td>COL</td>\n",
       "      <td>Colombia</td>\n",
       "      <td>Gustavo Petro Urrego</td>\n",
       "      <td>President</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023</td>\n",
       "      <td>78</td>\n",
       "      <td>JOR</td>\n",
       "      <td>Jordan</td>\n",
       "      <td>Abdullah II ibn Al Hussein</td>\n",
       "      <td>King</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023</td>\n",
       "      <td>78</td>\n",
       "      <td>POL</td>\n",
       "      <td>Poland</td>\n",
       "      <td>Andrzej Duda</td>\n",
       "      <td>President</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Session ISO Code                   Country  \\\n",
       "0  2023       78      BRA                   Brazil    \n",
       "1  2023       78      USA  United States of America   \n",
       "2  2023       78      COL                  Colombia   \n",
       "3  2023       78      JOR                    Jordan   \n",
       "4  2023       78      POL                    Poland   \n",
       "\n",
       "      Name of Person Speaking       Post Unnamed: 6  \n",
       "0   Luiz Inacio Lula da Silva  President        NaN  \n",
       "1             Joseph R. Biden  President        NaN  \n",
       "2        Gustavo Petro Urrego  President        NaN  \n",
       "3  Abdullah II ibn Al Hussein       King        NaN  \n",
       "4                Andrzej Duda  President        NaN  "
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_speakers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f094c95f-834e-4454-9442-0d28d4a66090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "      <th>country_code</th>\n",
       "      <th>year</th>\n",
       "      <th>country_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WSM_72_2017.txt</td>\n",
       "      <td>Samoa warmly welcomes the assumption of Mr. Mi...</td>\n",
       "      <td>WSM</td>\n",
       "      <td>2017</td>\n",
       "      <td>Samoa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KGZ_76_2021.txt</td>\n",
       "      <td>Mr. President, Mr. Secretary-General, ladies a...</td>\n",
       "      <td>KGZ</td>\n",
       "      <td>2021</td>\n",
       "      <td>Kyrgyzstan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MCO_70_2015.txt</td>\n",
       "      <td>The successful outcome of the United Nations S...</td>\n",
       "      <td>MCO</td>\n",
       "      <td>2015</td>\n",
       "      <td>Monaco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BGD_53_1998.txt</td>\n",
       "      <td>May I convey to you, Sir,\\non behalf of my del...</td>\n",
       "      <td>BGD</td>\n",
       "      <td>1998</td>\n",
       "      <td>Bangladesh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISL_28_1973.txt</td>\n",
       "      <td>ï»¿111.\\t Mr. President, at the outset permit me...</td>\n",
       "      <td>ISL</td>\n",
       "      <td>1973</td>\n",
       "      <td>Iceland</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech  \\\n",
       "0  WSM_72_2017.txt  Samoa warmly welcomes the assumption of Mr. Mi...   \n",
       "1  KGZ_76_2021.txt  Mr. President, Mr. Secretary-General, ladies a...   \n",
       "2  MCO_70_2015.txt  The successful outcome of the United Nations S...   \n",
       "3  BGD_53_1998.txt  May I convey to you, Sir,\\non behalf of my del...   \n",
       "4  ISL_28_1973.txt  ï»¿111.\\t Mr. President, at the outset permit me...   \n",
       "\n",
       "  country_code  year country_name  \n",
       "0          WSM  2017        Samoa  \n",
       "1          KGZ  2021   Kyrgyzstan  \n",
       "2          MCO  2015       Monaco  \n",
       "3          BGD  1998   Bangladesh  \n",
       "4          ISL  1973      Iceland  "
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "170a6cc9-fa93-454c-9f81-f489a84afe1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [filename, speech, country_code, year, country_name]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(df_raw[(df_raw['country_code'] == 'MEX') & (df_raw['year'] == 1982)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "672de39e-93ba-4808-82fd-f92b122f0843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "      <th>country_code</th>\n",
       "      <th>year</th>\n",
       "      <th>country_name</th>\n",
       "      <th>Year</th>\n",
       "      <th>ISO Code</th>\n",
       "      <th>Name of Person Speaking</th>\n",
       "      <th>Post</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WSM_72_2017.txt</td>\n",
       "      <td>Samoa warmly welcomes the assumption of Mr. Mi...</td>\n",
       "      <td>WSM</td>\n",
       "      <td>2017</td>\n",
       "      <td>Samoa</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>WSM</td>\n",
       "      <td>Mr. Tuilaepa Sailele Malielegaoi</td>\n",
       "      <td>Prime Minister and Minister for Foreign Affair...</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KGZ_76_2021.txt</td>\n",
       "      <td>Mr. President, Mr. Secretary-General, ladies a...</td>\n",
       "      <td>KGZ</td>\n",
       "      <td>2021</td>\n",
       "      <td>Kyrgyzstan</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>KGZ</td>\n",
       "      <td>Sadyr Japarov</td>\n",
       "      <td>President</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MCO_70_2015.txt</td>\n",
       "      <td>The successful outcome of the United Nations S...</td>\n",
       "      <td>MCO</td>\n",
       "      <td>2015</td>\n",
       "      <td>Monaco</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>MCO</td>\n",
       "      <td>Mr. Gilles Tonelli</td>\n",
       "      <td>Minister for Foreign Affairs</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BGD_53_1998.txt</td>\n",
       "      <td>May I convey to you, Sir,\\non behalf of my del...</td>\n",
       "      <td>BGD</td>\n",
       "      <td>1998</td>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>BGD</td>\n",
       "      <td>Abdus Samad Azad</td>\n",
       "      <td>Minister for Foreign Affairs</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISL_28_1973.txt</td>\n",
       "      <td>ï»¿111.\\t Mr. President, at the outset permit me...</td>\n",
       "      <td>ISL</td>\n",
       "      <td>1973</td>\n",
       "      <td>Iceland</td>\n",
       "      <td>1973.0</td>\n",
       "      <td>ISL</td>\n",
       "      <td>Agustsson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech  \\\n",
       "0  WSM_72_2017.txt  Samoa warmly welcomes the assumption of Mr. Mi...   \n",
       "1  KGZ_76_2021.txt  Mr. President, Mr. Secretary-General, ladies a...   \n",
       "2  MCO_70_2015.txt  The successful outcome of the United Nations S...   \n",
       "3  BGD_53_1998.txt  May I convey to you, Sir,\\non behalf of my del...   \n",
       "4  ISL_28_1973.txt  ï»¿111.\\t Mr. President, at the outset permit me...   \n",
       "\n",
       "  country_code  year country_name    Year ISO Code  \\\n",
       "0          WSM  2017        Samoa  2017.0      WSM   \n",
       "1          KGZ  2021   Kyrgyzstan  2021.0      KGZ   \n",
       "2          MCO  2015       Monaco  2015.0      MCO   \n",
       "3          BGD  1998   Bangladesh  1998.0      BGD   \n",
       "4          ISL  1973      Iceland  1973.0      ISL   \n",
       "\n",
       "            Name of Person Speaking  \\\n",
       "0  Mr. Tuilaepa Sailele Malielegaoi   \n",
       "1                     Sadyr Japarov   \n",
       "2                Mr. Gilles Tonelli   \n",
       "3                  Abdus Samad Azad   \n",
       "4                         Agustsson   \n",
       "\n",
       "                                                Post _merge  \n",
       "0  Prime Minister and Minister for Foreign Affair...   both  \n",
       "1                                         President    both  \n",
       "2                       Minister for Foreign Affairs   both  \n",
       "3                       Minister for Foreign Affairs   both  \n",
       "4                                                NaN   both  "
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged = df_raw.merge(\n",
    "    df_speakers[['Year', 'ISO Code', 'Name of Person Speaking', 'Post']],\n",
    "    left_on=['year', 'country_code'],\n",
    "    right_on=['Year', 'ISO Code'],\n",
    "    how='left',\n",
    "    indicator=True)\n",
    "\n",
    "df_merged.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "12a6be0c-1476-4f7f-9d23-1d8f5a9f6896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "      <th>country_code</th>\n",
       "      <th>year</th>\n",
       "      <th>country_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>SLE_17_1962.txt</td>\n",
       "      <td>34. Almost exactly a year ago today my Prime M...</td>\n",
       "      <td>SLE</td>\n",
       "      <td>1962</td>\n",
       "      <td>Sierra Leone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename                                             speech  \\\n",
       "164  SLE_17_1962.txt  34. Almost exactly a year ago today my Prime M...   \n",
       "\n",
       "    country_code  year  country_name  \n",
       "164          SLE  1962  Sierra Leone  "
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw[(df_raw['year'] == 1962) & (df_raw['country_name'] == \"Sierra Leone\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "604ceb1f-a9fc-49ec-aca8-fdc28ad7e73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            filename  year country_code              country_name\n",
      "11   GAB_49_1994.txt  1994          GAB                     Gabon\n",
      "147  CAF_18_1963.txt  1963          CAF  Central African Republic\n",
      "428  MOZ_49_1994.txt  1994          MOZ                Mozambique\n",
      "538  DOM_18_1963.txt  1963          DOM        Dominican Republic\n",
      "592  ECU_18_1963.txt  1963          ECU                   Ecuador\n",
      "663  UKR_18_1963.txt  1963          UKR                   Ukraine\n",
      "712  YMD_39_1984.txt  1984          YMD                Soth Yemen\n"
     ]
    }
   ],
   "source": [
    "# Merge with indicator and set unmatched rows to NA\n",
    "df_merged = df_raw.merge(\n",
    "    df_speakers[['Year', 'ISO Code', 'Name of Person Speaking', 'Post']],\n",
    "    left_on=['year', 'country_code'],\n",
    "    right_on=['Year', 'ISO Code'],\n",
    "    how='left',\n",
    "    indicator=True)\n",
    "\n",
    "# Get rows with no match in df_speakers\n",
    "unmatched = df_merged[df_merged['_merge'] == 'left_only']\n",
    "\n",
    "# Print unmatched rows with selected columns (panda sets them to NA by default)\n",
    "print(unmatched[['filename', 'year', 'country_code', 'country_name']])\n",
    "\n",
    "\n",
    "# Drop the '_merge' column from merged df\n",
    "df_merged = df_merged.drop(columns=['Year', 'ISO Code', '_merge'])\n",
    "\n",
    "# Rename columns\n",
    "df_merged = df_merged.rename(columns={\n",
    "    'Name of Person Speaking': 'speaker_name',\n",
    "    'Post': 'position'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "27a2539a-e2f6-47b6-b867-f3bab0cf8feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually search for the speakers of the Missings\n",
    "\n",
    "# Assign speaker names (or NA) based on country and year\n",
    "df_merged.loc[\n",
    "    (df_merged['country_name'] == 'European Union') & (df_merged['year'] == 2013),\n",
    "    'speaker_name'\n",
    "] = 'Mr. Herman Van Rompuy'\n",
    "\n",
    "df_merged.loc[\n",
    "    (df_merged['country_name'] == 'Sierra Leone') & (df_merged['year'] == 1962),\n",
    "    'speaker_name'\n",
    "] = np.nan\n",
    "\n",
    "\n",
    "# No reliable resource found for Sierra Leone\n",
    "# https://www.prnewswire.com/news-releases/eu-newsbrief-address-by-european-council-president-van-rompuy-to-the-un-general-assembly-225266212.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "2f9ba91d-cf5c-43e7-9fae-e35c868fbdf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "      <th>country_code</th>\n",
       "      <th>year</th>\n",
       "      <th>country_name</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WSM_72_2017.txt</td>\n",
       "      <td>Samoa warmly welcomes the assumption of Mr. Mi...</td>\n",
       "      <td>WSM</td>\n",
       "      <td>2017</td>\n",
       "      <td>Samoa</td>\n",
       "      <td>Mr. Tuilaepa Sailele Malielegaoi</td>\n",
       "      <td>Prime Minister and Minister for Foreign Affair...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KGZ_76_2021.txt</td>\n",
       "      <td>Mr. President, Mr. Secretary-General, ladies a...</td>\n",
       "      <td>KGZ</td>\n",
       "      <td>2021</td>\n",
       "      <td>Kyrgyzstan</td>\n",
       "      <td>Sadyr Japarov</td>\n",
       "      <td>President</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MCO_70_2015.txt</td>\n",
       "      <td>The successful outcome of the United Nations S...</td>\n",
       "      <td>MCO</td>\n",
       "      <td>2015</td>\n",
       "      <td>Monaco</td>\n",
       "      <td>Mr. Gilles Tonelli</td>\n",
       "      <td>Minister for Foreign Affairs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BGD_53_1998.txt</td>\n",
       "      <td>May I convey to you, Sir,\\non behalf of my del...</td>\n",
       "      <td>BGD</td>\n",
       "      <td>1998</td>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>Abdus Samad Azad</td>\n",
       "      <td>Minister for Foreign Affairs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISL_28_1973.txt</td>\n",
       "      <td>ï»¿111.\\t Mr. President, at the outset permit me...</td>\n",
       "      <td>ISL</td>\n",
       "      <td>1973</td>\n",
       "      <td>Iceland</td>\n",
       "      <td>Agustsson</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech  \\\n",
       "0  WSM_72_2017.txt  Samoa warmly welcomes the assumption of Mr. Mi...   \n",
       "1  KGZ_76_2021.txt  Mr. President, Mr. Secretary-General, ladies a...   \n",
       "2  MCO_70_2015.txt  The successful outcome of the United Nations S...   \n",
       "3  BGD_53_1998.txt  May I convey to you, Sir,\\non behalf of my del...   \n",
       "4  ISL_28_1973.txt  ï»¿111.\\t Mr. President, at the outset permit me...   \n",
       "\n",
       "  country_code  year country_name                      speaker_name  \\\n",
       "0          WSM  2017        Samoa  Mr. Tuilaepa Sailele Malielegaoi   \n",
       "1          KGZ  2021   Kyrgyzstan                     Sadyr Japarov   \n",
       "2          MCO  2015       Monaco                Mr. Gilles Tonelli   \n",
       "3          BGD  1998   Bangladesh                  Abdus Samad Azad   \n",
       "4          ISL  1973      Iceland                         Agustsson   \n",
       "\n",
       "                                            position  \n",
       "0  Prime Minister and Minister for Foreign Affair...  \n",
       "1                                         President   \n",
       "2                       Minister for Foreign Affairs  \n",
       "3                       Minister for Foreign Affairs  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424f152-b786-45bf-bf86-e169a900f628",
   "metadata": {},
   "source": [
    "### Create variable 'Amtssprache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "d1d2c2a6-23d6-4044-84cc-8785167e6b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries not matched in df_raw['country_name']:\n",
      "Anguilla\n",
      "Bermuda\n",
      "British Virgin Islands\n",
      "Cayman Islands\n",
      "Christmas Island\n",
      "Cook Islands\n",
      "Dominica\n",
      "Hong Kong\n",
      "Jersey\n",
      "Niue\n",
      "Norfolk Island\n",
      "Northern Mariana Islands\n",
      "Pitcairn Islands\n",
      "Sint Maarten\n",
      "South Sudan\n",
      "Turks and Caicos Islands\n"
     ]
    }
   ],
   "source": [
    "# Source for english as official language : https://gradschool.utk.edu/future-students/office-of-graduate-admissions/applying-to-graduate-school/admission-requirements/testing-requirements/countries-with-english-as-official-language/\n",
    "# They are quoting: https://www.cia.gov/the-world-factbook/field/languages/\n",
    "\n",
    "english_countries = [\n",
    "    \"Anguilla\", \"Antigua and Barbuda\", \"Bahamas\", \"Barbados\", \"Belize\", \"Belgium\",\n",
    "    \"Bermuda\", \"Botswana\", \"British Virgin Islands\", \"Burundi\", \"Cameroon\", \"Canada\",\n",
    "    \"Cayman Islands\", \"Christmas Island\", \"Cook Islands\", \"Dominica\", \"Fiji\", \"Gambia\",\n",
    "    \"Ghana\", \"Grenada\", \"Guyana\", \"Hong Kong\", \"India\", \"Ireland\", \"Jersey\", \"Kenya\",\n",
    "    \"Liberia\", \"Malawi\", \"Malta\", \"Marshall Islands\", \"Micronesia\",\n",
    "    \"Namibia\", \"New Zealand\", \"Nigeria\", \"Niue\", \"Norfolk Island\", \"Northern Mariana Islands\",\n",
    "    \"Pakistan\", \"Palau\", \"Papua New Guinea\", \"Philippines\", \"Pitcairn Islands\", \"Rwanda\",\n",
    "    \"Saint Kitts and Nevis\", \"Saint Lucia\", \"Samoa\", \"Seychelles\", \"Sierra Leone\", \"Singapore\",\n",
    "    \"Sint Maarten\", \"Solomon Islands\", \"Somalia\", \"South Africa\", \"South Sudan\", \"Sudan\",\n",
    "    \"Eswatini\", \"Tanzania\", \"Tonga\", \"Trinidad and Tobago\", \"Turks and Caicos Islands\",\n",
    "    \"Tuvalu\", \"Uganda\", \"Zambia\", \"Zimbabwe\"\n",
    "]\n",
    "\n",
    "# 2. Create dummy column\n",
    "df_raw['englisch_official_language'] = df_raw['country_name'].apply(\n",
    "    lambda x: 1 if x in english_countries else 0\n",
    ")\n",
    "\n",
    "# 3. Find countries in the list that did not match any entry in df_raw\n",
    "matched = set(df_raw['country_name'])\n",
    "unmatched = [country for country in english_countries if country not in matched]\n",
    "\n",
    "# 4. Print unmatched country names\n",
    "print(\"Countries not matched in df_raw['country_name']:\")\n",
    "for country in unmatched:\n",
    "    print(country)\n",
    "\n",
    "# All of these countries are either British Overseas Territories, Australian Territories, self-governing island territories or Special Administrative Regions\n",
    "    # None of the unmatched regions are UN Members\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "18be2d52-b8d1-4cb1-a74d-491a523f26c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "      <th>country_code</th>\n",
       "      <th>year</th>\n",
       "      <th>country_name</th>\n",
       "      <th>englisch_official_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WSM_72_2017.txt</td>\n",
       "      <td>Samoa warmly welcomes the assumption of Mr. Mi...</td>\n",
       "      <td>WSM</td>\n",
       "      <td>2017</td>\n",
       "      <td>Samoa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KGZ_76_2021.txt</td>\n",
       "      <td>Mr. President, Mr. Secretary-General, ladies a...</td>\n",
       "      <td>KGZ</td>\n",
       "      <td>2021</td>\n",
       "      <td>Kyrgyzstan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MCO_70_2015.txt</td>\n",
       "      <td>The successful outcome of the United Nations S...</td>\n",
       "      <td>MCO</td>\n",
       "      <td>2015</td>\n",
       "      <td>Monaco</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BGD_53_1998.txt</td>\n",
       "      <td>May I convey to you, Sir,\\non behalf of my del...</td>\n",
       "      <td>BGD</td>\n",
       "      <td>1998</td>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISL_28_1973.txt</td>\n",
       "      <td>ï»¿111.\\t Mr. President, at the outset permit me...</td>\n",
       "      <td>ISL</td>\n",
       "      <td>1973</td>\n",
       "      <td>Iceland</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech  \\\n",
       "0  WSM_72_2017.txt  Samoa warmly welcomes the assumption of Mr. Mi...   \n",
       "1  KGZ_76_2021.txt  Mr. President, Mr. Secretary-General, ladies a...   \n",
       "2  MCO_70_2015.txt  The successful outcome of the United Nations S...   \n",
       "3  BGD_53_1998.txt  May I convey to you, Sir,\\non behalf of my del...   \n",
       "4  ISL_28_1973.txt  ï»¿111.\\t Mr. President, at the outset permit me...   \n",
       "\n",
       "  country_code  year country_name  englisch_official_language  \n",
       "0          WSM  2017        Samoa                           1  \n",
       "1          KGZ  2021   Kyrgyzstan                           0  \n",
       "2          MCO  2015       Monaco                           0  \n",
       "3          BGD  1998   Bangladesh                           0  \n",
       "4          ISL  1973      Iceland                           0  "
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82540d8a-b656-45fb-aeb0-2fd5b4d7108a",
   "metadata": {},
   "source": [
    "## Create variable for permanent member security council"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "4fb92fe4-5c22-43fa-8b48-62ec2240d716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define permanent members of the UN Security Council\n",
    "permanent_members = ['RUS', 'USA', 'FRA', 'GBR', 'CHN']\n",
    "\n",
    "# Create dummy variable\n",
    "df_merged['security_council_permanent'] = df_merged['country_code'].isin(permanent_members).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "df2ccbc5-b32e-46bf-a93f-0106e4462358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    country_code    country_name  security_council_permanent  year\n",
      "28           FRA          France                           1  1948\n",
      "48           CHN           China                           1  2000\n",
      "76           FRA          France                           1  1970\n",
      "86           FRA          France                           1  1952\n",
      "119          FRA          France                           1  1949\n",
      "153          USA   United States                           1  2020\n",
      "178          USA   United States                           1  1949\n",
      "184          USA   United States                           1  1987\n",
      "205          GBR  United Kingdom                           1  1993\n",
      "233          USA   United States                           1  1982\n",
      "249          RUS          Russia                           1  1953\n",
      "253          USA   United States                           1  2017\n",
      "257          CHN           China                           1  1949\n",
      "314          RUS          Russia                           1  2009\n",
      "355          RUS          Russia                           1  1979\n",
      "460          CHN           China                           1  1950\n",
      "500          RUS          Russia                           1  1998\n",
      "501          RUS          Russia                           1  2019\n",
      "523          GBR  United Kingdom                           1  1953\n",
      "541          FRA          France                           1  1990\n",
      "576          RUS          Russia                           1  1948\n",
      "590          FRA          France                           1  1977\n",
      "594          RUS          Russia                           1  2011\n",
      "596          USA   United States                           1  1979\n",
      "622          CHN           China                           1  2021\n",
      "637          GBR  United Kingdom                           1  1998\n",
      "707          CHN           China                           1  2007\n",
      "724          GBR  United Kingdom                           1  1980\n",
      "732          CHN           China                           1  1952\n",
      "755          USA   United States                           1  2004\n",
      "793          GBR  United Kingdom                           1  1966\n"
     ]
    }
   ],
   "source": [
    "print(df_merged[df_merged['country_code'].isin(permanent_members)][\n",
    "    ['country_code', 'country_name', 'security_council_permanent', 'year']\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "6b7ed94c-7369-4b52-a84e-21bbdccd51ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(wd)\n",
    "\n",
    "# Save df_merged as a pickle file for quick future loading\n",
    "merged_pickle_path = r\".\\data\\un_corpus_merged.pkl\"\n",
    "df_merged.to_pickle(merged_pickle_path)\n",
    "\n",
    "# Export df as CSV \n",
    "merged_output_path = r\".\\data\\un_corpus_raw.csv\"\n",
    "df_merged.to_csv(merged_output_path, index=False, sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe0f4af-e939-4100-9618-5b522bdd72a1",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2661f431-a194-47d1-9a1c-89162bf85a91",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "cd37e08d-b4f2-459e-823c-3816a825ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Clean text by removing empty spaces, line breaks, hyphenation, stray characters, and escape quote ==\n",
    "\n",
    "# Define cleaning function\n",
    "def clean_text(content):\n",
    "    if pd.isna(content):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove line breaks and carriage returns\n",
    "    content = content.replace('\\n', ' ').replace('\\r', ' ')\n",
    "\n",
    "    # Collapse multiple spaces\n",
    "    content = ' '.join(content.split())\n",
    "\n",
    "    # Fix punctuation spacing (e.g. \"word,another\" â†’ \"word, another\")\n",
    "    content = re.sub(r'(?<=[.,])(?=[^\\s])', r' ', content)\n",
    "\n",
    "    # Remove hyphenation at line breaks (e.g. \"inter- national\" â†’ \"international\")\n",
    "    content = re.sub(r'-\\s', '', content)\n",
    "\n",
    "    ############NEW\n",
    "\n",
    "     # Replace hyphen between letters with a space to prevent merging words (e.g. \"russian-and\" â†’ \"russian and\")\n",
    "    content = re.sub(r'(?<=\\w)-(?=\\w)', ' ', content)\n",
    "    #################NEW\n",
    "\n",
    "    # Remove stray backslashes\n",
    "    content = content.replace(\"\\\\\", \"\")\n",
    "\n",
    "    # Escape double quotes for CSV safety\n",
    "    content = content.replace('\"', '\"\"')\n",
    "\n",
    "    return content\n",
    "\n",
    "# Apply cleaning to each speech\n",
    "df_raw['speech'] = df_raw['speech'].astype(str)  # Ensure column is string type\n",
    "df_clean = df_raw.copy()\n",
    "df_clean['speech'] = df_clean['speech'].apply(clean_text)\n",
    "\n",
    "# Drop rows with empty speeches after cleaning\n",
    "df_clean = df_clean[df_clean['speech'].str.strip().astype(bool)].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "7c6bd148-d5cf-4eb3-a66d-fc706e99c13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved clean speeches chunks in 'C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp'\n"
     ]
    }
   ],
   "source": [
    "# == Split cleaned data into chunks and save as separate files ==\n",
    "\n",
    "# Convert cleaned DataFrame to list of lists\n",
    "clean_data = df_clean[['filename', 'speech']].values.tolist()\n",
    "\n",
    "# Split cleaned data into 4 equal chunks\n",
    "data_id1 = clean_data[:int(len(clean_data)/4)]\n",
    "data_id2 = clean_data[int(len(clean_data)/4): int(2*len(clean_data)/4)]\n",
    "data_id3 = clean_data[int(2*len(clean_data)/4): int(3*len(clean_data)/4)]\n",
    "data_id4 = clean_data[int(3*len(clean_data)/4):]\n",
    "\n",
    "# Change directory to the temp folder\n",
    "os.chdir(data_temp)  # make sure `data_temp` exists and is defined\n",
    "\n",
    "# Save each chunk with joblib\n",
    "joblib.dump(data_id1, 'cleanspeeches_indexed1.pkl')\n",
    "joblib.dump(data_id2, 'cleanspeeches_indexed2.pkl')\n",
    "joblib.dump(data_id3, 'cleanspeeches_indexed3.pkl')\n",
    "joblib.dump(data_id4, 'cleanspeeches_indexed4.pkl')\n",
    "\n",
    "# Store list of cleaned data chunk paths to feed into preprocessing function later\n",
    "data_files = [\n",
    "    os.path.join(data_temp, 'cleanspeeches_indexed1.pkl'),\n",
    "    os.path.join(data_temp, 'cleanspeeches_indexed2.pkl'),\n",
    "    os.path.join(data_temp, 'cleanspeeches_indexed3.pkl'),\n",
    "    os.path.join(data_temp, 'cleanspeeches_indexed4.pkl')\n",
    "]\n",
    "\n",
    "print(f\"âœ… Saved clean speeches chunks in '{data_temp}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6e5109-8cf7-41e3-8fb9-8e7dde29b4e0",
   "metadata": {},
   "source": [
    "### Advanced Text Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "0aa75944-4b75-4aa4-9430-9aa6bd9ac934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Define function to Tokenize, eliminate digits, remove stopwords, lemmatize, POS-Tagging ==\n",
    "\n",
    "def pro1(lista):\n",
    "    # Remove punctuation\n",
    "    return [[row[0], row[1].translate(translator)] for row in lista]\n",
    "\n",
    "def pro2(lista):\n",
    "    # Tokenize and lowercase with gensim\n",
    "    return [[row[0], gensim.utils.simple_preprocess(row[1])] for row in lista]\n",
    "\n",
    "def pro3(lista):\n",
    "    # Remove digits\n",
    "        return [[row[0], [w for w in row[1] if not any(char.isdigit() for char in w)]] for row in lista]\n",
    "\n",
    "def pro4(lista):\n",
    "    # Drop short words\n",
    "    return [[row[0], [w for w in row[1] if len(w) > 2]] for row in lista]\n",
    "\n",
    "def tags_spacy(lista):\n",
    "    texts = [' '.join(row[1]) for row in lista]\n",
    "    docs = list(nlp.pipe(texts, batch_size=20, n_process=1))\n",
    "    result = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        filtered_tokens = [token.text for token in doc if token.tag_.startswith(('N', 'V', 'J'))]\n",
    "        result.append([lista[i][0], filtered_tokens])\n",
    "    return result\n",
    "\n",
    "\n",
    "def pro5(lista):\n",
    "    # Remove stopwords using SpaCy stopword list\n",
    "    return [[row[0], [w for w in row[1] if w not in SPACY_STOPWORDS]] for row in lista]\n",
    "\n",
    "def pro6(lista):\n",
    "      return [\n",
    "        [row[0], [stemmer.stem(token) for token in row[1]]]\n",
    "        for row in lista\n",
    "    ]\n",
    "   # texts = [' '.join(row[1]) for row in lista]\n",
    "   # docs = list(nlp.pipe(texts, batch_size=20, n_process=1))\n",
    "   # result = []\n",
    "   # for i, doc in enumerate(docs):\n",
    "    # lemmatized = [token.lemma_ for token in doc]\n",
    "     #    result.append([lista[i][0], lemmatized])\n",
    "  #  return result\n",
    "\n",
    "########################## Question for Max: They removed procedural words in the paper\n",
    "\n",
    "def dropnull(lista):\n",
    "    # Drop empty speeches\n",
    "    return [row for row in lista if len(' '.join(row[1])) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "524a274a-004c-4d87-9bb3-5f789a42b90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed1.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed1.pkl] Before tagging: 1.42s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed1.pkl] After tagging: 30.83s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed1.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed1.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed1.pkl] Done. Total time: 36.52s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed2.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed2.pkl] Before tagging: 1.49s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed2.pkl] After tagging: 29.44s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed2.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed2.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed2.pkl] Done. Total time: 34.36s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed3.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed3.pkl] Before tagging: 1.20s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed3.pkl] After tagging: 25.73s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed3.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed3.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed3.pkl] Done. Total time: 30.73s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed4.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed4.pkl] Before tagging: 1.39s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed4.pkl] After tagging: 27.83s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed4.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed4.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed4.pkl] Done. Total time: 32.77s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# == Create full pre-processing function and call it\n",
    "\n",
    "def preprocessing(data_name):\n",
    "    t0 = time.time()\n",
    "    print(f\"Starting preprocessing for {data_name}...\")\n",
    "\n",
    "    data = joblib.load(data_name)\n",
    "    data = pro1(data)\n",
    "    data = pro2(data)\n",
    "    data = pro3(data)\n",
    "    data = pro4(data)\n",
    "\n",
    "    print(f\"[{data_name}] Before tagging: {time.time() - t0:.2f}s\")\n",
    "    data = tags_spacy(data)\n",
    "    print(f\"[{data_name}] After tagging: {time.time() - t0:.2f}s\")\n",
    "\n",
    "    data = pro5(data)\n",
    "   # data = pro6(data)\n",
    "    data = dropnull(data)\n",
    "\n",
    "    # out_name = data_name.replace('cleanspeeches_', 'preprocessed_speeches_').replace('.pkl', '_temp.pkl')\n",
    "\n",
    "    # Store preprocessed corupus (before stemming) for wordcloud\n",
    "    filename_wordcloud = data_name.replace('cleanspeeches_', 'wordcloud_speeches_').replace('.pkl', '.pkl')\n",
    "    out_name_wordcloud = os.path.join(data_preprocessed, os.path.basename(filename_wordcloud))\n",
    "    joblib.dump(data, out_name_wordcloud)\n",
    "\n",
    "    # Apply stemming\n",
    "    data_stemmed = pro6(data)\n",
    "\n",
    "    filename_preprocessed = data_name.replace('cleanspeeches_', 'preprocessed_speeches_').replace('.pkl', '.pkl')\n",
    "    out_preprocessed = os.path.join(data_preprocessed, os.path.basename(filename_preprocessed))\n",
    "    joblib.dump(data_stemmed, out_preprocessed\n",
    "               )\n",
    "    print(f\"[{data_name}] Saved stemmed version: {out_preprocessed}\")\n",
    "\n",
    "    print(f\"[{data_name}] Done. Total time: {time.time() - t0:.2f}s\\n\")\n",
    "\n",
    "def main():\n",
    "    for fname in data_files:\n",
    "        preprocessing(fname)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "d3409987-485c-47c9-9430-01fa2c0e650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the pre-processed data\n",
    "preprocessed_files = [\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed1.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed2.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed3.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed4.pkl')\n",
    "]\n",
    "\n",
    "wordcloud_files = [\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed1.pkl'),\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed2.pkl'),\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed3.pkl'),\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed4.pkl')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "134ddbf7-9b95-4697-9d9c-f9c55b52ec80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d006e-6a23-453b-9c91-0e60bd3f8301",
   "metadata": {},
   "source": [
    "## Word-Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf2443e-4c76-41b2-bc16-e72d91c318a8",
   "metadata": {},
   "source": [
    "### Count frequencies of all tokens and display the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "154d978d-cddd-431c-9d53-73c9faf8fdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed1.pkl: removed words with freq < 10\n",
      "Processed C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed2.pkl: removed words with freq < 10\n",
      "Processed C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed3.pkl: removed words with freq < 10\n",
      "Processed C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed4.pkl: removed words with freq < 10\n",
      "\n",
      "[Stemmed] Top 100 most common words:\n",
      "nation: 16885\n",
      "unit: 13698\n",
      "countri: 12968\n",
      "intern: 11888\n",
      "develop: 10683\n",
      "world: 9676\n",
      "state: 9653\n",
      "peac: 9587\n",
      "peopl: 9054\n",
      "secur: 6133\n",
      "general: 5818\n",
      "govern: 5510\n",
      "econom: 5508\n",
      "year: 5039\n",
      "organ: 4989\n",
      "assembl: 4716\n",
      "right: 4668\n",
      "effort: 4304\n",
      "new: 4287\n",
      "problem: 4280\n",
      "human: 4114\n",
      "support: 4098\n",
      "continu: 4012\n",
      "polit: 3712\n",
      "communiti: 3702\n",
      "region: 3601\n",
      "time: 3418\n",
      "africa: 3418\n",
      "member: 3180\n",
      "session: 3117\n",
      "import: 3034\n",
      "work: 3032\n",
      "council: 3009\n",
      "need: 2899\n",
      "achiev: 2883\n",
      "hope: 2861\n",
      "war: 2843\n",
      "presid: 2798\n",
      "conflict: 2796\n",
      "south: 2777\n",
      "power: 2729\n",
      "republ: 2639\n",
      "global: 2628\n",
      "situat: 2597\n",
      "resolut: 2578\n",
      "concern: 2570\n",
      "principl: 2568\n",
      "order: 2513\n",
      "solut: 2495\n",
      "relat: 2468\n",
      "great: 2450\n",
      "forc: 2409\n",
      "oper: 2408\n",
      "confer: 2356\n",
      "polici: 2335\n",
      "social: 2330\n",
      "establish: 2318\n",
      "effect: 2275\n",
      "respect: 2262\n",
      "action: 2231\n",
      "nuclear: 2173\n",
      "commit: 2139\n",
      "african: 2099\n",
      "agreement: 2062\n",
      "independ: 2053\n",
      "interest: 2043\n",
      "charter: 2039\n",
      "chang: 2030\n",
      "contribut: 2022\n",
      "negoti: 2020\n",
      "issu: 2011\n",
      "progress: 2011\n",
      "weapon: 1990\n",
      "end: 1965\n",
      "system: 1963\n",
      "way: 1957\n",
      "today: 1922\n",
      "implement: 1896\n",
      "territori: 1880\n",
      "deleg: 1877\n",
      "process: 1876\n",
      "meet: 1873\n",
      "secretari: 1863\n",
      "respons: 1838\n",
      "east: 1838\n",
      "believ: 1837\n",
      "live: 1817\n",
      "express: 1806\n",
      "question: 1803\n",
      "challeng: 1786\n",
      "remain: 1782\n",
      "posit: 1773\n",
      "area: 1745\n",
      "arm: 1740\n",
      "repres: 1736\n",
      "cooper: 1735\n",
      "futur: 1730\n",
      "wish: 1719\n",
      "disarma: 1701\n",
      "law: 1700\n",
      "\n",
      "[Stemmed] Top 300 least common words:\n",
      "transmut: 1\n",
      "statementtim: 1\n",
      "freest: 1\n",
      "famagusta: 1\n",
      "unfreez: 1\n",
      "exhum: 1\n",
      "sprout: 1\n",
      "bpfa: 1\n",
      "csw: 1\n",
      "pregnanc: 1\n",
      "macroeconomi: 1\n",
      "trenchant: 1\n",
      "malawian: 1\n",
      "submers: 1\n",
      "tuna: 1\n",
      "quietest: 1\n",
      "cadenc: 1\n",
      "briefest: 1\n",
      "livabl: 1\n",
      "coemiss: 1\n",
      "feedback: 1\n",
      "blinken: 1\n",
      "kaselehli: 1\n",
      "wondrous: 1\n",
      "theirimmedi: 1\n",
      "odlum: 1\n",
      "charismat: 1\n",
      "rabbi: 1\n",
      "tulip: 1\n",
      "meetingar: 1\n",
      "fswapoj: 1\n",
      "refriger: 1\n",
      "godfath: 1\n",
      "unitaid: 1\n",
      "lra: 1\n",
      "fallout: 1\n",
      "ovat: 1\n",
      "islamophob: 1\n",
      "longoutstand: 1\n",
      "kamuuf: 1\n",
      "ikamuuf: 1\n",
      "shorelin: 1\n",
      "hkamuuf: 1\n",
      "eightieth: 1\n",
      "sheik: 1\n",
      "foam: 1\n",
      "daob: 1\n",
      "teribsel: 1\n",
      "unhatch: 1\n",
      "clam: 1\n",
      "spearfish: 1\n",
      "upskil: 1\n",
      "undcp: 1\n",
      "populist: 1\n",
      "ounc: 1\n",
      "shatt: 1\n",
      "ladd: 1\n",
      "nonpeac: 1\n",
      "lladd: 1\n",
      "bach: 1\n",
      "gangwon: 1\n",
      "yongbyon: 1\n",
      "dongchang: 1\n",
      "punggy: 1\n",
      "statesnorth: 1\n",
      "deris: 1\n",
      "avaric: 1\n",
      "farcic: 1\n",
      "onehundr: 1\n",
      "jink: 1\n",
      "pedlar: 1\n",
      "wax: 1\n",
      "unorgan: 1\n",
      "oilmen: 1\n",
      "ether: 1\n",
      "niceti: 1\n",
      "thirtysixth: 1\n",
      "nacionalista: 1\n",
      "movimiento: 1\n",
      "impetu: 1\n",
      "nauseam: 1\n",
      "kernel: 1\n",
      "betancourt: 1\n",
      "sandino: 1\n",
      "sahsran: 1\n",
      "wideninq: 1\n",
      "engross: 1\n",
      "warplan: 1\n",
      "bloat: 1\n",
      "max: 1\n",
      "hasenfus: 1\n",
      "ambassadress: 1\n",
      "evenhand: 1\n",
      "fart: 1\n",
      "brisk: 1\n",
      "calderÃ³n: 1\n",
      "keystrok: 1\n",
      "jablonski: 1\n",
      "henryk: 1\n",
      "persh: 1\n",
      "antisatellit: 1\n",
      "chemenko: 1\n",
      "konstantin: 1\n",
      "oxford: 1\n",
      "muse: 1\n",
      "skirt: 1\n",
      "miscellan: 1\n",
      "carrot: 1\n",
      "sevanda: 1\n",
      "reinterpret: 1\n",
      "closet: 1\n",
      "underplay: 1\n",
      "mazuria: 1\n",
      "sebastiani: 1\n",
      "horac: 1\n",
      "sejm: 1\n",
      "jaruzelski: 1\n",
      "wojciech: 1\n",
      "nationa: 1\n",
      "cracow: 1\n",
      "jagiellonian: 1\n",
      "alma: 1\n",
      "causa: 1\n",
      "honori: 1\n",
      "devout: 1\n",
      "superstructur: 1\n",
      "supin: 1\n",
      "whit: 1\n",
      "unsens: 1\n",
      "eir: 1\n",
      "wester: 1\n",
      "superbodi: 1\n",
      "inexperi: 1\n",
      "gurion: 1\n",
      "baromet: 1\n",
      "fraternit: 1\n",
      "egalit: 1\n",
      "libert: 1\n",
      "wilsonian: 1\n",
      "outgrow: 1\n",
      "shiver: 1\n",
      "brotherli: 1\n",
      "actionclos: 1\n",
      "blandish: 1\n",
      "rooftop: 1\n",
      "karaman: 1\n",
      "kosta: 1\n",
      "christofia: 1\n",
      "anthropocentr: 1\n",
      "synerget: 1\n",
      "allinclus: 1\n",
      "amphictyoni: 1\n",
      "bakoyanni: 1\n",
      "dora: 1\n",
      "waw: 1\n",
      "dprk: 1\n",
      "amc: 1\n",
      "effectu: 1\n",
      "solal: 1\n",
      "seigneur: 1\n",
      "connoisseur: 1\n",
      "toilsom: 1\n",
      "congnat: 1\n",
      "smalland: 1\n",
      "chaco: 1\n",
      "troth: 1\n",
      "bench: 1\n",
      "bipartit: 1\n",
      "anticolonialist: 1\n",
      "galley: 1\n",
      "treacheri: 1\n",
      "birthrat: 1\n",
      "gaoler: 1\n",
      "avalanch: 1\n",
      "calori: 1\n",
      "untrain: 1\n",
      "outdo: 1\n",
      "explic: 1\n",
      "stringenc: 1\n",
      "henderson: 1\n",
      "shrivel: 1\n",
      "naught: 1\n",
      "melkert: 1\n",
      "khamenei: 1\n",
      "roadway: 1\n",
      "resuscit: 1\n",
      "drawnout: 1\n",
      "mendaci: 1\n",
      "lame: 1\n",
      "maltreat: 1\n",
      "whet: 1\n",
      "recoup: 1\n",
      "bloodlet: 1\n",
      "highand: 1\n",
      "theatric: 1\n",
      "greenenergi: 1\n",
      "fossilfuel: 1\n",
      "schengen: 1\n",
      "brexit: 1\n",
      "visegrad: 1\n",
      "capek: 1\n",
      "karel: 1\n",
      "robot: 1\n",
      "allianz: 1\n",
      "inexperienc: 1\n",
      "zbrojovka: 1\n",
      "moto: 1\n",
      "jawa: 1\n",
      "auto: 1\n",
      "skoda: 1\n",
      "bata: 1\n",
      "footwear: 1\n",
      "automot: 1\n",
      "partak: 1\n",
      "unrespons: 1\n",
      "velvet: 1\n",
      "issuanc: 1\n",
      "agronomi: 1\n",
      "sep: 1\n",
      "haggl: 1\n",
      "hagana: 1\n",
      "er: 1\n",
      "tostat: 1\n",
      "interfet: 1\n",
      "gut: 1\n",
      "wherewith: 1\n",
      "twentyfourth: 1\n",
      "ahijo: 1\n",
      "dualiti: 1\n",
      "germansoviet: 1\n",
      "shave: 1\n",
      "lunar: 1\n",
      "id: 1\n",
      "lowmiddl: 1\n",
      "namibiaÃ­: 1\n",
      "delegationÃ­: 1\n",
      "bravest: 1\n",
      "dzebisov: 1\n",
      "svetlana: 1\n",
      "synagogu: 1\n",
      "wineskin: 1\n",
      "dÃ©mocratiqu: 1\n",
      "rÃ©publiqu: 1\n",
      "armÃ©: 1\n",
      "kamanyola: 1\n",
      "unsavouri: 1\n",
      "stonewal: 1\n",
      "hawa: 1\n",
      "landand: 1\n",
      "mechanist: 1\n",
      "mujahidin: 1\n",
      "dusk: 1\n",
      "sanguin: 1\n",
      "rudderless: 1\n",
      "anthrax: 1\n",
      "tobin: 1\n",
      "amsterdam: 1\n",
      "freedomlov: 1\n",
      "booklet: 1\n",
      "ffa: 1\n",
      "sprep: 1\n",
      "jigsaw: 1\n",
      "globalresourc: 1\n",
      "capitol: 1\n",
      "badg: 1\n",
      "symbolist: 1\n",
      "overstress: 1\n",
      "demiss: 1\n",
      "scoff: 1\n",
      "rebuf: 1\n",
      "desmond: 1\n",
      "hugh: 1\n",
      "unanticip: 1\n",
      "worldnowher: 1\n",
      "milieu: 1\n",
      "developmenti: 1\n",
      "elsewherether: 1\n",
      "encamp: 1\n",
      "bee: 1\n",
      "sebastopol: 1\n",
      "ogata: 1\n",
      "volcker: 1\n",
      "unwieldi: 1\n",
      "samorÃ©: 1\n",
      "itzhak: 1\n",
      "borja: 1\n",
      "carÃ¡qu: 1\n",
      "bahÃ­a: 1\n",
      "graffiti: 1\n",
      "uncontain: 1\n",
      "perdit: 1\n",
      "ghanaian: 1\n",
      "longerterm: 1\n",
      "turkishcypriot: 1\n",
      "threechamb: 1\n",
      "untru: 1\n",
      "thirtyninth: 1\n",
      "khoda: 1\n",
      "shonar: 1\n",
      "cvf: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Wordcloud] Top 100 most common words:\n",
      "nations: 13558\n",
      "united: 13502\n",
      "international: 11341\n",
      "world: 9473\n",
      "countries: 8560\n",
      "peace: 7860\n",
      "states: 6757\n",
      "development: 6213\n",
      "people: 5820\n",
      "general: 5698\n",
      "security: 5673\n",
      "economic: 5485\n",
      "assembly: 4628\n",
      "new: 4287\n",
      "country: 4266\n",
      "government: 4196\n",
      "organization: 3677\n",
      "human: 3573\n",
      "political: 3559\n",
      "efforts: 3495\n",
      "community: 3417\n",
      "africa: 3277\n",
      "peoples: 3230\n",
      "rights: 3135\n",
      "support: 3035\n",
      "session: 2970\n",
      "council: 2970\n",
      "time: 2807\n",
      "south: 2777\n",
      "years: 2585\n",
      "republic: 2561\n",
      "war: 2542\n",
      "state: 2538\n",
      "problems: 2532\n",
      "year: 2442\n",
      "great: 2434\n",
      "order: 2405\n",
      "national: 2336\n",
      "global: 2330\n",
      "developing: 2324\n",
      "situation: 2277\n",
      "social: 2274\n",
      "work: 2246\n",
      "president: 2170\n",
      "nuclear: 2169\n",
      "conference: 2152\n",
      "hope: 2100\n",
      "charter: 2024\n",
      "continue: 1970\n",
      "african: 1947\n",
      "important: 1931\n",
      "region: 1899\n",
      "today: 1869\n",
      "secretary: 1852\n",
      "relations: 1846\n",
      "need: 1840\n",
      "east: 1838\n",
      "progress: 1833\n",
      "respect: 1831\n",
      "problem: 1748\n",
      "policy: 1727\n",
      "peaceful: 1726\n",
      "future: 1720\n",
      "weapons: 1710\n",
      "process: 1706\n",
      "disarmament: 1701\n",
      "principles: 1700\n",
      "action: 1697\n",
      "solution: 1694\n",
      "end: 1670\n",
      "conflict: 1658\n",
      "member: 1639\n",
      "system: 1639\n",
      "way: 1625\n",
      "operation: 1615\n",
      "delegation: 1613\n",
      "resolution: 1568\n",
      "members: 1541\n",
      "cooperation: 1534\n",
      "law: 1516\n",
      "foreign: 1470\n",
      "right: 1463\n",
      "agreement: 1457\n",
      "independence: 1452\n",
      "resources: 1435\n",
      "present: 1428\n",
      "military: 1401\n",
      "fact: 1391\n",
      "question: 1386\n",
      "believe: 1380\n",
      "powers: 1355\n",
      "role: 1350\n",
      "measures: 1338\n",
      "possible: 1315\n",
      "negotiations: 1307\n",
      "developed: 1306\n",
      "special: 1292\n",
      "change: 1291\n",
      "crisis: 1284\n",
      "means: 1283\n",
      "\n",
      "[Wordcloud] Top 300 least common words:\n",
      "aggregates: 1\n",
      "ripening: 1\n",
      "transmutations: 1\n",
      "statementtime: 1\n",
      "freest: 1\n",
      "famagusta: 1\n",
      "unfreezing: 1\n",
      "exhumations: 1\n",
      "reunifies: 1\n",
      "absolving: 1\n",
      "sprout: 1\n",
      "empting: 1\n",
      "tuned: 1\n",
      "rearing: 1\n",
      "bpfa: 1\n",
      "showcasing: 1\n",
      "csw: 1\n",
      "pregnancy: 1\n",
      "macroeconomy: 1\n",
      "trenchant: 1\n",
      "fatalities: 1\n",
      "bargained: 1\n",
      "helpfulness: 1\n",
      "malawians: 1\n",
      "mozambiques: 1\n",
      "submersibles: 1\n",
      "shelves: 1\n",
      "tuna: 1\n",
      "dereliction: 1\n",
      "polluter: 1\n",
      "outlying: 1\n",
      "quietest: 1\n",
      "cadence: 1\n",
      "briefest: 1\n",
      "livable: 1\n",
      "pollutant: 1\n",
      "coemissions: 1\n",
      "feedback: 1\n",
      "blinken: 1\n",
      "kaselehlie: 1\n",
      "bristling: 1\n",
      "restarted: 1\n",
      "convenants: 1\n",
      "taxpayer: 1\n",
      "absentees: 1\n",
      "earmark: 1\n",
      "wondrous: 1\n",
      "theirimmediate: 1\n",
      "diagnosed: 1\n",
      "mince: 1\n",
      "blanketed: 1\n",
      "sensitizing: 1\n",
      "inherits: 1\n",
      "odlum: 1\n",
      "charismatic: 1\n",
      "rabbis: 1\n",
      "tulip: 1\n",
      "regulators: 1\n",
      "meetingare: 1\n",
      "fswapoj: 1\n",
      "refrigerator: 1\n",
      "godfathers: 1\n",
      "unitaid: 1\n",
      "lra: 1\n",
      "masse: 1\n",
      "widowers: 1\n",
      "fallout: 1\n",
      "ovation: 1\n",
      "deploys: 1\n",
      "islamophobic: 1\n",
      "longoutstanding: 1\n",
      "emboldens: 1\n",
      "shakes: 1\n",
      "idealize: 1\n",
      "kamuuf: 1\n",
      "ikamuuf: 1\n",
      "shoreline: 1\n",
      "hkamuuf: 1\n",
      "chanted: 1\n",
      "eightieth: 1\n",
      "abolishment: 1\n",
      "sheik: 1\n",
      "foam: 1\n",
      "daob: 1\n",
      "teribsel: 1\n",
      "metrics: 1\n",
      "unhatched: 1\n",
      "mirrors: 1\n",
      "clams: 1\n",
      "spearfish: 1\n",
      "upskill: 1\n",
      "palauans: 1\n",
      "skyrocket: 1\n",
      "soared: 1\n",
      "punches: 1\n",
      "acquainted: 1\n",
      "underscoring: 1\n",
      "undcp: 1\n",
      "synthesizing: 1\n",
      "populist: 1\n",
      "ailment: 1\n",
      "ounce: 1\n",
      "relinquishment: 1\n",
      "plummeting: 1\n",
      "shatt: 1\n",
      "ladd: 1\n",
      "nonpeaceful: 1\n",
      "lladd: 1\n",
      "transitioning: 1\n",
      "weathered: 1\n",
      "reconnect: 1\n",
      "bach: 1\n",
      "gangwon: 1\n",
      "yongbyon: 1\n",
      "dongchang: 1\n",
      "punggye: 1\n",
      "statesnorth: 1\n",
      "paine: 1\n",
      "exploding: 1\n",
      "derision: 1\n",
      "antarcticas: 1\n",
      "bloodiness: 1\n",
      "avarice: 1\n",
      "apartheids: 1\n",
      "farcical: 1\n",
      "enlarges: 1\n",
      "onehundred: 1\n",
      "jink: 1\n",
      "guesses: 1\n",
      "pedlars: 1\n",
      "wax: 1\n",
      "unorganized: 1\n",
      "oilmen: 1\n",
      "ether: 1\n",
      "battled: 1\n",
      "niceties: 1\n",
      "alphabetical: 1\n",
      "thirtysixth: 1\n",
      "muffled: 1\n",
      "nacionalista: 1\n",
      "movimiento: 1\n",
      "impetuous: 1\n",
      "sureness: 1\n",
      "complementation: 1\n",
      "bask: 1\n",
      "localization: 1\n",
      "nauseam: 1\n",
      "kernel: 1\n",
      "arrives: 1\n",
      "redeemers: 1\n",
      "immortality: 1\n",
      "betancourt: 1\n",
      "sandino: 1\n",
      "irritate: 1\n",
      "sahsran: 1\n",
      "segregating: 1\n",
      "hated: 1\n",
      "wideninq: 1\n",
      "engrossing: 1\n",
      "radars: 1\n",
      "warplanes: 1\n",
      "bloated: 1\n",
      "oner: 1\n",
      "mocked: 1\n",
      "fasters: 1\n",
      "max: 1\n",
      "naturalized: 1\n",
      "hasenfus: 1\n",
      "accrediting: 1\n",
      "photograph: 1\n",
      "ambassadress: 1\n",
      "evenhanded: 1\n",
      "fart: 1\n",
      "brisk: 1\n",
      "calderÃ³n: 1\n",
      "distributional: 1\n",
      "keystrokes: 1\n",
      "doctorates: 1\n",
      "jablonski: 1\n",
      "henryk: 1\n",
      "lowers: 1\n",
      "delineated: 1\n",
      "avow: 1\n",
      "pershing: 1\n",
      "cruise: 1\n",
      "antisatellite: 1\n",
      "chemenko: 1\n",
      "konstantin: 1\n",
      "oxford: 1\n",
      "shrugged: 1\n",
      "musing: 1\n",
      "skirt: 1\n",
      "interpreters: 1\n",
      "miscellaneous: 1\n",
      "dissonance: 1\n",
      "sticks: 1\n",
      "carrots: 1\n",
      "sevanda: 1\n",
      "reinterpret: 1\n",
      "twisting: 1\n",
      "manipulative: 1\n",
      "closet: 1\n",
      "underplays: 1\n",
      "temporized: 1\n",
      "sneered: 1\n",
      "mazuria: 1\n",
      "epitomize: 1\n",
      "sebastiani: 1\n",
      "horace: 1\n",
      "sejm: 1\n",
      "gloss: 1\n",
      "underwriters: 1\n",
      "banked: 1\n",
      "jaruzelski: 1\n",
      "wojciech: 1\n",
      "wisdoms: 1\n",
      "goodneighbour: 1\n",
      "germanic: 1\n",
      "penned: 1\n",
      "nationa: 1\n",
      "cracow: 1\n",
      "jagiellonian: 1\n",
      "alma: 1\n",
      "causa: 1\n",
      "honoris: 1\n",
      "conflicted: 1\n",
      "dismissal: 1\n",
      "branched: 1\n",
      "devout: 1\n",
      "dispensing: 1\n",
      "revolutionize: 1\n",
      "superstructure: 1\n",
      "citys: 1\n",
      "supine: 1\n",
      "obeyed: 1\n",
      "repudiating: 1\n",
      "whit: 1\n",
      "eliciting: 1\n",
      "bunche: 1\n",
      "expectant: 1\n",
      "unsensational: 1\n",
      "reformation: 1\n",
      "eire: 1\n",
      "camera: 1\n",
      "wester: 1\n",
      "superbody: 1\n",
      "kicks: 1\n",
      "inexperience: 1\n",
      "gurion: 1\n",
      "wounding: 1\n",
      "pitful: 1\n",
      "barometer: 1\n",
      "butchering: 1\n",
      "fraternite: 1\n",
      "egalite: 1\n",
      "liberte: 1\n",
      "germinate: 1\n",
      "wilsonian: 1\n",
      "outgrow: 1\n",
      "shivering: 1\n",
      "brotherliness: 1\n",
      "ministered: 1\n",
      "decencies: 1\n",
      "actionclosing: 1\n",
      "reds: 1\n",
      "blandishments: 1\n",
      "owning: 1\n",
      "rooftops: 1\n",
      "karamanlis: 1\n",
      "kostas: 1\n",
      "christofias: 1\n",
      "anthropocentric: 1\n",
      "disgraces: 1\n",
      "synergetic: 1\n",
      "allinclusive: 1\n",
      "foresees: 1\n",
      "habitability: 1\n",
      "amphictyony: 1\n",
      "bakoyannis: 1\n",
      "dora: 1\n",
      "waw: 1\n",
      "dprk: 1\n",
      "amc: 1\n",
      "effectual: 1\n",
      "mantra: 1\n",
      "mantras: 1\n",
      "litanies: 1\n",
      "solal: 1\n",
      "seigneur: 1\n",
      "belle: 1\n",
      "connoisseur: 1\n",
      "leached: 1\n",
      "divorcing: 1\n",
      "immaturity: 1\n",
      "anticolonial: 1\n",
      "detailing: 1\n",
      "toilsome: 1\n",
      "formulates: 1\n",
      "congnational: 1\n",
      "tactful: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit\\\\data\\\\freq\\\\word_counts_wordcloud.pkl']"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#== Count token frequencies ==\n",
    "\n",
    "def count_frequencies(filenames):\n",
    "    total_freq = Counter()\n",
    "    for fname in tqdm(filenames):\n",
    "        data = joblib.load(fname)\n",
    "        tokens = chain.from_iterable(row[1] for row in data if isinstance(row[1], list))\n",
    "        total_freq.update(tokens)\n",
    "    return total_freq\n",
    "\n",
    "def remove_rare_words(filenames, freqs, min_count=10):\n",
    "    for fname in filenames:\n",
    "        data = joblib.load(fname)\n",
    "        filtered_data = []\n",
    "        for doc_id, tokens in data:\n",
    "            filtered_tokens = [w for w in tokens if freqs.get(w, 0) >= min_count]\n",
    "            filtered_data.append([doc_id, filtered_tokens])\n",
    "        joblib.dump(filtered_data, fname)  # overwrite or save as new file\n",
    "        print(f\"Processed {fname}: removed words with freq < {min_count}\")\n",
    "\n",
    "# === Count for preprocessed (stemmed) speeches ===\n",
    "word_counts_stemmed = count_frequencies(preprocessed_files)\n",
    "\n",
    "remove_rare_words(preprocessed_files, word_counts_stemmed, min_count=10)\n",
    "\n",
    "print(\"\\n[Stemmed] Top 100 most common words:\")\n",
    "for word, count in word_counts_stemmed.most_common(100):\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\n[Stemmed] Top 300 least common words:\")\n",
    "for word, count in word_counts_stemmed.most_common()[:-301:-1]:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Save stemmed word counts\n",
    "save_path_stemmed = os.path.join(data_freq, 'word_counts_stemmed.pkl')\n",
    "joblib.dump(word_counts_stemmed, save_path_stemmed)\n",
    "\n",
    "# === Count for wordcloud (unstemmed) speeches ===\n",
    "word_counts_wordcloud = count_frequencies(wordcloud_files)\n",
    "\n",
    "print(\"\\n[Wordcloud] Top 100 most common words:\")\n",
    "for word, count in word_counts_wordcloud.most_common(100):\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\n[Wordcloud] Top 300 least common words:\")\n",
    "for word, count in word_counts_wordcloud.most_common()[:-301:-1]:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Save unstemmed word counts\n",
    "save_path_wordcloud = os.path.join(data_freq, 'word_counts_wordcloud.pkl')\n",
    "joblib.dump(word_counts_wordcloud, save_path_wordcloud)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "932ce36d-d21c-4f1c-aff7-899c321dcfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_c)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39acefa1-c504-478e-98e5-9d465963dc82",
   "metadata": {},
   "source": [
    "### Count the frequency of the dictionary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "9e28f481-7e20-486c-a6b3-8729d6aeaad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of affect dictionary:\n",
      "['forbid', 'unattract', 'cruelti', 'crappi', 'apathi', 'scari', 'unimpress', 'sin', 'dumbest', 'eas', 'agit', 'sob', 'shocker', 'tragedi', 'fabul', 'strongest', 'giver', 'sigh', 'aw', 'witch', 'hurtl', 'fucktard', 'cruel', 'glamor', 'funni', 'smarter', 'brillianc', 'irrate', 'alright', 'honest', 'profit', 'fearless', 'grievous', 'relax', 'isolationist', 'hah', 'shyness', 'poorest', 'cruelest', 'troublemak', 'disagre', 'agon', 'terror', 'fight', 'pleas', 'poor', 'crazi', 'hostil', 'stupid', 'damnat', 'vain', 'jade', 'heartless', 'nag', 'gloomi', 'damn', 'dishearten', 'pleaser', 'credit', 'warmth', 'greatest', 'whine', 'shame', 'angriest', 'envious', 'grin', 'blameless', 'sweeter', 'laidback', 'stupidest', 'unprotect', 'whiner', 'unlov', 'shake', 'boredom', 'fairer', 'weaker', 'wellb', 'bold', 'sucki', 'unsuccess', 'mourner', 'liken', 'defens', 'invigor', 'tedious', 'paranoid', 'cynic', 'dignifi', 'paranoia', 'sweetest', 'contented', 'humili', 'crush', 'terrif', 'alarm', 'asham', 'impati', 'fake', 'flirti', 'laziest', 'libertarian', 'empti', 'panicki', 'rape', 'contemptu', 'incent', 'reward', 'brutal', 'ignoramus', 'bereav', 'wimp', 'laugh', 'nicer', 'warm', 'resent', 'threat', 'hell', 'tortur', 'mad', 'worship', 'uncomfort', 'depress', 'unsaf', 'faultless', 'numb', 'humorless', 'murder', 'prettiest', 'jealous', 'tragic', 'discomfort', 'loveli', 'ungrat', 'divin', 'lamest', 'weaken', 'horribl', 'fuck', 'agoni', 'grievanc', 'ach', 'isolation', 'masoch', 'uglier', 'amaz', 'devilish', 'bitter', 'devil', 'smartest', 'glori', 'trite', 'vicious', 'unsavori', 'grossest', 'sinist', 'cheeri', 'dismay', 'easier', 'flatter', 'virtuous', 'egot', 'generos', 'cherish', 'heartwarm', 'advers', 'tough', 'adorn', 'overwhelm', 'devast', 'egotist', 'sociabl', 'lazier', 'affection', 'sarcasm', 'humor', 'funniest', 'startl', 'innoc', 'vomit', 'shyli', 'kiss', 'burdensom', 'melancholi', 'weakest', 'worthless', 'gentlest', 'proud', 'alarmist', 'pervert', 'demeanor', 'loneli', 'reassur', 'weird', 'passionless', 'loyalti', 'frustrat', 'weak', 'immor', 'hater', 'libertin', 'flirt', 'terribl', 'threaten', 'fatalist', 'melancholia', 'avers', 'prick', 'sucker', 'meanest', 'virtuos', 'benefici', 'neurot', 'disappoint', 'restless', 'romant', 'bastard', 'angri', 'flatteri', 'stun', 'bolder', 'amor', 'forbidden', 'hug', 'demean', 'disgust', 'truer', 'benefic', 'teas', 'stinki', 'condemnatori', 'turmoil', 'poison', 'cunt', 'gorgeous', 'harm', 'lousi', 'glorifi', 'piti', 'wimpish', 'impression', 'freakish', 'happiest', 'apathet', 'pitiabl', 'unpleas', 'suffer', 'miseri', 'toler', 'mess', 'casual', 'anxieti', 'stale', 'fieri', 'grudg', 'arrog', 'outrag', 'thrill', 'faithless', 'popular', 'vaniti', 'palat', 'fantasi', 'decay', 'domin', 'mood', 'proudest', 'scream', 'stress', 'calm', 'fatigu', 'dumber', 'benign', 'rude', 'steal', 'tension', 'enthusiast', 'intimid', 'sweetheart', 'prettier', 'perv', 'pathet', 'flawless', 'contradictori', 'distress', 'moron', 'gratifi', 'frighten', 'anguish', 'snob', 'defenseless', 'fantast', 'sad', 'thief', 'worsen', 'tear', 'handsom', 'attract', 'messi', 'smother', 'scarier', 'afraid', 'insult', 'disreput', 'wept', 'victim', 'traumat', 'prejudic', 'amus', 'decent', 'joyous', 'masochist', 'abandon', 'foe', 'pleasantri', 'shitless', 'strengthen', 'antagonist', 'liar', 'romantic', 'violenc', 'sentimentalist', 'gratif', 'decept', 'witchcraft', 'liveli', 'impressionist', 'guilt', 'griev', 'rotten', 'disgrac', 'insincer', 'pushi', 'enjoy', 'freaki', 'ador', 'grate', 'homesick', 'braver', 'battlefield', 'niceti', 'hooray', 'incompet', 'thiev', 'timid', 'adventur', 'needi', 'joyless', 'grimac', 'gloom', 'dump', 'horrid', 'magnific', 'unlucki', 'killer', 'uncontrol', 'funnier', 'ruin', 'weepi', 'play', 'encourag', 'fantas', 'irrit', 'sincer', 'rage', 'despis', 'unhappi', 'hate', 'bravest', 'harmless', 'hero', 'vigor', 'pervers', 'dumb', 'degrad', 'shameless', 'villaini', 'envi', 'aggress', 'fucker', 'courag', 'craziest', 'delici', 'scare', 'finer', 'bittersweet', 'trivial', 'wealthi', 'heroic', 'ecstat', 'irrat', 'prejudici', 'smug', 'stammer', 'feroci', 'phobic', 'villain', 'anger', 'fond', 'obsession', 'awesom', 'wors', 'compassion', 'weakl', 'calmer', 'inhibit', 'bother', 'rancid', 'hatr', 'beaten', 'opportunist', 'offens', 'haunt', 'appal', 'painless', 'pessimist', 'molest', 'lol', 'wrongdo', 'frantic', 'heroin', 'rejoic', 'woe', 'miser', 'meaner', 'saddest', 'mooch', 'attack', 'enthusiasm', 'joke', 'meritocraci', 'angrier', 'emot', 'weirdest', 'silliest', 'kisser', 'hurrah', 'crueler', 'nervous', 'dishonor', 'gracious', 'sicker', 'hopeless', 'delect', 'compliment', 'petrifi', 'cuter', 'reek', 'loveless', 'daze', 'weirder', 'triumphant', 'impolit', 'offend', 'richer', 'violent', 'crazier', 'comedown', 'heroism', 'stubborn', 'maniac', 'strain', 'broke', 'jerk', 'unkind', 'asshol', 'reveng', 'jealousi', 'damnabl', 'unimport', 'careless', 'gentler', 'bitch', 'shi', 'terrorist', 'rigid', 'harsh', 'passion', 'annoy', 'compass', 'geek', 'weirdo', 'agre', 'whore', 'splendor', 'suck', 'fun', 'chuckl', 'snobberi', 'mock', 'doom', 'reluct', 'mourn', 'boldest', 'helpless', 'trembl', 'pestil', 'curs', 'desper', 'harass', 'complain', 'joker', 'obsess', 'uneasi', 'nasti', 'greedi', 'belov', 'grim', 'upset', 'greed', 'honesti', 'phoni', 'feudal', 'loveliest', 'trauma', 'dwell', 'ecstasi', 'blame', 'legit', 'calmest', 'sickest', 'scariest', 'sarcast', 'cheat', 'fatal', 'stronger', 'energet', 'feroc', 'healthi', 'goddam', 'disast', 'flirtati', 'defeatist', 'glorious', 'terrifi', 'gossip', 'forbad', 'sicken', 'poorer', 'strangest', 'cute', 'struggl', 'woebegon', 'worri', 'gratitud', 'cheer', 'distraught', 'ok', 'vulner', 'blessed', 'fairest', 'lame', 'optim', 'grouch', 'fright', 'pessim', 'sadder', 'worst', 'despair', 'bash', 'intellectu', 'promis', 'prais', 'heartfelt', 'horror', 'crap', 'repress', 'prouder', 'fought', 'discourag', 'heal', 'lazi', 'precious', 'melanchol', 'isol', 'aggressor', 'giggl', 'idiot', 'twitchi', 'glorif', 'kid', 'selfish', 'smile', 'grief', 'hellish', 'shit', 'fume', 'bitchi', 'fool', 'trustworthi', 'optimist', 'dumpi', 'unwelcom', 'neat', 'nicest', 'furi', 'unfriend', 'ridicul', 'arguabl', 'shamefac', 'pervi', 'upbeat', 'wow', 'easygo', 'blameworthi', 'charmer', 'stink', 'phobia', 'adversari', 'sillier', 'distrust', 'dread', 'happier', 'vital', 'dissatisfact', 'insecur', 'assault', 'ineffect', 'panic', 'powerless', 'nightmar', 'wealth', 'enrag', 'splendid', 'foolish', 'ugliest', 'contradict', 'worrier', 'freak']\n",
      "Number of words in affect dictionary: 629\n",
      "Contents of cognition dictionary:\n",
      "['analyt', 'guess', 'suspect', 'coz', 'mysteri', 'unawar', 'proverbi', 'dissimilar', 'consequenti', 'stimul', 'occasion', 'rethink', 'purest', 'entireti', 'abnorm', 'referenti', 'differenti', 'diagnos', 'ambigu', 'concentr', 'undon', 'undecid', 'disclosur', 'fundamentalist', 'blur', 'reaction', 'variabl', 'recogniz', 'meant', 'reconstruct', 'supposit', 'acknowledg', 'choosi', 'accuraci', 'mislead', 'perceiv', 'hypothesi', 'unknow', 'cuz', 'reveal', 'potenti', 'puriti', 'persuad', 'indirect', 'generat', 'presumpt', 'clue', 'justif', 'evidenti', 'realize', 'deduct', 'causal', 'inevit', 'thinker', 'evidentiari', 'factor', 'undeni', 'unclear', 'pick', 'induc', 'accur', 'absolut', 'everytim', 'reconsider', 'diagnost', 'solut', 'diagnosi', 'discoveri', 'ration', 'unambigu', 'explicit', 'decis', 'insight', 'confess', 'fuzzi', 'led', 'lesson', 'theoriz', 'analyz', 'learner', 'split', 'enlighten', 'cos', 'respons', 'dubious', 'react', 'guarante', 'someday', 'blatant', 'undo', 'perceptu', 'infal', 'afterthought', 'inequ', 'feasibl', 'knowabl', 'approxim', 'logic', 'meaning', 'unknown', 'categor', 'trigger', 'incomplet', 'curious', 'attribut', 'pretend', 'reactionari', 'prove', 'meaningless', 'reactiv', 'proof', 'bet', 'purpos', 'suppos', 'discern', 'imagin', 'kinda', 'comprehens', 'sens', 'percept', 'everyday', 'rational', 'unquestion', 'random', 'implicit', 'clarif', 'theori', 'dunno', 'analysi', 'obedi', 'memor', 'genuin', 'evalu', 'explor', 'linkag', 'grasp', 'curios', 'methink', 'somewher', 'deduc', 'unwant', 'induct', 'unrel', 'outstand', 'discov', 'unresolv', 'exploratori', 'choos', 'perspect', 'inferenti', 'disclos', 'factual', 'arbitrari', 'conting', 'vagu', 'persuas', 'manipul', 'impli', 'somehow', 'sorta', 'borderlin', 'reconcil', 'anyhow', 'conscious', 'probabilist', 'precis', 'rearrang', 'referenc', 'obscur']\n",
      "Number of words in cognition dictionary: 169\n"
     ]
    }
   ],
   "source": [
    "# == Count dictionary words\n",
    "\n",
    "# Load dictionaries            ##### How did they come up with this dictionary? Why did they exclude words?\n",
    "\n",
    "affect_path = os.path.join(data_dict, 'dictionary_affect.pkl')\n",
    "cognition_path = os.path.join(data_dict, 'dictionary_cognition.pkl')\n",
    "\n",
    "with open(affect_path, 'rb') as f:\n",
    "    affect_dict = pickle.load(f)\n",
    "print(\"Contents of affect dictionary:\")\n",
    "print(affect_dict)\n",
    "print(\"Number of words in affect dictionary:\", len(affect_dict))\n",
    "\n",
    "with open(cognition_path, 'rb') as f:\n",
    "    cognition_dict = pickle.load(f)\n",
    "print(\"Contents of cognition dictionary:\")\n",
    "print(cognition_dict)\n",
    "print(\"Number of words in cognition dictionary:\", len(cognition_dict))\n",
    "\n",
    "affect = joblib.load(affect_path)\n",
    "cognition = joblib.load(cognition_path)\n",
    "\n",
    "a = [[i, word_counts_stemmed[i]] for i in affect if i in word_counts_stemmed]\n",
    "c = [[i, word_counts_stemmed[i]] for i in cognition if i in word_counts_stemmed]\n",
    "\n",
    "a = sorted(a, key=lambda x: x[1], reverse=True)\n",
    "c = sorted(c, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "a = [[i[0], f\"({i[1]}),\"] for i in a]\n",
    "c = [[i[0], f\"({i[1]}),\"] for i in c]\n",
    "\n",
    "a1 = ' '.join(str(r) for v in a for r in v)\n",
    "c1 = ' '.join(str(r) for v in c for r in v)\n",
    "\n",
    "affect_out_path = os.path.join(data_freq, \"affect_words.txt\")\n",
    "cog_out_path = os.path.join(data_freq, \"cog_words.txt\")\n",
    "\n",
    "os.makedirs(data_freq, exist_ok=True)  # ensure directory exists\n",
    "\n",
    "with open(affect_out_path, \"w\") as output:\n",
    "    output.write(a1)\n",
    "\n",
    "with open(cog_out_path, \"w\") as output:\n",
    "    output.write(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "0dffe87f-d712-4d71-9305-fb288cf06402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 words by weighted frequency:\n",
      "eb: 0.9990577868732752\n",
      "nudg: 0.9990577868732752\n",
      "bounc: 0.9990577868732752\n",
      "twentythird: 0.9990577868732752\n",
      "thse: 0.9990577868732752\n",
      "biketawa: 0.9990577868732752\n",
      "epidemiolog: 0.9990577868732752\n",
      "karachi: 0.9990577868732752\n",
      "bandar: 0.9990577868732752\n",
      "multimod: 0.9990577868732752\n",
      "antivir: 0.9990577868732752\n",
      "palmyra: 0.9990577868732752\n",
      "monegasqu: 0.9990577868732752\n",
      "oceanograph: 0.9990577868732752\n",
      "omnibus: 0.9990577868732752\n",
      "brace: 0.9990577868732752\n",
      "minster: 0.9990577868732752\n",
      "minsterÃ¢: 0.9990577868732752\n",
      "womenÃ¢: 0.9990577868732752\n",
      "communiquÃ£: 0.9990577868732752\n",
      "bangladeshÃ¢: 0.9990577868732752\n",
      "waje: 0.9990577868732752\n",
      "sunset: 0.9990577868732752\n",
      "movementÃ¢: 0.9990577868732752\n",
      "liv: 0.9990577868732752\n",
      "trawl: 0.9990577868732752\n",
      "licenc: 0.9990577868732752\n",
      "toussaint: 0.9990577868732752\n",
      "louvertur: 0.9990577868732752\n",
      "plautus: 0.9990577868732752\n",
      "solon: 0.9990577868732752\n",
      "pittacus: 0.9990577868732752\n",
      "ephialt: 0.9990577868732752\n",
      "cleisthen: 0.9990577868732752\n",
      "subhuman: 0.9990577868732752\n",
      "fraph: 0.9990577868732752\n",
      "bravo: 0.9990577868732752\n",
      "chez: 0.9990577868732752\n",
      "nous: 0.9990577868732752\n",
      "ecclesiast: 0.9990577868732752\n",
      "voodoo: 0.9990577868732752\n",
      "threequart: 0.9990577868732752\n",
      "frond: 0.9990577868732752\n",
      "ellipt: 0.9990577868732752\n",
      "rendezv: 0.9990577868732752\n",
      "acumen: 0.9990577868732752\n",
      "rocki: 0.9990577868732752\n",
      "marker: 0.9990577868732752\n",
      "chew: 0.9990577868732752\n",
      "perÃ³n: 0.9990577868732752\n",
      "guajira: 0.9990577868732752\n",
      "lugo: 0.9990577868732752\n",
      "theologian: 0.9990577868732752\n",
      "filmmak: 0.9990577868732752\n",
      "comandant: 0.9990577868732752\n",
      "deciph: 0.9990577868732752\n",
      "galeano: 0.9990577868732752\n",
      "companion: 0.9990577868732752\n",
      "abya: 0.9990577868732752\n",
      "yala: 0.9990577868732752\n",
      "prophesi: 0.9990577868732752\n",
      "cracker: 0.9990577868732752\n",
      "fernÃ¡ndez: 0.9990577868732752\n",
      "sulphur: 0.9990577868732752\n",
      "hello: 0.9990577868732752\n",
      "manuela: 0.9990577868732752\n",
      "saenz: 0.9990577868732752\n",
      "eloy: 0.9990577868732752\n",
      "patricia: 0.9990577868732752\n",
      "roda: 0.9990577868732752\n",
      "farther: 0.9990577868732752\n",
      "rubber: 0.9990577868732752\n",
      "cistern: 0.9990577868732752\n",
      "magician: 0.9990577868732752\n",
      "cavemen: 0.9990577868732752\n",
      "bosch: 0.9990577868732752\n",
      "cave: 0.9990577868732752\n",
      "maestra: 0.9990577868732752\n",
      "noam: 0.9990577868732752\n",
      "bookshop: 0.9990577868732752\n",
      "casa: 0.9990577868732752\n",
      "libro: 0.9990577868732752\n",
      "joÃ£o: 0.9990577868732752\n",
      "goulart: 0.9990577868732752\n",
      "jacobo: 0.9990577868732752\n",
      "Ã¡rbenz: 0.9990577868732752\n",
      "guzmÃ¡n: 0.9990577868732752\n",
      "scanner: 0.9990577868732752\n",
      "philip: 0.9990577868732752\n",
      "funÃ©: 0.9990577868732752\n",
      "skip: 0.9990577868732752\n",
      "nasa: 0.9990577868732752\n",
      "bricklay: 0.9990577868732752\n",
      "plumb: 0.9990577868732752\n",
      "carpentri: 0.9990577868732752\n",
      "passer: 0.9990577868732752\n",
      "gasolin: 0.9990577868732752\n",
      "istvÃ¡n: 0.9990577868732752\n",
      "revoluciÃ³n: 0.9990577868732752\n",
      "bosÃ©: 0.9990577868732752\n"
     ]
    }
   ],
   "source": [
    "# == Calculate weighted frequencies for all words\n",
    "\n",
    " # STEMMED OR NOT?\n",
    "# - downweights very common words by giving more importance to rare ones\n",
    "word_counts_stemmed = joblib.load(os.path.join(data_freq, 'word_counts_stemmed.pkl'))\n",
    "\n",
    "l = sum(word_counts_stemmed.values())\n",
    "\n",
    "a = 0.001\n",
    "word_counts_weighted = {k: a / (a + (v / l)) for k, v in word_counts_stemmed.items()}\n",
    "#for key in word_counts.keys():\n",
    " #   word_counts[key] = a / (a + (word_counts[key] / l))\n",
    "\n",
    "joblib.dump(word_counts_weighted, os.path.join(data_freq, 'word_counts_weighted.pkl'))\n",
    "\n",
    "################################################################################ ISSUE##################\n",
    "# To print top 100 by weighted values, sort the dictionary by value descending:\n",
    "top_100_weighted = sorted(word_counts_weighted.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "\n",
    "print(\"Top 100 words by weighted frequency:\")\n",
    "for word, weight in top_100_weighted:\n",
    "    print(f\"{word}: {weight}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0856a2-44f1-4304-b221-09a9ab72ad17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001e6aaf-101b-4912-a198-3d79f6d6e828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5ba7c0-33c7-4420-9cb0-fe54a94ee649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf1db25-1369-45bc-b6c4-ddddb77a3c52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
