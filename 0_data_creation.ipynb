{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b41cb22-e190-48d5-9926-7ac6fac8310d",
   "metadata": {},
   "source": [
    "# Emotion and Reason in Political Language: Examining the UN General speeches\n",
    "## Script 1: Preprocessing & Token Frequencies\n",
    "## Author: Sarah Franzen\n",
    "\n",
    "### Instructions BEFORE running this script:\n",
    "- Ensure all required packages are installed. If not, set `InstallPackages = TRUE` (see code cells below).  \n",
    "- Set your working directory appropriately.  \n",
    "- The script will automatically create the required folder structure.  \n",
    "- Later, you will be asked to download the data from:  \n",
    "  https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/0TJX8Y  \n",
    "  and store it **unzipped** inside the created folder *data_original*\n",
    "\n",
    "\n",
    "### Description: \n",
    "- Extract documents from their original txt documents and store them as one csv\n",
    "- Data Cleaning and Pre-Processing\n",
    "- Count word frequencies and weight themh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930ebd56-def9-4a16-91eb-78dbb7d037ae",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5588afc-105a-426e-9348-742a830a2732",
   "metadata": {},
   "source": [
    "## Setup, Installation and Verification of required Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "ebb8987a-a0da-453e-97e2-aafa209d0e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "InstallPackages = False # Set this to True to install the following packages \n",
    "\n",
    "if InstallPackages:\n",
    "    import sys\n",
    "\n",
    "    packages = [\n",
    "        \"pandas\",\n",
    "        \"nltk\",\n",
    "        \"spacy\",\n",
    "        \"numpy\",\n",
    "        \"gensim\",\n",
    "        \"pycountry\",\n",
    "        \"wordcloud\",\n",
    "        \"matplotlib\",\n",
    "        \"tqdm\"\n",
    "    ]\n",
    "\n",
    "    for package in packages:\n",
    "        !{sys.executable} -m pip install {package}\n",
    "\n",
    "\n",
    "DownloadAdditions = False # Set this to True to download these additional resources\n",
    "if DownloadAdditions:\n",
    "    nltk.download(\"punkt\")\n",
    "    nltk.download(\"averaged_perceptron_tagger\")\n",
    "    spacy.cli.download('en_core_web_lg')         # Download spaCy English model (large)\n",
    "\n",
    "#########################\n",
    "# Check if all packages are included\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "32d7d499-0563-46e0-9730-914c9f106d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Import standard and third-party libraries for data processing, NLP, and visualization ==\n",
    "\n",
    "import gensim\n",
    "import joblib\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import pycountry\n",
    "import random\n",
    "import re\n",
    "import spacy\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from multiprocessing import Pool, freeze_support\n",
    "from nltk.stem import SnowballStemmer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOPWORDS\n",
    "from string import punctuation\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# === Initialize NLP Tools ===\n",
    "\n",
    "translator = str.maketrans('', '', punctuation)\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "4d495156-bbb2-403c-86f2-83b47d8cb03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Folder structure created:\n",
      "- C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\n",
      "  - data_original\n",
      "  - dictionaries\n",
      "  - freq\n",
      "  - preprocessed\n",
      "  - temp\n",
      "  - tokenized\n"
     ]
    }
   ],
   "source": [
    "# === Set Working Directory and create folder structure ===\n",
    "\n",
    "# Prompt user to enter working directory path\n",
    "#wd = input(\"Please enter your working directory path (e.g., C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit): \").strip()\n",
    "\n",
    "# Change to the entered working directory\n",
    "#try:\n",
    "   # os.chdir(wd)\n",
    "    #print(f\"Working directory set to: {os.getcwd()}\")\n",
    "#except FileNotFoundError:\n",
    "   # print(\"ERROR: The directory you entered does not exist. Please restart and enter a valid path.\")\n",
    "    #exit(1)\n",
    "\n",
    "# Set your working directory (adjust this as needed)\n",
    "wd = r\"C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\"\n",
    "os.chdir(wd)\n",
    "\n",
    "# Set base path to current working directory\n",
    "base_path = Path.cwd()\n",
    "data_path = base_path / \"data\"\n",
    "\n",
    "# List of subfolders to create inside 'data'\n",
    "subfolders = [\"data_original\", \"dictionaries\", \"freq\", \"preprocessed\", \"temp\", \"tokenized\"]\n",
    "\n",
    "# Create 'data' folder if it doesn't exist\n",
    "data_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Create subfolders\n",
    "for name in subfolders:\n",
    "    (data_path / name).mkdir(exist_ok=True)\n",
    "\n",
    "print(\"\\nFolder structure created:\")\n",
    "print(f\"- {data_path}\")\n",
    "for name in subfolders:\n",
    "    print(f\"  - {name}\")\n",
    "\n",
    "# Prompt user to place raw data files\n",
    "#print(f\"\\nPlease place your raw data files (unzipped) into the folder:\\n  {data_path / 'data_original'}\")\n",
    "#input(\"Press Enter after you have placed the files to continue...\")\n",
    "\n",
    "#print(\"Continuing with the script...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "56fa9a21-650f-4d8e-8a21-3d9629655c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define Folder Paths ===\n",
    "\n",
    "# If an error occurs, make sure that you actually have these folders in your working directory\n",
    "data_c = os.path.join(wd, 'data')\n",
    "data_temp = os.path.join(data_c, 'temp')\n",
    "data_freq = os.path.join(data_c, 'freq')\n",
    "data_dict = os.path.join(data_c, 'dictionaries')\n",
    "data_preprocessed = os.path.join(data_c, 'preprocessed')\n",
    "data_tokenized = os.path.join(data_c, 'tokenized')\n",
    "fig = os.path.join(wd, 'fig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "296fc7d6-a2ac-406e-a322-6c9f2702ee1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\n"
     ]
    }
   ],
   "source": [
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bd410e-bc90-4e0b-aa08-a9bff13fab47",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff34b6-cbee-4f8c-8076-5f385f596b58",
   "metadata": {},
   "source": [
    "## Load and Prepare Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18067b9-6337-4fb3-a570-603083a7deae",
   "metadata": {},
   "source": [
    "### This chunk can be skipped at the moment\n",
    "### Think of proper header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "93d247a0-5ff2-4062-add9-f3267ae1fe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total speeches found: 10761\n",
      "\n",
      " Saved raw data with 10760 speeches to '.\\data\\un_corpus_raw.csv'\n"
     ]
    }
   ],
   "source": [
    "# == Load Sample from UN General Debate Corpus ==\n",
    "\n",
    "# Set Folder path containing the original TXT files    \n",
    "base_folder = r\".\\data\\data_original\\UN General Debate Corpus\\UNGDC_1946-2023\\TXT\"\n",
    "\n",
    "# Collect txt-files\n",
    "all_txt_files = []\n",
    "for root, dirs, files in os.walk(base_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.txt') and not file.startswith('._'):\n",
    "            all_txt_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Total speeches found: {len(all_txt_files)}\")\n",
    "\n",
    "# Randomly pick 800 files from the full collection   ###################################################### REMOVE AT LATER POINT\n",
    "sampled_files = random.sample(all_txt_files,10761)\n",
    "\n",
    "# Read the selected files into a list\n",
    "raw_data = []\n",
    "for filepath in sampled_files:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        raw_data.append({'filename': os.path.basename(filepath), 'speech': content})\n",
    "\n",
    "df_raw = pd.DataFrame(raw_data)\n",
    "\n",
    "# == Store as csv & pkl ==\n",
    "\n",
    "df_raw = df_raw[df_raw['filename'] != '.DS_Store-to-UTF-8.txt'].copy()\n",
    "\n",
    "raw_pickle_path = r\".\\data\\un_corpus_raw.pkl\"\n",
    "df_raw.to_pickle(raw_pickle_path)\n",
    "\n",
    "raw_output_path = r\".\\data\\un_corpus_raw.csv\"\n",
    "df_raw.to_csv(raw_output_path, index=False, sep=';', encoding='utf-8')\n",
    "\n",
    "print(f\"\\n Saved raw data with {len(df_raw)} speeches to '{raw_output_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "6405ff20-92fa-4c97-becb-ac5fff70f84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SMR_47_1992.txt</td>\n",
       "      <td>The recent accession of the Republic of San Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ROU_17_1962.txt</td>\n",
       "      <td>May I first of all, on behalf of the Romanian ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SLE_35_1980.txt</td>\n",
       "      <td>﻿During the middle of this year the heads of S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CUB_59_2004.txt</td>\n",
       "      <td>Every year at the United Nations, we go throug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LBN_45_1990.txt</td>\n",
       "      <td>﻿At the outset it gives me great pleasure to c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech\n",
       "0  SMR_47_1992.txt  The recent accession of the Republic of San Ma...\n",
       "1  ROU_17_1962.txt  May I first of all, on behalf of the Romanian ...\n",
       "2  SLE_35_1980.txt  ﻿During the middle of this year the heads of S...\n",
       "3  CUB_59_2004.txt  Every year at the United Nations, we go throug...\n",
       "4  LBN_45_1990.txt  ﻿At the outset it gives me great pleasure to c..."
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# == Load data & drop empty speeches ==\n",
    "\n",
    "df_raw = pd.read_pickle(r\".\\data\\un_corpus_raw.pkl\")\n",
    "\n",
    "# Drop empty speeches\n",
    "df_raw['speech'] = df_raw['speech'].astype(str)\n",
    "df_raw = df_raw[df_raw['speech'].str.strip() != ''].copy()\n",
    "\n",
    "df_raw.head()         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7980e945-9d56-4df3-99e5-acf491617568",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7d3162-ebbd-449b-a2be-1148fceda725",
   "metadata": {},
   "source": [
    "## Create new variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25710e79-5068-4d39-8023-f4f97513f165",
   "metadata": {},
   "source": [
    "#### New Variables: Year, Country Code and Country Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "162da07c-1112-43e8-a434-2bef0ed32de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min year: 1946\n",
      "Max year: 2023\n",
      "Missing codes: []\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "      <th>country_code</th>\n",
       "      <th>year</th>\n",
       "      <th>country_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SMR_47_1992.txt</td>\n",
       "      <td>The recent accession of the Republic of San Ma...</td>\n",
       "      <td>SMR</td>\n",
       "      <td>1992</td>\n",
       "      <td>San Marino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ROU_17_1962.txt</td>\n",
       "      <td>May I first of all, on behalf of the Romanian ...</td>\n",
       "      <td>ROU</td>\n",
       "      <td>1962</td>\n",
       "      <td>Romania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SLE_35_1980.txt</td>\n",
       "      <td>﻿During the middle of this year the heads of S...</td>\n",
       "      <td>SLE</td>\n",
       "      <td>1980</td>\n",
       "      <td>Sierra Leone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CUB_59_2004.txt</td>\n",
       "      <td>Every year at the United Nations, we go throug...</td>\n",
       "      <td>CUB</td>\n",
       "      <td>2004</td>\n",
       "      <td>Cuba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LBN_45_1990.txt</td>\n",
       "      <td>﻿At the outset it gives me great pleasure to c...</td>\n",
       "      <td>LBN</td>\n",
       "      <td>1990</td>\n",
       "      <td>Lebanon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech  \\\n",
       "0  SMR_47_1992.txt  The recent accession of the Republic of San Ma...   \n",
       "1  ROU_17_1962.txt  May I first of all, on behalf of the Romanian ...   \n",
       "2  SLE_35_1980.txt  ﻿During the middle of this year the heads of S...   \n",
       "3  CUB_59_2004.txt  Every year at the United Nations, we go throug...   \n",
       "4  LBN_45_1990.txt  ﻿At the outset it gives me great pleasure to c...   \n",
       "\n",
       "  country_code  year  country_name  \n",
       "0          SMR  1992    San Marino  \n",
       "1          ROU  1962       Romania  \n",
       "2          SLE  1980  Sierra Leone  \n",
       "3          CUB  2004          Cuba  \n",
       "4          LBN  1990       Lebanon  "
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# == Create variable: country code & year\n",
    "\n",
    "# Create contry_code and year variable\n",
    "df_raw['country_code'] = df_raw['filename'].str.extract(r'^([A-Z]{2,3})')\n",
    "df_raw['year'] = df_raw['filename'].str.extract(r'_(\\d{4})\\.txt$').astype(int)\n",
    "\n",
    "print(\"Min year:\", df_raw['year'].min())\n",
    "print(\"Max year:\", df_raw['year'].max())\n",
    "# Speeches range from 1946 to 2023\n",
    "\n",
    "# == Create variable: country_name by matching ISO country code \n",
    "code_to_name = {country.alpha_3: country.name for country in pycountry.countries}\n",
    "\n",
    "# Add custom short names and legacy codes\n",
    "custom_names = {\n",
    "    \"BOL\": \"Bolivia\",\n",
    "    \"COD\": \"The Democratic Republic of the Congo\",\n",
    "    \"IRN\": \"Iran\",\n",
    "    \"LAO\": \"Laos\",\n",
    "    \"MDA\": \"Moldova\",\n",
    "    \"PRK\": \"North Korea\",\n",
    "    \"PSE\": \"Palestine\",\n",
    "    \"RUS\": \"Russia\",\n",
    "    \"SYR\": \"Syria\",\n",
    "    \"TZA\": \"Tanzania\",\n",
    "    \"VAT\": \"Vatican City State\",\n",
    "    \"VEN\": \"Venezuela\",\n",
    "    \"VNM\": \"Vietnam\",\n",
    "    \"YMD\": \"South Yemen\",\n",
    "    \"YUG\": \"Yugoslavia\",\n",
    "    \"DDR\": \"East Germany\",\n",
    "    \"EU\": \"European Union\",\n",
    "    \"CSK\": \"Czechoslovakia\",\n",
    "    \"FSM\": \"Micronesia\",\n",
    "    \"KOR\": \"South Korea\"\n",
    "}\n",
    "\n",
    "code_to_name.update(custom_names)\n",
    "df_raw['country_name'] = df_raw['country_code'].map(code_to_name)\n",
    "\n",
    "# Check missing mappings\n",
    "missing = df_raw.loc[df_raw['country_name'].isna(), 'country_code'].unique()\n",
    "print(\"Missing codes:\", missing)\n",
    "\n",
    "# == Check structure ==\n",
    "\n",
    "df_raw.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "383d2904-f015-4c9d-9aeb-51a94f03f950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    country_code                          country_name\n",
      "0            AFG                           Afghanistan\n",
      "1            AGO                                Angola\n",
      "2            ALB                               Albania\n",
      "3            AND                               Andorra\n",
      "4            ARE                  United Arab Emirates\n",
      "5            ARG                             Argentina\n",
      "6            ARM                               Armenia\n",
      "7            ATG                   Antigua and Barbuda\n",
      "8            AUS                             Australia\n",
      "9            AUT                               Austria\n",
      "10           AZE                            Azerbaijan\n",
      "11           BDI                               Burundi\n",
      "12           BEL                               Belgium\n",
      "13           BEN                                 Benin\n",
      "14           BFA                          Burkina Faso\n",
      "15           BGD                            Bangladesh\n",
      "16           BGR                              Bulgaria\n",
      "17           BHR                               Bahrain\n",
      "18           BHS                               Bahamas\n",
      "19           BIH                Bosnia and Herzegovina\n",
      "20           BLR                               Belarus\n",
      "21           BLZ                                Belize\n",
      "22           BOL                               Bolivia\n",
      "23           BRA                                Brazil\n",
      "24           BRB                              Barbados\n",
      "25           BRN                     Brunei Darussalam\n",
      "26           BTN                                Bhutan\n",
      "27           BWA                              Botswana\n",
      "28           CAF              Central African Republic\n",
      "29           CAN                                Canada\n",
      "30           CHE                           Switzerland\n",
      "31           CHL                                 Chile\n",
      "32           CHN                                 China\n",
      "33           CIV                         Côte d'Ivoire\n",
      "34           CMR                              Cameroon\n",
      "35           COD  The Democratic Republic of the Congo\n",
      "36           COG                                 Congo\n",
      "37           COL                              Colombia\n",
      "38           COM                               Comoros\n",
      "39           CPV                            Cabo Verde\n",
      "40           CRI                            Costa Rica\n",
      "41           CSK                        Czechoslovakia\n",
      "42           CUB                                  Cuba\n",
      "43           CYP                                Cyprus\n",
      "44           CZE                               Czechia\n",
      "45           DDR                          East Germany\n",
      "46           DEU                               Germany\n",
      "47           DJI                              Djibouti\n",
      "48           DMA                              Dominica\n",
      "49           DNK                               Denmark\n",
      "50           DOM                    Dominican Republic\n",
      "51           DZA                               Algeria\n",
      "52           ECU                               Ecuador\n",
      "53           EGY                                 Egypt\n",
      "54           ERI                               Eritrea\n",
      "55           ESP                                 Spain\n",
      "56           EST                               Estonia\n",
      "57           ETH                              Ethiopia\n",
      "58            EU                        European Union\n",
      "59           FIN                               Finland\n",
      "60           FJI                                  Fiji\n",
      "61           FRA                                France\n",
      "62           FSM                            Micronesia\n",
      "63           GAB                                 Gabon\n",
      "64           GBR                        United Kingdom\n",
      "65           GEO                               Georgia\n",
      "66           GHA                                 Ghana\n",
      "67           GIN                                Guinea\n",
      "68           GMB                                Gambia\n",
      "69           GNB                         Guinea-Bissau\n",
      "70           GNQ                     Equatorial Guinea\n",
      "71           GRC                                Greece\n",
      "72           GRD                               Grenada\n",
      "73           GTM                             Guatemala\n",
      "74           GUY                                Guyana\n",
      "75           HND                              Honduras\n",
      "76           HRV                               Croatia\n",
      "77           HTI                                 Haiti\n",
      "78           HUN                               Hungary\n",
      "79           IDN                             Indonesia\n",
      "80           IND                                 India\n",
      "81           IRL                               Ireland\n",
      "82           IRN                                  Iran\n",
      "83           IRQ                                  Iraq\n",
      "84           ISL                               Iceland\n",
      "85           ISR                                Israel\n",
      "86           ITA                                 Italy\n",
      "87           JAM                               Jamaica\n",
      "88           JOR                                Jordan\n",
      "89           JPN                                 Japan\n",
      "90           KAZ                            Kazakhstan\n",
      "91           KEN                                 Kenya\n",
      "92           KGZ                            Kyrgyzstan\n",
      "93           KHM                              Cambodia\n",
      "94           KIR                              Kiribati\n",
      "95           KNA                 Saint Kitts and Nevis\n",
      "96           KOR                           South Korea\n",
      "97           KWT                                Kuwait\n",
      "98           LAO                                  Laos\n",
      "99           LBN                               Lebanon\n",
      "100          LBR                               Liberia\n",
      "101          LBY                                 Libya\n",
      "102          LCA                           Saint Lucia\n",
      "103          LIE                         Liechtenstein\n",
      "104          LKA                             Sri Lanka\n",
      "105          LSO                               Lesotho\n",
      "106          LTU                             Lithuania\n",
      "107          LUX                            Luxembourg\n",
      "108          LVA                                Latvia\n",
      "109          MAR                               Morocco\n",
      "110          MCO                                Monaco\n",
      "111          MDA                               Moldova\n",
      "112          MDG                            Madagascar\n",
      "113          MDV                              Maldives\n",
      "114          MEX                                Mexico\n",
      "115          MHL                      Marshall Islands\n",
      "116          MKD                       North Macedonia\n",
      "117          MLI                                  Mali\n",
      "118          MLT                                 Malta\n",
      "119          MMR                               Myanmar\n",
      "120          MNE                            Montenegro\n",
      "121          MNG                              Mongolia\n",
      "122          MOZ                            Mozambique\n",
      "123          MRT                            Mauritania\n",
      "124          MUS                             Mauritius\n",
      "125          MWI                                Malawi\n",
      "126          MYS                              Malaysia\n",
      "127          NAM                               Namibia\n",
      "128          NER                                 Niger\n",
      "129          NGA                               Nigeria\n",
      "130          NIC                             Nicaragua\n",
      "131          NLD                           Netherlands\n",
      "132          NOR                                Norway\n",
      "133          NPL                                 Nepal\n",
      "134          NRU                                 Nauru\n",
      "135          NZL                           New Zealand\n",
      "136          OMN                                  Oman\n",
      "137          PAK                              Pakistan\n",
      "138          PAN                                Panama\n",
      "139          PER                                  Peru\n",
      "140          PHL                           Philippines\n",
      "141          PLW                                 Palau\n",
      "142          PNG                      Papua New Guinea\n",
      "143          POL                                Poland\n",
      "144          PRK                           North Korea\n",
      "145          PRT                              Portugal\n",
      "146          PRY                              Paraguay\n",
      "147          PSE                             Palestine\n",
      "148          QAT                                 Qatar\n",
      "149          ROU                               Romania\n",
      "150          RUS                                Russia\n",
      "151          RWA                                Rwanda\n",
      "152          SAU                          Saudi Arabia\n",
      "153          SDN                                 Sudan\n",
      "154          SEN                               Senegal\n",
      "155          SGP                             Singapore\n",
      "156          SLB                       Solomon Islands\n",
      "157          SLE                          Sierra Leone\n",
      "158          SLV                           El Salvador\n",
      "159          SMR                            San Marino\n",
      "160          SOM                               Somalia\n",
      "161          SRB                                Serbia\n",
      "162          SSD                           South Sudan\n",
      "163          STP                 Sao Tome and Principe\n",
      "164          SUR                              Suriname\n",
      "165          SVK                              Slovakia\n",
      "166          SVN                              Slovenia\n",
      "167          SWE                                Sweden\n",
      "168          SWZ                              Eswatini\n",
      "169          SYC                            Seychelles\n",
      "170          SYR                                 Syria\n",
      "171          TCD                                  Chad\n",
      "172          TGO                                  Togo\n",
      "173          THA                              Thailand\n",
      "174          TJK                            Tajikistan\n",
      "175          TKM                          Turkmenistan\n",
      "176          TLS                           Timor-Leste\n",
      "177          TON                                 Tonga\n",
      "178          TTO                   Trinidad and Tobago\n",
      "179          TUN                               Tunisia\n",
      "180          TUR                               Türkiye\n",
      "181          TUV                                Tuvalu\n",
      "182          TZA                              Tanzania\n",
      "183          UGA                                Uganda\n",
      "184          UKR                               Ukraine\n",
      "185          URY                               Uruguay\n",
      "186          USA                         United States\n",
      "187          UZB                            Uzbekistan\n",
      "188          VAT                    Vatican City State\n",
      "189          VCT      Saint Vincent and the Grenadines\n",
      "190          VEN                             Venezuela\n",
      "191          VNM                               Vietnam\n",
      "192          VUT                               Vanuatu\n",
      "193          WSM                                 Samoa\n",
      "194          YEM                                 Yemen\n",
      "195          YMD                           South Yemen\n",
      "196          YUG                            Yugoslavia\n",
      "197          ZAF                          South Africa\n",
      "198          ZMB                                Zambia\n",
      "199          ZWE                              Zimbabwe\n"
     ]
    }
   ],
   "source": [
    "# == Check country names \n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(df_raw[['country_code', 'country_name']].drop_duplicates().sort_values('country_code').reset_index(drop=True))\n",
    "# Reset to default afterward\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5928538b-8519-40e2-bdee-ac72024f9f48",
   "metadata": {},
   "source": [
    "#### New Variable: Length of speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "911e2229-337f-4479-984c-3a6ab9dff5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average speech length (words): 2931.35\n",
      "20 shortest speeches:\n",
      "              filename                      country_name  year  \\\n",
      "9357   EGY_28_1973.txt                             Egypt  1973   \n",
      "6556   VCT_41_1986.txt  Saint Vincent and the Grenadines  1986   \n",
      "9393   BEN_71_2016.txt                             Benin  2016   \n",
      "10310  GNB_76_2021.txt                     Guinea-Bissau  2021   \n",
      "7398   HTI_01_1946.txt                             Haiti  1946   \n",
      "1541   RWA_76_2021.txt                            Rwanda  2021   \n",
      "5871   DDR_45_1990.txt                      East Germany  1990   \n",
      "3756   LTU_73_2018.txt                         Lithuania  2018   \n",
      "6328   RWA_69_2014.txt                            Rwanda  2014   \n",
      "1946   IRN_01_1946.txt                              Iran  1946   \n",
      "9704   LTU_72_2017.txt                         Lithuania  2017   \n",
      "10709  RWA_70_2015.txt                            Rwanda  2015   \n",
      "7140   ERI_75_2020.txt                           Eritrea  2020   \n",
      "7318   RWA_72_2017.txt                            Rwanda  2017   \n",
      "72     URY_02_1947.txt                           Uruguay  1947   \n",
      "5438   SAU_01_1946.txt                      Saudi Arabia  1946   \n",
      "6859   JOR_65_2010.txt                            Jordan  2010   \n",
      "7976   QAT_63_2008.txt                             Qatar  2008   \n",
      "2037   JOR_61_2006.txt                            Jordan  2006   \n",
      "3486   CUB_01_1946.txt                              Cuba  1946   \n",
      "\n",
      "       speech_length_words  \n",
      "9357                   423  \n",
      "6556                   462  \n",
      "9393                   463  \n",
      "10310                  480  \n",
      "7398                   485  \n",
      "1541                   492  \n",
      "5871                   493  \n",
      "3756                   508  \n",
      "6328                   523  \n",
      "1946                   531  \n",
      "9704                   537  \n",
      "10709                  539  \n",
      "7140                   540  \n",
      "7318                   540  \n",
      "72                     544  \n",
      "5438                   555  \n",
      "6859                   563  \n",
      "7976                   567  \n",
      "2037                   571  \n",
      "3486                   590  \n",
      "\n",
      "20 longest speeches:\n",
      "             filename country_name  year  speech_length_words\n",
      "3636  IND_15_1960.txt        India  1960                22003\n",
      "134   CUB_15_1960.txt         Cuba  1960                21777\n",
      "7287  RUS_06_1951.txt       Russia  1951                19045\n",
      "4306  GIN_17_1962.txt       Guinea  1962                18952\n",
      "841   IND_10_1955.txt        India  1955                18053\n",
      "8001  RUS_15_1960.txt       Russia  1960                17943\n",
      "7031  IND_14_1959.txt        India  1959                17526\n",
      "2656  IND_09_1954.txt        India  1954                17178\n",
      "7736  RUS_13_1958.txt       Russia  1958                16872\n",
      "6608  IND_08_1953.txt        India  1953                16080\n",
      "2834  IND_12_1957.txt        India  1957                14927\n",
      "4943  IND_16_1961.txt        India  1961                13951\n",
      "2118  RUS_16_1961.txt       Russia  1961                13941\n",
      "1825  GIN_15_1960.txt       Guinea  1960                13916\n",
      "7532  IND_11_1956.txt        India  1956                13482\n",
      "7914  IDN_15_1960.txt    Indonesia  1960                13463\n",
      "2724  IND_13_1958.txt        India  1958                12599\n",
      "235   RUS_17_1962.txt       Russia  1962                12255\n",
      "5667  RUS_09_1954.txt       Russia  1954                12055\n",
      "6504  RUS_14_1959.txt       Russia  1959                11959\n"
     ]
    }
   ],
   "source": [
    "# Add a new column: speech length in words\n",
    "df_raw['speech_length_words'] = df_raw['speech'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Calculate average length\n",
    "avg_length = df_raw['speech_length_words'].mean()\n",
    "print(\"Average speech length (words):\", round(avg_length, 2))\n",
    "\n",
    "# 20 shortest & longest speeches\n",
    "print(\"20 shortest speeches:\")\n",
    "print(df_raw.nsmallest(20, 'speech_length_words')[['filename', 'country_name', 'year', 'speech_length_words']])\n",
    "\n",
    "print(\"\\n20 longest speeches:\")\n",
    "print(df_raw.nlargest(20, 'speech_length_words')[['filename', 'country_name', 'year', 'speech_length_words']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424f152-b786-45bf-bf86-e169a900f628",
   "metadata": {},
   "source": [
    "#### New variable: English as Official Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "d1d2c2a6-23d6-4044-84cc-8785167e6b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries not matched in df_raw['country_name']:\n",
      "Anguilla\n",
      "Bermuda\n",
      "British Virgin Islands\n",
      "Cayman Islands\n",
      "Christmas Island\n",
      "Cook Islands\n",
      "Hong Kong\n",
      "Jersey\n",
      "Niue\n",
      "Norfolk Island\n",
      "Northern Mariana Islands\n",
      "Pitcairn Islands\n",
      "Sint Maarten\n",
      "Turks and Caicos Islands\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "      <th>country_code</th>\n",
       "      <th>year</th>\n",
       "      <th>country_name</th>\n",
       "      <th>speech_length_words</th>\n",
       "      <th>english_official_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SMR_47_1992.txt</td>\n",
       "      <td>The recent accession of the Republic of San Ma...</td>\n",
       "      <td>SMR</td>\n",
       "      <td>1992</td>\n",
       "      <td>San Marino</td>\n",
       "      <td>1451</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ROU_17_1962.txt</td>\n",
       "      <td>May I first of all, on behalf of the Romanian ...</td>\n",
       "      <td>ROU</td>\n",
       "      <td>1962</td>\n",
       "      <td>Romania</td>\n",
       "      <td>3993</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SLE_35_1980.txt</td>\n",
       "      <td>﻿During the middle of this year the heads of S...</td>\n",
       "      <td>SLE</td>\n",
       "      <td>1980</td>\n",
       "      <td>Sierra Leone</td>\n",
       "      <td>6137</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CUB_59_2004.txt</td>\n",
       "      <td>Every year at the United Nations, we go throug...</td>\n",
       "      <td>CUB</td>\n",
       "      <td>2004</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>1579</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LBN_45_1990.txt</td>\n",
       "      <td>﻿At the outset it gives me great pleasure to c...</td>\n",
       "      <td>LBN</td>\n",
       "      <td>1990</td>\n",
       "      <td>Lebanon</td>\n",
       "      <td>2761</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech  \\\n",
       "0  SMR_47_1992.txt  The recent accession of the Republic of San Ma...   \n",
       "1  ROU_17_1962.txt  May I first of all, on behalf of the Romanian ...   \n",
       "2  SLE_35_1980.txt  ﻿During the middle of this year the heads of S...   \n",
       "3  CUB_59_2004.txt  Every year at the United Nations, we go throug...   \n",
       "4  LBN_45_1990.txt  ﻿At the outset it gives me great pleasure to c...   \n",
       "\n",
       "  country_code  year  country_name  speech_length_words  \\\n",
       "0          SMR  1992    San Marino                 1451   \n",
       "1          ROU  1962       Romania                 3993   \n",
       "2          SLE  1980  Sierra Leone                 6137   \n",
       "3          CUB  2004          Cuba                 1579   \n",
       "4          LBN  1990       Lebanon                 2761   \n",
       "\n",
       "   english_official_language  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          1  \n",
       "3                          0  \n",
       "4                          0  "
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source for english as official language : https://gradschool.utk.edu/future-students/office-of-graduate-admissions/applying-to-graduate-school/admission-requirements/testing-requirements/countries-with-english-as-official-language/\n",
    "# They are quoting: https://www.cia.gov/the-world-factbook/field/languages/\n",
    "\n",
    "english_countries = [\n",
    "    \"Anguilla\", \"Antigua and Barbuda\", \"Bahamas\", \"Barbados\", \"Belize\", \"Belgium\",\n",
    "    \"Bermuda\", \"Botswana\", \"British Virgin Islands\", \"Burundi\", \"Cameroon\", \"Canada\",\n",
    "    \"Cayman Islands\", \"Christmas Island\", \"Cook Islands\", \"Dominica\", \"Fiji\", \"Gambia\",\n",
    "    \"Ghana\", \"Grenada\", \"Guyana\", \"Hong Kong\", \"India\", \"Ireland\", \"Jersey\", \"Kenya\",\n",
    "    \"Liberia\", \"Malawi\", \"Malta\", \"Marshall Islands\", \"Micronesia\",\n",
    "    \"Namibia\", \"New Zealand\", \"Nigeria\", \"Niue\", \"Norfolk Island\", \"Northern Mariana Islands\",\n",
    "    \"Pakistan\", \"Palau\", \"Papua New Guinea\", \"Philippines\", \"Pitcairn Islands\", \"Rwanda\",\n",
    "    \"Saint Kitts and Nevis\", \"Saint Lucia\", \"Samoa\", \"Seychelles\", \"Sierra Leone\", \"Singapore\",\n",
    "    \"Sint Maarten\", \"Solomon Islands\", \"Somalia\", \"South Africa\", \"South Sudan\", \"Sudan\",\n",
    "    \"Eswatini\", \"Tanzania\", \"Tonga\", \"Trinidad and Tobago\", \"Turks and Caicos Islands\",\n",
    "    \"Tuvalu\", \"Uganda\", \"Zambia\", \"Zimbabwe\"\n",
    "]\n",
    "\n",
    "# Create dummy column\n",
    "df_raw['english_official_language'] = df_raw['country_name'].apply(\n",
    "    lambda x: 1 if x in english_countries else 0\n",
    ")\n",
    "\n",
    "# Detect unmatched countries \n",
    "matched = set(df_raw['country_name'])\n",
    "unmatched = [country for country in english_countries if country not in matched]\n",
    "\n",
    "print(\"Countries not matched in df_raw['country_name']:\")\n",
    "for country in unmatched:\n",
    "    print(country)\n",
    "\n",
    "# All of these countries are either British Overseas Territories, Australian Territories, self-governing island territories or Special Administrative Regions\n",
    "    # None of the unmatched regions are UN Members\n",
    "\n",
    "# Check df with new variable english_official_language\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82540d8a-b656-45fb-aeb0-2fd5b4d7108a",
   "metadata": {},
   "source": [
    "#### New variable: Permanent member security council"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "4fb92fe4-5c22-43fa-8b48-62ec2240d716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      country_code    country_name  security_council_permanent  year\n",
      "23             CHN           China                           1  2019\n",
      "44             GBR  United Kingdom                           1  1983\n",
      "46             FRA          France                           1  2022\n",
      "54             USA   United States                           1  1978\n",
      "80             RUS          Russia                           1  1971\n",
      "...            ...             ...                         ...   ...\n",
      "10693          USA   United States                           1  1999\n",
      "10710          FRA          France                           1  1995\n",
      "10726          FRA          France                           1  1984\n",
      "10731          GBR  United Kingdom                           1  1968\n",
      "10756          FRA          France                           1  1952\n",
      "\n",
      "[383 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define permanent members of the UN Security Council and create dummy\n",
    "permanent_members = ['RUS', 'USA', 'FRA', 'GBR', 'CHN']\n",
    "\n",
    "df_raw['security_council_permanent'] = df_raw['country_code'].isin(permanent_members).astype(int)\n",
    "\n",
    "print(df_raw[df_raw['country_code'].isin(permanent_members)][\n",
    "    ['country_code', 'country_name', 'security_council_permanent', 'year']\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523547d9-9b10-473c-8c85-d1e2fc4fd3e1",
   "metadata": {},
   "source": [
    "#### New variables: Speaker, Position & Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "de783a81-efbf-49a6-96f6-ae6a93fa1573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              filename  year country_code  \\\n",
      "488     EU_66_2011.txt  2011           EU   \n",
      "627    ECU_18_1963.txt  1963          ECU   \n",
      "671    PRT_75_2020.txt  2020          PRT   \n",
      "814    GTM_26_1971.txt  1971          GTM   \n",
      "976    SYR_18_1963.txt  1963          SYR   \n",
      "1065   BEN_28_1973.txt  1973          BEN   \n",
      "1211   PRK_72_2017.txt  2017          PRK   \n",
      "1563   MYS_18_1963.txt  1963          MYS   \n",
      "1588   BEL_18_1963.txt  1963          BEL   \n",
      "1758   KWT_49_1994.txt  1994          KWT   \n",
      "1824   YMD_37_1982.txt  1982          YMD   \n",
      "2128   CYP_18_1963.txt  1963          CYP   \n",
      "2285   PRY_16_1961.txt  1961          PRY   \n",
      "2295   MDG_18_1963.txt  1963          MDG   \n",
      "2308   KEN_49_1994.txt  1994          KEN   \n",
      "2330   CUB_49_1994.txt  1994          CUB   \n",
      "2353   SDN_18_1963.txt  1963          SDN   \n",
      "2386   YMD_34_1979.txt  1979          YMD   \n",
      "2440   BFA_18_1963.txt  1963          BFA   \n",
      "2684   YMD_26_1971.txt  1971          YMD   \n",
      "2917   YMD_31_1976.txt  1976          YMD   \n",
      "3135   YMD_29_1974.txt  1974          YMD   \n",
      "3250   CSK_02_1947.txt  1947          CSK   \n",
      "3333    EU_69_2014.txt  2014           EU   \n",
      "3395   ZAF_71_2016.txt  2016          ZAF   \n",
      "3452   HTI_18_1963.txt  1963          HTI   \n",
      "3494   CZE_49_1994.txt  1994          CZE   \n",
      "3501   TUN_49_1994.txt  1994          TUN   \n",
      "4163   CUB_18_1963.txt  1963          CUB   \n",
      "4198   YMD_28_1973.txt  1973          YMD   \n",
      "4287   CAN_21_1966.txt  1966          CAN   \n",
      "4314   DNK_24_1969.txt  1969          DNK   \n",
      "4796   YMD_42_1987.txt  1987          YMD   \n",
      "4834   YMD_44_1989.txt  1989          YMD   \n",
      "4969   YMD_30_1975.txt  1975          YMD   \n",
      "5278   MEX_18_1963.txt  1963          MEX   \n",
      "5376   COD_18_1963.txt  1963          COD   \n",
      "5477   RWA_18_1963.txt  1963          RWA   \n",
      "5568   YMD_36_1981.txt  1981          YMD   \n",
      "5721   ETH_18_1963.txt  1963          ETH   \n",
      "5918   ZAF_18_1963.txt  1963          ZAF   \n",
      "5931   PRT_72_2017.txt  2017          PRT   \n",
      "5932   IND_18_1963.txt  1963          IND   \n",
      "5951   MOZ_49_1994.txt  1994          MOZ   \n",
      "6155   NOR_18_1963.txt  1963          NOR   \n",
      "6212   YMD_23_1968.txt  1968          YMD   \n",
      "6449   IRQ_18_1963.txt  1963          IRQ   \n",
      "6452   JAM_18_1963.txt  1963          JAM   \n",
      "6473   DOM_18_1963.txt  1963          DOM   \n",
      "6646   EGY_17_1962.txt  1962          EGY   \n",
      "6699   GIN_36_1981.txt  1981          GIN   \n",
      "6814   YMD_40_1985.txt  1985          YMD   \n",
      "6826   YMD_33_1978.txt  1978          YMD   \n",
      "6917   AUS_49_1994.txt  1994          AUS   \n",
      "6958   ZAF_39_1984.txt  1984          ZAF   \n",
      "6984   YMD_35_1980.txt  1980          YMD   \n",
      "7285   CAF_18_1963.txt  1963          CAF   \n",
      "7507    EU_68_2013.txt  2013           EU   \n",
      "7513   MLI_18_1963.txt  1963          MLI   \n",
      "7626   SLE_18_1963.txt  1963          SLE   \n",
      "7635   COM_56_2001.txt  2001          COM   \n",
      "7677   GIN_34_1979.txt  1979          GIN   \n",
      "8254   PRT_17_1962.txt  1962          PRT   \n",
      "8268   RUS_37_1982.txt  1982          RUS   \n",
      "8274   SWZ_56_2001.txt  2001          SWZ   \n",
      "8305   SOM_18_1963.txt  1963          SOM   \n",
      "8323   PRT_71_2016.txt  2016          PRT   \n",
      "8541   GAB_18_1963.txt  1963          GAB   \n",
      "8764   GAB_49_1994.txt  1994          GAB   \n",
      "8776   TZA_18_1963.txt  1963          TZA   \n",
      "8972   YMD_24_1969.txt  1969          YMD   \n",
      "9010   EGY_18_1963.txt  1963          EGY   \n",
      "9049   YMD_43_1988.txt  1988          YMD   \n",
      "9191   PHL_18_1963.txt  1963          PHL   \n",
      "9454   POL_18_1963.txt  1963          POL   \n",
      "9572   UKR_18_1963.txt  1963          UKR   \n",
      "9575   YMD_41_1986.txt  1986          YMD   \n",
      "9606   SAU_18_1963.txt  1963          SAU   \n",
      "9703   CHN_18_1963.txt  1963          CHN   \n",
      "9903   YMD_32_1977.txt  1977          YMD   \n",
      "10500  YMD_27_1972.txt  1972          YMD   \n",
      "10595  YMD_39_1984.txt  1984          YMD   \n",
      "10620  YMD_38_1983.txt  1983          YMD   \n",
      "10768  MEX_37_1982.txt  1982          MEX   \n",
      "\n",
      "                               country_name  \n",
      "488                          European Union  \n",
      "627                                 Ecuador  \n",
      "671                                Portugal  \n",
      "814                               Guatemala  \n",
      "976                                   Syria  \n",
      "1065                                  Benin  \n",
      "1211                            North Korea  \n",
      "1563                               Malaysia  \n",
      "1588                                Belgium  \n",
      "1758                                 Kuwait  \n",
      "1824                            South Yemen  \n",
      "2128                                 Cyprus  \n",
      "2285                               Paraguay  \n",
      "2295                             Madagascar  \n",
      "2308                                  Kenya  \n",
      "2330                                   Cuba  \n",
      "2353                                  Sudan  \n",
      "2386                            South Yemen  \n",
      "2440                           Burkina Faso  \n",
      "2684                            South Yemen  \n",
      "2917                            South Yemen  \n",
      "3135                            South Yemen  \n",
      "3250                         Czechoslovakia  \n",
      "3333                         European Union  \n",
      "3395                           South Africa  \n",
      "3452                                  Haiti  \n",
      "3494                                Czechia  \n",
      "3501                                Tunisia  \n",
      "4163                                   Cuba  \n",
      "4198                            South Yemen  \n",
      "4287                                 Canada  \n",
      "4314                                Denmark  \n",
      "4796                            South Yemen  \n",
      "4834                            South Yemen  \n",
      "4969                            South Yemen  \n",
      "5278                                 Mexico  \n",
      "5376   The Democratic Republic of the Congo  \n",
      "5477                                 Rwanda  \n",
      "5568                            South Yemen  \n",
      "5721                               Ethiopia  \n",
      "5918                           South Africa  \n",
      "5931                               Portugal  \n",
      "5932                                  India  \n",
      "5951                             Mozambique  \n",
      "6155                                 Norway  \n",
      "6212                            South Yemen  \n",
      "6449                                   Iraq  \n",
      "6452                                Jamaica  \n",
      "6473                     Dominican Republic  \n",
      "6646                                  Egypt  \n",
      "6699                                 Guinea  \n",
      "6814                            South Yemen  \n",
      "6826                            South Yemen  \n",
      "6917                              Australia  \n",
      "6958                           South Africa  \n",
      "6984                            South Yemen  \n",
      "7285               Central African Republic  \n",
      "7507                         European Union  \n",
      "7513                                   Mali  \n",
      "7626                           Sierra Leone  \n",
      "7635                                Comoros  \n",
      "7677                                 Guinea  \n",
      "8254                               Portugal  \n",
      "8268                                 Russia  \n",
      "8274                               Eswatini  \n",
      "8305                                Somalia  \n",
      "8323                               Portugal  \n",
      "8541                                  Gabon  \n",
      "8764                                  Gabon  \n",
      "8776                               Tanzania  \n",
      "8972                            South Yemen  \n",
      "9010                                  Egypt  \n",
      "9049                            South Yemen  \n",
      "9191                            Philippines  \n",
      "9454                                 Poland  \n",
      "9572                                Ukraine  \n",
      "9575                            South Yemen  \n",
      "9606                           Saudi Arabia  \n",
      "9703                                  China  \n",
      "9903                            South Yemen  \n",
      "10500                           South Yemen  \n",
      "10595                           South Yemen  \n",
      "10620                           South Yemen  \n",
      "10768                                Mexico  \n",
      "84 rows could not be matched\n",
      "    gender_dummy  count\n",
      "0       0 (male)   4537\n",
      "1     1 (female)    183\n",
      "2  NaN (unknown)   6057\n"
     ]
    }
   ],
   "source": [
    "# Supplmentary xlsx-file from the UN Dataset provides information on the speaker and their position\n",
    "\n",
    "# == Create variable speaker_name and position ==\n",
    "df_speakers = pd.read_excel(os.path.join(data_c, \"data_original\", \"UN General Debate Corpus\", \"Speakers_by_session.xlsx\"))\n",
    "\n",
    "df_speakers.head()\n",
    "\n",
    "# Merge new infrormation to dataframe\n",
    "df_merged = df_raw.merge(\n",
    "    df_speakers[['Year', 'ISO Code', 'Name of Person Speaking', 'Post']],\n",
    "    left_on=['year', 'country_code'],\n",
    "    right_on=['Year', 'ISO Code'],\n",
    "    how='left',\n",
    "    indicator=True)\n",
    "\n",
    "# Detect unmatched rows\n",
    "unmatched = df_merged[df_merged['_merge'] == 'left_only']\n",
    "unmatched_count = (df_merged['_merge'] == 'left_only').sum()\n",
    "\n",
    "print(unmatched[['filename', 'year', 'country_code', 'country_name']])\n",
    "print(f\"{unmatched_count} rows could not be matched\")\n",
    "\n",
    "# Clean up \n",
    "df_merged = df_merged.drop(columns=['Year', 'ISO Code', '_merge']).rename(columns={\n",
    "    'Name of Person Speaking': 'speaker_name',\n",
    "    'Post': 'position'\n",
    "})\n",
    "\n",
    "# == Create gender dummy ==\n",
    "df_merged['gender_dummy'] = df_merged['speaker_name'].apply(\n",
    "    lambda name: 0 if pd.notnull(name) and re.search(r'^(?:Mr|Sir)\\b', name, re.IGNORECASE)\n",
    "    else 1 if pd.notnull(name) and re.search(r'^(?:Mrs|Ms)\\b', name, re.IGNORECASE)\n",
    "    else None\n",
    ")\n",
    "\n",
    "# Count all values including NaN\n",
    "counts = df_merged['gender_dummy'].value_counts(dropna=False)\n",
    "\n",
    "# Build summary using .get() to handle missing keys\n",
    "gender_summary = pd.DataFrame({\n",
    "    'gender_dummy': ['0 (male)', '1 (female)', 'NaN (unknown)'],\n",
    "    'count': [\n",
    "        counts.get(0, 0),\n",
    "        counts.get(1, 0),\n",
    "        counts.get(np.nan, 0)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(gender_summary)\n",
    "\n",
    "# == Adjust position variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3cfdc9-d972-454b-9719-3285b1e5d3e2",
   "metadata": {},
   "source": [
    "Looking at the structure, highest position always seems to be mentioned first --> drop everything else if speaker has more than one position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "b560d87c-56d1-4a39-84e8-6f904a1ab77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379      NaN\n",
      "603      NaN\n",
      "955      NaN\n",
      "1121     NaN\n",
      "1492     NaN\n",
      "1947     NaN\n",
      "2108     NaN\n",
      "3227     NaN\n",
      "3380     NaN\n",
      "3489     NaN\n",
      "3586     NaN\n",
      "3879     NaN\n",
      "3880     NaN\n",
      "3935     NaN\n",
      "4812     NaN\n",
      "5015     NaN\n",
      "5233     NaN\n",
      "5407     NaN\n",
      "5444     NaN\n",
      "5516     NaN\n",
      "6370     NaN\n",
      "6568     NaN\n",
      "6660     NaN\n",
      "6681     NaN\n",
      "6745     NaN\n",
      "7078     NaN\n",
      "7273     NaN\n",
      "7406     NaN\n",
      "7678     NaN\n",
      "7906     NaN\n",
      "8185     NaN\n",
      "8421     NaN\n",
      "8715     NaN\n",
      "9046     NaN\n",
      "9130     NaN\n",
      "9514     NaN\n",
      "10025    NaN\n",
      "10136    NaN\n",
      "10606    NaN\n",
      "Name: position, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where year is 1946 and print their positions\n",
    "print(df_merged.loc[df_merged['year'] == 1946, 'position'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "969f9568-9b6c-454f-ab3f-c9fcdf50323a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Presidents ===\n",
      "President                                                                                                                                                    1612\n",
      "President                                                                                                                                                     200\n",
      "President of the Council of Ministers                                                                                                                           9\n",
      "President and Head of Government                                                                                                                                6\n",
      "President of the Government                                                                                                                                     5\n",
      "President of the European Council                                                                                                                               5\n",
      "Constitutional President                                                                                                                                        5\n",
      "President, Head of Government and Minister for Foreign Affairs and Immigration                                                                                  5\n",
      "President of the Presidency Council of the Government of National Accord                                                                                        4\n",
      "President and Commander-in-Chief of the Defence Forces                                                                                                          3\n",
      "interim president                                                                                                                                               2\n",
      "president                                                                                                                                                       2\n",
      "President of the Presidensy                                                                                                                                     1\n",
      "President and Minister for Defense                                                                                                                              1\n",
      "King and President of the Council of Ministers                                                                                                                  1\n",
      "President Republic                                                                                                                                              1\n",
      "President of the United Republic of Tanzania                                                                                                                    1\n",
      "President of the Arab Republic of Egypt                                                                                                                         1\n",
      "President of  the people's Republic of Mozambique                                                                                                               1\n",
      "President and Comander of the Armed Forces and Minister of Defence                                                                                              1\n",
      "President of the Council of State and of the Government                                                                                                         1\n",
      "President of the Government and Minister for Foreign Affairs and Commerce                                                                                       1\n",
      "President of the Federal Republic of Cameroon                                                                                                                   1\n",
      "President ad interim                                                                                                                                            1\n",
      "President of the Council of the State                                                                                                                           1\n",
      "President of the Republic of Mali and Chairman of the Conference of Heads of State of the Permanent Inter-State Committee on Drought Control in the Sahel       1\n",
      "President                                                                                                                                                       1\n",
      "First Executive President                                                                                                                                       1\n",
      "President and Head of State                                                                                                                                     1\n",
      "President of the Republic of Peru                                                                                                                               1\n",
      "President of the Republic of Pananma                                                                                                                            1\n",
      "President of Faso and President of the Council of Ministers of Burkina Faso                                                                                     1\n",
      "President of Democratic Kampuchea                                                                                                                               1\n",
      "President of the Federative Republic of Brazil                                                                                                                  1\n",
      "President of the Supreme National Council                                                                                                                       1\n",
      "President of the Councils of State and of Ministers                                                                                                             1\n",
      "President of the Eastern Republic of Uruguay                                                                                                                    1\n",
      "President                                                                                                                                                       1\n",
      "President of the Republic of Venezuela                                                                                                                          1\n",
      "President                                                                                                                                                       1\n",
      "President of Government                                                                                                                                         1\n",
      "President and comander in chief of the armed forces                                                                                                             1\n",
      "Acting President                                                                                                                                                1\n",
      "President of the National Council of Government                                                                                                                 1\n",
      "Name: position, dtype: int64\n",
      "\n",
      "=== Vice-Presidents ===\n",
      "Vice-President                                                                                                                                77\n",
      "Vice President                                                                                                                                16\n",
      "Vice-President                                                                                                                                 7\n",
      "vice-President                                                                                                                                 3\n",
      "Vice-President and Minister for Foreign Affairs                                                                                                2\n",
      "First Vice-President                                                                                                                           2\n",
      "First Vice-President of the Supreme Military Council and Commis·sioner for External Affairs of Equatorial Guinea                               1\n",
      "First Vice-President and Prime Minister                                                                                                        1\n",
      "First Vice-President and Minister for Foreign Affairs                                                                                          1\n",
      "Vice President and Misnter of External Relations                                                                                               1\n",
      "Vice-President of the Prime Minister's Council                                                                                                 1\n",
      "President of the Republic of Malawi, Minister for Defense and Commander-in-Chief of the Malawi Defense Force and the Malawi Police Service     1\n",
      "Vice-President and Minister for Women’s Affairs                                                                                                1\n",
      "First Vice-President                                                                                                                           1\n",
      "Vice President of the Republic                                                                                                                 1\n",
      "Name: position, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# All President (but not Vice)\n",
    "president_positions = position_counts[\n",
    "    position_counts.index.str.contains(r'\\bpresident\\b', case=False, na=False)\n",
    "    & ~position_counts.index.str.contains(\"vice\", case=False, na=False)\n",
    "]\n",
    "\n",
    "# All Vice-President / Vice President\n",
    "vice_president_positions = position_counts[\n",
    "    position_counts.index.str.contains(\"vice\", case=False, na=False)\n",
    "    & position_counts.index.str.contains(\"president\", case=False, na=False)\n",
    "]\n",
    "\n",
    "print(\"=== Presidents ===\")\n",
    "print(president_positions)\n",
    "\n",
    "print(\"\\n=== Vice-Presidents ===\")\n",
    "print(vice_president_positions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "6a1ce3d9-a03d-46f6-90aa-923660bc4474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_position(pos):\n",
    "    if pd.isna(pos):\n",
    "        return pos\n",
    "\n",
    "    pos = pos.strip()\n",
    "\n",
    "    # --- Fix common typos and extra spaces ---\n",
    "    pos = re.sub(r'\\s+', ' ', pos)  # collapse multiple spaces\n",
    "    pos_lower = pos.lower()\n",
    "\n",
    "     # Turn all ministers that deal with foreign affairs and international relations to \"Minister for Foreign Affairs\n",
    "    foreign_affairs_variants = [\n",
    "        'minister for foregn affairs',\n",
    "        'minister responsible for foreign affairs',\n",
    "        'minsiter for foreign and caricom affairs',\n",
    "        'minister for external affairs',\n",
    "        'minister of external relations',  # <-- added\n",
    "        'foreign minister',\n",
    "        'minister for international affairs and cooperation',\n",
    "        'minister for external relations',\n",
    "        'federal minister for european and international affairs',\n",
    "        'international cooperation',\n",
    "        'federal minister for foreign affairs',\n",
    "        'minister for foreign and caricom affairs',\n",
    "        'minister of foreign affairs and cooperation',\n",
    "        'minister for international relations and cooperation',\n",
    "        'ministry of external relations',\n",
    "        'acting minister for foreign affairs and international cooperation',\n",
    "        'ministry of foreign affairs',\n",
    "        'minister for foreign and political affairs',\n",
    "        'federal minister for europe, integration, and foreign affairs',\n",
    "        'federal minister for europe, integration and foreign affairs',\n",
    "        'deputy minister for foreign affairs',\n",
    "        'deputy minister foreign affairs',\n",
    "        'second minister for foreign affairs',\n",
    "        'second minister for foreign affairs and trade',\n",
    "        'vice minister for foreign affairs',\n",
    "        'minister of foreign and european affaris',\n",
    "        'minister of foreign affairs',\n",
    "        'minister for foreign',\n",
    "        'minister of foreign and european affairs and minister of immigration and asylum',\n",
    "        'minister for foreign affairs and senegalese living abroad',\n",
    "        'minister for foreign affairs with responsibility for brexit',\n",
    "        'minister for foreign affairs and investment promotion'\n",
    "    ]\n",
    "    if any(variant in pos_lower for variant in foreign_affairs_variants):\n",
    "        return \"Minister for Foreign Affairs\"\n",
    "\n",
    "    # --- Fix \"rime minister\" typo ---\n",
    "    pos = re.sub(r'(?i)\\brime[- ]?minister\\b', 'Prime Minister', pos)\n",
    "\n",
    "    # Normalize different versions of Head of Government, President, Prime Minsiter and Vice-President-\n",
    "    exact_matches = {\n",
    "        r'(?i)^president of (the )?government$': 'Head of Government',\n",
    "        r'(?i)^acting president$': 'President',\n",
    "        r'(?i)^interim president$': 'President',\n",
    "        r'(?i)^constitutional president$': 'President',\n",
    "        r'(?i)^first executive president$': 'President',\n",
    "        r'(?i)^first prime[- ]?minister$': 'Prime Minister',\n",
    "        r'(?i)^head of the goverment$': 'Head of Government',  # <-- catch typo + spaces\n",
    "        r'(?i)^head\\s+of\\s+govern?ment$': 'Head of Government',\n",
    "        r'(?i)^first vice[- ]?president$': 'Vice-President'\n",
    "    }\n",
    "    for pattern, replacement in exact_matches.items():\n",
    "        if re.fullmatch(pattern, pos):\n",
    "            return replacement\n",
    "\n",
    "    # --- Normalize prefixes ---\n",
    "    pos = re.sub(r'(?i)^first vice[- ]?president\\b', 'Vice-President', pos)\n",
    "    pos = re.sub(r'(?i)\\bprime[- ]?minister\\b', 'Prime Minister', pos)\n",
    "    pos = re.sub(r'(?i)\\bpresident\\b', 'President', pos)\n",
    "    pos = re.sub(r'(?i)\\bvice[- ]?president\\b', 'Vice-President', pos)\n",
    "\n",
    "    # --- Collapse primary roles if they appear at start ---\n",
    "    primary_roles = [\n",
    "        (r'(?i)^prime[- ]?minister\\b', 'Prime Minister'),\n",
    "        (r'(?i)^deputy prime[- ]?minister\\b', 'Deputy Prime Minister'),\n",
    "        (r'(?i)^president\\b', 'President'),\n",
    "        (r'(?i)^vice[- ]?president\\b', 'Vice-President'),\n",
    "        (r'(?i)^head of state\\b', 'Head of State'),\n",
    "        (r'(?i)^chairman of the council of ministers\\b', 'Chairman of the Council of Ministers'),\n",
    "        (r'(?i)^(crown prince|prince|king|emir|amir)\\b', 'Monarch'),\n",
    "        (r'(?i)^(un representative|permanent representative|delegation|chair of (the )?delegation|chair of diplomatic representative)\\b', 'Diplomatic Representative')\n",
    "    ]\n",
    "    for pattern, replacement in primary_roles:\n",
    "        if re.match(pattern, pos):\n",
    "            return replacement\n",
    "\n",
    "    # --- Monarchs ---\n",
    "    if re.search(r'(?i)\\b(crown prince|prince|king|emir|amir)\\b', pos):\n",
    "        return \"Monarch\"\n",
    "\n",
    "    # --- Head of State ---\n",
    "    if re.search(r'(?i)head of state', pos):\n",
    "        return \"Head of State\"\n",
    "        \n",
    "    # --- Diplomatic Representatives ---\n",
    "    if re.search(r'(?i)(un representative|permanent representative|delegation|chair of (the )?delegation|chair of diplomatic representative)', pos):\n",
    "        return \"Diplomatic Representative\"\n",
    "\n",
    "      # --- Religious Leaders ---\n",
    "    if re.search(r'(?i)\\b(pope|head of the church|sheikh)\\b', pos):\n",
    "        return \"Religious Leader\"\n",
    "\n",
    "    # --- Everything else ---\n",
    "    return \"Others\"\n",
    "\n",
    "# Apply\n",
    "df_merged[\"position\"] = df_merged[\"position\"].apply(normalize_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "14ececb8-262d-4270-8d52-5742a864cf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN                                     4693\n",
      "Minister for Foreign Affairs            2333\n",
      "President                               1878\n",
      "Prime Minister                           939\n",
      "Diplomatic Representative                333\n",
      "Deputy Prime Minister                    251\n",
      "Vice-President                           112\n",
      "Head of State                             74\n",
      "Monarch                                   56\n",
      "Others                                    53\n",
      "Head of Government                        48\n",
      "Chairman of the Council of Ministers       4\n",
      "Religious Leader                           3\n",
      "Name: position, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Pandas so einstellen, dass es alles ausgibt\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "# Alle Positionen mit Häufigkeit\n",
    "position_counts = df_merged['position'].value_counts(dropna=False)\n",
    "\n",
    "print(position_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41d97e2-840b-40eb-b032-197040a0246c",
   "metadata": {},
   "source": [
    "#### New Variable: Country (Year)\n",
    "\n",
    "This variable is later needed to create clean description plots and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "d647e412-7bd4-4190-9c34-39874b4620a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_merged.copy()\n",
    "df_merged['speech_label'] = df_merged['country_name'] + \" (\" + df_merged['year'].astype(str) + \")\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc05d65d-be72-42ae-b753-0940de5df17c",
   "metadata": {},
   "source": [
    "#### Save dataframe with all new variables as ?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "94a901c8-3fc4-4b33-a1ff-cede22bd196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(wd)\n",
    "\n",
    "# Save df_merged as a pickle file for quick future loading\n",
    "merged_pickle_path = r\".\\data\\un_corpus_merged.pkl\"\n",
    "df_merged.to_pickle(merged_pickle_path)\n",
    "\n",
    "# Export df as CSV \n",
    "merged_output_path = r\".\\data\\un_corpus_merged.csv\"\n",
    "df_merged.to_csv(merged_output_path, index=False, sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea36f19a-789d-4c62-a470-3e73a6d848bf",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe0f4af-e939-4100-9618-5b522bdd72a1",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2661f431-a194-47d1-9a1c-89162bf85a91",
   "metadata": {},
   "source": [
    "#### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd37e08d-b4f2-459e-823c-3816a825ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Clean text by removing empty spaces, line breaks, hyphenation, stray characters, and escape quote ==\n",
    "\n",
    "# Define cleaning function\n",
    "def clean_text(content):\n",
    "    if pd.isna(content):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove line breaks and carriage returns\n",
    "    content = content.replace('\\n', ' ').replace('\\r', ' ')\n",
    "\n",
    "    # Collapse multiple spaces\n",
    "    content = ' '.join(content.split())\n",
    "\n",
    "    # Ensure spacing after punctuation\n",
    "    content = re.sub(r'(?<=[.,])(?=[^\\s])', r' ', content)\n",
    "\n",
    "    # Remove hyphenation at line breaks (e.g. \"inter- national\" → \"international\")\n",
    "    content = re.sub(r'-\\s', '', content)\n",
    "\n",
    "     # Replace hyphen between letters with a space to prevent merging words (e.g. \"russian-and\" → \"russian and\")\n",
    "    content = re.sub(r'(?<=\\w)-(?=\\w)', ' ', content)\n",
    "\n",
    "    # Remove stray backslashes\n",
    "    content = content.replace(\"\\\\\", \"\")\n",
    "\n",
    "    return content\n",
    "\n",
    "# Apply cleaning to each speech\n",
    "df_raw['speech'] = df_raw['speech'].astype(str)  # Ensure column is string type\n",
    "df_clean = df_raw.copy()\n",
    "df_clean['speech'] = df_clean['speech'].apply(clean_text)\n",
    "\n",
    "# Drop rows with empty speeches after cleaning\n",
    "df_clean = df_clean[df_clean['speech'].str.strip().astype(bool)].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7c6bd148-d5cf-4eb3-a66d-fc706e99c13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved clean speeches chunks in 'C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp'\n"
     ]
    }
   ],
   "source": [
    "# == Split cleaned data into chunks and save as separate files ==\n",
    "\n",
    "# Convert cleaned DataFrame to list of lists\n",
    "clean_data = df_clean[['filename', 'speech']].values.tolist()\n",
    "\n",
    "# Split cleaned data into 4 equal chunks\n",
    "data_id1 = clean_data[:int(len(clean_data)/4)]\n",
    "data_id2 = clean_data[int(len(clean_data)/4): int(2*len(clean_data)/4)]\n",
    "data_id3 = clean_data[int(2*len(clean_data)/4): int(3*len(clean_data)/4)]\n",
    "data_id4 = clean_data[int(3*len(clean_data)/4):]\n",
    "\n",
    "# Change directory to the temp folder\n",
    "os.chdir(data_temp)  # make sure `data_temp` exists and is defined\n",
    "\n",
    "# Save each chunk with joblib\n",
    "joblib.dump(data_id1, 'clean_speeches_indexed1.pkl')\n",
    "joblib.dump(data_id2, 'clean_speeches_indexed2.pkl')\n",
    "joblib.dump(data_id3, 'clean_speeches_indexed3.pkl')\n",
    "joblib.dump(data_id4, 'clean_speeches_indexed4.pkl')\n",
    "\n",
    "# Store list of cleaned data chunk paths to feed into preprocessing function later\n",
    "clean_files = [\n",
    "    os.path.join(data_temp, 'clean_speeches_indexed1.pkl'),\n",
    "    os.path.join(data_temp, 'clean_speeches_indexed2.pkl'),\n",
    "    os.path.join(data_temp, 'clean_speeches_indexed3.pkl'),\n",
    "    os.path.join(data_temp, 'clean_speeches_indexed4.pkl')\n",
    "]\n",
    "\n",
    "print(f\"✅ Saved clean speeches chunks in '{data_temp}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6e5109-8cf7-41e3-8fb9-8e7dde29b4e0",
   "metadata": {},
   "source": [
    "### Advanced Text Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfca8dd9-c6d7-48c7-96b0-f5a449814336",
   "metadata": {},
   "source": [
    "#### Extend stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "11d056b5-6729-45f8-a233-3e8e6de1f69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 326 stopwords to C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\stopwords.pkl\n"
     ]
    }
   ],
   "source": [
    "# Get SpaCy stopwords\n",
    "SPACY_STOPWORDS = list(nlp.Defaults.stop_words)\n",
    "\n",
    "# Path to save\n",
    "stopwords_path = os.path.join(data_c, \"stopwords.pkl\")\n",
    "\n",
    "# Save stopwords\n",
    "joblib.dump(SPACY_STOPWORDS, stopwords_path)\n",
    "\n",
    "print(f\"Saved {len(SPACY_STOPWORDS)} stopwords to {stopwords_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0aa75944-4b75-4aa4-9430-9aa6bd9ac934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Define function to Tokenize, eliminate digits, remove stopwords, lemmatize, POS-Tagging ==\n",
    "\n",
    "def pro1(lista):\n",
    "    # Remove punctuation\n",
    "    return [[row[0], row[1].translate(translator)] for row in lista]\n",
    "\n",
    "def pro2(lista):\n",
    "    # Tokenize and lowercase with gensim\n",
    "    return [[row[0], gensim.utils.simple_preprocess(row[1])] for row in lista]\n",
    "\n",
    "def pro3(lista):\n",
    "    # Remove tokens that are only digits\n",
    "        a = [[row[0], [w for w in row[1] if not w.isdigit()]] for row in lista]\n",
    "        return a\n",
    "    \n",
    "def pro4(lista):\n",
    "    # Drop short words\n",
    "    return [[row[0], [w for w in row[1] if len(w) > 2]] for row in lista]\n",
    "\n",
    "def tags_spacy(lista):\n",
    "    texts = [' '.join(row[1]) for row in lista]\n",
    "    docs = list(nlp.pipe(texts, batch_size=20, n_process=1))\n",
    "    result = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        filtered_tokens = [token.text for token in doc if token.tag_.startswith(('N', 'V', 'J'))]\n",
    "        result.append([lista[i][0], filtered_tokens])\n",
    "    return result\n",
    "\n",
    "\n",
    "def pro5(lista):\n",
    "    # Remove stopwords using SpaCy stopword list\n",
    "    return [[row[0], [w for w in row[1] if w not in SPACY_STOPWORDS]] for row in lista]\n",
    "\n",
    "def pro6(lista):\n",
    "      return [\n",
    "        [row[0], [stemmer.stem(token) for token in row[1]]]\n",
    "        for row in lista\n",
    "    ]\n",
    "########################## Question for Max: They removed procedural words in the paper\n",
    "\n",
    "def dropnull(lista):\n",
    "    # Drop empty speeches\n",
    "    return [row for row in lista if len(' '.join(row[1])) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "524a274a-004c-4d87-9bb3-5f789a42b90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl] Before tagging: 1.10s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl] After tagging: 31.77s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed1.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl] Done. Total time: 37.41s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl] Before tagging: 1.04s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl] After tagging: 30.06s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed2.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl] Done. Total time: 35.52s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl] Before tagging: 0.88s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl] After tagging: 30.68s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed3.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl] Done. Total time: 35.81s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl] Before tagging: 0.92s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl] After tagging: 27.80s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed4.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl] Done. Total time: 32.34s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# == Create full pre-processing function and call it\n",
    "\n",
    "def preprocessing(data_name):\n",
    "    t0 = time.time()\n",
    "    print(f\"Starting preprocessing for {data_name}...\")\n",
    "\n",
    "    data = joblib.load(data_name)\n",
    "    data = pro1(data)\n",
    "    data = pro2(data)\n",
    "    data = pro3(data)\n",
    "    data = pro4(data)\n",
    "\n",
    "    print(f\"[{data_name}] Before tagging: {time.time() - t0:.2f}s\")\n",
    "    data = tags_spacy(data)\n",
    "    print(f\"[{data_name}] After tagging: {time.time() - t0:.2f}s\")\n",
    "\n",
    "    data = pro5(data)\n",
    "   # data = pro6(data)\n",
    "    data = dropnull(data)\n",
    "\n",
    "    # out_name = data_name.replace('cleanspeeches_', 'preprocessed_speeches_').replace('.pkl', '_temp.pkl')\n",
    "\n",
    "    # Store preprocessed corupus (before stemming) for wordcloud\n",
    "    filename_wordcloud = data_name.replace('clean_speeches_', 'wordcloud_speeches_').replace('.pkl', '.pkl')\n",
    "    out_name_wordcloud = os.path.join(data_preprocessed, os.path.basename(filename_wordcloud))\n",
    "    joblib.dump(data, out_name_wordcloud)\n",
    "\n",
    "    # Apply stemming\n",
    "    data_stemmed = pro6(data)\n",
    "\n",
    "    filename_preprocessed = data_name.replace('clean_speeches_', 'preprocessed_speeches_').replace('.pkl', '.pkl')\n",
    "    out_preprocessed = os.path.join(data_preprocessed, os.path.basename(filename_preprocessed))\n",
    "    joblib.dump(data_stemmed, out_preprocessed\n",
    "               )\n",
    "    print(f\"[{data_name}] Saved stemmed version: {out_preprocessed}\")\n",
    "\n",
    "    print(f\"[{data_name}] Done. Total time: {time.time() - t0:.2f}s\\n\")\n",
    "\n",
    "def main():\n",
    "    for fname in clean_files:\n",
    "        preprocessing(fname)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d3409987-485c-47c9-9430-01fa2c0e650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the pre-processed data\n",
    "preprocessed_files = [\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed1.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed2.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed3.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed4.pkl')\n",
    "]\n",
    "\n",
    "wordcloud_files = [\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed1.pkl'),\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed2.pkl'),\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed3.pkl'),\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed4.pkl')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "134ddbf7-9b95-4697-9d9c-f9c55b52ec80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73db5f7-566d-4276-a631-c1f33fab494d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d006e-6a23-453b-9c91-0e60bd3f8301",
   "metadata": {},
   "source": [
    "## Word-Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf2443e-4c76-41b2-bc16-e72d91c318a8",
   "metadata": {},
   "source": [
    "### Count frequencies of all tokens and display the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "154d978d-cddd-431c-9d53-73c9faf8fdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:02<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed1.pkl: removed words with freq < 10\n",
      "Processed C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed2.pkl: removed words with freq < 10\n",
      "Processed C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed3.pkl: removed words with freq < 10\n",
      "Processed C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed4.pkl: removed words with freq < 10\n",
      "\n",
      "[Stemmed] Top 50 most common words:\n",
      "nation: 12923\n",
      "unit: 10584\n",
      "countri: 10339\n",
      "intern: 9114\n",
      "develop: 8329\n",
      "state: 7886\n",
      "world: 7620\n",
      "peac: 7445\n",
      "peopl: 7201\n",
      "secur: 4740\n",
      "general: 4582\n",
      "govern: 4426\n",
      "econom: 4345\n",
      "organ: 3835\n",
      "assembl: 3768\n",
      "right: 3725\n",
      "year: 3677\n",
      "problem: 3446\n",
      "new: 3404\n",
      "effort: 3276\n",
      "continu: 3183\n",
      "human: 3091\n",
      "support: 3077\n",
      "polit: 2861\n",
      "communiti: 2847\n",
      "time: 2685\n",
      "region: 2632\n",
      "member: 2513\n",
      "session: 2478\n",
      "africa: 2466\n",
      "import: 2327\n",
      "war: 2314\n",
      "need: 2273\n",
      "council: 2266\n",
      "achiev: 2251\n",
      "work: 2228\n",
      "conflict: 2174\n",
      "hope: 2152\n",
      "situat: 2130\n",
      "principl: 2124\n",
      "power: 2082\n",
      "relat: 2081\n",
      "forc: 2074\n",
      "south: 2035\n",
      "presid: 1994\n",
      "republ: 1981\n",
      "oper: 1962\n",
      "global: 1956\n",
      "concern: 1950\n",
      "order: 1924\n",
      "\n",
      "[Stemmed] Top 50 least common words:\n",
      "simul: 1\n",
      "impenit: 1\n",
      "cession: 1\n",
      "shill: 1\n",
      "compagni: 1\n",
      "triptych: 1\n",
      "banker: 1\n",
      "léopold: 1\n",
      "sédar: 1\n",
      "roubl: 1\n",
      "bagaza: 1\n",
      "signer: 1\n",
      "meaber: 1\n",
      "botha: 1\n",
      "placatori: 1\n",
      "ah: 1\n",
      "ifad: 1\n",
      "cuter: 1\n",
      "mankinj: 1\n",
      "hardwon: 1\n",
      "praia: 1\n",
      "dizzi: 1\n",
      "lang: 1\n",
      "belfast: 1\n",
      "prorogu: 1\n",
      "irishmen: 1\n",
      "cakobau: 1\n",
      "nought: 1\n",
      "copra: 1\n",
      "perfection: 1\n",
      "canberra: 1\n",
      "transient: 1\n",
      "depolar: 1\n",
      "subjectiv: 1\n",
      "endanger: 1\n",
      "dovetail: 1\n",
      "refashion: 1\n",
      "warp: 1\n",
      "recast: 1\n",
      "obloquy: 1\n",
      "plc: 1\n",
      "uncomplet: 1\n",
      "constanc: 1\n",
      "bandaranaik: 1\n",
      "guyer: 1\n",
      "endu: 1\n",
      "sinaimor: 1\n",
      "theafrican: 1\n",
      "arabeuropean: 1\n",
      "vij: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:02<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Wordcloud] Top 50 most common words:\n",
      "united: 10469\n",
      "nations: 10232\n",
      "international: 8746\n",
      "world: 7458\n",
      "countries: 6812\n",
      "peace: 6071\n",
      "states: 5709\n",
      "development: 4755\n",
      "people: 4585\n",
      "general: 4468\n",
      "security: 4398\n",
      "economic: 4318\n",
      "assembly: 3700\n",
      "new: 3404\n",
      "country: 3392\n",
      "government: 3355\n",
      "organization: 2874\n",
      "political: 2748\n",
      "community: 2664\n",
      "human: 2663\n",
      "efforts: 2649\n",
      "peoples: 2611\n",
      "africa: 2389\n",
      "rights: 2367\n",
      "session: 2363\n",
      "support: 2272\n",
      "council: 2234\n",
      "time: 2204\n",
      "war: 2041\n",
      "south: 2033\n",
      "problems: 1997\n",
      "state: 1952\n",
      "republic: 1914\n",
      "situation: 1887\n",
      "great: 1869\n",
      "national: 1864\n",
      "developing: 1858\n",
      "years: 1854\n",
      "nuclear: 1843\n",
      "order: 1839\n",
      "year: 1817\n",
      "conference: 1748\n",
      "global: 1744\n",
      "social: 1717\n",
      "work: 1675\n",
      "relations: 1637\n",
      "charter: 1569\n",
      "hope: 1560\n",
      "continue: 1548\n",
      "african: 1547\n",
      "\n",
      "[Wordcloud] Top 50 least common words:\n",
      "irishmen: 1\n",
      "absolutes: 1\n",
      "cakobau: 1\n",
      "wings: 1\n",
      "venturing: 1\n",
      "nought: 1\n",
      "copra: 1\n",
      "perfectionism: 1\n",
      "scribed: 1\n",
      "dissimilarities: 1\n",
      "islanders: 1\n",
      "canberra: 1\n",
      "expired: 1\n",
      "exhilarating: 1\n",
      "transient: 1\n",
      "oligarchical: 1\n",
      "depolarization: 1\n",
      "subjectivism: 1\n",
      "endangerment: 1\n",
      "harbingers: 1\n",
      "dovetailing: 1\n",
      "wrench: 1\n",
      "refashioning: 1\n",
      "warped: 1\n",
      "recast: 1\n",
      "pervert: 1\n",
      "tri: 1\n",
      "obloquy: 1\n",
      "obviousness: 1\n",
      "plc: 1\n",
      "pointlessness: 1\n",
      "stigmas: 1\n",
      "scrupulousness: 1\n",
      "uncompleted: 1\n",
      "constancy: 1\n",
      "bandaranaike: 1\n",
      "supersede: 1\n",
      "stipulating: 1\n",
      "touchstones: 1\n",
      "guyer: 1\n",
      "enduing: 1\n",
      "sinaimore: 1\n",
      "theafrican: 1\n",
      "procrastinating: 1\n",
      "arabism: 1\n",
      "arabeuropean: 1\n",
      "voyages: 1\n",
      "supplanting: 1\n",
      "vij: 1\n",
      "accedes: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit\\\\data\\\\freq\\\\word_counts_wordcloud.pkl']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#== Count token frequencies ==\n",
    "\n",
    "def count_frequencies(filenames):\n",
    "    total_freq = Counter()\n",
    "    for fname in tqdm(filenames):\n",
    "        data = joblib.load(fname)\n",
    "        tokens = chain.from_iterable(row[1] for row in data if isinstance(row[1], list))\n",
    "        total_freq.update(tokens)\n",
    "    return total_freq\n",
    "\n",
    "def remove_rare_words(filenames, freqs, min_count=10):\n",
    "    for fname in filenames:\n",
    "        data = joblib.load(fname)\n",
    "        filtered_data = []\n",
    "        for doc_id, tokens in data:\n",
    "            filtered_tokens = [w for w in tokens if freqs.get(w, 0) >= min_count]\n",
    "            filtered_data.append([doc_id, filtered_tokens])\n",
    "        joblib.dump(filtered_data, fname)  # overwrite or save as new file\n",
    "        print(f\"Processed {fname}: removed words with freq < {min_count}\")\n",
    "\n",
    "# === Count for preprocessed (stemmed) speeches ===\n",
    "word_counts_stemmed = count_frequencies(preprocessed_files)\n",
    "\n",
    "remove_rare_words(preprocessed_files, word_counts_stemmed, min_count=10)\n",
    "\n",
    "print(\"\\n[Stemmed] Top 50 most common words:\")\n",
    "for word, count in word_counts_stemmed.most_common(50):\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\n[Stemmed] Top 50 least common words:\")\n",
    "for word, count in word_counts_stemmed.most_common()[-50:]:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Save stemmed word counts\n",
    "save_path_stemmed = os.path.join(data_freq, 'word_counts_stemmed.pkl')\n",
    "joblib.dump(word_counts_stemmed, save_path_stemmed)\n",
    "\n",
    "# === Count for wordcloud (unstemmed) speeches ===\n",
    "word_counts_wordcloud = count_frequencies(wordcloud_files)\n",
    "\n",
    "print(\"\\n[Wordcloud] Top 50 most common words:\")\n",
    "for word, count in word_counts_wordcloud.most_common(50):\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\n[Wordcloud] Top 50 least common words:\")\n",
    "for word, count in word_counts_wordcloud.most_common()[-50:]:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Save unstemmed word counts\n",
    "save_path_wordcloud = os.path.join(data_freq, 'word_counts_wordcloud.pkl')\n",
    "joblib.dump(word_counts_wordcloud, save_path_wordcloud)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "932ce36d-d21c-4f1c-aff7-899c321dcfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_c)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39acefa1-c504-478e-98e5-9d465963dc82",
   "metadata": {},
   "source": [
    "### Count the frequency of the dictionary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9e28f481-7e20-486c-a6b3-8729d6aeaad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of affect dictionary:\n",
      "['forbid', 'unattract', 'cruelti', 'crappi', 'apathi', 'scari', 'unimpress', 'sin', 'dumbest', 'eas', 'agit', 'sob', 'shocker', 'tragedi', 'fabul', 'strongest', 'giver', 'sigh', 'aw', 'witch', 'hurtl', 'fucktard', 'cruel', 'glamor', 'funni', 'smarter', 'brillianc', 'irrate', 'alright', 'honest', 'profit', 'fearless', 'grievous', 'relax', 'isolationist', 'hah', 'shyness', 'poorest', 'cruelest', 'troublemak', 'disagre', 'agon', 'terror', 'fight', 'pleas', 'poor', 'crazi', 'hostil', 'stupid', 'damnat', 'vain', 'jade', 'heartless', 'nag', 'gloomi', 'damn', 'dishearten', 'pleaser', 'credit', 'warmth', 'greatest', 'whine', 'shame', 'angriest', 'envious', 'grin', 'blameless', 'sweeter', 'laidback', 'stupidest', 'unprotect', 'whiner', 'unlov', 'shake', 'boredom', 'fairer', 'weaker', 'wellb', 'bold', 'sucki', 'unsuccess', 'mourner', 'liken', 'defens', 'invigor', 'tedious', 'paranoid', 'cynic', 'dignifi', 'paranoia', 'sweetest', 'contented', 'humili', 'crush', 'terrif', 'alarm', 'asham', 'impati', 'fake', 'flirti', 'laziest', 'libertarian', 'empti', 'panicki', 'rape', 'contemptu', 'incent', 'reward', 'brutal', 'ignoramus', 'bereav', 'wimp', 'laugh', 'nicer', 'warm', 'resent', 'threat', 'hell', 'tortur', 'mad', 'worship', 'uncomfort', 'depress', 'unsaf', 'faultless', 'numb', 'humorless', 'murder', 'prettiest', 'jealous', 'tragic', 'discomfort', 'loveli', 'ungrat', 'divin', 'lamest', 'weaken', 'horribl', 'fuck', 'agoni', 'grievanc', 'ach', 'isolation', 'masoch', 'uglier', 'amaz', 'devilish', 'bitter', 'devil', 'smartest', 'glori', 'trite', 'vicious', 'unsavori', 'grossest', 'sinist', 'cheeri', 'dismay', 'easier', 'flatter', 'virtuous', 'egot', 'generos', 'cherish', 'heartwarm', 'advers', 'tough', 'adorn', 'overwhelm', 'devast', 'egotist', 'sociabl', 'lazier', 'affection', 'sarcasm', 'humor', 'funniest', 'startl', 'innoc', 'vomit', 'shyli', 'kiss', 'burdensom', 'melancholi', 'weakest', 'worthless', 'gentlest', 'proud', 'alarmist', 'pervert', 'demeanor', 'loneli', 'reassur', 'weird', 'passionless', 'loyalti', 'frustrat', 'weak', 'immor', 'hater', 'libertin', 'flirt', 'terribl', 'threaten', 'fatalist', 'melancholia', 'avers', 'prick', 'sucker', 'meanest', 'virtuos', 'benefici', 'neurot', 'disappoint', 'restless', 'romant', 'bastard', 'angri', 'flatteri', 'stun', 'bolder', 'amor', 'forbidden', 'hug', 'demean', 'disgust', 'truer', 'benefic', 'teas', 'stinki', 'condemnatori', 'turmoil', 'poison', 'cunt', 'gorgeous', 'harm', 'lousi', 'glorifi', 'piti', 'wimpish', 'impression', 'freakish', 'happiest', 'apathet', 'pitiabl', 'unpleas', 'suffer', 'miseri', 'toler', 'mess', 'casual', 'anxieti', 'stale', 'fieri', 'grudg', 'arrog', 'outrag', 'thrill', 'faithless', 'popular', 'vaniti', 'palat', 'fantasi', 'decay', 'domin', 'mood', 'proudest', 'scream', 'stress', 'calm', 'fatigu', 'dumber', 'benign', 'rude', 'steal', 'tension', 'enthusiast', 'intimid', 'sweetheart', 'prettier', 'perv', 'pathet', 'flawless', 'contradictori', 'distress', 'moron', 'gratifi', 'frighten', 'anguish', 'snob', 'defenseless', 'fantast', 'sad', 'thief', 'worsen', 'tear', 'handsom', 'attract', 'messi', 'smother', 'scarier', 'afraid', 'insult', 'disreput', 'wept', 'victim', 'traumat', 'prejudic', 'amus', 'decent', 'joyous', 'masochist', 'abandon', 'foe', 'pleasantri', 'shitless', 'strengthen', 'antagonist', 'liar', 'romantic', 'violenc', 'sentimentalist', 'gratif', 'decept', 'witchcraft', 'liveli', 'impressionist', 'guilt', 'griev', 'rotten', 'disgrac', 'insincer', 'pushi', 'enjoy', 'freaki', 'ador', 'grate', 'homesick', 'braver', 'battlefield', 'niceti', 'hooray', 'incompet', 'thiev', 'timid', 'adventur', 'needi', 'joyless', 'grimac', 'gloom', 'dump', 'horrid', 'magnific', 'unlucki', 'killer', 'uncontrol', 'funnier', 'ruin', 'weepi', 'play', 'encourag', 'fantas', 'irrit', 'sincer', 'rage', 'despis', 'unhappi', 'hate', 'bravest', 'harmless', 'hero', 'vigor', 'pervers', 'dumb', 'degrad', 'shameless', 'villaini', 'envi', 'aggress', 'fucker', 'courag', 'craziest', 'delici', 'scare', 'finer', 'bittersweet', 'trivial', 'wealthi', 'heroic', 'ecstat', 'irrat', 'prejudici', 'smug', 'stammer', 'feroci', 'phobic', 'villain', 'anger', 'fond', 'obsession', 'awesom', 'wors', 'compassion', 'weakl', 'calmer', 'inhibit', 'bother', 'rancid', 'hatr', 'beaten', 'opportunist', 'offens', 'haunt', 'appal', 'painless', 'pessimist', 'molest', 'lol', 'wrongdo', 'frantic', 'heroin', 'rejoic', 'woe', 'miser', 'meaner', 'saddest', 'mooch', 'attack', 'enthusiasm', 'joke', 'meritocraci', 'angrier', 'emot', 'weirdest', 'silliest', 'kisser', 'hurrah', 'crueler', 'nervous', 'dishonor', 'gracious', 'sicker', 'hopeless', 'delect', 'compliment', 'petrifi', 'cuter', 'reek', 'loveless', 'daze', 'weirder', 'triumphant', 'impolit', 'offend', 'richer', 'violent', 'crazier', 'comedown', 'heroism', 'stubborn', 'maniac', 'strain', 'broke', 'jerk', 'unkind', 'asshol', 'reveng', 'jealousi', 'damnabl', 'unimport', 'careless', 'gentler', 'bitch', 'shi', 'terrorist', 'rigid', 'harsh', 'passion', 'annoy', 'compass', 'geek', 'weirdo', 'agre', 'whore', 'splendor', 'suck', 'fun', 'chuckl', 'snobberi', 'mock', 'doom', 'reluct', 'mourn', 'boldest', 'helpless', 'trembl', 'pestil', 'curs', 'desper', 'harass', 'complain', 'joker', 'obsess', 'uneasi', 'nasti', 'greedi', 'belov', 'grim', 'upset', 'greed', 'honesti', 'phoni', 'feudal', 'loveliest', 'trauma', 'dwell', 'ecstasi', 'blame', 'legit', 'calmest', 'sickest', 'scariest', 'sarcast', 'cheat', 'fatal', 'stronger', 'energet', 'feroc', 'healthi', 'goddam', 'disast', 'flirtati', 'defeatist', 'glorious', 'terrifi', 'gossip', 'forbad', 'sicken', 'poorer', 'strangest', 'cute', 'struggl', 'woebegon', 'worri', 'gratitud', 'cheer', 'distraught', 'ok', 'vulner', 'blessed', 'fairest', 'lame', 'optim', 'grouch', 'fright', 'pessim', 'sadder', 'worst', 'despair', 'bash', 'intellectu', 'promis', 'prais', 'heartfelt', 'horror', 'crap', 'repress', 'prouder', 'fought', 'discourag', 'heal', 'lazi', 'precious', 'melanchol', 'isol', 'aggressor', 'giggl', 'idiot', 'twitchi', 'glorif', 'kid', 'selfish', 'smile', 'grief', 'hellish', 'shit', 'fume', 'bitchi', 'fool', 'trustworthi', 'optimist', 'dumpi', 'unwelcom', 'neat', 'nicest', 'furi', 'unfriend', 'ridicul', 'arguabl', 'shamefac', 'pervi', 'upbeat', 'wow', 'easygo', 'blameworthi', 'charmer', 'stink', 'phobia', 'adversari', 'sillier', 'distrust', 'dread', 'happier', 'vital', 'dissatisfact', 'insecur', 'assault', 'ineffect', 'panic', 'powerless', 'nightmar', 'wealth', 'enrag', 'splendid', 'foolish', 'ugliest', 'contradict', 'worrier', 'freak']\n",
      "Number of words in affect dictionary: 629\n",
      "Contents of cognition dictionary:\n",
      "['analyt', 'guess', 'suspect', 'coz', 'mysteri', 'unawar', 'proverbi', 'dissimilar', 'consequenti', 'stimul', 'occasion', 'rethink', 'purest', 'entireti', 'abnorm', 'referenti', 'differenti', 'diagnos', 'ambigu', 'concentr', 'undon', 'undecid', 'disclosur', 'fundamentalist', 'blur', 'reaction', 'variabl', 'recogniz', 'meant', 'reconstruct', 'supposit', 'acknowledg', 'choosi', 'accuraci', 'mislead', 'perceiv', 'hypothesi', 'unknow', 'cuz', 'reveal', 'potenti', 'puriti', 'persuad', 'indirect', 'generat', 'presumpt', 'clue', 'justif', 'evidenti', 'realize', 'deduct', 'causal', 'inevit', 'thinker', 'evidentiari', 'factor', 'undeni', 'unclear', 'pick', 'induc', 'accur', 'absolut', 'everytim', 'reconsider', 'diagnost', 'solut', 'diagnosi', 'discoveri', 'ration', 'unambigu', 'explicit', 'decis', 'insight', 'confess', 'fuzzi', 'led', 'lesson', 'theoriz', 'analyz', 'learner', 'split', 'enlighten', 'cos', 'respons', 'dubious', 'react', 'guarante', 'someday', 'blatant', 'undo', 'perceptu', 'infal', 'afterthought', 'inequ', 'feasibl', 'knowabl', 'approxim', 'logic', 'meaning', 'unknown', 'categor', 'trigger', 'incomplet', 'curious', 'attribut', 'pretend', 'reactionari', 'prove', 'meaningless', 'reactiv', 'proof', 'bet', 'purpos', 'suppos', 'discern', 'imagin', 'kinda', 'comprehens', 'sens', 'percept', 'everyday', 'rational', 'unquestion', 'random', 'implicit', 'clarif', 'theori', 'dunno', 'analysi', 'obedi', 'memor', 'genuin', 'evalu', 'explor', 'linkag', 'grasp', 'curios', 'methink', 'somewher', 'deduc', 'unwant', 'induct', 'unrel', 'outstand', 'discov', 'unresolv', 'exploratori', 'choos', 'perspect', 'inferenti', 'disclos', 'factual', 'arbitrari', 'conting', 'vagu', 'persuas', 'manipul', 'impli', 'somehow', 'sorta', 'borderlin', 'reconcil', 'anyhow', 'conscious', 'probabilist', 'precis', 'rearrang', 'referenc', 'obscur']\n",
      "Number of words in cognition dictionary: 169\n"
     ]
    }
   ],
   "source": [
    "# == Count dictionary words\n",
    "\n",
    "# Load dictionaries            ##### How did they come up with this dictionary? Why did they exclude words?\n",
    "\n",
    "affect_path = os.path.join(data_dict, 'dictionary_affect.pkl')\n",
    "cognition_path = os.path.join(data_dict, 'dictionary_cognition.pkl')\n",
    "\n",
    "with open(affect_path, 'rb') as f:\n",
    "    affect_dict = pickle.load(f)\n",
    "print(\"Contents of affect dictionary:\")\n",
    "print(affect_dict)\n",
    "print(\"Number of words in affect dictionary:\", len(affect_dict))\n",
    "\n",
    "with open(cognition_path, 'rb') as f:\n",
    "    cognition_dict = pickle.load(f)\n",
    "print(\"Contents of cognition dictionary:\")\n",
    "print(cognition_dict)\n",
    "print(\"Number of words in cognition dictionary:\", len(cognition_dict))\n",
    "\n",
    "affect = joblib.load(affect_path)\n",
    "cognition = joblib.load(cognition_path)\n",
    "\n",
    "a = [[i, word_counts_stemmed[i]] for i in affect if i in word_counts_stemmed]\n",
    "c = [[i, word_counts_stemmed[i]] for i in cognition if i in word_counts_stemmed]\n",
    "\n",
    "a = sorted(a, key=lambda x: x[1], reverse=True)\n",
    "c = sorted(c, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "a = [[i[0], f\"({i[1]}),\"] for i in a]\n",
    "c = [[i[0], f\"({i[1]}),\"] for i in c]\n",
    "\n",
    "a1 = ' '.join(str(r) for v in a for r in v)\n",
    "c1 = ' '.join(str(r) for v in c for r in v)\n",
    "\n",
    "affect_out_path = os.path.join(data_freq, \"affect_words.txt\")\n",
    "cog_out_path = os.path.join(data_freq, \"cog_words.txt\")\n",
    "\n",
    "os.makedirs(data_freq, exist_ok=True)  # ensure directory exists\n",
    "\n",
    "with open(affect_out_path, \"w\") as output:\n",
    "    output.write(a1)\n",
    "\n",
    "with open(cog_out_path, \"w\") as output:\n",
    "    output.write(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0dffe87f-d712-4d71-9305-fb288cf06402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 words by weighted frequency:\n",
      "francophon: 0.9987871304841413\n",
      "kidal: 0.9987871304841413\n",
      "macki: 0.9987871304841413\n",
      "complexion: 0.9987871304841413\n",
      "everwiden: 0.9987871304841413\n",
      "pointer: 0.9987871304841413\n",
      "anteced: 0.9987871304841413\n",
      "buoyanc: 0.9987871304841413\n",
      "unachiev: 0.9987871304841413\n",
      "highwat: 0.9987871304841413\n",
      "wiriyamu: 0.9987871304841413\n",
      "chawola: 0.9987871304841413\n",
      "enthron: 0.9987871304841413\n",
      "allembrac: 0.9987871304841413\n",
      "ymca: 0.9987871304841413\n",
      "footbal: 0.9987871304841413\n",
      "schoolboy: 0.9987871304841413\n",
      "newcastl: 0.9987871304841413\n",
      "knipe: 0.9987871304841413\n",
      "porter: 0.9987871304841413\n",
      "phoney: 0.9987871304841413\n",
      "minc: 0.9987871304841413\n",
      "bloodier: 0.9987871304841413\n",
      "quit: 0.9987871304841413\n",
      "talleyrand: 0.9987871304841413\n",
      "metaphys: 0.9987871304841413\n",
      "blameworthi: 0.9987871304841413\n",
      "infract: 0.9987871304841413\n",
      "reprob: 0.9987871304841413\n",
      "thiam: 0.9987871304841413\n",
      "organizet: 0.9987871304841413\n",
      "savior: 0.9987871304841413\n",
      "seaman: 0.9987871304841413\n",
      "impressionthat: 0.9987871304841413\n",
      "toolkit: 0.9987871304841413\n",
      "evergreat: 0.9987871304841413\n",
      "diabet: 0.9987871304841413\n",
      "pueril: 0.9987871304841413\n",
      "untruth: 0.9987871304841413\n",
      "aquina: 0.9987871304841413\n",
      "skirmish: 0.9987871304841413\n",
      "interlud: 0.9987871304841413\n",
      "xii: 0.9987871304841413\n",
      "somat: 0.9987871304841413\n",
      "rapacki: 0.9987871304841413\n",
      "inferno: 0.9987871304841413\n",
      "dah: 0.9987871304841413\n",
      "longoverdu: 0.9987871304841413\n",
      "icj: 0.9987871304841413\n",
      "shab: 0.9987871304841413\n",
      "slipperi: 0.9987871304841413\n",
      "riidig: 0.9987871304841413\n",
      "habomai: 0.9987871304841413\n",
      "shikotan: 0.9987871304841413\n",
      "kunashiri: 0.9987871304841413\n",
      "etorofu: 0.9987871304841413\n",
      "kuril: 0.9987871304841413\n",
      "straiten: 0.9987871304841413\n",
      "amadou: 0.9987871304841413\n",
      "toumani: 0.9987871304841413\n",
      "almati: 0.9987871304841413\n",
      "fieldin: 0.9987871304841413\n",
      "axiomat: 0.9987871304841413\n",
      "aryamehr: 0.9987871304841413\n",
      "harvard: 0.9987871304841413\n",
      "shelterless: 0.9987871304841413\n",
      "siberia: 0.9987871304841413\n",
      "latinamerican: 0.9987871304841413\n",
      "martinez: 0.9987871304841413\n",
      "saenz: 0.9987871304841413\n",
      "hipoteca: 0.9987871304841413\n",
      "asegurada: 0.9987871304841413\n",
      "instituto: 0.9987871304841413\n",
      "investigacion: 0.9987871304841413\n",
      "tecnologica: 0.9987871304841413\n",
      "electricidad: 0.9987871304841413\n",
      "comercio: 0.9987871304841413\n",
      "desarrollo: 0.9987871304841413\n",
      "economico: 0.9987871304841413\n",
      "fondo: 0.9987871304841413\n",
      "seguro: 0.9987871304841413\n",
      "deposito: 0.9987871304841413\n",
      "comis: 0.9987871304841413\n",
      "ejecutiva: 0.9987871304841413\n",
      "cooperacion: 0.9987871304841413\n",
      "agricola: 0.9987871304841413\n",
      "minera: 0.9987871304841413\n",
      "aviacion: 0.9987871304841413\n",
      "viscount: 0.9987871304841413\n",
      "miami: 0.9987871304841413\n",
      "ferrocarril: 0.9987871304841413\n",
      "habana: 0.9987871304841413\n",
      "britishown: 0.9987871304841413\n",
      "rca: 0.9987871304841413\n",
      "newsprint: 0.9987871304841413\n",
      "bagass: 0.9987871304841413\n",
      "hanabanilla: 0.9987871304841413\n",
      "toa: 0.9987871304841413\n",
      "gallon: 0.9987871304841413\n",
      "esso: 0.9987871304841413\n"
     ]
    }
   ],
   "source": [
    "# == Calculate weighted frequencies for all words\n",
    "\n",
    "# - downweights very common words by giving more importance to rare ones\n",
    "word_counts_stemmed = joblib.load(os.path.join(data_freq, 'word_counts_stemmed.pkl'))\n",
    "\n",
    "l = sum(word_counts_stemmed.values())\n",
    "\n",
    "a = 0.001\n",
    "word_counts_weighted = {k: a / (a + (v / l)) for k, v in word_counts_stemmed.items()}\n",
    "#for key in word_counts.keys():\n",
    " #   word_counts[key] = a / (a + (word_counts[key] / l))\n",
    "\n",
    "joblib.dump(word_counts_weighted, os.path.join(data_freq, 'word_counts_weighted.pkl'))\n",
    "\n",
    "################################################################################ ISSUE##################\n",
    "# To print top 100 by weighted values, sort the dictionary by value descending:\n",
    "top_100_weighted = sorted(word_counts_weighted.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "\n",
    "print(\"Top 100 words by weighted frequency:\")\n",
    "for word, weight in top_100_weighted:\n",
    "    print(f\"{word}: {weight}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a7ec60-2d73-4c68-948c-592970c4ad93",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df626ded-5e49-4a34-a3d1-315ba370226f",
   "metadata": {},
   "source": [
    "## Final Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0d0856a2-44f1-4304-b221-09a9ab72ad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_c)\n",
    "os.chdir(data_freq)\n",
    "\n",
    "count = joblib.load('word_counts_stemmed.pkl')  # load stemmed counts\n",
    "\n",
    "# For each speech only keep tokens that appear at least 10x\n",
    "\n",
    "def select(lista):\n",
    "    for i in range(len(lista)):\n",
    "        x = lista[i][0]\n",
    "        y = lista[i][1]\n",
    "        y = [w for w in y if count.get(w, 0) >= 10]\n",
    "        lista[i] = [x, y]\n",
    "    return lista\n",
    "\n",
    "for data_path in preprocessed_files:\n",
    "    data = joblib.load(data_path)\n",
    "    data = select(data)\n",
    "    cleaned_path = data_path.replace('.pkl', '_final.pkl')\n",
    "    joblib.dump(data, cleaned_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
