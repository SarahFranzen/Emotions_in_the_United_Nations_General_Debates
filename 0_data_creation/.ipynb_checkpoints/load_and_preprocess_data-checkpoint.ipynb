{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2810fe05-2a3c-437a-917c-a9833d901eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion and Reason in Political Language: Examining the UN General speeches\n",
    "### Sarah Franzen\n",
    "\n",
    "# Description: \n",
    "# - Extract documents from their original txt and store them as one csv\n",
    "# - \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5588afc-105a-426e-9348-742a830a2732",
   "metadata": {},
   "source": [
    "## Setup, Installation and Verification of required Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebb8987a-a0da-453e-97e2-aafa209d0e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "InstallPackages = False # Set this to True to install the following packages \n",
    "\n",
    "if InstallPackages:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install pandas\n",
    "    !{sys.executable} -m pip install nltk\n",
    "    !{sys.executable} -m pip install spacy\n",
    "    !{sys.executable} -m pip install numpy\n",
    "    !{sys.executable} -m pip install gensim\n",
    "\n",
    "#########################\n",
    "# Check if all packages are included\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56fa9a21-650f-4d8e-8a21-3d9629655c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import specific functions and classes from libraries\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random # for shorter samples\n",
    "from string import punctuation\n",
    "\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import gensim\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "from multiprocessing import Pool, freeze_support\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize punctuation translator for removal, POS tagger and Snowball Stemmer\n",
    "translator = str.maketrans('', '', punctuation)\n",
    "tagger = nltk.perceptron.PerceptronTagger()\n",
    "# stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Set working directory (please adjust)\n",
    "\n",
    "wd = r\"C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\"\n",
    "os.chdir(wd)\n",
    "\n",
    "# Define other folder paths\n",
    "\n",
    "data_c = os.path.join(wd, 'data')\n",
    "data_temp = os.path.join(data_c, 'temp')\n",
    "\n",
    "data_freq = os.path.join(data_c, 'freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c851fa4-f925-40d8-bfeb-873863cc2312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK stopwords are available.\n",
      "spaCy model is available.\n",
      "NLTK 'punkt' tokenizer is available.\n"
     ]
    }
   ],
   "source": [
    "# Set DownloadAdditions to True if you need to download these additional resources.\n",
    "\n",
    "DownloadAdditions = False\n",
    "if DownloadAdditions:\n",
    "    nltk.download('stopwords')                   # Download NLTK stopwords corpus\n",
    "    spacy.cli.download('en_core_web_lg')         # Download spaCy English model (large)\n",
    "    nltk.download('punkt')                       # Download the NLTK tokenizer models\n",
    "\n",
    "# Ensure the NLTK stopword, en_core_web_lg and NLTK tokenizer models are available\n",
    "\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "    print(\"NLTK stopwords are available.\")\n",
    "except LookupError:\n",
    "    print(\"NLTK stopwords are not available.\")\n",
    "\n",
    "# Ensure the spaCy model is available\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_lg')\n",
    "    print(\"spaCy model is available.\")\n",
    "except OSError:\n",
    "    print(\"spaCy model is not available.\")\n",
    "\n",
    "# Ensure the NLTK 'punkt' tokenizer is available\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print(\"NLTK 'punkt' tokenizer is available.\")\n",
    "except LookupError:\n",
    "    print(\"NLTK 'punkt' tokenizer is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "296fc7d6-a2ac-406e-a322-6c9f2702ee1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\n"
     ]
    }
   ],
   "source": [
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff34b6-cbee-4f8c-8076-5f385f596b58",
   "metadata": {},
   "source": [
    "## Load and Prepare Corpus for Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93d247a0-5ff2-4062-add9-f3267ae1fe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Saved raw data with 300 speeches to '.\\data\\un_corpus_raw.csv'\n"
     ]
    }
   ],
   "source": [
    "# Load data from txt-files into one csv-file \n",
    "\n",
    "# Define the folder path for the original data      \n",
    "base_folder = r\".\\data\\data_original\\UN General Debate Corpus\\UNGDC_1946-2023\\TXT\"\n",
    "\n",
    "# STEP 1: First gather all matching file paths\n",
    "all_txt_files = []\n",
    "for root, dirs, files in os.walk(base_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.txt') and not file.startswith('._'):\n",
    "            all_txt_files.append(os.path.join(root, file))\n",
    "\n",
    "# STEP 2: Sample 300 randomly\n",
    "sampled_files = random.sample(all_txt_files, 300)\n",
    "\n",
    "# STEP 3: Now use your preferred file-reading structure\n",
    "raw_data = []\n",
    "for filepath in sampled_files:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        raw_data.append({'filename': os.path.basename(filepath), 'speech': content})\n",
    "        \n",
    "'''\n",
    "# For entire data\n",
    "# Create a list to hold the data\n",
    "raw_data = []\n",
    "\n",
    "# Walk through all subfolders and process each .txt file\n",
    "for root, dirs, files in os.walk(base_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.txt') and not file.startswith('._'):\n",
    "            filepath = os.path.join(root, file)\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "\n",
    "                raw_data.append({'filename': file, 'speech': content})\n",
    "\n",
    "'''\n",
    "\n",
    "# Create DataFrame from the collected data\n",
    "df_raw = pd.DataFrame(raw_data)\n",
    "\n",
    "# Save df_raw so it doesn't have to be created every time again\n",
    "raw_pickle_path = r\".\\data\\un_corpus_raw.pkl\"\n",
    "df_raw.to_pickle(raw_pickle_path)\n",
    "\n",
    "# Save DataFrame to CSV with semicolon separator so Excel opens it correctly\n",
    "raw_output_path = r\".\\data\\un_corpus_raw.csv\"\n",
    "df_raw.to_csv(raw_output_path, index=False, sep=';', encoding='utf-8')\n",
    "\n",
    "print(f\"\\n✅ Saved raw data with {len(df_raw)} speeches to '{raw_output_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6405ff20-92fa-4c97-becb-ac5fff70f84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSO_28_1973.txt</td>\n",
       "      <td>﻿210.\\t It is a source of great pleasure for m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COD_42_1987.txt</td>\n",
       "      <td>﻿Mr, President, at this forty-second session, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ROU_42_1987.txt</td>\n",
       "      <td>﻿Your election, Sir, to the high post of Presi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISR_29_1974.txt</td>\n",
       "      <td>At the outset of my remarks, I wish to associa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FRA_01_1946.txt</td>\n",
       "      <td>Before I speak of anything else I would like f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech\n",
       "0  LSO_28_1973.txt  ﻿210.\\t It is a source of great pleasure for m...\n",
       "1  COD_42_1987.txt  ﻿Mr, President, at this forty-second session, ...\n",
       "2  ROU_42_1987.txt  ﻿Your election, Sir, to the high post of Presi...\n",
       "3  ISR_29_1974.txt  At the outset of my remarks, I wish to associa...\n",
       "4  FRA_01_1946.txt  Before I speak of anything else I would like f..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load df_raw\n",
    "df_raw = pd.read_pickle(r\".\\data\\un_corpus_raw.pkl\")\n",
    "\n",
    "# View df to understand structure\n",
    "df_raw.head()         # Shows the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe0f4af-e939-4100-9618-5b522bdd72a1",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2661f431-a194-47d1-9a1c-89162bf85a91",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd37e08d-b4f2-459e-823c-3816a825ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean speeches\n",
    "\n",
    "def clean_text(content):\n",
    "    if pd.isna(content):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove line breaks and carriage returns\n",
    "    content = content.replace('\\n', ' ').replace('\\r', ' ')\n",
    "\n",
    "    # Collapse multiple spaces\n",
    "    content = ' '.join(content.split())\n",
    "\n",
    "    # Fix punctuation spacing (e.g. \"word,another\" → \"word, another\")\n",
    "    content = re.sub(r'(?<=[.,])(?=[^\\s])', r' ', content)\n",
    "\n",
    "    # Remove hyphenation at line breaks (e.g. \"inter- national\" → \"international\")\n",
    "    content = re.sub(r'-\\s', '', content)\n",
    "\n",
    "    # Remove stray backslashes\n",
    "    content = content.replace(\"\\\\\", \"\")\n",
    "\n",
    "    # Escape double quotes for CSV safety\n",
    "    content = content.replace('\"', '\"\"')\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "# Apply cleaning to each speech\n",
    "df_raw['speech'] = df_raw['speech'].astype(str)  # Ensure column is string type\n",
    "df_clean = df_raw.copy()\n",
    "df_clean['speech'] = df_clean['speech'].apply(clean_text)\n",
    "\n",
    "# Drop rows with empty speeches after cleaning\n",
    "df_clean = df_clean[df_clean['speech'].str.strip().astype(bool)].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c6bd148-d5cf-4eb3-a66d-fc706e99c13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved clean speeches chunks in 'C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp'\n"
     ]
    }
   ],
   "source": [
    "# Convert cleaned DataFrame to list of lists\n",
    "clean_data = df_clean[['filename', 'speech']].values.tolist()\n",
    "\n",
    "# Split cleaned data into 4 equal chunks\n",
    "data_id1 = clean_data[:int(len(clean_data)/4)]\n",
    "data_id2 = clean_data[int(len(clean_data)/4): int(2*len(clean_data)/4)]\n",
    "data_id3 = clean_data[int(2*len(clean_data)/4): int(3*len(clean_data)/4)]\n",
    "data_id4 = clean_data[int(3*len(clean_data)/4):]\n",
    "\n",
    "# Change directory to the temp folder\n",
    "os.chdir(data_temp)  # make sure `data_temp` exists and is defined\n",
    "\n",
    "# Save each chunk with joblib\n",
    "joblib.dump(data_id1, 'cleanspeeches_indexed1_n.pkl')\n",
    "joblib.dump(data_id2, 'cleanspeeches_indexed2_n.pkl')\n",
    "joblib.dump(data_id3, 'cleanspeeches_indexed3_n.pkl')\n",
    "joblib.dump(data_id4, 'cleanspeeches_indexed4_n.pkl')\n",
    "\n",
    "print(f\"✅ Saved clean speeches chunks in '{data_temp}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ea943e7-9f7d-4b1b-b011-29484b1cd4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the chunk filenames\n",
    "data_files = [\n",
    "    'cleanspeeches_indexed1_n.pkl',\n",
    "    'cleanspeeches_indexed2_n.pkl',\n",
    "    'cleanspeeches_indexed3_n.pkl',\n",
    "    'cleanspeeches_indexed4_n.pkl'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d9f2174-797b-451b-b629-34eaebb53ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\n"
     ]
    }
   ],
   "source": [
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6e5109-8cf7-41e3-8fb9-8e7dde29b4e0",
   "metadata": {},
   "source": [
    "### Tokenize, Eliminate digits and stopwords, POS-Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1204b7-fe27-4fda-8cc1-4a9b3a3066d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#   Functions                   ###\n",
    "###################################\n",
    "\n",
    "os.chdir(data_c)\n",
    "\n",
    "def pro1(lista):\n",
    "    a = [[row[0], row[1].translate(translator)] for row in lista]\n",
    "    return a\n",
    "\n",
    "# Tokenize etc\n",
    "def pro2(lista):\n",
    "    a = [[row[0], gensim.utils.simple_preprocess(row[1])] for row in lista]\n",
    "    return a\n",
    "\n",
    "# Eliminate digits\n",
    "def pro3(lista):\n",
    "    a = [[row[0], [w for w in row[1] if not w.isdigit()]] for row in lista]\n",
    "    return a\n",
    "\n",
    "# Drop words that are too short\n",
    "def pro4(lista):\n",
    "    a = [[row[0], [w for w in row[1] if len(w)>2]] for row in lista]\n",
    "    return a\n",
    "\n",
    "# Tag parts of speech and keep only nouns, verbs and adjectives\n",
    "def tags(lista):\n",
    "    t = [[row[0], tagger.tag(row[1])] for row in lista]\n",
    "    t = [[row[0], [i[0] for i in row[1] if i[1].startswith(('N', 'V', 'J'))]] for row in t]\n",
    "    return t\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def pro5(lista):\n",
    "    a = [[row[0], [w for w in row[1] if w not in stop_words]] for row in lista]\n",
    "    return a\n",
    "\n",
    "# Stem\n",
    "#def pro5(lista):\n",
    "  #  a = [[row[0], [stemmer.stem(word) for word in row[1]]] for row in lista]\n",
    "   # return a\n",
    "\n",
    "# Drop empty speeches\n",
    "def dropnull(lista):\n",
    "    a = [row for row in lista if len(' '.join(row[1]))>0]\n",
    "    return a\n",
    "\n",
    "\n",
    "###################################\n",
    "#   Main                       ###\n",
    "###################################\n",
    "\n",
    "def preprocessing(data_name):\n",
    "    data = joblib.load(data_name)\n",
    "    data = pro1(data)\n",
    "    data = pro2(data)\n",
    "    data = pro3(data)\n",
    "    data = pro4(data)\n",
    "    data = tags(data)\n",
    "    data = pro5(data)\n",
    "    data = dropnull(data)\n",
    "    lab = data_name.replace('.pkl', '') + '_temp.pkl'\n",
    "    joblib.dump(data, lab)\n",
    "\n",
    "\n",
    "###################################\n",
    "#      Multiprocessing          ###\n",
    "###################################\n",
    "\n",
    "os.chdir(data_c)\n",
    "\n",
    "def main():\n",
    "    with Pool(4) as pool:\n",
    "        pool.starmap(preprocessing, data_files)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    freeze_support()\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154d978d-cddd-431c-9d53-73c9faf8fdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_frequencies(filenames):\n",
    "    total_freq = Counter()\n",
    "    for fname in tqdm(filenames):\n",
    "        data = joblib.load(fname)\n",
    "        tokens = chain.from_iterable(row[1] for row in data)\n",
    "        total_freq.update(tokens)\n",
    "    return total_freq\n",
    "\n",
    "freqs = count_frequencies(data_files)\n",
    "\n",
    "joblib.dump(freqs, r\"path_to_save\\word_counts.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfa341d-021b-4f1b-8af5-2ca7db70947f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
