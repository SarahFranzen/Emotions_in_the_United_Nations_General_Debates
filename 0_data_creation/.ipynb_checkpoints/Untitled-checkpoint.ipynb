{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3abcf21-b313-460c-a6bf-b5cb4445c445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "\n",
    "import os\n",
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from multiprocessing import Pool, freeze_support\n",
    "\n",
    "# Initialize punctuation translator for removal, POS tagger and Snowball Stemmer\n",
    "translator = str.maketrans('', '', punctuation)\n",
    "tagger = nltk.perceptron.PerceptronTagger()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Set working directory (please adjust)\n",
    "\n",
    "wd = r\"C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\"\n",
    "os.chdir(wd)\n",
    "\n",
    "# Define your temp folder path\n",
    "\n",
    "data_temp = r\".\\data\\temp\"\n",
    "\n",
    "data_c = r\".\\data\"\n",
    "\n",
    "# Load df_raw\n",
    "df_raw = pd.read_pickle(r\".\\data\\un_corpus_raw.pkl\")\n",
    "\n",
    "def clean_text(content):\n",
    "    if pd.isna(content):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove line breaks and carriage returns\n",
    "    content = content.replace('\\n', ' ').replace('\\r', ' ')\n",
    "\n",
    "    # Collapse multiple spaces\n",
    "    content = ' '.join(content.split())\n",
    "\n",
    "    # Fix punctuation spacing (e.g. \"word,another\" → \"word, another\")\n",
    "    content = re.sub(r'(?<=[.,])(?=[^\\s])', r' ', content)\n",
    "\n",
    "    # Remove hyphenation at line breaks (e.g. \"inter- national\" → \"international\")\n",
    "    content = re.sub(r'-\\s', '', content)\n",
    "\n",
    "    # Remove stray backslashes\n",
    "    content = content.replace(\"\\\\\", \"\")\n",
    "\n",
    "    # Escape double quotes for CSV safety\n",
    "    content = content.replace('\"', '\"\"')\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "# Apply cleaning to each speech\n",
    "df_raw['speech'] = df_raw['speech'].astype(str)  # Ensure column is string type\n",
    "df_clean = df_raw.copy()\n",
    "df_clean['speech'] = df_clean['speech'].apply(clean_text)\n",
    "\n",
    "# Drop rows with empty speeches after cleaning\n",
    "df_clean = df_clean[df_clean['speech'].str.strip().astype(bool)].reset_index(drop=True)\n",
    "\n",
    "# Convert cleaned DataFrame to list of lists\n",
    "clean_data = df_clean[['filename', 'speech']].values.tolist()\n",
    "\n",
    "# Split cleaned data into 4 equal chunks\n",
    "data_id1 = clean_data[:int(len(clean_data)/4)]\n",
    "data_id2 = clean_data[int(len(clean_data)/4): int(2*len(clean_data)/4)]\n",
    "data_id3 = clean_data[int(2*len(clean_data)/4): int(3*len(clean_data)/4)]\n",
    "data_id4 = clean_data[int(3*len(clean_data)/4):]\n",
    "\n",
    "# Change directory to the temp folder\n",
    "os.chdir(data_temp)  # make sure `data_temp` exists and is defined\n",
    "\n",
    "# Save each chunk with joblib\n",
    "joblib.dump(data_id1, 'cleanspeeches_indexed1_n.pkl')\n",
    "joblib.dump(data_id2, 'cleanspeeches_indexed2_n.pkl')\n",
    "joblib.dump(data_id3, 'cleanspeeches_indexed3_n.pkl')\n",
    "joblib.dump(data_id4, 'cleanspeeches_indexed4_n.pkl')\n",
    "\n",
    "print(f\"✅ Saved clean speeches chunks in '{data_temp}'\")\n",
    "\n",
    "###################################\n",
    "#   Functions                   ###\n",
    "###################################\n",
    "\n",
    "def pro1(lista):\n",
    "    a = [[row[0], row[1].translate(translator)] for row in lista]\n",
    "    return a\n",
    "\n",
    "# Tokenize etc\n",
    "def pro2(lista):\n",
    "    a = [[row[0], gensim.utils.simple_preprocess(row[1])] for row in lista]\n",
    "    return a\n",
    "\n",
    "# Eliminate digits\n",
    "def pro3(lista):\n",
    "    a = [[row[0], [w for w in row[1] if not w.isdigit()]] for row in lista]\n",
    "    return a\n",
    "\n",
    "# Drop words that are too short\n",
    "def pro4(lista):\n",
    "    a = [[row[0], [w for w in row[1] if len(w)>2]] for row in lista]\n",
    "    return a\n",
    "\n",
    "# Tag parts of speech and keep only some\n",
    "def tags(lista):\n",
    "    t = [[row[0], tagger.tag(row[1])] for row in lista]\n",
    "    t = [[row[0], [i[0] for i in row[1] if i[1].startswith(('N', 'V', 'J'))]] for row in t]\n",
    "    return t\n",
    "\n",
    "# Stem\n",
    "def pro5(lista):\n",
    "    a = [[row[0], [stemmer.stem(word) for word in row[1]]] for row in lista]\n",
    "    return a\n",
    "\n",
    "# Eliminate Stopwords\n",
    "os.chdir(data_c)\n",
    "stopwords = joblib.load('stopwords.pkl')\n",
    "proc = joblib.load('procedural_words.pkl')\n",
    "stopwords = set(stopwords).union(proc)\n",
    "del proc\n",
    "def pro6(lista):\n",
    "    for i in range(len(lista)):\n",
    "        x = lista[i][0]\n",
    "        y = lista[i][1]\n",
    "        y = [w for w in y if w not in stopwords]\n",
    "        lista[i] = [x, y]\n",
    "    return lista\n",
    "\n",
    "# Drop empty speeches\n",
    "def dropnull(lista):\n",
    "    a = [row for row in lista if len(' '.join(row[1]))>0]\n",
    "    return a\n",
    "\n",
    "\n",
    "###################################\n",
    "#   Main                       ###\n",
    "###################################\n",
    "\n",
    "\n",
    "def preprocessing(data_name):\n",
    "    data = joblib.load(data_name)\n",
    "    data = pro1(data)\n",
    "    data = pro2(data)\n",
    "    data = pro3(data)\n",
    "    data = pro4(data)\n",
    "    data = tags(data)\n",
    "    data = pro5(data)\n",
    "    data = pro6(data)\n",
    "    data = dropnull(data)\n",
    "    lab = data_name.replace('.pkl', '') + '_temp.pkl'\n",
    "    joblib.dump(data, lab)\n",
    "\n",
    "\n",
    "###################################\n",
    "#      Multiprocessing          ###\n",
    "###################################\n",
    "\n",
    "\n",
    "data_files = [[a] for a in data_files]\n",
    "\n",
    "\n",
    "def main():\n",
    "    with Pool(4) as pool:\n",
    "        pool.starmap(preprocessing, data_files)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    freeze_support()\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
