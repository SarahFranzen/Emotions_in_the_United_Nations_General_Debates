{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b41cb22-e190-48d5-9926-7ac6fac8310d",
   "metadata": {},
   "source": [
    "# Emotion and Reason in Political Language: Examining the UN General speeches\n",
    "## Script 1: Preprocessing & Token Frequencies\n",
    "### by Sarah Franzen\n",
    "\n",
    "### Description: \n",
    "#### - Extract documents from their original txt documents and store them as one csv\n",
    "#### - Data Cleaning and Pre-Processing\n",
    "#### - Count word frequencies and weight them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5588afc-105a-426e-9348-742a830a2732",
   "metadata": {},
   "source": [
    "## Setup, Installation and Verification of required Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ebb8987a-a0da-453e-97e2-aafa209d0e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "InstallPackages = False # Set this to True to install the following packages \n",
    "\n",
    "if InstallPackages:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install pandas\n",
    "    !{sys.executable} -m pip install nltk\n",
    "    !{sys.executable} -m pip install spacy\n",
    "    !{sys.executable} -m pip install numpy\n",
    "    !{sys.executable} -m pip install gensim\n",
    "    !{sys.executable} -m pip install pycountry\n",
    "    !{sys.executable} -m pip install wordcloud matplotlib\n",
    "\n",
    "#########################\n",
    "# Check if all packages are included\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "56fa9a21-650f-4d8e-8a21-3d9629655c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Import standard and third-party libraries for data processing, NLP, and visualization ==\n",
    "\n",
    "import gensim\n",
    "import joblib\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import pycountry\n",
    "import random\n",
    "import re\n",
    "import spacy\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "#from matplotlib.colors import ListedColormap\n",
    "from multiprocessing import Pool, freeze_support\n",
    "# from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOPWORDS\n",
    "from string import punctuation\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Initialize NLP Tools ===\n",
    "\n",
    "# Translator to remove punctuation\n",
    "translator = str.maketrans('', '', punctuation)\n",
    "\n",
    "# POS tagger (not used by SpaCy, but optionally available via NLTK)\n",
    "tagger = nltk.perceptron.PerceptronTagger()\n",
    "\n",
    "# Load SpaCy English model with unnecessary components disabled\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "# === Set Working Directory ===\n",
    "\n",
    "# Set your working directory (adjust this as needed)\n",
    "wd = r\"C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\"\n",
    "os.chdir(wd)\n",
    "\n",
    "# === Define Folder Paths ===\n",
    "\n",
    "# Make sure that you have these folders in your working directory\n",
    "data_c = os.path.join(wd, 'data')\n",
    "data_temp = os.path.join(data_c, 'temp')\n",
    "data_freq = os.path.join(data_c, 'freq')\n",
    "data_dict = os.path.join(data_c, 'dictionaries')\n",
    "data_preprocessed = os.path.join(data_c, 'preprocessed')\n",
    "data_tokenized = os.path.join(data_c, 'tokenized')\n",
    "fig = os.path.join(wd, 'Code/0_descriptives/fig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "296fc7d6-a2ac-406e-a322-6c9f2702ee1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\n"
     ]
    }
   ],
   "source": [
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8c851fa4-f925-40d8-bfeb-873863cc2312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set DownloadAdditions to True if you need to download these additional resources.\n",
    "\n",
    "DownloadAdditions = False\n",
    "if DownloadAdditions:\n",
    "    spacy.cli.download('en_core_web_lg')         # Download spaCy English model (large)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff34b6-cbee-4f8c-8076-5f385f596b58",
   "metadata": {},
   "source": [
    "## Load and Prepare Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18067b9-6337-4fb3-a570-603083a7deae",
   "metadata": {},
   "source": [
    "This chunk can be skipped at the moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "93d247a0-5ff2-4062-add9-f3267ae1fe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¾ Total speeches found: 10761\n",
      "\n",
      "âœ… Saved raw data with 10760 speeches to '.\\data\\un_corpus_raw.csv'\n"
     ]
    }
   ],
   "source": [
    "# == Load and Save Sample from UN General Debate Corpus ==             ######################## ADJUST LATER\n",
    "\n",
    "# Set Folder path containing the original TXT files    \n",
    "base_folder = r\".\\data\\data_original\\UN General Debate Corpus\\UNGDC_1946-2023\\TXT\"\n",
    "\n",
    "#  Gather all relevant txt-files\n",
    "all_txt_files = []\n",
    "for root, dirs, files in os.walk(base_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.txt') and not file.startswith('._'):\n",
    "            all_txt_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"ðŸ§¾ Total speeches found: {len(all_txt_files)}\")\n",
    "\n",
    "# Randomly pick 800 files from the full collection   ###################################################### REMOVE AT LATER POINT\n",
    "sampled_files = random.sample(all_txt_files,10761)\n",
    "\n",
    "# Read the selected files into a list\n",
    "raw_data = []\n",
    "for filepath in sampled_files:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        raw_data.append({'filename': os.path.basename(filepath), 'speech': content})\n",
    "\n",
    "# Create DataFrame from the collected speeches\n",
    "df_raw = pd.DataFrame(raw_data)\n",
    "\n",
    "df_raw = df_raw[df_raw['filename'] != '.DS_Store-to-UTF-8.txt'].copy()\n",
    "\n",
    "# Save df_raw as a pickle file for quick future loading\n",
    "raw_pickle_path = r\".\\data\\un_corpus_raw.pkl\"\n",
    "df_raw.to_pickle(raw_pickle_path)\n",
    "\n",
    "# Export df as CSV \n",
    "raw_output_path = r\".\\data\\un_corpus_raw.csv\"\n",
    "df_raw.to_csv(raw_output_path, index=False, sep=';', encoding='utf-8')\n",
    "\n",
    "print(f\"\\nâœ… Saved raw data with {len(df_raw)} speeches to '{raw_output_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6405ff20-92fa-4c97-becb-ac5fff70f84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GIN_77_2022.txt</td>\n",
       "      <td>At the outset of my remarks, I wish to convey ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ITA_43_1988.txt</td>\n",
       "      <td>ï»¿\\nMr. President, on behalf of the Italian Gov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SWE_26_1971.txt</td>\n",
       "      <td>1 Mr. President, it gives me great pleasure to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BRB_61_2006.txt</td>\n",
       "      <td>I am pleased to \\njoin with preceding speakers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LBY_15_1960.txt</td>\n",
       "      <td>Once again, Mr. President, allow me to congrat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech\n",
       "0  GIN_77_2022.txt  At the outset of my remarks, I wish to convey ...\n",
       "1  ITA_43_1988.txt  ï»¿\\nMr. President, on behalf of the Italian Gov...\n",
       "2  SWE_26_1971.txt  1 Mr. President, it gives me great pleasure to...\n",
       "3  BRB_61_2006.txt  I am pleased to \\njoin with preceding speakers...\n",
       "4  LBY_15_1960.txt  Once again, Mr. President, allow me to congrat..."
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# == Check if everything worked ==\n",
    "\n",
    "# Load df_raw\n",
    "df_raw = pd.read_pickle(r\".\\data\\un_corpus_raw.pkl\")\n",
    "\n",
    "# View df to check structure\n",
    "df_raw.head()         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "bf7400af-50fd-4be3-9daa-07925089dc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with missing years:\n",
      " Empty DataFrame\n",
      "Columns: [filename]\n",
      "Index: []\n",
      "Number of rows with missing year: 0\n"
     ]
    }
   ],
   "source": [
    "# Extract the year (may include NaNs if no match)\n",
    "df_raw['year'] = df_raw['filename'].str.extract(r'_(\\d{4})\\.txt$')\n",
    "\n",
    "# Print rows where year is NaN\n",
    "na_rows = df_raw[df_raw['year'].isna()]\n",
    "print(\"Rows with missing years:\\n\", na_rows[['filename']])\n",
    "\n",
    "# Optional: check how many rows are affected\n",
    "print(f\"Number of rows with missing year: {len(na_rows)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "162da07c-1112-43e8-a434-2bef0ed32de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min year: 1946\n",
      "Max year: 2023\n",
      "Missing codes: []\n",
      "df_raw saved to C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\un_corpus_raw.pkl\n"
     ]
    }
   ],
   "source": [
    "# == Create new variables: year, country_code and country_name ==\n",
    "\n",
    "# Extract country code (first 3 letters) and year (last 4 digits before .txt)\n",
    "df_raw['country_code'] = df_raw['filename'].str.extract(r'^([A-Z]{2,3})')\n",
    "df_raw['year'] = df_raw['filename'].str.extract(r'_(\\d{4})\\.txt$').astype(int)\n",
    "\n",
    "print(\"Min year:\", df_raw['year'].min())\n",
    "print(\"Max year:\", df_raw['year'].max())\n",
    "\n",
    "# Match country codes to country names\n",
    "code_to_name = {country.alpha_3: country.name for country in pycountry.countries}\n",
    "\n",
    "# Add custom short names and legacy codes\n",
    "custom_names = {\n",
    "    \"BOL\": \"Bolivia\",\n",
    "    \"COD\": \"The Democratic Republic of the Congo\",\n",
    "    \"IRN\": \"Iran\",\n",
    "    \"LAO\": \"Laos\",\n",
    "    \"MDA\": \"Moldova\",\n",
    "    \"PRK\": \"North Korea\",\n",
    "    \"PSE\": \"Palestine\",\n",
    "    \"RUS\": \"Russia\",\n",
    "    \"SYR\": \"Syria\",\n",
    "    \"TZA\": \"Tanzania\",\n",
    "    \"VAT\": \"Vatican City State\",\n",
    "    \"VEN\": \"Venezuela\",\n",
    "    \"VNM\": \"Vietnam\",\n",
    "    #\"YD\":  \"South Yemen\",\n",
    "    \"YMD\": \"Soth Yemen\",\n",
    "    \"YUG\": \"Yugoslavia\",\n",
    "    \"DDR\": \"East Germany\",\n",
    "    \"SUN\": \"Soviet Union\",\n",
    "    \"EU\": \"European Union\",\n",
    "    \"CSK\": \"Czechoslovakia\",\n",
    "    \"FSM\": \"Micronesia\",\n",
    "    \"KOR\": \"South Korea\"\n",
    "    \n",
    "}\n",
    "\n",
    "# Update the main mapping with custom names\n",
    "code_to_name.update(custom_names)\n",
    "\n",
    "# Map with updated dictionary\n",
    "df_raw['country_name'] = df_raw['country_code'].map(code_to_name)\n",
    "\n",
    "# Check missing mappings\n",
    "missing = df_raw.loc[df_raw['country_name'].isna(), 'country_code'].unique()\n",
    "print(\"Missing codes:\", missing)\n",
    "\n",
    "# Check structure of the df\n",
    "df_raw.head() \n",
    "\n",
    "save_path = os.path.join(data_c, 'un_corpus_raw.pkl')\n",
    "df_raw.to_pickle(save_path)\n",
    "print(f\"df_raw saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "383d2904-f015-4c9d-9aeb-51a94f03f950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    country_code                          country_name\n",
      "0            AFG                           Afghanistan\n",
      "1            AGO                                Angola\n",
      "2            ALB                               Albania\n",
      "3            AND                               Andorra\n",
      "4            ARE                  United Arab Emirates\n",
      "5            ARG                             Argentina\n",
      "6            ARM                               Armenia\n",
      "7            ATG                   Antigua and Barbuda\n",
      "8            AUS                             Australia\n",
      "9            AUT                               Austria\n",
      "10           AZE                            Azerbaijan\n",
      "11           BDI                               Burundi\n",
      "12           BEL                               Belgium\n",
      "13           BEN                                 Benin\n",
      "14           BFA                          Burkina Faso\n",
      "15           BGD                            Bangladesh\n",
      "16           BGR                              Bulgaria\n",
      "17           BHR                               Bahrain\n",
      "18           BHS                               Bahamas\n",
      "19           BIH                Bosnia and Herzegovina\n",
      "20           BLR                               Belarus\n",
      "21           BLZ                                Belize\n",
      "22           BOL                               Bolivia\n",
      "23           BRA                                Brazil\n",
      "24           BRB                              Barbados\n",
      "25           BRN                     Brunei Darussalam\n",
      "26           BTN                                Bhutan\n",
      "27           BWA                              Botswana\n",
      "28           CAF              Central African Republic\n",
      "29           CAN                                Canada\n",
      "30           CHE                           Switzerland\n",
      "31           CHL                                 Chile\n",
      "32           CHN                                 China\n",
      "33           CIV                         CÃ´te d'Ivoire\n",
      "34           CMR                              Cameroon\n",
      "35           COD  The Democratic Republic of the Congo\n",
      "36           COG                                 Congo\n",
      "37           COL                              Colombia\n",
      "38           COM                               Comoros\n",
      "39           CPV                            Cabo Verde\n",
      "40           CRI                            Costa Rica\n",
      "41           CSK                        Czechoslovakia\n",
      "42           CUB                                  Cuba\n",
      "43           CYP                                Cyprus\n",
      "44           CZE                               Czechia\n",
      "45           DDR                          East Germany\n",
      "46           DEU                               Germany\n",
      "47           DJI                              Djibouti\n",
      "48           DMA                              Dominica\n",
      "49           DNK                               Denmark\n",
      "50           DOM                    Dominican Republic\n",
      "51           DZA                               Algeria\n",
      "52           ECU                               Ecuador\n",
      "53           EGY                                 Egypt\n",
      "54           ERI                               Eritrea\n",
      "55           ESP                                 Spain\n",
      "56           EST                               Estonia\n",
      "57           ETH                              Ethiopia\n",
      "58            EU                        European Union\n",
      "59           FIN                               Finland\n",
      "60           FJI                                  Fiji\n",
      "61           FRA                                France\n",
      "62           FSM                            Micronesia\n",
      "63           GAB                                 Gabon\n",
      "64           GBR                        United Kingdom\n",
      "65           GEO                               Georgia\n",
      "66           GHA                                 Ghana\n",
      "67           GIN                                Guinea\n",
      "68           GMB                                Gambia\n",
      "69           GNB                         Guinea-Bissau\n",
      "70           GNQ                     Equatorial Guinea\n",
      "71           GRC                                Greece\n",
      "72           GRD                               Grenada\n",
      "73           GTM                             Guatemala\n",
      "74           GUY                                Guyana\n",
      "75           HND                              Honduras\n",
      "76           HRV                               Croatia\n",
      "77           HTI                                 Haiti\n",
      "78           HUN                               Hungary\n",
      "79           IDN                             Indonesia\n",
      "80           IND                                 India\n",
      "81           IRL                               Ireland\n",
      "82           IRN                                  Iran\n",
      "83           IRQ                                  Iraq\n",
      "84           ISL                               Iceland\n",
      "85           ISR                                Israel\n",
      "86           ITA                                 Italy\n",
      "87           JAM                               Jamaica\n",
      "88           JOR                                Jordan\n",
      "89           JPN                                 Japan\n",
      "90           KAZ                            Kazakhstan\n",
      "91           KEN                                 Kenya\n",
      "92           KGZ                            Kyrgyzstan\n",
      "93           KHM                              Cambodia\n",
      "94           KIR                              Kiribati\n",
      "95           KNA                 Saint Kitts and Nevis\n",
      "96           KOR                           South Korea\n",
      "97           KWT                                Kuwait\n",
      "98           LAO                                  Laos\n",
      "99           LBN                               Lebanon\n",
      "100          LBR                               Liberia\n",
      "101          LBY                                 Libya\n",
      "102          LCA                           Saint Lucia\n",
      "103          LIE                         Liechtenstein\n",
      "104          LKA                             Sri Lanka\n",
      "105          LSO                               Lesotho\n",
      "106          LTU                             Lithuania\n",
      "107          LUX                            Luxembourg\n",
      "108          LVA                                Latvia\n",
      "109          MAR                               Morocco\n",
      "110          MCO                                Monaco\n",
      "111          MDA                               Moldova\n",
      "112          MDG                            Madagascar\n",
      "113          MDV                              Maldives\n",
      "114          MEX                                Mexico\n",
      "115          MHL                      Marshall Islands\n",
      "116          MKD                       North Macedonia\n",
      "117          MLI                                  Mali\n",
      "118          MLT                                 Malta\n",
      "119          MMR                               Myanmar\n",
      "120          MNE                            Montenegro\n",
      "121          MNG                              Mongolia\n",
      "122          MOZ                            Mozambique\n",
      "123          MRT                            Mauritania\n",
      "124          MUS                             Mauritius\n",
      "125          MWI                                Malawi\n",
      "126          MYS                              Malaysia\n",
      "127          NAM                               Namibia\n",
      "128          NER                                 Niger\n",
      "129          NGA                               Nigeria\n",
      "130          NIC                             Nicaragua\n",
      "131          NLD                           Netherlands\n",
      "132          NOR                                Norway\n",
      "133          NPL                                 Nepal\n",
      "134          NRU                                 Nauru\n",
      "135          NZL                           New Zealand\n",
      "136          OMN                                  Oman\n",
      "137          PAK                              Pakistan\n",
      "138          PAN                                Panama\n",
      "139          PER                                  Peru\n",
      "140          PHL                           Philippines\n",
      "141          PLW                                 Palau\n",
      "142          PNG                      Papua New Guinea\n",
      "143          POL                                Poland\n",
      "144          PRK                           North Korea\n",
      "145          PRT                              Portugal\n",
      "146          PRY                              Paraguay\n",
      "147          PSE                             Palestine\n",
      "148          QAT                                 Qatar\n",
      "149          ROU                               Romania\n",
      "150          RUS                                Russia\n",
      "151          RWA                                Rwanda\n",
      "152          SAU                          Saudi Arabia\n",
      "153          SDN                                 Sudan\n",
      "154          SEN                               Senegal\n",
      "155          SGP                             Singapore\n",
      "156          SLB                       Solomon Islands\n",
      "157          SLE                          Sierra Leone\n",
      "158          SLV                           El Salvador\n",
      "159          SMR                            San Marino\n",
      "160          SOM                               Somalia\n",
      "161          SRB                                Serbia\n",
      "162          SSD                           South Sudan\n",
      "163          STP                 Sao Tome and Principe\n",
      "164          SUR                              Suriname\n",
      "165          SVK                              Slovakia\n",
      "166          SVN                              Slovenia\n",
      "167          SWE                                Sweden\n",
      "168          SWZ                              Eswatini\n",
      "169          SYC                            Seychelles\n",
      "170          SYR                                 Syria\n",
      "171          TCD                                  Chad\n",
      "172          TGO                                  Togo\n",
      "173          THA                              Thailand\n",
      "174          TJK                            Tajikistan\n",
      "175          TKM                          Turkmenistan\n",
      "176          TLS                           Timor-Leste\n",
      "177          TON                                 Tonga\n",
      "178          TTO                   Trinidad and Tobago\n",
      "179          TUN                               Tunisia\n",
      "180          TUR                               TÃ¼rkiye\n",
      "181          TUV                                Tuvalu\n",
      "182          TZA                              Tanzania\n",
      "183          UGA                                Uganda\n",
      "184          UKR                               Ukraine\n",
      "185          URY                               Uruguay\n",
      "186          USA                         United States\n",
      "187          UZB                            Uzbekistan\n",
      "188          VAT                    Vatican City State\n",
      "189          VCT      Saint Vincent and the Grenadines\n",
      "190          VEN                             Venezuela\n",
      "191          VNM                               Vietnam\n",
      "192          VUT                               Vanuatu\n",
      "193          WSM                                 Samoa\n",
      "194          YEM                                 Yemen\n",
      "195          YMD                            Soth Yemen\n",
      "196          YUG                            Yugoslavia\n",
      "197          ZAF                          South Africa\n",
      "198          ZMB                                Zambia\n",
      "199          ZWE                              Zimbabwe\n"
     ]
    }
   ],
   "source": [
    "# == Check the country names \n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(df_raw[['country_code', 'country_name']].drop_duplicates().sort_values('country_code').reset_index(drop=True))\n",
    "# Reset to default afterward\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe0f4af-e939-4100-9618-5b522bdd72a1",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2661f431-a194-47d1-9a1c-89162bf85a91",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "cd37e08d-b4f2-459e-823c-3816a825ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Clean text by removing empty spaces, line breaks, hyphenation, stray characters, and escape quote ==\n",
    "\n",
    "# Define cleaning function\n",
    "def clean_text(content):\n",
    "    if pd.isna(content):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove line breaks and carriage returns\n",
    "    content = content.replace('\\n', ' ').replace('\\r', ' ')\n",
    "\n",
    "    # Collapse multiple spaces\n",
    "    content = ' '.join(content.split())\n",
    "\n",
    "    # Fix punctuation spacing (e.g. \"word,another\" â†’ \"word, another\")\n",
    "    content = re.sub(r'(?<=[.,])(?=[^\\s])', r' ', content)\n",
    "\n",
    "    # Remove hyphenation at line breaks (e.g. \"inter- national\" â†’ \"international\")\n",
    "    content = re.sub(r'-\\s', '', content)\n",
    "\n",
    "    ############NEW\n",
    "\n",
    "     # Replace hyphen between letters with a space to prevent merging words (e.g. \"russian-and\" â†’ \"russian and\")\n",
    "    content = re.sub(r'(?<=\\w)-(?=\\w)', ' ', content)\n",
    "    #################NEW\n",
    "\n",
    "    # Remove stray backslashes\n",
    "    content = content.replace(\"\\\\\", \"\")\n",
    "\n",
    "    # Escape double quotes for CSV safety\n",
    "    content = content.replace('\"', '\"\"')\n",
    "\n",
    "    return content\n",
    "\n",
    "# Apply cleaning to each speech\n",
    "df_raw['speech'] = df_raw['speech'].astype(str)  # Ensure column is string type\n",
    "df_clean = df_raw.copy()\n",
    "df_clean['speech'] = df_clean['speech'].apply(clean_text)\n",
    "\n",
    "# Drop rows with empty speeches after cleaning\n",
    "df_clean = df_clean[df_clean['speech'].str.strip().astype(bool)].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7c6bd148-d5cf-4eb3-a66d-fc706e99c13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved clean speeches chunks in 'C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp'\n"
     ]
    }
   ],
   "source": [
    "# == Split cleaned data into chunks and save as separate files ==\n",
    "\n",
    "# Convert cleaned DataFrame to list of lists\n",
    "clean_data = df_clean[['filename', 'speech']].values.tolist()\n",
    "\n",
    "# Split cleaned data into 4 equal chunks\n",
    "data_id1 = clean_data[:int(len(clean_data)/4)]\n",
    "data_id2 = clean_data[int(len(clean_data)/4): int(2*len(clean_data)/4)]\n",
    "data_id3 = clean_data[int(2*len(clean_data)/4): int(3*len(clean_data)/4)]\n",
    "data_id4 = clean_data[int(3*len(clean_data)/4):]\n",
    "\n",
    "# Change directory to the temp folder\n",
    "os.chdir(data_temp)  # make sure `data_temp` exists and is defined\n",
    "\n",
    "# Save each chunk with joblib\n",
    "joblib.dump(data_id1, 'cleanspeeches_indexed1.pkl')\n",
    "joblib.dump(data_id2, 'cleanspeeches_indexed2.pkl')\n",
    "joblib.dump(data_id3, 'cleanspeeches_indexed3.pkl')\n",
    "joblib.dump(data_id4, 'cleanspeeches_indexed4.pkl')\n",
    "\n",
    "# Store list of cleaned data chunk paths to feed into preprocessing function later\n",
    "data_files = [\n",
    "    os.path.join(data_temp, 'cleanspeeches_indexed1.pkl'),\n",
    "    os.path.join(data_temp, 'cleanspeeches_indexed2.pkl'),\n",
    "    os.path.join(data_temp, 'cleanspeeches_indexed3.pkl'),\n",
    "    os.path.join(data_temp, 'cleanspeeches_indexed4.pkl')\n",
    "]\n",
    "\n",
    "print(f\"âœ… Saved clean speeches chunks in '{data_temp}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6e5109-8cf7-41e3-8fb9-8e7dde29b4e0",
   "metadata": {},
   "source": [
    "### Advanced Text Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0aa75944-4b75-4aa4-9430-9aa6bd9ac934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Define function to Tokenize, eliminate digits, remove stopwords, lemmatize, POS-Tagging ==\n",
    "\n",
    "def pro1(lista):\n",
    "    # Remove punctuation\n",
    "    return [[row[0], row[1].translate(translator)] for row in lista]\n",
    "\n",
    "def pro2(lista):\n",
    "    # Tokenize and lowercase with gensim\n",
    "    return [[row[0], gensim.utils.simple_preprocess(row[1])] for row in lista]\n",
    "\n",
    "def pro3(lista):\n",
    "    # Remove digits\n",
    "        return [[row[0], [w for w in row[1] if not any(char.isdigit() for char in w)]] for row in lista]\n",
    "\n",
    "def pro4(lista):\n",
    "    # Drop short words\n",
    "    return [[row[0], [w for w in row[1] if len(w) > 2]] for row in lista]\n",
    "\n",
    "def tags_spacy(lista):\n",
    "    texts = [' '.join(row[1]) for row in lista]\n",
    "    docs = list(nlp.pipe(texts, batch_size=20, n_process=1))\n",
    "    result = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        filtered_tokens = [token.text for token in doc if token.tag_.startswith(('N', 'V', 'J'))]\n",
    "        result.append([lista[i][0], filtered_tokens])\n",
    "    return result\n",
    "\n",
    "\n",
    "def pro5(lista):\n",
    "    # Remove stopwords using SpaCy stopword list\n",
    "    return [[row[0], [w for w in row[1] if w not in SPACY_STOPWORDS]] for row in lista]\n",
    "\n",
    "def pro6(lista):\n",
    "      return [\n",
    "        [row[0], [stemmer.stem(token) for token in row[1]]]\n",
    "        for row in lista\n",
    "    ]\n",
    "   # texts = [' '.join(row[1]) for row in lista]\n",
    "   # docs = list(nlp.pipe(texts, batch_size=20, n_process=1))\n",
    "   # result = []\n",
    "   # for i, doc in enumerate(docs):\n",
    "    # lemmatized = [token.lemma_ for token in doc]\n",
    "     #    result.append([lista[i][0], lemmatized])\n",
    "  #  return result\n",
    "\n",
    "########################## Question for Max: They removed procedural words in the paper\n",
    "\n",
    "def dropnull(lista):\n",
    "    # Drop empty speeches\n",
    "    return [row for row in lista if len(' '.join(row[1])) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "524a274a-004c-4d87-9bb3-5f789a42b90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed1.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed1.pkl] Before tagging: 24.05s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed1.pkl] After tagging: 465.79s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed1.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed1.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed1.pkl] Done. Total time: 546.51s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed2.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed2.pkl] Before tagging: 21.03s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed2.pkl] After tagging: 445.62s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed2.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed2.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed2.pkl] Done. Total time: 524.96s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed3.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed3.pkl] Before tagging: 21.67s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed3.pkl] After tagging: 451.89s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed3.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed3.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed3.pkl] Done. Total time: 529.86s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed4.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed4.pkl] Before tagging: 20.02s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed4.pkl] After tagging: 433.82s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed4.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed4.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed4.pkl] Done. Total time: 512.10s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# == Create full pre-processing function and call it\n",
    "\n",
    "def preprocessing(data_name):\n",
    "    t0 = time.time()\n",
    "    print(f\"Starting preprocessing for {data_name}...\")\n",
    "\n",
    "    data = joblib.load(data_name)\n",
    "    data = pro1(data)\n",
    "    data = pro2(data)\n",
    "    data = pro3(data)\n",
    "    data = pro4(data)\n",
    "\n",
    "    print(f\"[{data_name}] Before tagging: {time.time() - t0:.2f}s\")\n",
    "    data = tags_spacy(data)\n",
    "    print(f\"[{data_name}] After tagging: {time.time() - t0:.2f}s\")\n",
    "\n",
    "    data = pro5(data)\n",
    "   # data = pro6(data)\n",
    "    data = dropnull(data)\n",
    "\n",
    "    # out_name = data_name.replace('cleanspeeches_', 'preprocessed_speeches_').replace('.pkl', '_temp.pkl')\n",
    "\n",
    "    # original filename replacement\n",
    "    filename_wordcloud = data_name.replace('cleanspeeches_', 'wordcloud_speeches_').replace('.pkl', '.pkl')\n",
    "\n",
    "    # full path in data_preprocessed folder\n",
    "    out_name_wordcloud = os.path.join(data_preprocessed, os.path.basename(filename_wordcloud))\n",
    "\n",
    "  #  joblib.dump(data, out_name)\n",
    "    joblib.dump(data, out_name_wordcloud)\n",
    "\n",
    "    data_stemmed = pro6(data)\n",
    "\n",
    "    filename_preprocessed = data_name.replace('cleanspeeches_', 'preprocessed_speeches_').replace('.pkl', '.pkl')\n",
    "    out_preprocessed = os.path.join(data_preprocessed, os.path.basename(filename_preprocessed))\n",
    "    joblib.dump(data_stemmed, out_preprocessed)\n",
    "    print(f\"[{data_name}] Saved stemmed version: {out_preprocessed}\")\n",
    "\n",
    "    print(f\"[{data_name}] Done. Total time: {time.time() - t0:.2f}s\\n\")\n",
    "\n",
    "def main():\n",
    "    for fname in data_files:\n",
    "        preprocessing(fname)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d3409987-485c-47c9-9430-01fa2c0e650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the pre-processed data\n",
    "preprocessed_files = [\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed1.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed2.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed3.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed4.pkl')\n",
    "]\n",
    "\n",
    "wordcloud_files = [\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed1.pkl'),\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed2.pkl'),\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed3.pkl'),\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed4.pkl')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "134ddbf7-9b95-4697-9d9c-f9c55b52ec80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d006e-6a23-453b-9c91-0e60bd3f8301",
   "metadata": {},
   "source": [
    "## Word-Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf2443e-4c76-41b2-bc16-e72d91c318a8",
   "metadata": {},
   "source": [
    "### Count frequencies of all tokens and display the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "154d978d-cddd-431c-9d53-73c9faf8fdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:29<00:00,  7.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed1.pkl: removed words with freq < 10\n",
      "Processed C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed2.pkl: removed words with freq < 10\n",
      "Processed C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed3.pkl: removed words with freq < 10\n",
      "Processed C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed4.pkl: removed words with freq < 10\n",
      "\n",
      "[Stemmed] Top 100 most common words:\n",
      "nation: 232332\n",
      "unit: 186036\n",
      "countri: 176251\n",
      "intern: 159425\n",
      "develop: 145364\n",
      "peac: 133382\n",
      "world: 131630\n",
      "state: 128681\n",
      "peopl: 126148\n",
      "secur: 84579\n",
      "general: 76542\n",
      "govern: 74954\n",
      "econom: 72977\n",
      "organ: 68141\n",
      "right: 65874\n",
      "year: 65829\n",
      "assembl: 63469\n",
      "new: 58990\n",
      "effort: 56971\n",
      "problem: 56698\n",
      "human: 56072\n",
      "support: 54788\n",
      "continu: 53001\n",
      "communiti: 48791\n",
      "region: 48258\n",
      "polit: 48075\n",
      "time: 47283\n",
      "member: 42574\n",
      "africa: 42224\n",
      "session: 41637\n",
      "need: 41400\n",
      "war: 41154\n",
      "import: 40624\n",
      "council: 40598\n",
      "work: 40082\n",
      "achiev: 39915\n",
      "hope: 38156\n",
      "power: 38070\n",
      "conflict: 36935\n",
      "situat: 36044\n",
      "presid: 35955\n",
      "principl: 35891\n",
      "resolut: 34895\n",
      "global: 34852\n",
      "republ: 34447\n",
      "forc: 34210\n",
      "south: 34176\n",
      "great: 33856\n",
      "relat: 33671\n",
      "order: 33397\n",
      "oper: 33385\n",
      "concern: 33087\n",
      "action: 32165\n",
      "nuclear: 32048\n",
      "solut: 31817\n",
      "establish: 31476\n",
      "confer: 31242\n",
      "polici: 30489\n",
      "social: 30420\n",
      "effect: 30095\n",
      "respect: 29986\n",
      "commit: 29895\n",
      "independ: 29179\n",
      "chang: 28244\n",
      "interest: 28089\n",
      "charter: 27837\n",
      "african: 27484\n",
      "agreement: 27482\n",
      "progress: 27428\n",
      "weapon: 27203\n",
      "today: 27148\n",
      "system: 26648\n",
      "end: 26142\n",
      "contribut: 26116\n",
      "way: 25926\n",
      "process: 25909\n",
      "respons: 25813\n",
      "territori: 25794\n",
      "issu: 25624\n",
      "negoti: 25609\n",
      "believ: 24961\n",
      "deleg: 24959\n",
      "cooper: 24859\n",
      "implement: 24784\n",
      "question: 24769\n",
      "live: 24696\n",
      "east: 24570\n",
      "repres: 24190\n",
      "remain: 24043\n",
      "meet: 23878\n",
      "futur: 23357\n",
      "secretari: 23320\n",
      "arm: 22988\n",
      "result: 22961\n",
      "posit: 22913\n",
      "wish: 22880\n",
      "challeng: 22844\n",
      "express: 22724\n",
      "law: 22584\n",
      "area: 22239\n",
      "\n",
      "[Stemmed] Top 300 least common words:\n",
      "dutchand: 1\n",
      "eustatius: 1\n",
      "podul: 1\n",
      "tamaulipa: 1\n",
      "workprogram: 1\n",
      "reynoso: 1\n",
      "leant: 1\n",
      "moet: 1\n",
      "fresher: 1\n",
      "namnow: 1\n",
      "necessart: 1\n",
      "ihf: 1\n",
      "detaij: 1\n",
      "sanc: 1\n",
      "defendswil: 1\n",
      "waterretent: 1\n",
      "agrimet: 1\n",
      "shinto: 1\n",
      "jah: 1\n",
      "yahweh: 1\n",
      "westat: 1\n",
      "mariejosephin: 1\n",
      "cancela: 1\n",
      "gasana: 1\n",
      "setnghor: 1\n",
      "cernin: 1\n",
      "ussb: 1\n",
      "radyard: 1\n",
      "mummifi: 1\n",
      "mwakwer: 1\n",
      "annihili: 1\n",
      "planÃ®: 1\n",
      "Ã¬marshal: 1\n",
      "africanÃ­: 1\n",
      "multipillar: 1\n",
      "hippocrat: 1\n",
      "georgescu: 1\n",
      "calin: 1\n",
      "regon: 1\n",
      "systemÃ¢: 1\n",
      "herewith: 1\n",
      "tdbc: 1\n",
      "tian: 1\n",
      "shushenskaya: 1\n",
      "sayano: 1\n",
      "luÃ­: 1\n",
      "pluripolar: 1\n",
      "swapqj: 1\n",
      "wellresourc: 1\n",
      "unobscur: 1\n",
      "vladivosfaock: 1\n",
      "gat: 1\n",
      "iwe: 1\n",
      "lei: 1\n",
      "wfe: 1\n",
      "luxembourgeois: 1\n",
      "belgo: 1\n",
      "savintbi: 1\n",
      "ruacana: 1\n",
      "caluequ: 1\n",
      "tchipa: 1\n",
      "canaval: 1\n",
      "vwo: 1\n",
      "coluni: 1\n",
      "woodward: 1\n",
      "demona: 1\n",
      "oxtobi: 1\n",
      "willard: 1\n",
      "nes: 1\n",
      "kowloon: 1\n",
      "kroon: 1\n",
      "tunc: 1\n",
      "escori: 1\n",
      "ldi: 1\n",
      "theangola: 1\n",
      "salazarand: 1\n",
      "almadou: 1\n",
      "lassa: 1\n",
      "ordeath: 1\n",
      "yersin: 1\n",
      "ocÃ©an: 1\n",
      "maison: 1\n",
      "visoki: 1\n",
      "peric: 1\n",
      "vucitrn: 1\n",
      "gobulji: 1\n",
      "dakovica: 1\n",
      "gasic: 1\n",
      "dragica: 1\n",
      "istok: 1\n",
      "dubrava: 1\n",
      "pumpalov: 1\n",
      "radoj: 1\n",
      "sophouli: 1\n",
      "vima: 1\n",
      "griswold: 1\n",
      "vert: 1\n",
      "chamuriot: 1\n",
      "varkiza: 1\n",
      "voulgari: 1\n",
      "leeper: 1\n",
      "roatta: 1\n",
      "vaich: 1\n",
      "conpen: 1\n",
      "blurt: 1\n",
      "ewn: 1\n",
      "misbehaviour: 1\n",
      "autobiographi: 1\n",
      "moulvi: 1\n",
      "yabusin: 1\n",
      "baraq: 1\n",
      "excepticn: 1\n",
      "dyspepsia: 1\n",
      "rancid: 1\n",
      "purgat: 1\n",
      "sahouri: 1\n",
      "chihana: 1\n",
      "chakufwa: 1\n",
      "grimi: 1\n",
      "achievementin: 1\n",
      "sensori: 1\n",
      "jsoviet: 1\n",
      "iov: 1\n",
      "peaco: 1\n",
      "rabbuh: 1\n",
      "outspend: 1\n",
      "ageless: 1\n",
      "gediz: 1\n",
      "periscop: 1\n",
      "milligramm: 1\n",
      "slaveown: 1\n",
      "ploughman: 1\n",
      "tera: 1\n",
      "swpo: 1\n",
      "dirug: 1\n",
      "tlte: 1\n",
      "ecru: 1\n",
      "vecchio: 1\n",
      "palazzo: 1\n",
      "niccolÃ²: 1\n",
      "pira: 1\n",
      "erbil: 1\n",
      "unsmi: 1\n",
      "satisfactorili: 1\n",
      "tallest: 1\n",
      "tsoy: 1\n",
      "zhannetta: 1\n",
      "soliloquy: 1\n",
      "reynold: 1\n",
      "yudhoyno: 1\n",
      "apertur: 1\n",
      "republish: 1\n",
      "sledgehamm: 1\n",
      "roxa: 1\n",
      "vada: 1\n",
      "thera: 1\n",
      "sharaf: 1\n",
      "ohira: 1\n",
      "hideaway: 1\n",
      "grunitzki: 1\n",
      "stard: 1\n",
      "turnbul: 1\n",
      "jaffna: 1\n",
      "monolingu: 1\n",
      "intertogoles: 1\n",
      "regularis: 1\n",
      "namibi: 1\n",
      "mangosuthu: 1\n",
      "transdnestria: 1\n",
      "hrushevskiy: 1\n",
      "hivand: 1\n",
      "nevski: 1\n",
      "islamica: 1\n",
      "melchoir: 1\n",
      "buildingof: 1\n",
      "lancet: 1\n",
      "underth: 1\n",
      "kigh: 1\n",
      "pollyannaish: 1\n",
      "filigre: 1\n",
      "ruta: 1\n",
      "fÃ©type: 1\n",
      "martinet: 1\n",
      "kwanza: 1\n",
      "kilimo: 1\n",
      "sylvo: 1\n",
      "provokinq: 1\n",
      "wasn: 1\n",
      "feiffer: 1\n",
      "consession: 1\n",
      "bolivarisn: 1\n",
      "conclusionof: 1\n",
      "suleimani: 1\n",
      "canist: 1\n",
      "destruction: 1\n",
      "citizensto: 1\n",
      "ideus: 1\n",
      "daniloff: 1\n",
      "zakharov: 1\n",
      "gennadi: 1\n",
      "haramarskjold: 1\n",
      "edinburgh: 1\n",
      "dogged: 1\n",
      "saliniz: 1\n",
      "savann: 1\n",
      "meurt: 1\n",
      "ouvriÃ¨r: 1\n",
      "ninefold: 1\n",
      "bigart: 1\n",
      "pelÃ¡ez: 1\n",
      "merab: 1\n",
      "merril: 1\n",
      "stearn: 1\n",
      "fanni: 1\n",
      "grecian: 1\n",
      "gorgon: 1\n",
      "foxhol: 1\n",
      "unemot: 1\n",
      "awjiam: 1\n",
      "wouh: 1\n",
      "natidn: 1\n",
      "wough: 1\n",
      "cajleii: 1\n",
      "nde: 1\n",
      "tajk: 1\n",
      "ahnati: 1\n",
      "rutil: 1\n",
      "indespens: 1\n",
      "overshot: 1\n",
      "unverv: 1\n",
      "koelberg: 1\n",
      "toxicologist: 1\n",
      "mazaruni: 1\n",
      "scintilla: 1\n",
      "vve: 1\n",
      "atcancun: 1\n",
      "tiie: 1\n",
      "kruger: 1\n",
      "osra: 1\n",
      "adumin: 1\n",
      "maalei: 1\n",
      "renit: 1\n",
      "navalnyj: 1\n",
      "defund: 1\n",
      "traderul: 1\n",
      "abdulssalam: 1\n",
      "nikolay: 1\n",
      "wain: 1\n",
      "bibinagar: 1\n",
      "shabir: 1\n",
      "goradz: 1\n",
      "libetalÃ­: 1\n",
      "stellenbosch: 1\n",
      "noncompromis: 1\n",
      "cakobau: 1\n",
      "giorgia: 1\n",
      "tbdulla: 1\n",
      "knowact: 1\n",
      "peacepreach: 1\n",
      "verhaeren: 1\n",
      "reti: 1\n",
      "sangoulÃ©: 1\n",
      "turntabl: 1\n",
      "copu: 1\n",
      "pronunciamento: 1\n",
      "rechart: 1\n",
      "meati: 1\n",
      "societie: 1\n",
      "stopwatch: 1\n",
      "bhopal: 1\n",
      "pashinyan: 1\n",
      "unhuman: 1\n",
      "yegiazarian: 1\n",
      "mher: 1\n",
      "timespecif: 1\n",
      "karapetian: 1\n",
      "varuzhan: 1\n",
      "sangach: 1\n",
      "guliyev: 1\n",
      "asgarov: 1\n",
      "dilgam: 1\n",
      "odkb: 1\n",
      "birthpang: 1\n",
      "kleptocraci: 1\n",
      "theos: 1\n",
      "faoj: 1\n",
      "lemon: 1\n",
      "ecclesi: 1\n",
      "poldo: 1\n",
      "sill: 1\n",
      "chakma: 1\n",
      "wuch: 1\n",
      "boumedidn: 1\n",
      "djbouti: 1\n",
      "respectth: 1\n",
      "cuas: 1\n",
      "neuralg: 1\n",
      "winzer: 1\n",
      "garman: 1\n",
      "vojciech: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:29<00:00,  7.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Wordcloud] Top 100 most common words:\n",
      "nations: 185541\n",
      "united: 183294\n",
      "international: 152545\n",
      "world: 128865\n",
      "countries: 117093\n",
      "peace: 110332\n",
      "states: 90068\n",
      "development: 85218\n",
      "people: 80324\n",
      "security: 78421\n",
      "general: 74810\n",
      "economic: 72599\n",
      "assembly: 62091\n",
      "new: 58990\n",
      "country: 57215\n",
      "government: 56448\n",
      "organization: 50483\n",
      "human: 48889\n",
      "efforts: 46335\n",
      "political: 45844\n",
      "peoples: 45773\n",
      "community: 44912\n",
      "rights: 44034\n",
      "support: 40749\n",
      "africa: 40620\n",
      "council: 39845\n",
      "session: 39784\n",
      "time: 38904\n",
      "war: 36554\n",
      "state: 34415\n",
      "south: 34165\n",
      "great: 33644\n",
      "problems: 33302\n",
      "republic: 33286\n",
      "years: 33171\n",
      "national: 33042\n",
      "year: 32566\n",
      "order: 32067\n",
      "nuclear: 32003\n",
      "developing: 31781\n",
      "situation: 31337\n",
      "global: 30966\n",
      "work: 30014\n",
      "social: 29765\n",
      "conference: 28324\n",
      "hope: 27770\n",
      "charter: 27624\n",
      "president: 27511\n",
      "today: 26394\n",
      "continue: 26269\n",
      "important: 26160\n",
      "need: 25962\n",
      "region: 25912\n",
      "african: 25701\n",
      "relations: 25428\n",
      "progress: 25116\n",
      "principles: 24764\n",
      "east: 24565\n",
      "action: 24210\n",
      "respect: 23983\n",
      "weapons: 23796\n",
      "problem: 23396\n",
      "future: 23232\n",
      "secretary: 23216\n",
      "process: 23136\n",
      "peaceful: 23037\n",
      "operation: 22410\n",
      "end: 22112\n",
      "member: 22015\n",
      "conflict: 21982\n",
      "cooperation: 21980\n",
      "system: 21926\n",
      "disarmament: 21845\n",
      "policy: 21736\n",
      "solution: 21730\n",
      "delegation: 21571\n",
      "way: 21416\n",
      "independence: 20876\n",
      "right: 20732\n",
      "resolution: 20679\n",
      "members: 20559\n",
      "law: 20075\n",
      "resources: 19874\n",
      "agreement: 19508\n",
      "powers: 19342\n",
      "foreign: 19207\n",
      "role: 18961\n",
      "believe: 18928\n",
      "question: 18798\n",
      "present: 18563\n",
      "change: 18414\n",
      "military: 18353\n",
      "democratic: 18205\n",
      "fact: 18034\n",
      "means: 17757\n",
      "trade: 17507\n",
      "possible: 17446\n",
      "measures: 17391\n",
      "union: 17250\n",
      "developed: 17208\n",
      "\n",
      "[Wordcloud] Top 300 least common words:\n",
      "countys: 1\n",
      "dutchand: 1\n",
      "eustatius: 1\n",
      "podul: 1\n",
      "tamaulipas: 1\n",
      "workprogram: 1\n",
      "appall: 1\n",
      "maliks: 1\n",
      "reynoso: 1\n",
      "leant: 1\n",
      "moet: 1\n",
      "fresher: 1\n",
      "namnow: 1\n",
      "necessart: 1\n",
      "ihf: 1\n",
      "detaijs: 1\n",
      "curs: 1\n",
      "sanc: 1\n",
      "defendswill: 1\n",
      "waterretention: 1\n",
      "agrimet: 1\n",
      "shinto: 1\n",
      "jah: 1\n",
      "yahweh: 1\n",
      "dishonoring: 1\n",
      "westat: 1\n",
      "mariejosephine: 1\n",
      "saucers: 1\n",
      "denudes: 1\n",
      "mortes: 1\n",
      "cancela: 1\n",
      "gasana: 1\n",
      "setnghor: 1\n",
      "cernin: 1\n",
      "ussb: 1\n",
      "radyard: 1\n",
      "boiler: 1\n",
      "mummified: 1\n",
      "percussion: 1\n",
      "inanities: 1\n",
      "mwakwere: 1\n",
      "annihiliation: 1\n",
      "enclosures: 1\n",
      "planÃ®: 1\n",
      "Ã¬marshall: 1\n",
      "africanÃ­s: 1\n",
      "multipillared: 1\n",
      "hippocratic: 1\n",
      "georgescu: 1\n",
      "calin: 1\n",
      "regon: 1\n",
      "systemÃ¢s: 1\n",
      "herewith: 1\n",
      "tdbc: 1\n",
      "progenitor: 1\n",
      "tian: 1\n",
      "shushenskaya: 1\n",
      "sayano: 1\n",
      "tarrying: 1\n",
      "luÃ­s: 1\n",
      "pluripolar: 1\n",
      "swapqj: 1\n",
      "redeemable: 1\n",
      "subcontract: 1\n",
      "wellresourced: 1\n",
      "familiarization: 1\n",
      "unobscured: 1\n",
      "vladivosfaock: 1\n",
      "gat: 1\n",
      "iwe: 1\n",
      "lei: 1\n",
      "seismograph: 1\n",
      "wfe: 1\n",
      "luxembourgers: 1\n",
      "luxembourgeoise: 1\n",
      "belgo: 1\n",
      "economique: 1\n",
      "generalizing: 1\n",
      "gambits: 1\n",
      "savintbi: 1\n",
      "ruacana: 1\n",
      "calueque: 1\n",
      "tchipa: 1\n",
      "canavale: 1\n",
      "lounges: 1\n",
      "programfs: 1\n",
      "vwo: 1\n",
      "colunialism: 1\n",
      "woodward: 1\n",
      "demona: 1\n",
      "oxtoby: 1\n",
      "willard: 1\n",
      "nes: 1\n",
      "kowloon: 1\n",
      "worshipper: 1\n",
      "kroon: 1\n",
      "tunc: 1\n",
      "kozyrevs: 1\n",
      "escorial: 1\n",
      "ldi: 1\n",
      "theangola: 1\n",
      "salazarand: 1\n",
      "adepts: 1\n",
      "lubricants: 1\n",
      "impressment: 1\n",
      "almadou: 1\n",
      "lassa: 1\n",
      "ordeath: 1\n",
      "yersin: 1\n",
      "ocÃ©ans: 1\n",
      "maison: 1\n",
      "visoki: 1\n",
      "peric: 1\n",
      "vucitrn: 1\n",
      "gobulji: 1\n",
      "dakovica: 1\n",
      "gasic: 1\n",
      "dragica: 1\n",
      "istok: 1\n",
      "dubrava: 1\n",
      "pumpalovic: 1\n",
      "radoje: 1\n",
      "repeals: 1\n",
      "sophoulis: 1\n",
      "vima: 1\n",
      "griswold: 1\n",
      "verting: 1\n",
      "chamuriot: 1\n",
      "varkiza: 1\n",
      "voulgaris: 1\n",
      "leeper: 1\n",
      "roatta: 1\n",
      "vaich: 1\n",
      "conpenned: 1\n",
      "blurt: 1\n",
      "indisputably: 1\n",
      "ewn: 1\n",
      "misbehaviour: 1\n",
      "autobiography: 1\n",
      "moulvi: 1\n",
      "yabusins: 1\n",
      "baraq: 1\n",
      "excepticn: 1\n",
      "dyspepsia: 1\n",
      "rancid: 1\n",
      "purgative: 1\n",
      "sahouri: 1\n",
      "chihana: 1\n",
      "chakufwa: 1\n",
      "grimy: 1\n",
      "achievementin: 1\n",
      "sensory: 1\n",
      "jsoviet: 1\n",
      "ioving: 1\n",
      "peaco: 1\n",
      "rabbuh: 1\n",
      "outspend: 1\n",
      "monarchists: 1\n",
      "ageless: 1\n",
      "anatolie: 1\n",
      "gediz: 1\n",
      "periscopes: 1\n",
      "slots: 1\n",
      "milligramme: 1\n",
      "clank: 1\n",
      "lubricated: 1\n",
      "slaveowner: 1\n",
      "ploughman: 1\n",
      "tera: 1\n",
      "swpo: 1\n",
      "dirug: 1\n",
      "tlte: 1\n",
      "ecru: 1\n",
      "vecchio: 1\n",
      "palazzo: 1\n",
      "niccolÃ²: 1\n",
      "pira: 1\n",
      "erbil: 1\n",
      "leet: 1\n",
      "olympism: 1\n",
      "achieveable: 1\n",
      "unsmis: 1\n",
      "performer: 1\n",
      "satisfactorily: 1\n",
      "symbolical: 1\n",
      "tallest: 1\n",
      "tsoy: 1\n",
      "zhannetta: 1\n",
      "misplacing: 1\n",
      "soliloquy: 1\n",
      "reynolds: 1\n",
      "opined: 1\n",
      "yudhoyno: 1\n",
      "aperture: 1\n",
      "attemptting: 1\n",
      "republished: 1\n",
      "sledgehammer: 1\n",
      "roxas: 1\n",
      "vada: 1\n",
      "thera: 1\n",
      "sharaf: 1\n",
      "ohira: 1\n",
      "desalinized: 1\n",
      "salinized: 1\n",
      "hideaway: 1\n",
      "grunitzky: 1\n",
      "stard: 1\n",
      "turnbull: 1\n",
      "jaffna: 1\n",
      "interviewer: 1\n",
      "monolingual: 1\n",
      "intertogolese: 1\n",
      "regularise: 1\n",
      "namibi: 1\n",
      "mangosuthu: 1\n",
      "myriads: 1\n",
      "transdnestria: 1\n",
      "hrushevskiy: 1\n",
      "unaffordability: 1\n",
      "hivand: 1\n",
      "nevsky: 1\n",
      "islamica: 1\n",
      "melchoir: 1\n",
      "buildingof: 1\n",
      "lancet: 1\n",
      "unquiet: 1\n",
      "underthe: 1\n",
      "kigh: 1\n",
      "relearned: 1\n",
      "inis: 1\n",
      "purses: 1\n",
      "offload: 1\n",
      "colloquialism: 1\n",
      "pollyannaish: 1\n",
      "filigree: 1\n",
      "ruta: 1\n",
      "fÃ©type: 1\n",
      "martinets: 1\n",
      "kwanza: 1\n",
      "kilimo: 1\n",
      "infesting: 1\n",
      "sylvo: 1\n",
      "provokinq: 1\n",
      "wasn: 1\n",
      "feiffer: 1\n",
      "consessional: 1\n",
      "bolivarisn: 1\n",
      "conclusionof: 1\n",
      "suleimani: 1\n",
      "canisters: 1\n",
      "starters: 1\n",
      "destructionism: 1\n",
      "citizensto: 1\n",
      "ideus: 1\n",
      "daniloff: 1\n",
      "zakharov: 1\n",
      "gennadi: 1\n",
      "haramarskjold: 1\n",
      "edinburgh: 1\n",
      "doggedness: 1\n",
      "conundrums: 1\n",
      "bumbling: 1\n",
      "salinizating: 1\n",
      "savannization: 1\n",
      "meurt: 1\n",
      "gallop: 1\n",
      "ouvriÃ¨re: 1\n",
      "ninefold: 1\n",
      "bigart: 1\n",
      "pelÃ¡ez: 1\n",
      "meraber: 1\n",
      "merrill: 1\n",
      "stearns: 1\n",
      "fannie: 1\n",
      "freddie: 1\n",
      "grecian: 1\n",
      "gorgon: 1\n",
      "foxhole: 1\n",
      "rabbits: 1\n",
      "rafters: 1\n",
      "unemotional: 1\n",
      "awjiam: 1\n",
      "hornet: 1\n",
      "wouh: 1\n",
      "natidns: 1\n",
      "lipped: 1\n",
      "wough: 1\n",
      "cajleii: 1\n",
      "nde: 1\n",
      "tajk: 1\n",
      "ahnaty: 1\n",
      "bloodiness: 1\n",
      "lifejackets: 1\n",
      "rutile: 1\n",
      "indespensable: 1\n",
      "floodgate: 1\n",
      "soloists: 1\n",
      "referencing: 1\n",
      "overshot: 1\n",
      "unverving: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit\\\\data\\\\freq\\\\word_counts_wordcloud.pkl']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#== Count token frequencies ==\n",
    "\n",
    "def count_frequencies(filenames):\n",
    "    total_freq = Counter()\n",
    "    for fname in tqdm(filenames):\n",
    "        data = joblib.load(fname)\n",
    "        tokens = chain.from_iterable(row[1] for row in data if isinstance(row[1], list))\n",
    "        total_freq.update(tokens)\n",
    "    return total_freq\n",
    "\n",
    "def remove_rare_words(filenames, freqs, min_count=10):\n",
    "    for fname in filenames:\n",
    "        data = joblib.load(fname)\n",
    "        filtered_data = []\n",
    "        for doc_id, tokens in data:\n",
    "            filtered_tokens = [w for w in tokens if freqs.get(w, 0) >= min_count]\n",
    "            filtered_data.append([doc_id, filtered_tokens])\n",
    "        joblib.dump(filtered_data, fname)  # overwrite or save as new file\n",
    "        print(f\"Processed {fname}: removed words with freq < {min_count}\")\n",
    "\n",
    "# === Count for preprocessed (stemmed) speeches ===\n",
    "word_counts_stemmed = count_frequencies(preprocessed_files)\n",
    "\n",
    "remove_rare_words(preprocessed_files, word_counts_stemmed, min_count=10)\n",
    "\n",
    "print(\"\\n[Stemmed] Top 100 most common words:\")\n",
    "for word, count in word_counts_stemmed.most_common(100):\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\n[Stemmed] Top 300 least common words:\")\n",
    "for word, count in word_counts_stemmed.most_common()[:-301:-1]:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Save stemmed word counts\n",
    "save_path_stemmed = os.path.join(data_freq, 'word_counts_stemmed.pkl')\n",
    "joblib.dump(word_counts_stemmed, save_path_stemmed)\n",
    "\n",
    "# === Count for wordcloud (unstemmed) speeches ===\n",
    "word_counts_wordcloud = count_frequencies(wordcloud_files)\n",
    "\n",
    "print(\"\\n[Wordcloud] Top 100 most common words:\")\n",
    "for word, count in word_counts_wordcloud.most_common(100):\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\n[Wordcloud] Top 300 least common words:\")\n",
    "for word, count in word_counts_wordcloud.most_common()[:-301:-1]:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Save unstemmed word counts\n",
    "save_path_wordcloud = os.path.join(data_freq, 'word_counts_wordcloud.pkl')\n",
    "joblib.dump(word_counts_wordcloud, save_path_wordcloud)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "932ce36d-d21c-4f1c-aff7-899c321dcfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_c)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39acefa1-c504-478e-98e5-9d465963dc82",
   "metadata": {},
   "source": [
    "### Count the frequency of the dictionary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "9e28f481-7e20-486c-a6b3-8729d6aeaad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of affect dictionary:\n",
      "['forbid', 'unattract', 'cruelti', 'crappi', 'apathi', 'scari', 'unimpress', 'sin', 'dumbest', 'eas', 'agit', 'sob', 'shocker', 'tragedi', 'fabul', 'strongest', 'giver', 'sigh', 'aw', 'witch', 'hurtl', 'fucktard', 'cruel', 'glamor', 'funni', 'smarter', 'brillianc', 'irrate', 'alright', 'honest', 'profit', 'fearless', 'grievous', 'relax', 'isolationist', 'hah', 'shyness', 'poorest', 'cruelest', 'troublemak', 'disagre', 'agon', 'terror', 'fight', 'pleas', 'poor', 'crazi', 'hostil', 'stupid', 'damnat', 'vain', 'jade', 'heartless', 'nag', 'gloomi', 'damn', 'dishearten', 'pleaser', 'credit', 'warmth', 'greatest', 'whine', 'shame', 'angriest', 'envious', 'grin', 'blameless', 'sweeter', 'laidback', 'stupidest', 'unprotect', 'whiner', 'unlov', 'shake', 'boredom', 'fairer', 'weaker', 'wellb', 'bold', 'sucki', 'unsuccess', 'mourner', 'liken', 'defens', 'invigor', 'tedious', 'paranoid', 'cynic', 'dignifi', 'paranoia', 'sweetest', 'contented', 'humili', 'crush', 'terrif', 'alarm', 'asham', 'impati', 'fake', 'flirti', 'laziest', 'libertarian', 'empti', 'panicki', 'rape', 'contemptu', 'incent', 'reward', 'brutal', 'ignoramus', 'bereav', 'wimp', 'laugh', 'nicer', 'warm', 'resent', 'threat', 'hell', 'tortur', 'mad', 'worship', 'uncomfort', 'depress', 'unsaf', 'faultless', 'numb', 'humorless', 'murder', 'prettiest', 'jealous', 'tragic', 'discomfort', 'loveli', 'ungrat', 'divin', 'lamest', 'weaken', 'horribl', 'fuck', 'agoni', 'grievanc', 'ach', 'isolation', 'masoch', 'uglier', 'amaz', 'devilish', 'bitter', 'devil', 'smartest', 'glori', 'trite', 'vicious', 'unsavori', 'grossest', 'sinist', 'cheeri', 'dismay', 'easier', 'flatter', 'virtuous', 'egot', 'generos', 'cherish', 'heartwarm', 'advers', 'tough', 'adorn', 'overwhelm', 'devast', 'egotist', 'sociabl', 'lazier', 'affection', 'sarcasm', 'humor', 'funniest', 'startl', 'innoc', 'vomit', 'shyli', 'kiss', 'burdensom', 'melancholi', 'weakest', 'worthless', 'gentlest', 'proud', 'alarmist', 'pervert', 'demeanor', 'loneli', 'reassur', 'weird', 'passionless', 'loyalti', 'frustrat', 'weak', 'immor', 'hater', 'libertin', 'flirt', 'terribl', 'threaten', 'fatalist', 'melancholia', 'avers', 'prick', 'sucker', 'meanest', 'virtuos', 'benefici', 'neurot', 'disappoint', 'restless', 'romant', 'bastard', 'angri', 'flatteri', 'stun', 'bolder', 'amor', 'forbidden', 'hug', 'demean', 'disgust', 'truer', 'benefic', 'teas', 'stinki', 'condemnatori', 'turmoil', 'poison', 'cunt', 'gorgeous', 'harm', 'lousi', 'glorifi', 'piti', 'wimpish', 'impression', 'freakish', 'happiest', 'apathet', 'pitiabl', 'unpleas', 'suffer', 'miseri', 'toler', 'mess', 'casual', 'anxieti', 'stale', 'fieri', 'grudg', 'arrog', 'outrag', 'thrill', 'faithless', 'popular', 'vaniti', 'palat', 'fantasi', 'decay', 'domin', 'mood', 'proudest', 'scream', 'stress', 'calm', 'fatigu', 'dumber', 'benign', 'rude', 'steal', 'tension', 'enthusiast', 'intimid', 'sweetheart', 'prettier', 'perv', 'pathet', 'flawless', 'contradictori', 'distress', 'moron', 'gratifi', 'frighten', 'anguish', 'snob', 'defenseless', 'fantast', 'sad', 'thief', 'worsen', 'tear', 'handsom', 'attract', 'messi', 'smother', 'scarier', 'afraid', 'insult', 'disreput', 'wept', 'victim', 'traumat', 'prejudic', 'amus', 'decent', 'joyous', 'masochist', 'abandon', 'foe', 'pleasantri', 'shitless', 'strengthen', 'antagonist', 'liar', 'romantic', 'violenc', 'sentimentalist', 'gratif', 'decept', 'witchcraft', 'liveli', 'impressionist', 'guilt', 'griev', 'rotten', 'disgrac', 'insincer', 'pushi', 'enjoy', 'freaki', 'ador', 'grate', 'homesick', 'braver', 'battlefield', 'niceti', 'hooray', 'incompet', 'thiev', 'timid', 'adventur', 'needi', 'joyless', 'grimac', 'gloom', 'dump', 'horrid', 'magnific', 'unlucki', 'killer', 'uncontrol', 'funnier', 'ruin', 'weepi', 'play', 'encourag', 'fantas', 'irrit', 'sincer', 'rage', 'despis', 'unhappi', 'hate', 'bravest', 'harmless', 'hero', 'vigor', 'pervers', 'dumb', 'degrad', 'shameless', 'villaini', 'envi', 'aggress', 'fucker', 'courag', 'craziest', 'delici', 'scare', 'finer', 'bittersweet', 'trivial', 'wealthi', 'heroic', 'ecstat', 'irrat', 'prejudici', 'smug', 'stammer', 'feroci', 'phobic', 'villain', 'anger', 'fond', 'obsession', 'awesom', 'wors', 'compassion', 'weakl', 'calmer', 'inhibit', 'bother', 'rancid', 'hatr', 'beaten', 'opportunist', 'offens', 'haunt', 'appal', 'painless', 'pessimist', 'molest', 'lol', 'wrongdo', 'frantic', 'heroin', 'rejoic', 'woe', 'miser', 'meaner', 'saddest', 'mooch', 'attack', 'enthusiasm', 'joke', 'meritocraci', 'angrier', 'emot', 'weirdest', 'silliest', 'kisser', 'hurrah', 'crueler', 'nervous', 'dishonor', 'gracious', 'sicker', 'hopeless', 'delect', 'compliment', 'petrifi', 'cuter', 'reek', 'loveless', 'daze', 'weirder', 'triumphant', 'impolit', 'offend', 'richer', 'violent', 'crazier', 'comedown', 'heroism', 'stubborn', 'maniac', 'strain', 'broke', 'jerk', 'unkind', 'asshol', 'reveng', 'jealousi', 'damnabl', 'unimport', 'careless', 'gentler', 'bitch', 'shi', 'terrorist', 'rigid', 'harsh', 'passion', 'annoy', 'compass', 'geek', 'weirdo', 'agre', 'whore', 'splendor', 'suck', 'fun', 'chuckl', 'snobberi', 'mock', 'doom', 'reluct', 'mourn', 'boldest', 'helpless', 'trembl', 'pestil', 'curs', 'desper', 'harass', 'complain', 'joker', 'obsess', 'uneasi', 'nasti', 'greedi', 'belov', 'grim', 'upset', 'greed', 'honesti', 'phoni', 'feudal', 'loveliest', 'trauma', 'dwell', 'ecstasi', 'blame', 'legit', 'calmest', 'sickest', 'scariest', 'sarcast', 'cheat', 'fatal', 'stronger', 'energet', 'feroc', 'healthi', 'goddam', 'disast', 'flirtati', 'defeatist', 'glorious', 'terrifi', 'gossip', 'forbad', 'sicken', 'poorer', 'strangest', 'cute', 'struggl', 'woebegon', 'worri', 'gratitud', 'cheer', 'distraught', 'ok', 'vulner', 'blessed', 'fairest', 'lame', 'optim', 'grouch', 'fright', 'pessim', 'sadder', 'worst', 'despair', 'bash', 'intellectu', 'promis', 'prais', 'heartfelt', 'horror', 'crap', 'repress', 'prouder', 'fought', 'discourag', 'heal', 'lazi', 'precious', 'melanchol', 'isol', 'aggressor', 'giggl', 'idiot', 'twitchi', 'glorif', 'kid', 'selfish', 'smile', 'grief', 'hellish', 'shit', 'fume', 'bitchi', 'fool', 'trustworthi', 'optimist', 'dumpi', 'unwelcom', 'neat', 'nicest', 'furi', 'unfriend', 'ridicul', 'arguabl', 'shamefac', 'pervi', 'upbeat', 'wow', 'easygo', 'blameworthi', 'charmer', 'stink', 'phobia', 'adversari', 'sillier', 'distrust', 'dread', 'happier', 'vital', 'dissatisfact', 'insecur', 'assault', 'ineffect', 'panic', 'powerless', 'nightmar', 'wealth', 'enrag', 'splendid', 'foolish', 'ugliest', 'contradict', 'worrier', 'freak']\n",
      "Number of words in affect dictionary: 629\n",
      "Contents of cognition dictionary:\n",
      "['analyt', 'guess', 'suspect', 'coz', 'mysteri', 'unawar', 'proverbi', 'dissimilar', 'consequenti', 'stimul', 'occasion', 'rethink', 'purest', 'entireti', 'abnorm', 'referenti', 'differenti', 'diagnos', 'ambigu', 'concentr', 'undon', 'undecid', 'disclosur', 'fundamentalist', 'blur', 'reaction', 'variabl', 'recogniz', 'meant', 'reconstruct', 'supposit', 'acknowledg', 'choosi', 'accuraci', 'mislead', 'perceiv', 'hypothesi', 'unknow', 'cuz', 'reveal', 'potenti', 'puriti', 'persuad', 'indirect', 'generat', 'presumpt', 'clue', 'justif', 'evidenti', 'realize', 'deduct', 'causal', 'inevit', 'thinker', 'evidentiari', 'factor', 'undeni', 'unclear', 'pick', 'induc', 'accur', 'absolut', 'everytim', 'reconsider', 'diagnost', 'solut', 'diagnosi', 'discoveri', 'ration', 'unambigu', 'explicit', 'decis', 'insight', 'confess', 'fuzzi', 'led', 'lesson', 'theoriz', 'analyz', 'learner', 'split', 'enlighten', 'cos', 'respons', 'dubious', 'react', 'guarante', 'someday', 'blatant', 'undo', 'perceptu', 'infal', 'afterthought', 'inequ', 'feasibl', 'knowabl', 'approxim', 'logic', 'meaning', 'unknown', 'categor', 'trigger', 'incomplet', 'curious', 'attribut', 'pretend', 'reactionari', 'prove', 'meaningless', 'reactiv', 'proof', 'bet', 'purpos', 'suppos', 'discern', 'imagin', 'kinda', 'comprehens', 'sens', 'percept', 'everyday', 'rational', 'unquestion', 'random', 'implicit', 'clarif', 'theori', 'dunno', 'analysi', 'obedi', 'memor', 'genuin', 'evalu', 'explor', 'linkag', 'grasp', 'curios', 'methink', 'somewher', 'deduc', 'unwant', 'induct', 'unrel', 'outstand', 'discov', 'unresolv', 'exploratori', 'choos', 'perspect', 'inferenti', 'disclos', 'factual', 'arbitrari', 'conting', 'vagu', 'persuas', 'manipul', 'impli', 'somehow', 'sorta', 'borderlin', 'reconcil', 'anyhow', 'conscious', 'probabilist', 'precis', 'rearrang', 'referenc', 'obscur']\n",
      "Number of words in cognition dictionary: 169\n"
     ]
    }
   ],
   "source": [
    "# == Count dictionary words\n",
    "\n",
    "# Load dictionaries            ##### How did they come up with this dictionary? Why did they exclude words?\n",
    "\n",
    "affect_path = os.path.join(data_dict, 'dictionary_affect.pkl')\n",
    "cognition_path = os.path.join(data_dict, 'dictionary_cognition.pkl')\n",
    "\n",
    "with open(affect_path, 'rb') as f:\n",
    "    affect_dict = pickle.load(f)\n",
    "print(\"Contents of affect dictionary:\")\n",
    "print(affect_dict)\n",
    "print(\"Number of words in affect dictionary:\", len(affect_dict))\n",
    "\n",
    "with open(cognition_path, 'rb') as f:\n",
    "    cognition_dict = pickle.load(f)\n",
    "print(\"Contents of cognition dictionary:\")\n",
    "print(cognition_dict)\n",
    "print(\"Number of words in cognition dictionary:\", len(cognition_dict))\n",
    "\n",
    "affect = joblib.load(affect_path)\n",
    "cognition = joblib.load(cognition_path)\n",
    "\n",
    "a = [[i, word_counts_stemmed[i]] for i in affect if i in word_counts_stemmed]\n",
    "c = [[i, word_counts_stemmed[i]] for i in cognition if i in word_counts_stemmed]\n",
    "\n",
    "a = sorted(a, key=lambda x: x[1], reverse=True)\n",
    "c = sorted(c, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "a = [[i[0], f\"({i[1]}),\"] for i in a]\n",
    "c = [[i[0], f\"({i[1]}),\"] for i in c]\n",
    "\n",
    "a1 = ' '.join(str(r) for v in a for r in v)\n",
    "c1 = ' '.join(str(r) for v in c for r in v)\n",
    "\n",
    "affect_out_path = os.path.join(data_freq, \"affect_words.txt\")\n",
    "cog_out_path = os.path.join(data_freq, \"cog_words.txt\")\n",
    "\n",
    "os.makedirs(data_freq, exist_ok=True)  # ensure directory exists\n",
    "\n",
    "with open(affect_out_path, \"w\") as output:\n",
    "    output.write(a1)\n",
    "\n",
    "with open(cog_out_path, \"w\") as output:\n",
    "    output.write(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0dffe87f-d712-4d71-9305-fb288cf06402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 words by weighted frequency:\n",
      "disloy: 0.9993014671495019\n",
      "prick: 0.9993014671495019\n",
      "litr: 0.9993014671495019\n",
      "chen: 0.9993014671495019\n",
      "dualism: 0.9993014671495019\n",
      "praia: 0.9993014671495019\n",
      "lexicon: 0.9993014671495019\n",
      "insuffer: 0.9993014671495019\n",
      "siad: 0.9993014671495019\n",
      "nothing: 0.9993014671495019\n",
      "neat: 0.9993014671495019\n",
      "sojourn: 0.9993014671495019\n",
      "therefor: 0.9993014671495019\n",
      "cento: 0.9993014671495019\n",
      "lish: 0.9993014671495019\n",
      "pyrrhic: 0.9993014671495019\n",
      "tenth: 0.9993014671495019\n",
      "mondlan: 0.9993014671495019\n",
      "resolu: 0.9993014671495019\n",
      "tive: 0.9993014671495019\n",
      "kiloton: 0.9993014671495019\n",
      "atyp: 0.9993014671495019\n",
      "dan: 0.9993014671495019\n",
      "midrand: 0.9993014671495019\n",
      "mahamadou: 0.9993014671495019\n",
      "shrank: 0.9993014671495019\n",
      "unorthodox: 0.9993014671495019\n",
      "foci: 0.9993014671495019\n",
      "finer: 0.9993014671495019\n",
      "gust: 0.9993014671495019\n",
      "underworld: 0.9993014671495019\n",
      "bluster: 0.9993014671495019\n",
      "sneer: 0.9993014671495019\n",
      "shirt: 0.9993014671495019\n",
      "stealth: 0.9993014671495019\n",
      "brigand: 0.9993014671495019\n",
      "diaz: 0.9993014671495019\n",
      "outdo: 0.9993014671495019\n",
      "aquif: 0.9993014671495019\n",
      "edc: 0.9993014671495019\n",
      "shorn: 0.9993014671495019\n",
      "gatumba: 0.9993014671495019\n",
      "sus: 0.9993014671495019\n",
      "engross: 0.9993014671495019\n",
      "gobbl: 0.9993014671495019\n",
      "vaclav: 0.9993014671495019\n",
      "bazaar: 0.9993014671495019\n",
      "affabl: 0.9993014671495019\n",
      "floyd: 0.9993014671495019\n",
      "championship: 0.9993014671495019\n",
      "blasphem: 0.9993014671495019\n",
      "primev: 0.9993014671495019\n",
      "rat: 0.9993014671495019\n",
      "cannabi: 0.9993014671495019\n",
      "unprofit: 0.9993014671495019\n",
      "trifl: 0.9993014671495019\n",
      "tommi: 0.9993014671495019\n",
      "carriacou: 0.9993014671495019\n",
      "banquet: 0.9993014671495019\n",
      "unco: 0.9993014671495019\n",
      "bundl: 0.9993014671495019\n",
      "joÃ£o: 0.9993014671495019\n",
      "midwiv: 0.9993014671495019\n",
      "malleabl: 0.9993014671495019\n",
      "unpromis: 0.9993014671495019\n",
      "eastward: 0.9993014671495019\n",
      "alyaksandr: 0.9993014671495019\n",
      "israelit: 0.9993014671495019\n",
      "sapien: 0.9993014671495019\n",
      "callaghan: 0.9993014671495019\n",
      "panamian: 0.9993014671495019\n",
      "mohtarma: 0.9993014671495019\n",
      "peshawar: 0.9993014671495019\n",
      "headed: 0.9993014671495019\n",
      "unfreez: 0.9993014671495019\n",
      "scarcer: 0.9993014671495019\n",
      "evangel: 0.9993014671495019\n",
      "unsung: 0.9993014671495019\n",
      "guayaquil: 0.9993014671495019\n",
      "condor: 0.9993014671495019\n",
      "unenforc: 0.9993014671495019\n",
      "wean: 0.9993014671495019\n",
      "clairvoy: 0.9993014671495019\n",
      "ketumil: 0.9993014671495019\n",
      "marcoussi: 0.9993014671495019\n",
      "progeni: 0.9993014671495019\n",
      "gunpowd: 0.9993014671495019\n",
      "comport: 0.9993014671495019\n",
      "drunk: 0.9993014671495019\n",
      "margai: 0.9993014671495019\n",
      "firework: 0.9993014671495019\n",
      "kekkonen: 0.9993014671495019\n",
      "pam: 0.9993014671495019\n",
      "catherin: 0.9993014671495019\n",
      "akashi: 0.9993014671495019\n",
      "robl: 0.9993014671495019\n",
      "orlean: 0.9993014671495019\n",
      "paso: 0.9993014671495019\n",
      "skopj: 0.9993014671495019\n",
      "cocktail: 0.9993014671495019\n"
     ]
    }
   ],
   "source": [
    "# == Calculate weighted frequencies for all words\n",
    "\n",
    " # STEMMED OR NOT?\n",
    "# - downweights very common words by giving more importance to rare ones\n",
    "word_counts_stemmed = joblib.load(os.path.join(data_freq, 'word_counts_stemmed.pkl'))\n",
    "\n",
    "l = sum(word_counts_stemmed.values())\n",
    "\n",
    "a = 0.001\n",
    "word_counts_weighted = {k: a / (a + (v / l)) for k, v in word_counts.items()}\n",
    "#for key in word_counts.keys():\n",
    " #   word_counts[key] = a / (a + (word_counts[key] / l))\n",
    "\n",
    "joblib.dump(word_counts_weighted, os.path.join(data_freq, 'word_counts_weighted.pkl'))\n",
    "\n",
    "################################################################################ ISSUE##################\n",
    "# To print top 100 by weighted values, sort the dictionary by value descending:\n",
    "top_100_weighted = sorted(word_counts_weighted.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "\n",
    "print(\"Top 100 words by weighted frequency:\")\n",
    "for word, weight in top_100_weighted:\n",
    "    print(f\"{word}: {weight}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0856a2-44f1-4304-b221-09a9ab72ad17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001e6aaf-101b-4912-a198-3d79f6d6e828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5ba7c0-33c7-4420-9cb0-fe54a94ee649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf1db25-1369-45bc-b6c4-ddddb77a3c52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
