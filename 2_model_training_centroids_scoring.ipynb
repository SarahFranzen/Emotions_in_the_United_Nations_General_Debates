{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ca6f721-48ad-495c-8e5f-e60790225709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "from pathlib import Path\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "# from gensim.summarization.textcleaner import get_sentences\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from random import shuffle\n",
    "import nltk\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.tokenize import sent_tokenize\n",
    "tagger = nltk.perceptron.PerceptronTagger()\n",
    "import joblib\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from string import punctuation\n",
    "from scipy.spatial.distance import cosine\n",
    "import glob\n",
    "from multiprocessing import Pool, freeze_support\n",
    "\n",
    "\n",
    "# === Initialize NLP Tools ===\n",
    "\n",
    "translator = str.maketrans('', '', punctuation) \n",
    "tagger = nltk.perceptron.PerceptronTagger()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "# === Set Working Directory ===\n",
    "\n",
    "# Set your working directory (adjust this as needed)\n",
    "wd = Path(r\"C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\")\n",
    "wd_models = wd / \"models\"\n",
    "wd_results = wd / \"results\"\n",
    "\n",
    "\n",
    "# === Define Folder Paths ===\n",
    "\n",
    "# Make sure that you have these folders in your working directory\n",
    "data_c = wd / \"data\"\n",
    "data_temp = data_c / \"temp\"\n",
    "data_freq = data_c / \"freq\"\n",
    "data_dict = data_c / \"dictionaries\"\n",
    "data_preprocessed = data_c / \"preprocessed\"\n",
    "fig = wd / \"Code\" / \"0_data_preparation_descriptives\" / \"fig\"\n",
    "\n",
    "# Upload ressources\n",
    "stopwords = joblib.load(data_c / \"stopwords.pkl\")\n",
    "word_counts_stemmed = joblib.load(data_freq / \"word_counts_stemmed.pkl\")\n",
    "word_counts_weighted = joblib.load(data_freq / \"word_counts_weighted.pkl\")\n",
    "affect_dic = joblib.load(data_dict / 'dictionary_affect.pkl')\n",
    "cognition_dic = joblib.load(data_dict / 'dictionary_cognition.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "125227ca-a0b5-43d7-956d-7d49a66f81b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_temp)\n",
    "cleaned_files = [\n",
    "    str(data_temp / 'clean_speeches_indexed1.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed2.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed3.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed4.pkl')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af0997e8-9da3-45c3-bdd4-7d38a708a474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl processed\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed1.pkl saved\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl processed\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed2.pkl saved\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl processed\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed3.pkl saved\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl processed\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed4.pkl saved\n"
     ]
    }
   ],
   "source": [
    "def extract_sentences(dataname):\n",
    "    data = joblib.load(dataname)\n",
    "    data = [a[1] for a in data]  # keep only text, no id\n",
    "\n",
    "    sentences = []\n",
    "    for doc in data:\n",
    "        sentences += sent_tokenize(doc)  # use nltk's sent_tokenize here\n",
    "\n",
    "    sentences = [item for item in sentences if len(item.split()) > 1]\n",
    "    sentences = [gensim.utils.simple_preprocess(item) for item in sentences]\n",
    "\n",
    "    sentences = [[a for a in s if not a.isdigit()] for s in sentences]\n",
    "    sentences = [[a for a in s if len(a) > 2] for s in sentences]\n",
    "\n",
    "    sentences = [tagger.tag(s) for s in sentences]\n",
    "    sentences = [[i[0] for i in s if i[1].startswith(('N', 'V', 'J'))] for s in sentences]\n",
    "\n",
    "    sentences = [[stemmer.stem(i) for i in s] for s in sentences]\n",
    "    sentences = [[a for a in s if a not in stopwords] for s in sentences]\n",
    "    sentences = [[a for a in s if word_counts_stemmed[a] >= 10] for s in sentences]\n",
    "\n",
    "    sentences = [s for s in sentences if len(s) > 1]  # eliminate empty\n",
    "    shuffle(sentences)\n",
    "\n",
    "    lab = dataname.replace('clean_speeches_', 'sentences_').replace('_.pkl', '.pkl')\n",
    "    print(f'{dataname} processed')\n",
    "    joblib.dump(sentences, lab)\n",
    "    print(f'{lab} saved')\n",
    "\n",
    "# Run for all your files\n",
    "for fname in cleaned_files:\n",
    "    extract_sentences(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f1a2e83-e5f4-436b-84c9-0d9d51f3b761",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_files = [\n",
    "    'sentences_indexed1.pkl',\n",
    "    'sentences_indexed2.pkl',\n",
    "    'sentences_indexed3.pkl',\n",
    "    'sentences_indexed4.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "465bc73c-70b6-4dbf-a379-8c5235f83a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for dataname in sentences_files:  # <-- your list of sentence files\n",
    "    data = joblib.load(dataname)\n",
    "    dataset.extend(data)  # extend instead of append if you want all sentences in a single list\n",
    "\n",
    "# === Model training ===\n",
    "w2v = Word2Vec(\n",
    "    sentences=dataset,    # iterator that loops over tokenized sentences\n",
    "    vector_size=300,      # Word vector dimensionality (use `vector_size` in newer gensim)\n",
    "    window=8,             # Context window size\n",
    "    min_count=10,         # Minimum word count\n",
    "    workers=8,            # Number of threads\n",
    "    sample=1e-3,          # Downsample setting for frequent words\n",
    "    epochs=10             # Number of iterations over the corpus\n",
    ")\n",
    "\n",
    "# Optimize memory usage (optional)\n",
    "w2v.wv.fill_norms()  # only works in older gensim versions\n",
    "\n",
    "# Save model\n",
    "wd_models.mkdir(parents=True, exist_ok=True)  # create folder if it doesn't exist\n",
    "w2v.save(str(wd_models / 'w2v-vectors_8_300.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d94142c-1e7f-4b0b-bfec-98604fc84304",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec.load(str(wd_models / \"w2v-vectors_8_300.pkl\"))\n",
    "word_vectors = w2v.wv # Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b75bd3b-2957-42d1-a89c-89e5b3ea44f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['centroids/cog_centroid.pkl']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################################\n",
    "# Find the centroid             ###\n",
    "###################################\n",
    "\n",
    "def findcentroid(text, model):\n",
    "    vecs = [model.wv[w] * word_counts_weighted[w] for w in text if w in model.wv]\n",
    "    vecs = [v for v in vecs if len(v) > 0]\n",
    "    centroid = np.mean(vecs, axis=0)\n",
    "    centroid = centroid.reshape(1, -1)\n",
    "    return centroid\n",
    "\n",
    "\n",
    "affect_centroid = findcentroid(affect_dic, w2v)\n",
    "cog_centroid = findcentroid(cognition_dic, w2v)\n",
    "\n",
    "\n",
    "###################################\n",
    "# Save                          ###\n",
    "###################################\n",
    "\n",
    "os.chdir(data_c)\n",
    "joblib.dump(affect_centroid, 'centroids/affect_centroid.pkl')\n",
    "joblib.dump(cog_centroid, 'centroids/cog_centroid.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bcab4536-fca7-468d-ae93-6e8da17cbfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affect centroid vector:\n",
      " [[ 5.24660684e-02  1.33741319e-01 -8.37163553e-02  1.05239064e-01\n",
      "   5.16792433e-03 -2.74070710e-01  3.96473110e-02  2.18954951e-01\n",
      "  -1.44893322e-02  4.20026295e-02 -2.16143996e-01 -4.11862917e-02\n",
      "   7.74786696e-02  2.32501794e-02 -2.76877433e-01 -7.23662078e-02\n",
      "   8.59653279e-02  4.53436635e-02  4.13618274e-02 -9.77352187e-02\n",
      "  -1.24463655e-01 -1.70920249e-02 -1.19915092e-02 -1.06752619e-01\n",
      "   2.31602266e-01 -5.45996949e-02 -1.07075058e-01  9.65755805e-03\n",
      "   2.44047716e-02 -1.78962246e-01  1.51880831e-01 -4.10291441e-02\n",
      "  -1.21981084e-01  1.12318166e-01 -3.43517736e-02 -1.33787235e-02\n",
      "  -9.91963074e-02 -2.22331107e-01  1.52626354e-02 -8.01816881e-02\n",
      "   8.87374133e-02  1.89637672e-02  7.91744441e-02 -2.94685904e-02\n",
      "   1.27999365e-01  1.49500817e-01 -1.02076359e-01  5.58054410e-02\n",
      "  -5.84671162e-02  5.05480124e-03  6.21940494e-02  1.62476990e-02\n",
      "   5.66442125e-02  3.07438914e-02 -1.92840993e-02  1.90077394e-01\n",
      "  -4.98453565e-02 -6.03052368e-03  1.99061669e-02 -6.09210879e-02\n",
      "  -4.93408404e-02 -7.82785639e-02 -8.52646455e-02  7.26972595e-02\n",
      "   1.56443655e-01  3.12775150e-02  1.49572212e-02  8.43622535e-03\n",
      "  -1.70928687e-02  2.50717229e-03 -7.58111253e-02 -2.29032245e-02\n",
      "   1.49099916e-01 -1.57318246e-02 -5.30730467e-03  9.70881954e-02\n",
      "  -1.85933515e-01 -5.74945919e-02 -1.56483561e-01  7.01437294e-02\n",
      "   2.69697476e-02 -1.73489273e-01 -6.12388849e-02  2.23954096e-01\n",
      "   1.42881393e-01  8.63142908e-02 -1.00408651e-01 -2.40650075e-03\n",
      "   1.18613787e-01  3.60103846e-02  4.71639223e-02 -1.05582707e-01\n",
      "   1.98272038e-02  1.27415419e-01 -1.17221591e-03  1.09918840e-01\n",
      "   8.63058269e-02 -1.57024078e-02 -6.40412979e-03  1.17443100e-01\n",
      "   3.29520628e-02 -5.25118317e-03  9.36569944e-02 -8.16257149e-02\n",
      "   4.72375490e-02 -7.87842646e-02 -6.80840984e-02  1.20117463e-01\n",
      "  -6.41625226e-02  5.58609813e-02 -7.48419985e-02  2.37602349e-02\n",
      "  -9.66946185e-02  1.41148612e-01  1.89116210e-01  6.03209548e-02\n",
      "   1.37902088e-02 -5.69409467e-02  7.22394660e-02 -3.97475772e-02\n",
      "  -2.85728835e-02  7.11942390e-02 -2.02882197e-02 -4.63152267e-02\n",
      "  -3.11675891e-02  1.66154858e-02  4.30401154e-02 -1.69750601e-01\n",
      "   2.75685787e-02 -7.80396909e-02 -3.30751278e-02  4.30723876e-02\n",
      "   1.22387715e-01  1.50531046e-02 -7.04301447e-02 -3.03268414e-02\n",
      "   5.79138771e-02 -1.00792103e-01 -8.16281661e-02 -1.91417098e-01\n",
      "  -1.41485035e-02  9.32467449e-03  3.18244509e-02 -2.72846362e-03\n",
      "  -4.46219258e-02 -1.21025994e-01 -7.64683262e-02 -1.03000505e-02\n",
      "   1.16867095e-01  7.86234885e-02 -3.52789909e-02 -5.84217608e-02\n",
      "  -1.92575857e-01 -1.21203765e-01  2.38533765e-02 -6.06492488e-03\n",
      "   5.40263727e-02  3.61431949e-02 -5.46988994e-02  8.64224136e-02\n",
      "   8.53754431e-02  5.17181680e-02 -4.25122716e-02  1.01524457e-01\n",
      "   4.43221955e-03  5.13041653e-02 -6.54493049e-02 -1.23727284e-01\n",
      "   4.21535037e-02  6.68925419e-02  1.04306504e-01  7.30840862e-02\n",
      "  -8.53556767e-03  3.49023156e-02 -5.28143607e-02  5.68920635e-02\n",
      "  -7.77753368e-02 -2.40843259e-02 -2.40607411e-02 -9.80029851e-02\n",
      "  -6.93689883e-02  8.94186087e-03 -1.45798167e-02 -8.45219865e-02\n",
      "   1.06950611e-01  5.05732484e-02  2.65587624e-02  1.91058904e-01\n",
      "   1.02497332e-01  2.58252546e-02 -1.49039142e-02  6.25954270e-02\n",
      "  -1.04391448e-01  4.53983359e-02 -5.18669114e-02  7.89777115e-02\n",
      "   1.00034088e-01 -4.73993681e-02  7.79403895e-02 -1.01451798e-04\n",
      "  -4.33573686e-02  5.16955070e-02 -9.85722095e-02  1.33415267e-01\n",
      "  -4.50326614e-02  1.14363534e-02  6.87386142e-03  5.62991649e-02\n",
      "  -4.45401967e-02  2.83449626e-04  2.82226354e-02 -4.54303510e-02\n",
      "  -1.06997199e-01 -3.87956426e-02  4.40339232e-03 -1.54852003e-01\n",
      "  -1.20678447e-01 -2.67070860e-01 -3.37760895e-02 -1.02089249e-01\n",
      "   2.57180274e-01 -7.11166952e-03 -6.73172027e-02 -9.81583670e-02\n",
      "  -6.76084012e-02 -8.90071541e-02 -2.15335861e-02 -9.72495452e-02\n",
      "  -2.00679183e-01  1.97065901e-02  6.64022043e-02 -2.41529588e-02\n",
      "  -7.66150132e-02  1.59842148e-01 -8.50933511e-03 -3.32215503e-02\n",
      "  -6.43050745e-02 -6.89091440e-03 -1.21555425e-01 -2.42295966e-01\n",
      "   1.15506798e-01 -1.13165684e-01 -8.33416730e-02  3.62406597e-02\n",
      "   7.15542436e-02 -1.79031432e-01  1.70500726e-01  1.85578018e-02\n",
      "  -2.37454418e-02  1.16999984e-01  3.47319171e-02  4.33601849e-02\n",
      "  -3.52968201e-02  3.91983707e-03 -8.87433738e-02 -2.02447340e-01\n",
      "   1.38049498e-01  1.17870025e-01 -1.27082378e-01 -1.72439978e-01\n",
      "   6.67379275e-02  2.62498893e-02 -1.16287380e-01 -1.64270863e-01\n",
      "  -8.76115188e-02 -9.93531346e-02  5.05787320e-02  5.46579715e-03\n",
      "   7.14922547e-02 -2.92643104e-02 -1.26078367e-01  1.52949365e-02\n",
      "  -5.41680865e-02 -1.19867742e-01  2.68643230e-01  2.14125756e-02\n",
      "   1.24869253e-02 -1.33705391e-02 -1.01256326e-01 -6.73368648e-02\n",
      "   1.60032183e-01  8.78561065e-02 -6.27142424e-03  1.04039505e-01\n",
      "  -1.85305458e-02 -5.79676479e-02 -1.22196317e-01  7.84773603e-02\n",
      "  -3.74878314e-03  1.33187041e-01  1.89137831e-02  1.27589032e-01\n",
      "   1.22164860e-01  6.84829354e-02  1.62429422e-01  8.78862590e-02\n",
      "  -9.55244675e-02 -7.21910223e-02  4.64114025e-02 -1.12392522e-01]]\n",
      "\n",
      "Cognition centroid vector:\n",
      " [[ 2.04290301e-02  6.06963150e-02  6.46494180e-02 -1.78950131e-02\n",
      "   3.45924273e-02 -1.87524706e-02  2.40216702e-02  3.89238335e-02\n",
      "   9.16544290e-04  1.00556061e-01 -2.42012460e-02 -6.31225761e-03\n",
      "   8.64164066e-03  6.98904097e-02 -1.02633402e-01 -5.62938601e-02\n",
      "   5.76517470e-02  1.35743627e-02  9.49366577e-03 -2.99016908e-02\n",
      "  -6.02416657e-02  2.12638639e-02  1.89515799e-02 -3.86136323e-02\n",
      "   4.76493426e-02 -9.48678553e-02  9.84032378e-02  2.74043232e-02\n",
      "  -3.81191000e-02 -1.11656308e-01 -3.81163731e-02  1.03399176e-02\n",
      "  -7.82255754e-02  4.99707386e-02 -1.25739217e-01  5.86590916e-02\n",
      "  -1.14123868e-02 -1.76653098e-02 -1.40716936e-02  4.42812368e-02\n",
      "   6.29331693e-02  7.40236323e-03  8.09336361e-03  7.08181709e-02\n",
      "   3.49295475e-02  4.82174866e-02 -5.43634454e-03  2.01544613e-02\n",
      "   2.00977898e-03  4.44379970e-02  6.98522711e-03  9.10002738e-02\n",
      "  -1.89166106e-02 -1.50339240e-02  6.10418878e-02  3.22393365e-02\n",
      "  -4.02575433e-02 -9.65851545e-02  5.90698980e-02 -1.39599144e-01\n",
      "  -4.42435890e-02  8.47064983e-03 -3.34776044e-02 -2.08442211e-02\n",
      "   8.95435587e-02 -1.50669336e-01  1.61039326e-02  9.75588411e-02\n",
      "  -3.02880798e-02 -1.04472160e-01  1.49072418e-02  5.52059151e-02\n",
      "   4.42746244e-02 -8.67312104e-02 -1.09864753e-02  8.78723264e-02\n",
      "  -4.82411161e-02 -6.24638833e-02 -1.97760388e-02  1.07177868e-01\n",
      "   2.01697201e-02 -8.95186141e-02  3.04000210e-02  2.50027161e-02\n",
      "   3.90787870e-02  3.48126926e-02 -1.00091010e-01  1.11912228e-01\n",
      "   7.12549910e-02  4.15959489e-03  6.55395836e-02 -8.17750543e-02\n",
      "  -7.60441786e-03 -2.76381001e-02  4.10398878e-02  1.07952282e-01\n",
      "   5.58146238e-02 -3.77769669e-04 -5.93701228e-02  7.54943714e-02\n",
      "  -2.73150746e-02 -3.64986286e-02  7.63549358e-02 -8.74101520e-02\n",
      "   6.63188770e-02 -9.03043672e-02 -4.58039232e-02  1.08971344e-02\n",
      "   7.66002759e-03  8.51804018e-02 -8.97173360e-02 -5.05232997e-02\n",
      "  -7.19578415e-02  7.22849891e-02  6.44121468e-02  4.29119589e-03\n",
      "  -5.33498488e-02 -1.95239354e-02  7.12788552e-02 -5.27302846e-02\n",
      "   1.49310101e-04 -1.64488852e-02  9.09438357e-02  9.99539346e-02\n",
      "  -1.98234003e-02  8.13973472e-02  7.15387538e-02 -6.77359328e-02\n",
      "   2.97614839e-02 -8.96045566e-02  2.74539459e-02  8.82500410e-02\n",
      "  -1.55846775e-02  5.90432249e-02 -2.91534290e-02  5.21314740e-02\n",
      "   6.39148429e-02 -5.15190437e-02 -4.25304845e-02 -9.64592546e-02\n",
      "   7.08794817e-02  1.63840298e-02 -5.47617152e-02  5.30579537e-02\n",
      "  -2.94266306e-02 -4.09597531e-02 -1.53317407e-01 -2.56003719e-02\n",
      "   1.46766752e-01  4.16368172e-02 -2.84242686e-02 -4.88601103e-02\n",
      "  -9.72722992e-02 -1.37884368e-03  2.07442772e-02  4.30455022e-02\n",
      "   3.10014258e-03 -2.11002193e-02 -5.49794883e-02  8.01798552e-02\n",
      "   8.54526088e-02  2.76021436e-02 -6.75067380e-02  5.30950055e-02\n",
      "  -1.56633735e-01 -1.63641572e-02 -7.00469464e-02 -3.14601436e-02\n",
      "   1.42836443e-03  5.54005727e-02  1.05126530e-01  6.51978329e-03\n",
      "  -2.09139511e-02  7.44439065e-02 -1.88727351e-03 -3.14676911e-02\n",
      "  -1.64849535e-01 -1.64996460e-02 -4.49477788e-03 -6.74360022e-02\n",
      "  -9.47351903e-02 -1.59923977e-04 -3.23319202e-03 -4.94633652e-02\n",
      "  -2.52278969e-02 -9.60285496e-03  2.93891621e-03  6.79351911e-02\n",
      "   6.18728064e-02 -9.07896161e-02  1.81001835e-02  4.12857458e-02\n",
      "  -5.45036569e-02  3.44111584e-02  9.83550306e-03 -2.20419951e-02\n",
      "   7.49371424e-02 -3.87536660e-02 -5.52388094e-02 -1.21530682e-01\n",
      "  -4.94339503e-03  3.66801620e-02 -4.43777144e-02  3.27366926e-02\n",
      "  -8.78816396e-02  1.28627578e-02 -1.80716544e-01 -8.43566749e-03\n",
      "  -1.38693094e-01 -2.42005335e-04  2.89508943e-02 -2.72477996e-02\n",
      "  -4.96413670e-02 -1.81363337e-02 -3.35483924e-02 -1.64366990e-01\n",
      "  -3.76432166e-02 -4.26715203e-02 -6.26132861e-02  1.84932742e-02\n",
      "  -1.22156637e-02 -4.99827564e-02 -6.71879128e-02 -4.80315797e-02\n",
      "   4.64174971e-02 -3.47177424e-02 -3.81513722e-02 -3.77234742e-02\n",
      "  -1.24959424e-01  3.80566642e-02 -5.05282022e-02  3.57671864e-02\n",
      "  -6.37177378e-02  5.00349589e-02  9.30776596e-02  3.43137905e-02\n",
      "  -9.02028158e-02  2.81201326e-04 -4.51720692e-02 -8.50883350e-02\n",
      "   4.27893065e-02 -9.02435482e-02 -2.43770070e-02 -8.89611468e-02\n",
      "   2.52306797e-02 -5.42018339e-02  4.40172404e-02 -1.96963008e-02\n",
      "   1.63008962e-02  6.88964576e-02  4.05598292e-03  9.02453251e-03\n",
      "  -7.76122361e-02 -2.21910942e-02 -1.05692908e-01 -2.82796212e-02\n",
      "   5.72470389e-02  7.45823011e-02 -6.50958791e-02 -4.91106100e-02\n",
      "   6.26294389e-02 -5.07175475e-02 -7.93257821e-03 -5.00703156e-02\n",
      "  -6.49564490e-02 -8.48433748e-02 -2.18070019e-02  1.66872807e-03\n",
      "   2.39361543e-02  1.29891303e-03 -1.00052878e-01 -1.36871950e-03\n",
      "   6.69569476e-03  1.03603015e-02  1.04135029e-01  9.10385698e-02\n",
      "   3.19096558e-02 -2.35576276e-02 -1.35131064e-03  5.70858307e-02\n",
      "   6.12748824e-02  5.19571006e-02  3.11208945e-02 -4.30413745e-02\n",
      "   2.52719037e-03 -1.71675570e-02 -8.52433741e-02  9.13567469e-02\n",
      "   6.92987442e-02  1.56799391e-01 -4.95204031e-02  7.55189955e-02\n",
      "   4.46337946e-02  4.20525745e-02  5.73564582e-02  1.13008037e-01\n",
      "  -1.88240577e-02 -9.90363955e-02  2.99227405e-02 -4.65502441e-02]]\n",
      "\n",
      "Shape of affect centroid: (1, 300)\n",
      "Shape of cognition centroid: (1, 300)\n",
      "\n",
      "Affect centroid summary: min, max, mean: -0.27687743 0.26864323 -0.0034895032\n",
      "Cognition centroid summary: min, max, mean: -0.18071654 0.15679939 -0.00367845\n"
     ]
    }
   ],
   "source": [
    "# Print the vectors\n",
    "print(\"Affect centroid vector:\\n\", affect_centroid)\n",
    "print(\"\\nCognition centroid vector:\\n\", cog_centroid)\n",
    "\n",
    "# Optional: shape and stats\n",
    "print(\"\\nShape of affect centroid:\", affect_centroid.shape)\n",
    "print(\"Shape of cognition centroid:\", cog_centroid.shape)\n",
    "\n",
    "print(\"\\nAffect centroid summary: min, max, mean:\", \n",
    "      np.min(affect_centroid), np.max(affect_centroid), np.mean(affect_centroid))\n",
    "print(\"Cognition centroid summary: min, max, mean:\", \n",
    "      np.min(cog_centroid), np.max(cog_centroid), np.mean(cog_centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c8316f-f794-4bcd-87c9-578aa82f88b0",
   "metadata": {},
   "source": [
    "## Emotionality Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3376a4f6-a002-4179-8614-dfef96113180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set wd to data_preprocessed\n",
    "os.chdir(data_preprocessed)\n",
    "\n",
    "# === Load preprocessed speech data ===\n",
    "\n",
    "preprocessed_files = [\n",
    "    joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed1.pkl')),\n",
    "    joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed2.pkl')),\n",
    "    joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed3.pkl')),\n",
    "    joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed4.pkl'))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbd1f95-c7f4-41f1-a292-542ee34ae882",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Define Functions              ###\n",
    "###################################\n",
    "\n",
    "def documentvecweight(lista):\n",
    "    out = []\n",
    "    lista = [i for i in lista if len(i[1]) > 0]\n",
    "    for s in lista:\n",
    "        vecs = [w2v.wv[w] * word_counts_weighted[w] for w in s[1] if w in w2v.wv]\n",
    "        if len(vecs) == 0:\n",
    "            a = np.nan\n",
    "            c = np.nan\n",
    "            score = np.nan\n",
    "        else:\n",
    "            v = np.mean(vecs, axis=0).reshape(1, -1)\n",
    "            a = cosine(v, affect_centroid)\n",
    "            c = cosine(v, cog_centroid)\n",
    "            score = (1 + 1 - a) / (1 + 1 - c)\n",
    "        out.append([s[0], a, c, score])\n",
    "    return out\n",
    "\n",
    "\n",
    "def main_function(file_path, idx):\n",
    "    dataset = joblib.load(file_path)\n",
    "    data = documentvecweight(dataset)\n",
    "    lab = os.path.join(wd_data, f'temp_distances_main_{idx}.pkl')\n",
    "    joblib.dump(data, lab)\n",
    "\n",
    "###################################\n",
    "#      Multiprocessing          ###\n",
    "###################################\n",
    "\n",
    "def main():\n",
    "    # Build list of file paths to your preprocessed chunks\n",
    "    files = [\n",
    "        os.path.join(data_preprocessed, f'preprocessed_speeches_indexed{i+1}.pkl')\n",
    "        for i in range(4)\n",
    "    ]\n",
    "\n",
    "    # Each worker receives (file_path, index)\n",
    "    with Pool(len(files)) as pool:\n",
    "        pool.starmap(main_function, [(f, i+1) for i, f in enumerate(files)])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    freeze_support()\n",
    "    main()\n",
    "\n",
    "###################################\n",
    "#      Recompose everything     ###\n",
    "###################################\n",
    "\n",
    "DATA_temp = [os.path.join(wd_data, f'temp_distances_main_{i+1}.pkl') for i in range(len(preprocessed_files))]\n",
    "\n",
    "tot = []\n",
    "for dataname in DATA_temp:\n",
    "    d = joblib.load(dataname)\n",
    "    tot += d\n",
    "\n",
    "tot_df = pd.DataFrame(tot, columns=['filename', 'affect_d', 'cognition_d', 'score'])\n",
    "joblib.dump(tot_df, os.path.join(wd_data, 'distances_10epochs.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7249a85f-e9b3-4b16-ae74-59e954b72ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your main corpus CSV\n",
    "un_corpus_merged = pd.read_csv(os.path.join(wd_data, \"un_corpus_merged.csv\"))\n",
    "\n",
    "# Merge on filename\n",
    "un_corpus_merged = un_corpus_merged.merge(tot_df, on=\"filename\", how=\"left\")\n",
    "\n",
    "# Save updated merged dataframe\n",
    "un_corpus_merged.to_csv(os.path.join(wd_data, \"un_corpus_merged_with_scores.csv\"), index=False)\n",
    "\n",
    "# Optionally also save as pickle for faster later use\n",
    "joblib.dump(un_corpus_merged, os.path.join(wd_data, \"un_corpus_merged_with_scores.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfd907f-f4da-4b49-b7dd-6cf96bf329b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
