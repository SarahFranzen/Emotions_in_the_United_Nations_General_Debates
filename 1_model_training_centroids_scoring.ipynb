{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a39bd0b5-2ac1-499d-8938-2fdfe4c3d93d",
   "metadata": {},
   "source": [
    "# Development of Emotion and Reasoning in the General Speeches of the United Nations: A text-based machine learning approach\n",
    "## Script 1: Model Traning, Calculation Centroids & Speech Scoring\n",
    "### Author: Sarah Franzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6ca6f721-48ad-495c-8e5f-e60790225709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Import libraries for data processing and NLP ==\n",
    "\n",
    "import os\n",
    "import gensim\n",
    "from pathlib import Path\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from random import shuffle\n",
    "import nltk\n",
    "from gensim.utils import simple_preprocess\n",
    "import joblib\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from string import punctuation\n",
    "from scipy.spatial.distance import cosine\n",
    "import glob\n",
    "import spacy\n",
    "from multiprocessing import Pool, freeze_support\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# === Initialize NLP Tools ===\n",
    "\n",
    "translator = str.maketrans('', '', punctuation) \n",
    "tagger = nltk.perceptron.PerceptronTagger()\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# === Set Working Directory ===\n",
    "\n",
    "# Prompt user to enter working directory path\n",
    "#wd = input(\"Please enter your working directory path (e.g., C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit): \").strip()\n",
    "\n",
    "# Change to the entered working directory\n",
    "#try:\n",
    "   # os.chdir(wd)\n",
    "    #print(f\"Working directory set to: {os.getcwd()}\")\n",
    "#except FileNotFoundError:\n",
    "   # print(\"ERROR: The directory you entered does not exist. Please restart and enter a valid path.\")\n",
    "    #exit(1)\n",
    "\n",
    "# Set your working directory (adjust this as needed)\n",
    "#wd = r\"C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\"\n",
    "#os.chdir(wd)\n",
    "\n",
    "# Set your working directory (adjust this as needed)\n",
    "wd = Path(r\"C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\")\n",
    "\n",
    "# === Create new Folders ===\n",
    "additional_folders = [\"results\", \"models\"]\n",
    "\n",
    "# Create/check folders directly in wd\n",
    "for folder in additional_folders:\n",
    "    folder_path = Path(wd) / folder\n",
    "    folder_path.mkdir(exist_ok=True)\n",
    "    print(f\"Folder checked/created: {folder_path}\")\n",
    "\n",
    "# === Define Folder Paths ===\n",
    "\n",
    "# Folders were already created in the script 0_data_creation\n",
    "data_c = wd / \"data\"\n",
    "data_temp = data_c / \"temp\"\n",
    "data_freq = data_c / \"freq\"\n",
    "data_dict = data_c / \"dictionaries\"\n",
    "data_sent = data_c / \"sentences\"\n",
    "data_preprocessed = data_c / \"preprocessed\"\n",
    "fig_dir = wd /\"fig\"\n",
    "\n",
    "wd_results = wd / \"results\" # THESE FOLDERS NEED TO BE CREATED\n",
    "wd_models = wd / \"models\" # THESE FOLDERS NEED TO BE CREATED\n",
    "\n",
    "# Upload ressources\n",
    "stopwords = joblib.load(data_c / \"stopwords.pkl\")              #### this is from the replication package! Issue for replication\n",
    "word_counts = joblib.load(data_freq / \"word_counts.pkl\")\n",
    "word_counts_weighted = joblib.load(data_freq / \"word_counts_weighted.pkl\")\n",
    "affect_dic = joblib.load(data_dict / 'dictionary_affect.pkl')  ### same issue here!\n",
    "cognition_dic = joblib.load(data_dict / 'dictionary_cognition.pkl') ### same issue here! must also be fixed in 0_data_creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "125227ca-a0b5-43d7-956d-7d49a66f81b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_temp)\n",
    "cleaned_files = [\n",
    "    str(data_temp / 'clean_speeches_indexed1.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed2.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed3.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed4.pkl')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855a7f37-bbf9-4259-8a07-7e2a589f1de4",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2645f36-e935-4b0e-b5e6-7461f398d887",
   "metadata": {},
   "source": [
    "### Sentence Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "af0997e8-9da3-45c3-bdd4-7d38a708a474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl processed\n",
      "Unique tokens: 10778\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed1.pkl saved\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl processed\n",
      "Unique tokens: 10793\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed2.pkl saved\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl processed\n",
      "Unique tokens: 10797\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed3.pkl saved\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl processed\n",
      "Unique tokens: 10803\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed4.pkl saved\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_sent)\n",
    "\n",
    "# Function to split cleaned speeches (clean_speeches) into sentences, tokenize, clean, tag, stem, filter, and save them.\n",
    "\n",
    "def extract_sentences(dataname):\n",
    "    data = joblib.load(dataname)\n",
    "    data = [a[1] for a in data]  # keep only text, no id\n",
    "\n",
    "    sentences = []\n",
    "    for doc in data:\n",
    "        sentences += sent_tokenize(doc)  # use nltk's sent_tokenize here\n",
    "\n",
    "    sentences = [item for item in sentences if len(item.split()) > 1]\n",
    "    sentences = [gensim.utils.simple_preprocess(item) for item in sentences]\n",
    "\n",
    "    sentences = [[a for a in s if not a.isdigit()] for s in sentences]\n",
    "    sentences = [[a for a in s if len(a) > 2] for s in sentences]\n",
    "\n",
    "    sentences = [tagger.tag(s) for s in sentences]\n",
    "    sentences = [[i[0] for i in s if i[1].startswith(('N', 'V', 'J'))] for s in sentences]\n",
    "\n",
    "    sentences = [[stemmer.stem(i) for i in s] for s in sentences]\n",
    "    sentences = [[a for a in s if a not in stopwords] for s in sentences]\n",
    "    sentences = [[a for a in s if word_counts[a] >= 10] for s in sentences]\n",
    "\n",
    "    sentences = [s for s in sentences if len(s) > 1]\n",
    "    shuffle(sentences)\n",
    "\n",
    "    lab = dataname.replace('clean_speeches_', 'sentences_').replace('_.pkl', '.pkl')\n",
    "    print(f'{dataname} processed')\n",
    "    joblib.dump(sentences, lab)\n",
    "    \n",
    "    unique_tokens = set(token for s in sentences for token in s)\n",
    "    print(f\"Unique tokens: {len(unique_tokens)}\")\n",
    "\n",
    "    print(f'{lab} saved')\n",
    "\n",
    "for fname in cleaned_files:\n",
    "    extract_sentences(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fddaea83-7d1f-4b14-85b9-401b2e727c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_files = [\n",
    "    os.path.join(data_temp, 'sentences_indexed1.pkl'),\n",
    "    os.path.join(data_temp, 'sentences_indexed2.pkl'),\n",
    "    os.path.join(data_temp, 'sentences_indexed3.pkl'),\n",
    "    os.path.join(data_temp, 'sentences_indexed4.pkl')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c41580ba-2954-4d84-a574-e9c3d4c8ef1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens across all files: 10932\n"
     ]
    }
   ],
   "source": [
    "# == Get sum of unique tokens ==\n",
    "all_unique_tokens = set()\n",
    "\n",
    "for dataname in sentences_files:\n",
    "    data = joblib.load(dataname)  # load list of tokenized sentences\n",
    "    for sentence in data:\n",
    "        all_unique_tokens.update(sentence)  # add tokens to the set\n",
    "\n",
    "print(f\"Total unique tokens across all files: {len(all_unique_tokens)}\")\n",
    "\n",
    "sentences_files = [\n",
    "    'sentences_indexed1.pkl',\n",
    "    'sentences_indexed2.pkl',\n",
    "    'sentences_indexed3.pkl',\n",
    "    'sentences_indexed4.pkl']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28938326-25d1-43cc-b0a8-4400dc138584",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d8d0e2-3d81-41ea-90c4-6d36bc9fa11c",
   "metadata": {},
   "source": [
    "### Train Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "465bc73c-70b6-4dbf-a379-8c5235f83a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for dataname in sentences_files:  \n",
    "    data = joblib.load(dataname)\n",
    "    dataset.extend(data) \n",
    "\n",
    "# === Model training ===\n",
    "w2v = Word2Vec(\n",
    "    sentences=dataset,    \n",
    "    vector_size=300,      # Dimension of the vector\n",
    "    window=8,             # Context window size\n",
    "    min_count=10,         # Minimum word count\n",
    "    workers=8,            # Number of threads\n",
    "    sample=1e-3,          # Downsample setting for frequent words\n",
    "    epochs=10             # Number of iterations over the corpus\n",
    ")\n",
    "\n",
    "# Optimize memory usage (optional)\n",
    "w2v.wv.fill_norms() \n",
    "\n",
    "# Save model\n",
    "wd_models.mkdir(parents=True, exist_ok=True) \n",
    "w2v.save(str(wd_models / 'w2v-vectors_8_300.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9d94142c-1e7f-4b0b-bfec-98604fc84304",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec.load(str(wd_models / \"w2v-vectors_8_300.pkl\"))\n",
    "word_vectors = w2v.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aa347e-aad5-477c-a52a-40a84a7684c6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420620dc-4af4-4a55-a7b6-1827949d1b17",
   "metadata": {},
   "source": [
    "### Calculate Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5b75bd3b-2957-42d1-a89c-89e5b3ea44f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['centroids/cog_centroid.pkl']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# == Calculation ==\n",
    "def findcentroid(text, model):\n",
    "    vecs = [model.wv[w] * word_counts_weighted[w] for w in text if w in model.wv]\n",
    "    vecs = [v for v in vecs if len(v) > 0]\n",
    "    centroid = np.mean(vecs, axis=0)\n",
    "    #centroid = centroid.reshape(1, -1)\n",
    "    return centroid\n",
    "\n",
    "\n",
    "affect_centroid = findcentroid(affect_dic, w2v)\n",
    "cog_centroid = findcentroid(cognition_dic, w2v)\n",
    "\n",
    "os.chdir(data_c)\n",
    "joblib.dump(affect_centroid, 'centroids/affect_centroid.pkl')\n",
    "joblib.dump(cog_centroid, 'centroids/cog_centroid.pkl')\n",
    "\n",
    "# == Overview Vectors ==\n",
    "print(\"Affect centroid vector:\\n\", affect_centroid)\n",
    "print(\"\\nCognition centroid vector:\\n\", cog_centroid)\n",
    "\n",
    "# Shape and stats\n",
    "print(\"\\nShape of affect centroid:\", affect_centroid.shape)\n",
    "print(\"Shape of cognition centroid:\", cog_centroid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af347b1-4f5f-401a-8ca0-1c33eb7776b1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c8316f-f794-4bcd-87c9-578aa82f88b0",
   "metadata": {},
   "source": [
    "### Emotionality Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3376a4f6-a002-4179-8614-dfef96113180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set wd to data_preprocessed\n",
    "os.chdir(data_preprocessed)\n",
    "\n",
    "# Load preprocessed speech data # NOR SURE IF USING THIS FILE IS CORRECT\n",
    "\n",
    "preprocessed_final_files = [\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed1_final.pkl')),\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed2_final.pkl')),\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed3_final.pkl')),\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed4_final.pkl'))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1cbd1f95-c7f4-41f1-a292-542ee34ae882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit\\\\data\\\\distances_10epochs.pkl']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to compute weighted document vectors and derive affective/cognitive distances and scores\n",
    "\n",
    "def documentvecweight(lista):\n",
    "    out = []\n",
    "    lista = [i for i in lista if len(i[1]) > 0]\n",
    "    for s in lista:\n",
    "        # Compute weighted word vectors for each token present in the Word2Vec model\n",
    "        vecs = [w2v.wv[w] * word_counts_weighted[w] for w in s[1] if w in w2v.wv]\n",
    "        if len(vecs) == 0:\n",
    "            a = np.nan\n",
    "            c = np.nan\n",
    "            score = np.nan\n",
    "        else:\n",
    "            # Compute mean vector for each speech\n",
    "            v = np.mean(vecs, axis=0)\n",
    "             # Cosine distance to affective centroid\n",
    "            a = cosine(v, affect_centroid)\n",
    "            # Cosine distance to cognitive centroid\n",
    "            c = cosine(v, cog_centroid)\n",
    "            score = (1 + 1 - a) / (1 + 1 - c)\n",
    "        out.append([s[0], a, c, score])\n",
    "    return out\n",
    "\n",
    "\n",
    "def main_function(file_path, idx):\n",
    "    dataset = joblib.load(file_path)\n",
    "    data = documentvecweight(dataset)\n",
    "    lab = os.path.join(data_c, f'distances_main_{idx}.pkl')\n",
    "    joblib.dump(data, lab)\n",
    "\n",
    "# Main loop: process all preprocessed speech files\n",
    "def main():\n",
    "    files = [\n",
    "        os.path.join(data_preprocessed, f'preprocessed_speeches_indexed{i+1}_final.pkl') #Changed!\n",
    "        for i in range(4)\n",
    "    ]\n",
    "    for i, f in enumerate(files, start=1):\n",
    "        main_function(f, i)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# Merge all distance files into a single DataFrame\n",
    "DATA_temp = [os.path.join(wd_results, f'distances_main_{i+1}.pkl') for i in range(4)]\n",
    "\n",
    "tot = []\n",
    "for dataname in DATA_temp:\n",
    "    d = joblib.load(dataname)\n",
    "    tot += d\n",
    "\n",
    "tot_df = pd.DataFrame(tot, columns=['filename', 'affect_d', 'cognition_d', 'score'])\n",
    "joblib.dump(tot_df, os.path.join(wd_results, 'distances_10epochs.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fb955ab4-243d-470d-a967-e0af1e474c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          filename  affect_d  cognition_d     score\n",
      "0  COL_03_1948.txt  1.159868     0.600979  0.600514\n",
      "1  HTI_47_1992.txt  0.474285     0.886777  1.370539\n",
      "2  LKA_46_1991.txt  1.416345     1.189522  0.720137\n",
      "3  HND_05_1950.txt  0.747536     0.270929  0.724357\n",
      "4  GHA_25_1970.txt  1.368114     0.735999  0.499909\n",
      "Shape: (10760, 4)\n",
      "           affect_d   cognition_d         score\n",
      "count  10760.000000  10760.000000  10760.000000\n",
      "mean       1.196353      0.909197      0.738105\n",
      "std        0.259407      0.201238      0.199796\n",
      "min        0.293323      0.262322      0.310588\n",
      "25%        1.032945      0.769638      0.587802\n",
      "50%        1.235943      0.924176      0.712655\n",
      "75%        1.392101      1.065732      0.868452\n",
      "max        1.741876      1.455069      1.613793\n",
      "              filename  affect_d  cognition_d     score\n",
      "0      COL_03_1948.txt  1.159868     0.600979  0.600514\n",
      "1      HTI_47_1992.txt  0.474285     0.886777  1.370539\n",
      "2      LKA_46_1991.txt  1.416345     1.189522  0.720137\n",
      "3      HND_05_1950.txt  0.747536     0.270929  0.724357\n",
      "4      GHA_25_1970.txt  1.368114     0.735999  0.499909\n",
      "...                ...       ...          ...       ...\n",
      "10755  SLV_66_2011.txt  0.668358     0.701658  1.025648\n",
      "10756  AUT_23_1968.txt  1.509631     0.904342  0.447556\n",
      "10757  CIV_24_1969.txt  0.446327     0.626288  1.131003\n",
      "10758  CYP_68_2013.txt  1.432286     0.850868  0.494037\n",
      "10759  MWI_34_1979.txt  1.512555     0.978339  0.477110\n",
      "\n",
      "[10760 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Print the first few rows\n",
    "print(tot_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8c295cd4-4fa0-470f-b67c-4f584f7a3f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load df_merged and merge with tot_df by filename \n",
    "un_corpus_merged = pd.read_csv(os.path.join(data_c, \"un_corpus_merged.csv\"), sep=';', encoding='utf-8') \n",
    "un_corpus_scored = un_corpus_merged.merge(tot_df, on=\"filename\", how=\"left\")\n",
    "\n",
    "joblib.dump(un_corpus_scored, os.path.join(data_c, \"un_corpus_scored.pkl\"))\n",
    "\n",
    "un_corpus_scored.to_csv(\n",
    "    os.path.join(wd_results, \"un_corpus_scored.csv\"),\n",
    "    sep=';', \n",
    "    index=False, \n",
    "    encoding='utf-8'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d8d13667-1eb3-482e-83ca-5969a229750a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              filename                                             speech  \\\n",
      "0      COL_03_1948.txt  Mr. URDANETA-ARBELAEZ declared that when the s...   \n",
      "1      HTI_47_1992.txt  In the name of the \\npeople of Haiti, I am hap...   \n",
      "2      LKA_46_1991.txt  ﻿I have the honour to convey to you, Sir, and ...   \n",
      "3      HND_05_1950.txt  It is indeed difficult to find words which do ...   \n",
      "4      GHA_25_1970.txt  121.\\t I should like to begin by congratulatin...   \n",
      "...                ...                                                ...   \n",
      "10755  SLV_66_2011.txt  \\nThis new session of the Assembly of the Unit...   \n",
      "10756  AUT_23_1968.txt  1. May I ask you, Sir, to convey to the Presid...   \n",
      "10757  CIV_24_1969.txt  83. Permit me to take this opportunity to pay ...   \n",
      "10758  CYP_68_2013.txt  It is a great honour to \\naddress the General ...   \n",
      "10759  MWI_34_1979.txt  ﻿Allow me, Mr. President, first to join the re...   \n",
      "\n",
      "      country_code  year   country_name  speech_length_words  \\\n",
      "0              COL  1948       Colombia                 1795   \n",
      "1              HTI  1992          Haiti                 3432   \n",
      "2              LKA  1991      Sri Lanka                 2434   \n",
      "3              HND  1950       Honduras                 1111   \n",
      "4              GHA  1970          Ghana                 4031   \n",
      "...            ...   ...            ...                  ...   \n",
      "10755          SLV  2011    El Salvador                 2190   \n",
      "10756          AUT  1968        Austria                 3594   \n",
      "10757          CIV  1969  Côte d'Ivoire                 2746   \n",
      "10758          CYP  2013         Cyprus                 2052   \n",
      "10759          MWI  1979         Malawi                 3190   \n",
      "\n",
      "       english_official_language  security_council_permanent  \\\n",
      "0                              0                           0   \n",
      "1                              0                           0   \n",
      "2                              0                           0   \n",
      "3                              0                           0   \n",
      "4                              1                           0   \n",
      "...                          ...                         ...   \n",
      "10755                          0                           0   \n",
      "10756                          0                           0   \n",
      "10757                          0                           0   \n",
      "10758                          0                           0   \n",
      "10759                          1                           0   \n",
      "\n",
      "                          speaker_name           position  gender_dummy  \\\n",
      "0               Mr. URDANETA-ARBELAEZ                 NaN           0.0   \n",
      "1                             ARISTIDE  (Vice-) President           NaN   \n",
      "2                              Kalpage                NaN           NaN   \n",
      "3                           Mr. CARIAS                NaN           0.0   \n",
      "4                            Mr. OWUSU                NaN           0.0   \n",
      "...                                ...                ...           ...   \n",
      "10755  Carlos Mauricio Funes Cartagena  (Vice-) President           NaN   \n",
      "10756                    Mr. WALDHEIM                 NaN           0.0   \n",
      "10757                        Mr. USHER                NaN           0.0   \n",
      "10758               Nicos Anastasiades  (Vice-) President           NaN   \n",
      "10759                          Matenje                NaN           NaN   \n",
      "\n",
      "               speech_label  affect_d  cognition_d     score  \n",
      "0           Colombia (1948)  1.159868     0.600979  0.600514  \n",
      "1              Haiti (1992)  0.474285     0.886777  1.370539  \n",
      "2          Sri Lanka (1991)  1.416345     1.189522  0.720137  \n",
      "3           Honduras (1950)  0.747536     0.270929  0.724357  \n",
      "4              Ghana (1970)  1.368114     0.735999  0.499909  \n",
      "...                     ...       ...          ...       ...  \n",
      "10755    El Salvador (2011)  0.668358     0.701658  1.025648  \n",
      "10756        Austria (1968)  1.509631     0.904342  0.447556  \n",
      "10757  Côte d'Ivoire (1969)  0.446327     0.626288  1.131003  \n",
      "10758         Cyprus (2013)  1.432286     0.850868  0.494037  \n",
      "10759         Malawi (1979)  1.512555     0.978339  0.477110  \n",
      "\n",
      "[10760 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "print(un_corpus_scored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1210b88f-cf13-4da0-807b-77764cf0d439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count where affect_d is NaN: 0\n",
      "Count where affect_d is not NaN: 10760\n"
     ]
    }
   ],
   "source": [
    "# Count where affect_d is NaN\n",
    "nan_count = un_corpus_scored['affect_d'].isna().sum()\n",
    "\n",
    "# Count where affect_d is not NaN\n",
    "not_nan_count = un_corpus_scored['affect_d'].notna().sum()\n",
    "\n",
    "print(\"Count where affect_d is NaN:\", nan_count)\n",
    "print(\"Count where affect_d is not NaN:\", not_nan_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
