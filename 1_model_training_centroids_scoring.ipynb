{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a39bd0b5-2ac1-499d-8938-2fdfe4c3d93d",
   "metadata": {},
   "source": [
    "# Development of Emotion and Reasoning in the General Speeches of the United Nations: A text-based machine learning approach\n",
    "## Script 1: Model Traning, Calculation Centroids & Speech Scoring\n",
    "### Author: Sarah Franzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6ca6f721-48ad-495c-8e5f-e60790225709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder checked/created: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\results\n",
      "Folder checked/created: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\models\n"
     ]
    }
   ],
   "source": [
    "# == Import libraries for data processing and NLP ==\n",
    "\n",
    "import os\n",
    "import gensim\n",
    "from pathlib import Path\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from random import shuffle\n",
    "import nltk\n",
    "from gensim.utils import simple_preprocess\n",
    "import joblib\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from string import punctuation\n",
    "from scipy.spatial.distance import cosine\n",
    "import glob\n",
    "import spacy\n",
    "from multiprocessing import Pool, freeze_support\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# === Initialize NLP Tools ===\n",
    "\n",
    "translator = str.maketrans('', '', punctuation) \n",
    "tagger = nltk.perceptron.PerceptronTagger()\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# === Set Working Directory ===\n",
    "\n",
    "# Prompt user to enter working directory path\n",
    "wd = input(\"Please enter your working directory path (e.g., C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit): \").strip()\n",
    "\n",
    "# Change to the entered working directory\n",
    "try:\n",
    "    os.chdir(wd)\n",
    "    print(f\"Working directory set to: {os.getcwd()}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: The directory you entered does not exist. Please restart and enter a valid path.\")\n",
    "    exit(1)\n",
    "\n",
    "# === Define Folder Paths ===\n",
    "\n",
    "# Folders were already created in the script 0_data_creation\n",
    "data_c = wd / \"data\"\n",
    "data_temp = data_c / \"temp\"\n",
    "data_freq = data_c / \"freq\"\n",
    "data_dict = data_c / \"dictionaries\"\n",
    "data_sent = data_c / \"sentences\"\n",
    "data_preprocessed = data_c / \"preprocessed\"\n",
    "data_results = data_c / \"results\"\n",
    "data_models = data_c / \"models\" \n",
    "\n",
    "# Upload ressources\n",
    "stopwords = joblib.load(data_c / \"stopwords.pkl\")         \n",
    "word_counts = joblib.load(data_freq / \"word_counts.pkl\")\n",
    "word_counts_weighted = joblib.load(data_freq / \"word_counts_weighted.pkl\")\n",
    "affect_dic = joblib.load(data_dict / 'dictionary_affect.pkl')\n",
    "cognition_dic = joblib.load(data_dict / 'dictionary_cognition.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "125227ca-a0b5-43d7-956d-7d49a66f81b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_temp)\n",
    "cleaned_files = [\n",
    "    str(data_temp / 'clean_speeches_indexed1.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed2.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed3.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed4.pkl')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855a7f37-bbf9-4259-8a07-7e2a589f1de4",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2645f36-e935-4b0e-b5e6-7461f398d887",
   "metadata": {},
   "source": [
    "### Sentence Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af0997e8-9da3-45c3-bdd4-7d38a708a474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of very short sentences being dropped: 4928\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl processed\n",
      "Unique tokens: 12289\n",
      "Example sentences (first 5):\n",
      "['thousand', 'class', 'peasant', 'middl', 'class', 'famili', 'held', 'yoke', 'poverti', 'result', 'labour', 'black', 'list']\n",
      "['technic', 'cooper', 'assist', 'activ', 'develop', 'crimin', 'justic', 'capac', 'develop', 'accord', 'high', 'prioriti']\n",
      "['conclud', 'share', 'tradit', 'bless', 'mauri', 'tabomoa']\n",
      "['overwhelm', 'concern', 'entir', 'world', 'prevent', 'nuclear', 'war', 'accompani', 'threat', 'total', 'annihil']\n",
      "['decis', 'taken', 'object', 'halt', 'action', 'govern', 'cuba', 'known', 'design', 'promot', 'financ', 'direct', 'subvers', 'movement', 'latin', 'american']\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed1.pkl saved\n",
      "Number of very short sentences being dropped: 4990\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl processed\n",
      "Unique tokens: 12314\n",
      "Example sentences (first 5):\n",
      "['frank', 'discuss', 'safer', 'hidden', 'bitter']\n",
      "['valu', 'special', 'suprem', 'signific']\n",
      "['region', 'conflict', 'lead', 'unjustifi', 'destruct', 'resourc', 'necessari', 'develop', 'involv']\n",
      "['right', 'palestinian', 'humanitarian', 'necess', 'polit', 'necess', 'prerequisit', 'peac']\n",
      "['want', 'mention', 'point', 'particular', 'today']\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed2.pkl saved\n",
      "Number of very short sentences being dropped: 5655\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl processed\n",
      "Unique tokens: 12303\n",
      "Example sentences (first 5):\n",
      "['address', 'migrat']\n",
      "['crime', 'commit', 'war', 'ukrain', 'live', 'proof', 'thereof']\n",
      "['great', 'forum', 'hallmark', 'freedom', 'express', 'problem', 'discuss']\n",
      "['children', 'vaccin']\n",
      "['view', 'said', 'urgent', 'need', 'continu', 'effort', 'forg', 'new', 'fairer', 'intern', 'econom', 'order', 'peac', 'dividend', 'deriv', 'eas', 'bipolar', 'tension', 'deploy', 'develop', 'need', 'world', 'disadvantag', 'region']\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed3.pkl saved\n",
      "Number of very short sentences being dropped: 5209\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl processed\n",
      "Unique tokens: 12318\n",
      "Example sentences (first 5):\n",
      "['better', 'account', 'link', 'intern', 'secur', 'econom', 'develop']\n",
      "['hand', 'enhanc', 'effect', 'streamlin', 'structur', 'revit', 'secur', 'council']\n",
      "['ratifi', 'tokyo', 'convent', 'support', 'new', 'convent', 'forthcom', 'hagu', 'confer']\n",
      "['permiss', 'withheld']\n",
      "['day', 'come', 'coloni', 'disappear', 'discuss']\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed4.pkl saved\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_sent)\n",
    "\n",
    "# Function to split cleaned speeches (clean_speeches) into sentences, tokenize, clean, tag, stem, filter, and save them.\n",
    "\n",
    "def extract_sentences(dataname):\n",
    "    data = joblib.load(dataname)\n",
    "    data = [a[1] for a in data]  # keep only text, no id\n",
    "\n",
    "    sentences = []\n",
    "    for doc in data:\n",
    "        sentences += sent_tokenize(doc)  # use nltk's sent_tokenize here\n",
    "\n",
    "    sentences = [item for item in sentences if len(item.split()) > 1]\n",
    "    sentences = [gensim.utils.simple_preprocess(item) for item in sentences]\n",
    "\n",
    "    sentences = [[a for a in s if not a.isdigit()] for s in sentences]\n",
    "    sentences = [[a for a in s if len(a) > 2] for s in sentences]\n",
    "\n",
    "    sentences = [tagger.tag(s) for s in sentences]\n",
    "    sentences = [[i[0] for i in s if i[1].startswith(('N', 'V', 'J'))] for s in sentences]\n",
    "\n",
    "    sentences = [[stemmer.stem(i) for i in s] for s in sentences]\n",
    "    sentences = [[a for a in s if a not in stopwords] for s in sentences]\n",
    "    sentences = [[a for a in s if word_counts[a] >= 10] for s in sentences]\n",
    "\n",
    "    dropped_count = sum(1 for s in sentences if len(s) <= 1)\n",
    "    print(f\"Number of very short sentences being dropped: {dropped_count}\")\n",
    "\n",
    "    sentences = [s for s in sentences if len(s) > 1]\n",
    "    shuffle(sentences)\n",
    "\n",
    "    lab = dataname.replace('clean_speeches_', 'sentences_').replace('_.pkl', '.pkl')\n",
    "    print(f'{dataname} processed')\n",
    "    joblib.dump(sentences, lab)\n",
    "    \n",
    "    unique_tokens = set(token for s in sentences for token in s)\n",
    "    print(f\"Unique tokens: {len(unique_tokens)}\")\n",
    "\n",
    "    # Print preview of first 5 processed sentences\n",
    "    print(\"Example sentences (first 5):\")\n",
    "    for s in sentences[:5]:\n",
    "        print(s)\n",
    "\n",
    "\n",
    "    print(f'{lab} saved')\n",
    "\n",
    "for fname in cleaned_files:\n",
    "    extract_sentences(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d9ae013e-bb3f-43d5-98e1-cd113108f0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentences (first 5):\n",
      "['thousand', 'class', 'peasant', 'middl', 'class', 'famili', 'held', 'yoke', 'poverti', 'result', 'labour', 'black', 'list']\n",
      "['technic', 'cooper', 'assist', 'activ', 'develop', 'crimin', 'justic', 'capac', 'develop', 'accord', 'high', 'prioriti']\n",
      "['conclud', 'share', 'tradit', 'bless', 'mauri', 'tabomoa']\n",
      "['overwhelm', 'concern', 'entir', 'world', 'prevent', 'nuclear', 'war', 'accompani', 'threat', 'total', 'annihil']\n",
      "['decis', 'taken', 'object', 'halt', 'action', 'govern', 'cuba', 'known', 'design', 'promot', 'financ', 'direct', 'subvers', 'movement', 'latin', 'american']\n"
     ]
    }
   ],
   "source": [
    "# Ppick the first file to see how the sentence split looks like\n",
    "file_path = os.path.join(data_temp, 'sentences_indexed1.pkl')\n",
    "\n",
    "sentences = joblib.load(file_path)\n",
    "\n",
    "print(\"Example sentences (first 5):\")\n",
    "for s in sentences[:5]:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fddaea83-7d1f-4b14-85b9-401b2e727c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_files = [\n",
    "    os.path.join(data_temp, 'sentences_indexed1.pkl'),\n",
    "    os.path.join(data_temp, 'sentences_indexed2.pkl'),\n",
    "    os.path.join(data_temp, 'sentences_indexed3.pkl'),\n",
    "    os.path.join(data_temp, 'sentences_indexed4.pkl')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c41580ba-2954-4d84-a574-e9c3d4c8ef1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens across all files: 12480\n"
     ]
    }
   ],
   "source": [
    "# == Get sum of unique tokens ==\n",
    "all_unique_tokens = set()\n",
    "\n",
    "for dataname in sentences_files:\n",
    "    data = joblib.load(dataname) \n",
    "    for sentence in data:\n",
    "        all_unique_tokens.update(sentence) \n",
    "\n",
    "print(f\"Total unique tokens across all files: {len(all_unique_tokens)}\")\n",
    "\n",
    "sentences_files = [\n",
    "    'sentences_indexed1.pkl',\n",
    "    'sentences_indexed2.pkl',\n",
    "    'sentences_indexed3.pkl',\n",
    "    'sentences_indexed4.pkl']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28938326-25d1-43cc-b0a8-4400dc138584",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d8d0e2-3d81-41ea-90c4-6d36bc9fa11c",
   "metadata": {},
   "source": [
    "### Train Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "465bc73c-70b6-4dbf-a379-8c5235f83a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for dataname in sentences_files:  \n",
    "    data = joblib.load(dataname)\n",
    "    dataset.extend(data) \n",
    "\n",
    "# === Model training ===\n",
    "w2v = Word2Vec(\n",
    "    sentences=dataset,    \n",
    "    vector_size=300,      # Dimension of the vector\n",
    "    window=8,             # Context window size\n",
    "    min_count=10,         # Minimum word count\n",
    "    workers=8,            # Number of threads\n",
    "    sample=1e-3,          # Downsample setting for frequent words\n",
    "    epochs=10,            # Number of iterations over the corpus\n",
    "    seeds=12\n",
    ")\n",
    "\n",
    "\n",
    "w2v.wv.fill_norms() \n",
    "\n",
    "# Save model\n",
    "wd_models.mkdir(parents=True, exist_ok=True) \n",
    "w2v.save(str(wd_models / 'w2v-vectors_8_300.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9d94142c-1e7f-4b0b-bfec-98604fc84304",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec.load(str(wd_models / \"w2v-vectors_8_300.pkl\"))\n",
    "word_vectors = w2v.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aa347e-aad5-477c-a52a-40a84a7684c6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420620dc-4af4-4a55-a7b6-1827949d1b17",
   "metadata": {},
   "source": [
    "### Calculate Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5b75bd3b-2957-42d1-a89c-89e5b3ea44f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affect centroid vector:\n",
      " [-0.0736924  -0.02615644  0.08641668  0.11732437  0.1528423   0.24897972\n",
      "  0.14337324  0.23880306  0.03186733 -0.30849063  0.15468785 -0.06546544\n",
      " -0.04732803 -0.04446076 -0.22914626  0.13254197  0.23568878 -0.2516948\n",
      "  0.27152127  0.1225189  -0.17335808  0.18330842  0.09306473  0.15133058\n",
      "  0.7069783  -0.1064485  -0.2377337   0.10650271  0.07058039 -0.02874511\n",
      " -0.1373579  -0.00215214 -0.2274221   0.20105839  0.19398606  0.2080281\n",
      " -0.1449763  -0.32416582  0.27860564 -0.10978469 -0.05237249  0.16225633\n",
      "  0.30137467  0.01800226  0.07078476 -0.05441128 -0.14705361 -0.11145094\n",
      " -0.01225633  0.27423495 -0.01179702  0.10230152 -0.01881051  0.07042564\n",
      "  0.06828374  0.20510022  0.33411083  0.14199427  0.2653098   0.01785835\n",
      " -0.08340826  0.18939826  0.0608794  -0.0969384   0.29247656  0.1895983\n",
      " -0.14109848  0.1671956   0.2214419   0.16436344 -0.14708567  0.05495733\n",
      "  0.06584255 -0.1595384   0.25399894 -0.15582773 -0.21654704 -0.00891316\n",
      " -0.07640563  0.0519471  -0.15984344 -0.01271576 -0.32011428  0.08339294\n",
      "  0.14735752  0.02005898 -0.14874484  0.208093    0.04877556 -0.05670404\n",
      " -0.14054027 -0.19197895 -0.06434821 -0.10112014  0.20500611  0.16662452\n",
      "  0.12654448 -0.11547649 -0.19872701  0.43805644  0.03644086 -0.16134882\n",
      " -0.1312577  -0.08593116  0.14085518 -0.11040089  0.01293639 -0.1275062\n",
      " -0.175959    0.02305109 -0.08047939 -0.28586516  0.14861697  0.23539585\n",
      " -0.22360724  0.39554292  0.07252868 -0.07743754 -0.022134   -0.263537\n",
      " -0.18179502  0.01394797  0.06103098  0.26050976 -0.04224603  0.03893179\n",
      "  0.14483185 -0.18661499  0.08144999  0.068212   -0.14247061 -0.4637823\n",
      "  0.07358968 -0.03193435 -0.04253104 -0.13396882  0.08158698  0.02973696\n",
      " -0.10643926 -0.25827652 -0.04433974 -0.0318344   0.06494446  0.38797057\n",
      "  0.09860552 -0.08179712 -0.33382    -0.1216852  -0.00339924  0.14103244\n",
      " -0.0133276  -0.19204195 -0.47384953  0.13059443 -0.08931024  0.08741381\n",
      " -0.01818811 -0.5004614   0.36112228 -0.13002822  0.0333863   0.23529488\n",
      " -0.10178875 -0.27903697 -0.36224917  0.24063629 -0.02569608 -0.04737279\n",
      "  0.16622044  0.08650041 -0.15418476  0.3898698   0.25085565  0.23476033\n",
      "  0.26433718 -0.28456628 -0.05921459 -0.09750993 -0.32377845 -0.26868114\n",
      " -0.15726411  0.16008322  0.02022971  0.06346749 -0.03379624 -0.17228957\n",
      " -0.055652    0.04454141  0.20783873 -0.33961082 -0.24416685  0.2157427\n",
      "  0.39571378  0.3342576  -0.12730461 -0.26830116  0.0449307  -0.16762422\n",
      " -0.01412968  0.01240163 -0.3277784   0.20968916 -0.02867691  0.18997681\n",
      "  0.09626898 -0.20797363 -0.06266199  0.01354601 -0.09147438 -0.24368973\n",
      " -0.23219013  0.10311365 -0.02132035  0.21098192 -0.01688356 -0.27413985\n",
      "  0.05990645  0.05430752 -0.45537475 -0.06756576  0.04178552  0.0546632\n",
      " -0.32213587  0.21790212 -0.00714276 -0.04751773  0.17730238  0.13731444\n",
      " -0.30815396  0.08782486 -0.12146024 -0.07604317 -0.18813226  0.0260689\n",
      " -0.17607914  0.22621502 -0.0912291  -0.09223882  0.0203841  -0.02355617\n",
      " -0.35247838 -0.00097013  0.13902794  0.14849551 -0.26702794  0.01665162\n",
      "  0.19580133 -0.10266028  0.05357096  0.26269332  0.45993754 -0.00130289\n",
      " -0.03940531 -0.02094955 -0.19559526  0.30510712  0.07680225  0.17196691\n",
      " -0.14148982 -0.25272977 -0.17447309  0.27184996 -0.04904357  0.09558898\n",
      " -0.13509867 -0.05863379 -0.26114422 -0.1607416  -0.07683264 -0.12594049\n",
      "  0.01769488  0.2869428   0.09631028  0.1386265  -0.10703796  0.5674834\n",
      " -0.06051096  0.29687947  0.18914911  0.0431157   0.11159064 -0.09802766\n",
      "  0.04487929 -0.09655403  0.11031584  0.24239942 -0.17866172  0.0124073\n",
      " -0.03150321  0.20749171 -0.21279274  0.21862072 -0.16776645  0.2128758\n",
      "  0.26998648  0.203921   -0.13385954 -0.30499098 -0.26542932  0.12891759]\n",
      "\n",
      "Cognition centroid vector:\n",
      " [ 3.08720358e-02  3.83230648e-03 -3.92176397e-03  1.11793242e-01\n",
      " -3.13015543e-02  4.69884351e-02  2.00697154e-01 -2.95301098e-02\n",
      "  1.14721231e-01 -2.09563106e-01  7.73110017e-02  9.11013968e-03\n",
      "  2.29626879e-01 -2.72588003e-02 -1.72142431e-01 -4.38307319e-03\n",
      " -1.60998404e-01 -1.37924001e-01  1.03317454e-01  2.81078935e-01\n",
      " -1.19013034e-01  7.80490227e-03  1.90364122e-01 -3.56994122e-02\n",
      "  2.29697302e-01 -1.14099585e-01 -7.87856728e-02 -2.35503450e-01\n",
      "  6.02168515e-02  2.24150009e-02  1.68067473e-03 -6.75207451e-02\n",
      " -2.43618488e-01 -5.07714599e-02  2.07289793e-02  3.03027272e-01\n",
      "  1.69035178e-02 -1.59944043e-01  6.41809478e-02  7.08624274e-02\n",
      " -2.13451788e-01  2.11086553e-02  1.37070358e-01  1.31302075e-02\n",
      " -9.00398195e-02 -1.28010616e-01 -2.34206412e-02 -1.37361795e-01\n",
      "  1.35334700e-01  3.57158035e-02  6.69466704e-02 -7.31804445e-02\n",
      "  6.31625354e-02 -1.56378478e-01 -6.63122237e-02  2.60336369e-01\n",
      "  2.62564391e-01 -1.44223303e-01 -1.16466498e-02  1.78619400e-01\n",
      " -3.53788674e-01  1.03084847e-01 -1.03187032e-01  4.23961096e-02\n",
      "  1.38874695e-01  2.31044218e-01 -1.12360856e-02 -2.26867571e-02\n",
      "  1.59734815e-01  1.15352340e-01 -4.22938429e-02  1.86363593e-01\n",
      "  1.25004515e-01  7.39660040e-02  4.26714830e-02 -9.29171890e-02\n",
      " -1.15350448e-01  3.63360867e-02  1.72389477e-01  1.48696154e-01\n",
      " -9.74307358e-02 -1.72802448e-01 -6.85737655e-02 -1.56864464e-01\n",
      " -3.27811367e-03  9.69638973e-02 -9.39567238e-02 -3.40687507e-03\n",
      "  1.40759662e-01  2.38110516e-02 -9.67912748e-02 -1.50317475e-01\n",
      "  3.15169878e-02  1.94459140e-01 -4.91117015e-02  3.10355593e-02\n",
      "  1.02554057e-02  1.09405629e-01 -1.14236519e-01  5.09394854e-02\n",
      " -2.15645015e-01 -1.89275354e-01  1.77970573e-01 -1.43066958e-01\n",
      "  3.06769274e-02  2.91279424e-02 -1.17182285e-01 -2.35178936e-02\n",
      "  2.22916260e-01  2.30255891e-02  1.02178063e-02 -1.34218797e-01\n",
      " -3.41943018e-02  1.44431785e-01 -2.48467207e-01  5.50289415e-02\n",
      " -1.46111608e-01 -1.12051435e-01  2.13880464e-02 -1.25448242e-01\n",
      " -2.18794812e-02 -3.07387603e-03  1.00551493e-01  2.21439511e-01\n",
      "  3.40393037e-02  1.50964722e-01  2.04637479e-02  2.18334183e-01\n",
      " -7.83797130e-02 -2.73028594e-02 -4.56530191e-02 -1.39723793e-01\n",
      " -1.31445989e-01  1.79434642e-01 -6.56679943e-02 -2.14948818e-01\n",
      " -8.18674043e-02  8.66051391e-02  6.53279722e-02 -7.92680979e-02\n",
      " -1.17903382e-01  7.58886263e-02  1.99693851e-02 -3.79110351e-02\n",
      "  1.86247140e-01  1.20483570e-01 -6.49838522e-02  1.38837069e-01\n",
      "  8.80882442e-02  1.41659498e-01 -1.33904628e-02 -1.86589137e-01\n",
      " -7.09101409e-02  5.64449243e-02 -2.53959954e-01  2.38772094e-01\n",
      "  2.14014016e-02 -1.71673074e-01  1.66717276e-01  1.12757884e-01\n",
      " -2.00031430e-01  8.61493349e-02 -1.40430778e-01 -1.82242170e-01\n",
      " -2.76751250e-01 -4.90618013e-02 -3.68351601e-02 -1.51931280e-02\n",
      "  1.00174338e-01  2.43216276e-01 -3.83194759e-02 -2.52904221e-02\n",
      "  1.96489573e-01 -6.13737740e-02 -2.39363890e-02 -1.43796694e-03\n",
      "  9.50243399e-02 -1.21400625e-01 -1.36301205e-01  1.25779644e-01\n",
      " -4.36339565e-02 -3.54442820e-02 -1.48244165e-02 -5.26708774e-02\n",
      "  1.49129972e-01 -1.73776254e-01 -1.75704569e-01  3.37219983e-03\n",
      " -4.27311882e-02 -2.54511803e-01 -2.50258416e-01 -1.65570259e-01\n",
      "  1.88777670e-01 -7.90780112e-02  2.28312165e-01  2.62312498e-02\n",
      " -1.22423254e-01  1.31242871e-02  8.73392150e-02 -6.10231161e-02\n",
      " -4.24469262e-02  1.77404180e-01  2.29899660e-02  9.70903486e-02\n",
      " -8.51157382e-02 -2.39395928e-02 -1.97316095e-01 -2.20979199e-01\n",
      "  1.52407408e-01 -1.24394462e-01 -5.79537377e-02  1.93887129e-01\n",
      " -7.34788890e-04  3.29346389e-01 -3.55784260e-02 -9.07562524e-02\n",
      "  1.48234162e-02  1.28493890e-01 -5.58898486e-02  5.16538806e-02\n",
      "  1.30156323e-01  6.58873171e-02 -9.70784854e-03  2.60458827e-01\n",
      " -9.46867988e-02 -5.64268231e-02  6.20201007e-02 -6.35874942e-02\n",
      " -8.03044364e-02  2.17933729e-01  8.77749175e-02 -1.83247969e-01\n",
      " -6.46370649e-02  1.11014761e-01 -1.10774554e-01 -1.35184661e-01\n",
      " -1.49419224e-02 -1.79180846e-01  4.82318811e-02 -3.92045155e-02\n",
      "  1.58084389e-02  7.20575377e-02 -1.83389097e-01 -2.28663743e-01\n",
      " -2.99080640e-01  2.11480126e-01 -2.07572207e-01 -5.99888712e-03\n",
      "  2.27830082e-01  1.65922679e-02  2.11387575e-01  1.38359636e-01\n",
      "  4.90427800e-02 -5.63999154e-02  3.08817718e-02  1.52967006e-01\n",
      " -1.36720642e-01  8.66000578e-02 -1.60425603e-01 -2.05395147e-02\n",
      " -1.57521039e-01  1.02247551e-01 -1.51937589e-01 -4.72298674e-02\n",
      "  6.31606728e-02 -2.57108152e-01 -1.50280222e-01 -1.32376060e-01\n",
      " -4.55243438e-02  9.48359966e-02  7.43126646e-02 -9.09292512e-03\n",
      " -1.68872356e-01 -1.04852401e-01  1.48284778e-01 -1.93535894e-01\n",
      "  2.02794418e-01  2.86973789e-02 -2.28803232e-02  9.50218365e-02\n",
      "  6.25297949e-02 -2.37889588e-01 -5.82003221e-02 -7.75309429e-02\n",
      " -2.65530460e-02  3.23422835e-04 -1.11411884e-01  3.16718183e-02\n",
      "  3.46131832e-03  1.30839378e-01 -2.36187920e-01  1.12698510e-01\n",
      " -3.87172960e-02 -8.39263871e-02  1.19256511e-01  2.26068810e-01\n",
      "  1.06251635e-01 -1.01135455e-01  1.05927596e-02  2.25393549e-01]\n",
      "\n",
      "Shape of affect centroid: (300,)\n",
      "Shape of cognition centroid: (300,)\n"
     ]
    }
   ],
   "source": [
    "# == Calculation ==\n",
    "def findcentroid(text, model):\n",
    "    vecs = [model.wv[w] * word_counts_weighted[w] \n",
    "        for w in text \n",
    "        if w in model.wv and w in word_counts_weighted]\n",
    "    vecs = [v for v in vecs if len(v) > 0]\n",
    "    centroid = np.mean(vecs, axis=0)\n",
    "    #centroid = centroid.reshape(1, -1)\n",
    "    return centroid\n",
    "\n",
    "\n",
    "affect_centroid = findcentroid(affect_dic, w2v)\n",
    "cog_centroid = findcentroid(cognition_dic, w2v)\n",
    "\n",
    "os.chdir(data_c)\n",
    "joblib.dump(affect_centroid, 'centroids/affect_centroid.pkl')\n",
    "joblib.dump(cog_centroid, 'centroids/cog_centroid.pkl')\n",
    "\n",
    "# == Overview Vectors ==\n",
    "print(\"Affect centroid vector:\\n\", affect_centroid)\n",
    "print(\"\\nCognition centroid vector:\\n\", cog_centroid)\n",
    "\n",
    "# Shape and stats\n",
    "print(\"\\nShape of affect centroid:\", affect_centroid.shape)\n",
    "print(\"Shape of cognition centroid:\", cog_centroid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af347b1-4f5f-401a-8ca0-1c33eb7776b1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c8316f-f794-4bcd-87c9-578aa82f88b0",
   "metadata": {},
   "source": [
    "### Emotionality Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3376a4f6-a002-4179-8614-dfef96113180",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_preprocessed)\n",
    "\n",
    "# Load preprocessed speech data \n",
    "\n",
    "preprocessed_final_files = [\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed1_final.pkl')),\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed2_final.pkl')),\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed3_final.pkl')),\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed4_final.pkl'))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1cbd1f95-c7f4-41f1-a292-542ee34ae882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit\\\\results\\\\distances_10epochs.pkl']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to compute weighted document vectors and derive affective/cognitive distances and scores\n",
    "\n",
    "def documentvecweight(lista):\n",
    "    out = []\n",
    "    lista = [i for i in lista if len(i[1]) > 0]\n",
    "    for s in lista:\n",
    "        # Compute weighted word vectors for each token present in the Word2Vec model\n",
    "        vecs = [w2v.wv[w] * word_counts_weighted[w] for w in s[1] if w in w2v.wv]\n",
    "        if len(vecs) == 0:\n",
    "            a = np.nan\n",
    "            c = np.nan\n",
    "            score = np.nan\n",
    "        else:\n",
    "            # Compute mean vector for each speech\n",
    "            v = np.mean(vecs, axis=0)\n",
    "             # Cosine distance to affective centroid\n",
    "            a = cosine(v, affect_centroid)\n",
    "            # Cosine distance to cognitive centroid\n",
    "            c = cosine(v, cog_centroid)\n",
    "            score = (1 + 1 - a) / (1 + 1 - c)\n",
    "        out.append([s[0], a, c, score])\n",
    "    return out\n",
    "\n",
    "\n",
    "def main_function(file_path, idx):\n",
    "    dataset = joblib.load(file_path)\n",
    "    data = documentvecweight(dataset)\n",
    "    lab = os.path.join(wd_results, f'distances_main_{idx}.pkl')\n",
    "    joblib.dump(data, lab)\n",
    "\n",
    "# Main loop: process all preprocessed speech files\n",
    "def main():\n",
    "    files = [\n",
    "        os.path.join(data_preprocessed, f'preprocessed_speeches_indexed{i+1}_final.pkl') #Changed!\n",
    "        for i in range(4)\n",
    "    ]\n",
    "    for i, f in enumerate(files, start=1):\n",
    "        main_function(f, i)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# Merge all distance files into one df\n",
    "DATA_temp = [os.path.join(wd_results, f'distances_main_{i+1}.pkl') for i in range(4)]\n",
    "\n",
    "tot = []\n",
    "for dataname in DATA_temp:\n",
    "    d = joblib.load(dataname)\n",
    "    tot += d\n",
    "\n",
    "tot_df = pd.DataFrame(tot, columns=['filename', 'affect_d', 'cognition_d', 'score'])\n",
    "joblib.dump(tot_df, os.path.join(wd_results, 'distances_10epochs.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fb955ab4-243d-470d-a967-e0af1e474c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          filename  affect_d  cognition_d     score\n",
      "0  HTI_70_2015.txt  1.364583     1.170656  0.766168\n",
      "1  PRY_58_2003.txt  0.784654     0.861974  1.067943\n",
      "2  GMB_72_2017.txt  1.183515     1.094364  0.901559\n",
      "3  SLV_04_1949.txt  1.309417     0.809819  0.580234\n",
      "4  LBY_56_2001.txt  1.131063     0.935927  0.816614\n"
     ]
    }
   ],
   "source": [
    "print(tot_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8c295cd4-4fa0-470f-b67c-4f584f7a3f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load df_merged and merge with tot_df by filename \n",
    "un_corpus_merged = pd.read_csv(os.path.join(data_c, \"un_corpus_merged.csv\"), sep=';', encoding='utf-8') \n",
    "un_corpus_scored = un_corpus_merged.merge(tot_df, on=\"filename\", how=\"left\")\n",
    "\n",
    "joblib.dump(un_corpus_scored, os.path.join(wd_results, \"un_corpus_scored.pkl\"))\n",
    "\n",
    "un_corpus_scored.to_csv(\n",
    "    os.path.join(wd_results, \"un_corpus_scored.csv\"),\n",
    "    sep=';', \n",
    "    index=False, \n",
    "    encoding='utf-8'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d8d13667-1eb3-482e-83ca-5969a229750a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              filename                                             speech  \\\n",
      "0      HTI_70_2015.txt  Mr. President, I would like to express my warm...   \n",
      "1      PRY_58_2003.txt  ﻿Two hundred years after the first cry of free...   \n",
      "2      GMB_72_2017.txt  With warm greetings to all members of the Gene...   \n",
      "3      SLV_04_1949.txt  Mr. Castro stated that the election of General...   \n",
      "4      LBY_56_2001.txt  ﻿At the\\noutset, I would like to congratulate ...   \n",
      "...                ...                                                ...   \n",
      "10947  BIH_73_2018.txt  It is my honour to address the Assembly for th...   \n",
      "10948  AFG_36_1981.txt  On behalf of the delegation of the Democratic ...   \n",
      "10949  SDN_43_1988.txt  ﻿It gives me great pleasure to extend to you. ...   \n",
      "10950  IRN_45_1990.txt  I should like to express my sincere congratula...   \n",
      "10951  CHL_29_1974.txt  The delegation of Chile participates in this i...   \n",
      "\n",
      "      country_code  year            country_name  speech_length_words  \\\n",
      "0              HTI  2015                   Haiti                 1601   \n",
      "1              PRY  2003                Paraguay                 1385   \n",
      "2              GMB  2017                  Gambia                 1696   \n",
      "3              SLV  1949             El Salvador                 2211   \n",
      "4              LBY  2001                   Libya                 4110   \n",
      "...            ...   ...                     ...                  ...   \n",
      "10947          BIH  2018  Bosnia and Herzegovina                 1892   \n",
      "10948          AFG  1981             Afghanistan                 5524   \n",
      "10949          SDN  1988                   Sudan                 3201   \n",
      "10950          IRN  1990                    Iran                 3678   \n",
      "10951          CHL  1974                   Chile                 3579   \n",
      "\n",
      "       english_official_language  security_council_permanent  \\\n",
      "0                              0                           0   \n",
      "1                              0                           0   \n",
      "2                              1                           0   \n",
      "3                              0                           0   \n",
      "4                              0                           0   \n",
      "...                          ...                         ...   \n",
      "10947                          0                           0   \n",
      "10948                          0                           0   \n",
      "10949                          1                           0   \n",
      "10950                          0                           0   \n",
      "10951                          0                           0   \n",
      "\n",
      "                       speaker_name                               position  \\\n",
      "0        Mr. Michel Joseph Martelly                      (Vice-) President   \n",
      "1             Nicanor Duarte Frutos                      (Vice-) President   \n",
      "2                  Mr. Adama Barrow                      (Vice-) President   \n",
      "3                       Mr. Castro                                     NaN   \n",
      "4      Abdurrahman Mohamed Shalghem  (Deputy) Minister for Foreign Affairs   \n",
      "...                             ...                                    ...   \n",
      "10947         Mr. Bakir Izetbegoviƈ                                 Others   \n",
      "10948                     Mr. DOST                                     NaN   \n",
      "10949                     Abu Salih                                    NaN   \n",
      "10950                      Velayati                                    NaN   \n",
      "10951                  Mr. Carvajal                                    NaN   \n",
      "\n",
      "       gender_dummy                   speech_label  affect_d  cognition_d  \\\n",
      "0               0.0                   Haiti (2015)  1.364583     1.170656   \n",
      "1               NaN                Paraguay (2003)  0.784654     0.861974   \n",
      "2               0.0                  Gambia (2017)  1.183515     1.094364   \n",
      "3               0.0             El Salvador (1949)  1.309417     0.809819   \n",
      "4               NaN                   Libya (2001)  1.131063     0.935927   \n",
      "...             ...                            ...       ...          ...   \n",
      "10947           0.0  Bosnia and Herzegovina (2018)  1.110090     0.938381   \n",
      "10948           0.0             Afghanistan (1981)  1.136330     1.036821   \n",
      "10949           NaN                   Sudan (1988)  1.348914     1.092352   \n",
      "10950           NaN                    Iran (1990)  1.109228     0.938882   \n",
      "10951           0.0                   Chile (1974)  1.388806     0.933527   \n",
      "\n",
      "          score  \n",
      "0      0.766168  \n",
      "1      1.067943  \n",
      "2      0.901559  \n",
      "3      0.580234  \n",
      "4      0.816614  \n",
      "...         ...  \n",
      "10947  0.838258  \n",
      "10948  0.896688  \n",
      "10949  0.717333  \n",
      "10950  0.839465  \n",
      "10951  0.573098  \n",
      "\n",
      "[10952 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "print(un_corpus_scored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1210b88f-cf13-4da0-807b-77764cf0d439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count where affect_d is NaN: 0\n",
      "Count where affect_d is not NaN: 10952\n"
     ]
    }
   ],
   "source": [
    "# Count where affect_d is NaN\n",
    "nan_count = un_corpus_scored['affect_d'].isna().sum()\n",
    "\n",
    "print(\"Count where affect_d is NaN:\", nan_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
