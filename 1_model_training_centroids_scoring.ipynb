{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6ca6f721-48ad-495c-8e5f-e60790225709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "from pathlib import Path\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "# from gensim.summarization.textcleaner import get_sentences\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from random import shuffle\n",
    "import nltk\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.tokenize import sent_tokenize\n",
    "tagger = nltk.perceptron.PerceptronTagger()\n",
    "import joblib\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from string import punctuation\n",
    "from scipy.spatial.distance import cosine\n",
    "import glob\n",
    "from multiprocessing import Pool, freeze_support\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# === Initialize NLP Tools ===\n",
    "\n",
    "translator = str.maketrans('', '', punctuation) \n",
    "tagger = nltk.perceptron.PerceptronTagger()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "# === Set Working Directory ===\n",
    "\n",
    "# Set your working directory (adjust this as needed)\n",
    "wd = Path(r\"C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\")\n",
    "wd_models = wd / \"models\"\n",
    "wd_results = wd / \"results\"\n",
    "\n",
    "\n",
    "# === Define Folder Paths ===\n",
    "\n",
    "# Make sure that you have these folders in your working directory\n",
    "data_c = wd / \"data\"\n",
    "data_temp = data_c / \"temp\"\n",
    "data_freq = data_c / \"freq\"\n",
    "data_dict = data_c / \"dictionaries\"\n",
    "data_preprocessed = data_c / \"preprocessed\"\n",
    "fig_dir = wd /\"fig\"\n",
    "\n",
    "# Upload ressources\n",
    "stopwords = joblib.load(data_c / \"stopwords.pkl\")\n",
    "word_counts = joblib.load(data_freq / \"word_counts_stemmed.pkl\")\n",
    "word_counts_weighted = joblib.load(data_freq / \"word_counts_weighted.pkl\")\n",
    "affect_dic = joblib.load(data_dict / 'dictionary_affect.pkl')\n",
    "cognition_dic = joblib.load(data_dict / 'dictionary_cognition.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "125227ca-a0b5-43d7-956d-7d49a66f81b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_temp)\n",
    "cleaned_files = [\n",
    "    str(data_temp / 'clean_speeches_indexed1.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed2.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed3.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed4.pkl')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "af0997e8-9da3-45c3-bdd4-7d38a708a474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl processed\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed1.pkl saved\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl processed\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed2.pkl saved\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl processed\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed3.pkl saved\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl processed\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed4.pkl saved\n"
     ]
    }
   ],
   "source": [
    "def extract_sentences(dataname):\n",
    "    data = joblib.load(dataname)\n",
    "    data = [a[1] for a in data]  # keep only text, no id\n",
    "\n",
    "    sentences = []\n",
    "    for doc in data:\n",
    "        sentences += sent_tokenize(doc)  # use nltk's sent_tokenize here\n",
    "\n",
    "    sentences = [item for item in sentences if len(item.split()) > 1]\n",
    "    sentences = [gensim.utils.simple_preprocess(item) for item in sentences]\n",
    "\n",
    "    sentences = [[a for a in s if not a.isdigit()] for s in sentences]\n",
    "    sentences = [[a for a in s if len(a) > 2] for s in sentences]\n",
    "\n",
    "    sentences = [tagger.tag(s) for s in sentences]\n",
    "    sentences = [[i[0] for i in s if i[1].startswith(('N', 'V', 'J'))] for s in sentences]\n",
    "\n",
    "    sentences = [[stemmer.stem(i) for i in s] for s in sentences]\n",
    "    sentences = [[a for a in s if a not in stopwords] for s in sentences]\n",
    "    sentences = [[a for a in s if word_counts_stemmed[a] >= 10] for s in sentences]\n",
    "\n",
    "    sentences = [s for s in sentences if len(s) > 1]  # eliminate empty\n",
    "    shuffle(sentences)\n",
    "\n",
    "    lab = dataname.replace('clean_speeches_', 'sentences_').replace('_.pkl', '.pkl')\n",
    "    print(f'{dataname} processed')\n",
    "    joblib.dump(sentences, lab)\n",
    "    print(f'{lab} saved')\n",
    "\n",
    "# Run for all your files\n",
    "for fname in cleaned_files:\n",
    "    extract_sentences(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4f1a2e83-e5f4-436b-84c9-0d9d51f3b761",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_files = [\n",
    "    'sentences_indexed1.pkl',\n",
    "    'sentences_indexed2.pkl',\n",
    "    'sentences_indexed3.pkl',\n",
    "    'sentences_indexed4.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "465bc73c-70b6-4dbf-a379-8c5235f83a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for dataname in sentences_files:  # <-- your list of sentence files\n",
    "    data = joblib.load(dataname)\n",
    "    dataset.extend(data)  # extend instead of append if you want all sentences in a single list\n",
    "\n",
    "# === Model training ===\n",
    "w2v = Word2Vec(\n",
    "    sentences=dataset,    # iterator that loops over tokenized sentences\n",
    "    vector_size=300,      # Word vector dimensionality (use `vector_size` in newer gensim)\n",
    "    window=8,             # Context window size\n",
    "    min_count=10,         # Minimum word count\n",
    "    workers=8,            # Number of threads\n",
    "    sample=1e-3,          # Downsample setting for frequent words\n",
    "    epochs=10             # Number of iterations over the corpus\n",
    ")\n",
    "\n",
    "# Optimize memory usage (optional)\n",
    "w2v.wv.fill_norms()  # only works in older gensim versions\n",
    "\n",
    "# Save model\n",
    "wd_models.mkdir(parents=True, exist_ok=True)  # create folder if it doesn't exist\n",
    "w2v.save(str(wd_models / 'w2v-vectors_8_300.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9d94142c-1e7f-4b0b-bfec-98604fc84304",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec.load(str(wd_models / \"w2v-vectors_8_300.pkl\"))\n",
    "word_vectors = w2v.wv # Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5b75bd3b-2957-42d1-a89c-89e5b3ea44f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['centroids/cog_centroid.pkl']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Calculate centroids ===\n",
    "\n",
    "def findcentroid(text, model):\n",
    "    vecs = [model.wv[w] * word_counts_weighted[w] for w in text if w in model.wv]\n",
    "    vecs = [v for v in vecs if len(v) > 0]\n",
    "    centroid = np.mean(vecs, axis=0)\n",
    "    #centroid = centroid.reshape(1, -1)\n",
    "    return centroid\n",
    "\n",
    "\n",
    "affect_centroid = findcentroid(affect_dic, w2v)\n",
    "cog_centroid = findcentroid(cognition_dic, w2v)\n",
    "\n",
    "os.chdir(data_c)\n",
    "joblib.dump(affect_centroid, 'centroids/affect_centroid.pkl')\n",
    "joblib.dump(cog_centroid, 'centroids/cog_centroid.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bcab4536-fca7-468d-ae93-6e8da17cbfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affect centroid vector:\n",
      " [-0.00608798 -0.03492771  0.01405908  0.02630128 -0.04857672 -0.1278637\n",
      "  0.1206017   0.24284929  0.02220609  0.06530752 -0.03228097 -0.03097843\n",
      "  0.01043473 -0.14732435 -0.26938754  0.00691377  0.03254289  0.08645286\n",
      "  0.16637743  0.05675362 -0.18768284 -0.01955027  0.01128447 -0.00183581\n",
      "  0.27018696  0.03270628 -0.19852409 -0.09102426 -0.0511265  -0.11699844\n",
      "  0.03494072 -0.14906853  0.02323899  0.02690839 -0.04481664  0.07235346\n",
      " -0.00102059 -0.26483372  0.08222571 -0.19196938 -0.02331756  0.02588117\n",
      "  0.12817642 -0.00528961  0.1212317   0.0529688  -0.04989932 -0.05130346\n",
      " -0.12776153  0.10621514  0.07343408  0.03053066  0.04956501  0.04847677\n",
      " -0.0810184   0.04587177  0.06071044  0.05726838  0.05954647 -0.13661206\n",
      " -0.17127532  0.12201256 -0.00514129  0.01362167 -0.0237526  -0.00784076\n",
      "  0.09162332 -0.02886288 -0.05266345 -0.00068738 -0.1333322   0.08877458\n",
      "  0.09628373 -0.07945921 -0.03453472 -0.00555583 -0.1404627   0.02281174\n",
      " -0.02893426  0.11431761 -0.11188828 -0.08158167  0.00703358  0.10704376\n",
      "  0.14128105  0.06096183 -0.19201176  0.1076658   0.08304327  0.0826102\n",
      "  0.08580855 -0.12853216  0.03667345 -0.06003178 -0.03503788  0.11238112\n",
      "  0.0773386  -0.06054499  0.00920645  0.2235894   0.11839999  0.02335892\n",
      "  0.03758086 -0.08538561  0.11077404 -0.11736421 -0.02301962 -0.06099238\n",
      " -0.07686206  0.16624801 -0.09664118 -0.00542958  0.02434729  0.13284379\n",
      "  0.19718587 -0.01341319 -0.03741692  0.05841286  0.08376162 -0.06413547\n",
      "  0.10260589  0.09935543  0.01412937  0.09531108 -0.18101366  0.02469476\n",
      "  0.1494961  -0.14443028  0.09220549  0.00349566 -0.03960231  0.04692056\n",
      " -0.06809377  0.01977565  0.07338735 -0.01811147  0.02081391 -0.11945107\n",
      " -0.08057814 -0.22375901  0.04433751 -0.18246028 -0.0805832   0.173961\n",
      " -0.04077337 -0.0510954  -0.08290318 -0.12940882  0.02676463 -0.00096433\n",
      " -0.09607068 -0.18348414 -0.16937107 -0.05610067  0.08805645  0.17625439\n",
      "  0.00256999 -0.17286679  0.09566554  0.1145538   0.06560454 -0.03043864\n",
      " -0.23659194  0.15219547 -0.12255015 -0.00638475 -0.06347311  0.00134704\n",
      "  0.0485315   0.12289951  0.06190211  0.04723495 -0.00951528 -0.02263062\n",
      " -0.06338616 -0.04991376  0.02054655 -0.03336126 -0.03357362 -0.00896056\n",
      " -0.1124716   0.16686048 -0.01609734 -0.13941771  0.02755202  0.03466563\n",
      "  0.06212798  0.2043878   0.16521674 -0.15489821 -0.04987665  0.07621783\n",
      " -0.07892925  0.13638641  0.07830697 -0.1004322   0.13801938  0.00257074\n",
      "  0.06939184 -0.02739366 -0.13836467  0.0771796  -0.02370373 -0.02567075\n",
      " -0.01198084  0.06949946 -0.00863769 -0.01567849  0.01042307  0.00653705\n",
      " -0.01330487 -0.07860426 -0.12450209  0.03147594  0.11176845 -0.17060877\n",
      " -0.0699186  -0.26624784 -0.11709484 -0.04575396  0.18520568  0.01465413\n",
      " -0.09761661  0.00999214 -0.23954724 -0.07852969 -0.00745661  0.05749271\n",
      " -0.12310061  0.1038575   0.0387992  -0.05450378 -0.16514614  0.19196838\n",
      " -0.12852104 -0.02209334 -0.02078131  0.03565396  0.00503317 -0.14671831\n",
      " -0.076465    0.00691508 -0.15203984 -0.02069701  0.02947978 -0.15968786\n",
      "  0.09319419  0.00126077  0.05187615  0.12788835 -0.06309159  0.04254289\n",
      " -0.05179913  0.04750826 -0.12934251 -0.02702374  0.18524     0.05898768\n",
      " -0.1721581  -0.13367027  0.0030255   0.1543935  -0.14434302 -0.12671083\n",
      " -0.00747641 -0.09036781 -0.07556838 -0.05273265  0.16870846 -0.0864988\n",
      " -0.0968219   0.0805323  -0.07083113 -0.22418112  0.24047449 -0.02964553\n",
      " -0.01018261  0.08963169 -0.123133   -0.09333872  0.15046014  0.01926907\n",
      "  0.11434572  0.09598079  0.01265598 -0.07031662 -0.18746525  0.08898495\n",
      " -0.03765037  0.06258395  0.06647455  0.06307168  0.04926045  0.05202749\n",
      "  0.08232675  0.08679482  0.08486    -0.10451081 -0.0537765   0.00581169]\n",
      "\n",
      "Cognition centroid vector:\n",
      " [-0.1014078   0.05965905  0.07141224  0.01638387  0.02432498 -0.03969603\n",
      "  0.07285945  0.10960772 -0.02086224  0.00537332  0.01444421 -0.03670366\n",
      " -0.01076547 -0.05885947 -0.06068022 -0.03575108  0.04221709 -0.0538765\n",
      "  0.05085201  0.06271876 -0.07861108 -0.00708246  0.01660975  0.04193647\n",
      "  0.07449386  0.00969687 -0.06816451 -0.03785657  0.01047037 -0.03066943\n",
      " -0.01058726 -0.08251187 -0.06325672 -0.08625098  0.11498929 -0.04257582\n",
      "  0.02539821 -0.04058612  0.04323614 -0.05728641 -0.07504106  0.00334278\n",
      "  0.057156   -0.02986874 -0.12773861 -0.00187735 -0.01477697 -0.08568919\n",
      "  0.03538385  0.0966242   0.02127776  0.00959625 -0.0122596  -0.02023827\n",
      " -0.0058334   0.00891852  0.09857504  0.10412538  0.01437397 -0.07458788\n",
      " -0.10172649  0.04068533  0.01576663 -0.05658461  0.03766038 -0.1158967\n",
      "  0.10980058 -0.0424881  -0.02217417 -0.02108081  0.08402517  0.05518861\n",
      "  0.03800667 -0.00501193 -0.07242339 -0.06762952 -0.02729583 -0.02876917\n",
      "  0.02862118  0.14715242 -0.01171396 -0.12820567 -0.05166724 -0.07099247\n",
      "  0.13089053  0.05393724 -0.08643185 -0.03726859  0.11337919  0.14687744\n",
      "  0.00450543  0.0128725   0.01895306 -0.04349522 -0.01857803  0.05280319\n",
      " -0.02923503 -0.02558841 -0.07292973  0.01449771 -0.06902077  0.00078349\n",
      "  0.15220709 -0.06143325  0.09760409  0.05951314  0.02782028 -0.09977279\n",
      "  0.0062689  -0.04446254 -0.06844661 -0.00577621  0.00927621  0.11139233\n",
      "  0.19620572  0.01399949 -0.11994429  0.05747211  0.00145024 -0.15726903\n",
      " -0.06223185  0.04789481  0.00568959 -0.01160095 -0.07274365  0.05751702\n",
      "  0.02324988 -0.07372583 -0.03105467 -0.07980186 -0.03773203 -0.00998637\n",
      "  0.04086884  0.01996573 -0.05367181 -0.03546279  0.04721295  0.0311294\n",
      " -0.0458462  -0.09400963 -0.06797511 -0.06659102  0.02313448  0.02523543\n",
      "  0.01668355 -0.02210973 -0.08944203 -0.04263106  0.11537274  0.06132314\n",
      " -0.11006727 -0.07175831  0.00140356 -0.04967478  0.00162662  0.16905008\n",
      " -0.00185546 -0.12181432 -0.06350829  0.05064846  0.03566645  0.03563657\n",
      " -0.11023346  0.09029848 -0.11727304 -0.05606202  0.0092887  -0.02531806\n",
      "  0.00462987  0.0855284   0.00680548  0.14616545  0.07540241 -0.02585155\n",
      " -0.04448003  0.03667782 -0.01738392  0.02470021 -0.04378181  0.00928023\n",
      "  0.08042421 -0.00448327 -0.03259194 -0.01521773  0.01679775 -0.09935609\n",
      " -0.04013482  0.01728192 -0.03092547 -0.11740879  0.01029376 -0.01465794\n",
      "  0.02093821  0.08132435 -0.01012782  0.00624464 -0.00792727  0.05203957\n",
      " -0.02378359  0.0560134  -0.04938285  0.15209433 -0.02507141  0.04786128\n",
      " -0.05435337 -0.02070329 -0.05995424  0.01206634 -0.05344838  0.05917652\n",
      " -0.01572918 -0.00147067 -0.10902803  0.0947766   0.06610876 -0.124373\n",
      " -0.0858063   0.00679611 -0.06048153  0.00451376  0.06963751 -0.02479719\n",
      " -0.02111311  0.17144215  0.08166187  0.05068495  0.00906728  0.0348625\n",
      " -0.06286268  0.0024541   0.01093778  0.00735913 -0.10228674  0.02873663\n",
      "  0.08146569  0.01880704  0.01565062 -0.02862291  0.04020135 -0.07319952\n",
      " -0.01270507 -0.03947137 -0.06349946 -0.06424736 -0.07270009 -0.04160501\n",
      "  0.08102399  0.07526733  0.00969754  0.11366306 -0.05835915  0.00362654\n",
      "  0.07762612 -0.02127409 -0.15528612  0.01297421  0.07954404  0.09076885\n",
      " -0.16161694 -0.03880472  0.04222551  0.06811394 -0.04307172 -0.0837286\n",
      "  0.03002396 -0.06771707 -0.03843885  0.03487705  0.01792924  0.01756553\n",
      " -0.02975403 -0.02017561 -0.00068989 -0.122773    0.06775378  0.04549939\n",
      "  0.06198118  0.0798042  -0.03372212  0.03455005  0.07116128 -0.01503538\n",
      "  0.0846708  -0.02160333  0.0663139  -0.07374489 -0.08011299 -0.02171838\n",
      "  0.08515228 -0.0200342   0.01613863  0.0192716   0.0105387   0.11960819\n",
      "  0.00918673  0.04856278  0.07897425 -0.14903145 -0.11500253  0.07032952]\n",
      "\n",
      "Shape of affect centroid: (300,)\n",
      "Shape of cognition centroid: (300,)\n",
      "\n",
      "Affect centroid summary: min, max, mean: -0.26938754 0.27018696 -0.0046231896\n",
      "Cognition centroid summary: min, max, mean: -0.16161694 0.19620572 -0.0014675402\n"
     ]
    }
   ],
   "source": [
    "# Print the vectors\n",
    "print(\"Affect centroid vector:\\n\", affect_centroid)\n",
    "print(\"\\nCognition centroid vector:\\n\", cog_centroid)\n",
    "\n",
    "# Optional: shape and stats\n",
    "print(\"\\nShape of affect centroid:\", affect_centroid.shape)\n",
    "print(\"Shape of cognition centroid:\", cog_centroid.shape)\n",
    "\n",
    "print(\"\\nAffect centroid summary: min, max, mean:\", \n",
    "      np.min(affect_centroid), np.max(affect_centroid), np.mean(affect_centroid))\n",
    "print(\"Cognition centroid summary: min, max, mean:\", \n",
    "      np.min(cog_centroid), np.max(cog_centroid), np.mean(cog_centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c8316f-f794-4bcd-87c9-578aa82f88b0",
   "metadata": {},
   "source": [
    "## Emotionality Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3376a4f6-a002-4179-8614-dfef96113180",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Set wd to data_preprocessed\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(data_preprocessed)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# === Load preprocessed speech data ===\u001b[39;00m\n\u001b[0;32m      6\u001b[0m preprocessed_final_files \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      7\u001b[0m     joblib\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_preprocessed, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed_speeches_indexed1_final.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[0;32m      8\u001b[0m     joblib\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_preprocessed, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed_speeches_indexed2_final.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[0;32m      9\u001b[0m     joblib\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_preprocessed, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed_speeches_indexed3_final.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[0;32m     10\u001b[0m     joblib\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_preprocessed, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed_speeches_indexed4_final.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     11\u001b[0m ]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Set wd to data_preprocessed\n",
    "os.chdir(data_preprocessed)\n",
    "\n",
    "# === Load preprocessed speech data ===\n",
    "\n",
    "preprocessed_final_files = [\n",
    "    joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed1_final.pkl')),\n",
    "    joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed2_final.pkl')),\n",
    "    joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed3_final.pkl')),\n",
    "    joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed4_final.pkl'))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1cbd1f95-c7f4-41f1-a292-542ee34ae882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit\\\\data\\\\distances_10epochs.pkl']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################################\n",
    "# Define Functions              ###\n",
    "###################################\n",
    "\n",
    "# apparently is missing deleting intermediate files\n",
    "\n",
    "def documentvecweight(lista):\n",
    "    out = []\n",
    "    lista = [i for i in lista if len(i[1]) > 0]\n",
    "    for s in lista:\n",
    "        vecs = [w2v.wv[w] * word_counts_weighted[w] for w in s[1] if w in w2v.wv]\n",
    "        if len(vecs) == 0:\n",
    "            a = np.nan\n",
    "            c = np.nan\n",
    "            score = np.nan\n",
    "        else:\n",
    "            v = np.mean(vecs, axis=0)\n",
    "            a = cosine(v, affect_centroid)\n",
    "            c = cosine(v, cog_centroid)\n",
    "            score = (1 + 1 - a) / (1 + 1 - c)\n",
    "        out.append([s[0], a, c, score])\n",
    "    return out\n",
    "\n",
    "\n",
    "def main_function(file_path, idx):\n",
    "    dataset = joblib.load(file_path)\n",
    "    data = documentvecweight(dataset)\n",
    "    lab = os.path.join(data_c, f'temp_distances_main_{idx}.pkl')\n",
    "    joblib.dump(data, lab)\n",
    "\n",
    "\n",
    "###################################\n",
    "#      Run main directly        ###\n",
    "###################################\n",
    "\n",
    "def main():\n",
    "    files = [\n",
    "        os.path.join(data_preprocessed, f'preprocessed_speeches_indexed{i+1}_final.pkl') #Changed!\n",
    "        for i in range(4)\n",
    "    ]\n",
    "    for i, f in enumerate(files, start=1):\n",
    "        main_function(f, i)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "###################################\n",
    "#      Recompose everything     ###\n",
    "###################################\n",
    "\n",
    "DATA_temp = [os.path.join(data_c, f'temp_distances_main_{i+1}.pkl') for i in range(4)]\n",
    "\n",
    "tot = []\n",
    "for dataname in DATA_temp:\n",
    "    d = joblib.load(dataname)\n",
    "    tot += d\n",
    "\n",
    "tot_df = pd.DataFrame(tot, columns=['filename', 'affect_d', 'cognition_d', 'score'])\n",
    "joblib.dump(tot_df, os.path.join(data_c, 'distances_10epochs.pkl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fb955ab4-243d-470d-a967-e0af1e474c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          filename  affect_d  cognition_d     score\n",
      "0  SYR_04_1949.txt  0.392178     0.400979  1.005504\n",
      "1  PRK_57_2002.txt  1.300450     0.974174  0.681938\n",
      "2  LVA_76_2021.txt  1.200172     0.801189  0.667184\n",
      "3  RWA_29_1974.txt  0.920350     0.533893  0.736406\n",
      "4  BFA_26_1971.txt  1.026254     0.635050  0.713393\n",
      "Shape: (1005, 4)\n",
      "          affect_d  cognition_d        score\n",
      "count  1005.000000  1005.000000  1005.000000\n",
      "mean      1.130575     0.768413     0.701871\n",
      "std       0.228257     0.156835     0.141048\n",
      "min       0.392178     0.293237     0.405621\n",
      "25%       0.999124     0.666527     0.598453\n",
      "50%       1.164945     0.785873     0.687966\n",
      "75%       1.303989     0.883953     0.788198\n",
      "max       1.580940     1.231918     1.223759\n",
      "             filename  affect_d  cognition_d     score\n",
      "0     SYR_04_1949.txt  0.392178     0.400979  1.005504\n",
      "1     PRK_57_2002.txt  1.300450     0.974174  0.681938\n",
      "2     LVA_76_2021.txt  1.200172     0.801189  0.667184\n",
      "3     RWA_29_1974.txt  0.920350     0.533893  0.736406\n",
      "4     BFA_26_1971.txt  1.026254     0.635050  0.713393\n",
      "...               ...       ...          ...       ...\n",
      "1000  MNG_59_2004.txt  1.291955     0.908864  0.648907\n",
      "1001  CIV_61_2006.txt  1.369215     0.919090  0.583569\n",
      "1002  PRY_50_1995.txt  1.345120     0.896265  0.593331\n",
      "1003  FIN_72_2017.txt  1.181881     0.870792  0.724507\n",
      "1004  SLB_63_2008.txt  1.342532     1.024759  0.674159\n",
      "\n",
      "[1005 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the first few rows\n",
    "print(tot_df.head())\n",
    "\n",
    "# Optionally, print the shape to see how many documents were processed\n",
    "print(\"Shape:\", tot_df.shape)\n",
    "\n",
    "# Print a quick summary\n",
    "print(tot_df.describe())\n",
    "\n",
    "# Or print the full DataFrame (if small)\n",
    "print(tot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "8c295cd4-4fa0-470f-b67c-4f584f7a3f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your main corpus CSV \n",
    "un_corpus_merged = pd.read_csv(os.path.join(data_c, \"un_corpus_merged.csv\"), sep=';', encoding='utf-8') \n",
    "# Merge on filename \n",
    "un_corpus_merged = un_corpus_merged.merge(tot_df, on=\"filename\", how=\"left\")\n",
    "\n",
    "# Optionally save back as pickle\n",
    "joblib.dump(un_corpus_merged, os.path.join(data_c, \"un_corpus_merged_with_scores.pkl\"))\n",
    "\n",
    "un_corpus_merged.to_csv(\n",
    "    os.path.join(data_c, \"un_corpus_merged_with_scores.csv\"),\n",
    "    sep=';', \n",
    "    index=False, \n",
    "    encoding='utf-8'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d8d13667-1eb3-482e-83ca-5969a229750a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             filename                                             speech  \\\n",
      "0     SYR_04_1949.txt  Fayez EL-KHOURI Bey explained that although hi...   \n",
      "1     PRK_57_2002.txt  ﻿I would like to\\ncongratulate you, Sir, on yo...   \n",
      "2     LVA_76_2021.txt  Let me begin by congratulating the Minister fo...   \n",
      "3     RWA_29_1974.txt   Mr. President, your unanimous election to the...   \n",
      "4     BFA_26_1971.txt  56.\\t Mr. President, allow me to congratulate ...   \n",
      "...               ...                                                ...   \n",
      "1010  MNG_59_2004.txt  I\\nwish to extend to you, Sir, my delegation’s...   \n",
      "1011  CIV_61_2006.txt  First, Madam, I should like to congratulate yo...   \n",
      "1012  PRY_50_1995.txt  At the outset, allow me to express my best\\nwi...   \n",
      "1013  FIN_72_2017.txt  Our thoughts today are with the people of Mexi...   \n",
      "1014  SLB_63_2008.txt  At the outset, Sir, \\nallow me to offer the wa...   \n",
      "\n",
      "     country_code  year     country_name  speech_length_words  \\\n",
      "0             SYR  1949            Syria                 2930   \n",
      "1             PRK  2002      North Korea                 2011   \n",
      "2             LVA  2021           Latvia                 1693   \n",
      "3             RWA  1974           Rwanda                 4963   \n",
      "4             BFA  1971     Burkina Faso                 4537   \n",
      "...           ...   ...              ...                  ...   \n",
      "1010          MNG  2004         Mongolia                 2221   \n",
      "1011          CIV  2006    Côte d'Ivoire                 2520   \n",
      "1012          PRY  1995         Paraguay                 3699   \n",
      "1013          FIN  2017          Finland                 1286   \n",
      "1014          SLB  2008  Solomon Islands                 2764   \n",
      "\n",
      "      english_official_language  security_council_permanent  \\\n",
      "0                             0                           0   \n",
      "1                             0                           0   \n",
      "2                             0                           0   \n",
      "3                             1                           0   \n",
      "4                             0                           0   \n",
      "...                         ...                         ...   \n",
      "1010                          0                           0   \n",
      "1011                          0                           0   \n",
      "1012                          0                           0   \n",
      "1013                          0                           0   \n",
      "1014                          1                           0   \n",
      "\n",
      "                      speaker_name                               position  \\\n",
      "0                  Fayez EL-KHOURI                                    NaN   \n",
      "1                      Choe Su Hon  (Deputy) Minister for Foreign Affairs   \n",
      "2                     Egils Levits                      (Vice-) President   \n",
      "3                    Mr. Nzekalije                                    NaN   \n",
      "4                      Mr. CONOMBO                                    NaN   \n",
      "...                            ...                                    ...   \n",
      "1010  Mr. Radnaabazaryn ALTANGEREL              Diplomatic Representative   \n",
      "1011        Mr. Youssouf BAKAYOKO   (Deputy) Minister for Foreign Affairs   \n",
      "1012          Mr. Ramírez Boettner  (Deputy) Minister for Foreign Affairs   \n",
      "1013            Mr. Sauli Niinistö                      (Vice-) President   \n",
      "1014                 Derrick Sikua                (Deputy) Prime Minister   \n",
      "\n",
      "      gender_dummy            speech_label  affect_d  cognition_d     score  \n",
      "0              NaN            Syria (1949)  0.392178     0.400979  1.005504  \n",
      "1              NaN      North Korea (2002)  1.300450     0.974174  0.681938  \n",
      "2              NaN           Latvia (2021)  1.200172     0.801189  0.667184  \n",
      "3              0.0           Rwanda (1974)  0.920350     0.533893  0.736406  \n",
      "4              0.0     Burkina Faso (1971)  1.026254     0.635050  0.713393  \n",
      "...            ...                     ...       ...          ...       ...  \n",
      "1010           0.0         Mongolia (2004)  1.291955     0.908864  0.648907  \n",
      "1011           0.0    Côte d'Ivoire (2006)  1.369215     0.919090  0.583569  \n",
      "1012           0.0         Paraguay (1995)  1.345120     0.896265  0.593331  \n",
      "1013           0.0          Finland (2017)  1.181881     0.870792  0.724507  \n",
      "1014           NaN  Solomon Islands (2008)  1.342532     1.024759  0.674159  \n",
      "\n",
      "[1015 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "print(un_corpus_merged)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
