{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b41cb22-e190-48d5-9926-7ac6fac8310d",
   "metadata": {},
   "source": [
    "# Development of Emotion and Reasoning in the General Speeches of the United Nations: A text-based machine learning approach\n",
    "## Additional Analysis: Different Calculation of the Weighted Frequencies\n",
    "### Author: Sarah Franzen\n",
    "\n",
    "### Description\r\n",
    "\n",
    "In the replication package, weighted frequencies are calculated on the full preprocessed corpus (35,009 unique words; 4,500,778 tokens), while the embedding corpus drops words occurring fewer than 10 times (9,453 unique words; 4,286,666 tokens). This script examines whether calculating weighted frequencies after removing these low-frequency words makes any difference.ce.\r\n",
    "rence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930ebd56-def9-4a16-91eb-78dbb7d037ae",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5588afc-105a-426e-9348-742a830a2732",
   "metadata": {},
   "source": [
    "## Setup, Installation of required Packages and Libraries & Folder Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "32d7d499-0563-46e0-9730-914c9f106d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Import libraries for data processing and NLP ==\n",
    "\n",
    "import gensim\n",
    "import joblib\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import pycountry\n",
    "import random\n",
    "import re\n",
    "import spacy\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from multiprocessing import Pool, freeze_support\n",
    "from nltk.stem import SnowballStemmer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOPWORDS\n",
    "from string import punctuation\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "# === Initialize NLP Tools ===\n",
    "\n",
    "translator = str.maketrans('', '', punctuation)\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "tagger = nltk.perceptron.PerceptronTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4d495156-bbb2-403c-86f2-83b47d8cb03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your working directory path (e.g., C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit):  C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory set to: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\n"
     ]
    }
   ],
   "source": [
    "# === Set Working Directory and create folder structure ===\n",
    "\n",
    "# Prompt user to enter working directory path\n",
    "wd = Path(input(\"Please enter your working directory path (e.g., C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit): \").strip())\n",
    "\n",
    "# Change to the entered working directory\n",
    "\n",
    "#wd = Path(r\"C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\")\n",
    "\n",
    "# Change to the entered working directory\n",
    "try:\n",
    "    os.chdir(wd)\n",
    "    print(f\"Working directory set to: {os.getcwd()}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: The directory you entered does not exist. Please restart and enter a valid path.\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "56fa9a21-650f-4d8e-8a21-3d9629655c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define Folder Paths ===\n",
    "\n",
    "# If an error occurs, make sure that you actually have these folders in your working directory\n",
    "\n",
    "data_c = wd / \"data\"\n",
    "data_temp = data_c / \"temp\"\n",
    "data_freq = data_c / \"freq\"\n",
    "data_dict = data_c / \"dictionaries\"\n",
    "data_preprocessed = data_c / \"preprocessed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "296fc7d6-a2ac-406e-a322-6c9f2702ee1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\n"
     ]
    }
   ],
   "source": [
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "68962929-b407-40d9-8d1c-e943061eeb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = joblib.load(os.path.join(data_c, \"un_corpus_merged.pkl\"))\n",
    "\n",
    "preprocessed_files = [\n",
    "    os.path.join(data_preprocessed, f\"preprocessed_speeches_indexed{i}.pkl\") \n",
    "    for i in range(1, 5)  # adjust the range if you have more/less files\n",
    "]\n",
    "\n",
    "preprocessed_data = []\n",
    "for fpath in preprocessed_files:\n",
    "    data = joblib.load(fpath)\n",
    "    preprocessed_data.extend(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bd410e-bc90-4e0b-aa08-a9bff13fab47",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d006e-6a23-453b-9c91-0e60bd3f8301",
   "metadata": {},
   "source": [
    "## Word-Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf2443e-4c76-41b2-bc16-e72d91c318a8",
   "metadata": {},
   "source": [
    "### Count frequencies of all tokens and display the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "154d978d-cddd-431c-9d53-73c9faf8fdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:16<00:00,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Stemmed] Top 50 most common words:\n",
      "econom: 73475\n",
      "human: 57178\n",
      "problem: 56809\n",
      "region: 48963\n",
      "achiev: 39487\n",
      "global: 36179\n",
      "africa: 34787\n",
      "nuclear: 32246\n",
      "solut: 32199\n",
      "social: 30684\n",
      "charter: 28158\n",
      "african: 27745\n",
      "weapon: 26744\n",
      "contribut: 26390\n",
      "respons: 26218\n",
      "negoti: 25658\n",
      "implement: 25093\n",
      "cannot: 22646\n",
      "ensur: 22450\n",
      "area: 22357\n",
      "disarma: 21892\n",
      "increas: 21585\n",
      "strengthen: 20530\n",
      "promot: 20382\n",
      "role: 19597\n",
      "non: 19390\n",
      "decis: 18832\n",
      "propos: 18690\n",
      "climat: 17991\n",
      "threat: 17597\n",
      "goal: 17426\n",
      "crisi: 17155\n",
      "terror: 16682\n",
      "stabil: 16576\n",
      "struggl: 14857\n",
      "aggress: 14819\n",
      "toward: 14590\n",
      "palestinian: 14093\n",
      "soviet: 13992\n",
      "financi: 13807\n",
      "poverti: 13372\n",
      "europ: 13173\n",
      "democraci: 13087\n",
      "share: 12802\n",
      "purpos: 12612\n",
      "suffer: 12543\n",
      "dialogu: 12385\n",
      "european: 12324\n",
      "popul: 12253\n",
      "regim: 12026\n",
      "\n",
      "[Stemmed] Top 50 least common words:\n",
      "gridlock: 10\n",
      "tiraspol: 10\n",
      "niyazov: 10\n",
      "kinkel: 10\n",
      "herat: 10\n",
      "turquois: 10\n",
      "softwar: 10\n",
      "overexploit: 10\n",
      "débi: 10\n",
      "mandab: 10\n",
      "bakili: 10\n",
      "rss: 10\n",
      "midrand: 10\n",
      "sukhumi: 10\n",
      "olara: 10\n",
      "highhanded: 10\n",
      "mohéli: 10\n",
      "habibi: 10\n",
      "copax: 10\n",
      "skip: 10\n",
      "femin: 10\n",
      "pathogen: 10\n",
      "litr: 10\n",
      "marcoussi: 10\n",
      "eln: 10\n",
      "sixparti: 10\n",
      "longoverdu: 10\n",
      "gatumba: 10\n",
      "peopleí: 10\n",
      "noncommunic: 10\n",
      "multilay: 10\n",
      "unitaid: 10\n",
      "gini: 10\n",
      "icu: 10\n",
      "tabomoa: 10\n",
      "sampaio: 10\n",
      "countercycl: 10\n",
      "outsourc: 10\n",
      "tfg: 10\n",
      "icglr: 10\n",
      "muskoka: 10\n",
      "homophobia: 10\n",
      "mahamadou: 10\n",
      "ddrr: 10\n",
      "hudaydah: 10\n",
      "repurpos: 10\n",
      "obrador: 10\n",
      "pronor: 10\n",
      "jusc: 10\n",
      "sancon: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit\\\\data\\\\freq\\\\removed_lowfreq_words_word_counts.pkl']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#== Count token frequencies ==\n",
    "\n",
    "def count_frequencies(filenames):\n",
    "    total_freq = Counter()\n",
    "    for fname in tqdm(filenames):\n",
    "        data = joblib.load(fname)\n",
    "        tokens = chain.from_iterable(row[1] for row in data if isinstance(row[1], list))\n",
    "        total_freq.update(tokens)\n",
    "    return total_freq\n",
    "\n",
    "# Remove Words that appear less than 10x times\n",
    "removed_lowfreq_words_word_counts = count_frequencies(preprocessed_files)\n",
    "\n",
    "removed_lowfreq_words_word_counts  = Counter({w: c for w, c in removed_lowfreq_words_word_counts.items() if c >= 10})\n",
    "\n",
    "print(\"\\n[Stemmed] Top 50 most common words:\")\n",
    "for word, count in removed_lowfreq_words_word_counts.most_common(50):\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\n[Stemmed] Top 50 least common words:\")\n",
    "for word, count in removed_lowfreq_words_word_counts.most_common()[-50:]:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Save stemmed word counts\n",
    "save_path = os.path.join(data_freq, 'removed_lowfreq_words_word_counts.pkl')\n",
    "joblib.dump(removed_lowfreq_words_word_counts, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0ef220ae-b830-42a8-81e6-06a6f0b723a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 9473\n"
     ]
    }
   ],
   "source": [
    "num_unique_words = len(removed_lowfreq_words_word_counts)\n",
    "print(f\"Number of unique words: {num_unique_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "932ce36d-d21c-4f1c-aff7-899c321dcfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_c)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39acefa1-c504-478e-98e5-9d465963dc82",
   "metadata": {},
   "source": [
    "### Count the frequency of the dictionary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9e28f481-7e20-486c-a6b3-8729d6aeaad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of affect dictionary:\n",
      "['forbid', 'unattract', 'cruelti', 'crappi', 'apathi', 'scari', 'unimpress', 'sin', 'dumbest', 'eas', 'agit', 'sob', 'shocker', 'tragedi', 'fabul', 'strongest', 'giver', 'sigh', 'aw', 'witch', 'hurtl', 'fucktard', 'cruel', 'glamor', 'funni', 'smarter', 'brillianc', 'irrate', 'alright', 'honest', 'profit', 'fearless', 'grievous', 'relax', 'isolationist', 'hah', 'shyness', 'poorest', 'cruelest', 'troublemak', 'disagre', 'agon', 'terror', 'fight', 'pleas', 'poor', 'crazi', 'hostil', 'stupid', 'damnat', 'vain', 'jade', 'heartless', 'nag', 'gloomi', 'damn', 'dishearten', 'pleaser', 'credit', 'warmth', 'greatest', 'whine', 'shame', 'angriest', 'envious', 'grin', 'blameless', 'sweeter', 'laidback', 'stupidest', 'unprotect', 'whiner', 'unlov', 'shake', 'boredom', 'fairer', 'weaker', 'wellb', 'bold', 'sucki', 'unsuccess', 'mourner', 'liken', 'defens', 'invigor', 'tedious', 'paranoid', 'cynic', 'dignifi', 'paranoia', 'sweetest', 'contented', 'humili', 'crush', 'terrif', 'alarm', 'asham', 'impati', 'fake', 'flirti', 'laziest', 'libertarian', 'empti', 'panicki', 'rape', 'contemptu', 'incent', 'reward', 'brutal', 'ignoramus', 'bereav', 'wimp', 'laugh', 'nicer', 'warm', 'resent', 'threat', 'hell', 'tortur', 'mad', 'worship', 'uncomfort', 'depress', 'unsaf', 'faultless', 'numb', 'humorless', 'murder', 'prettiest', 'jealous', 'tragic', 'discomfort', 'loveli', 'ungrat', 'divin', 'lamest', 'weaken', 'horribl', 'fuck', 'agoni', 'grievanc', 'ach', 'isolation', 'masoch', 'uglier', 'amaz', 'devilish', 'bitter', 'devil', 'smartest', 'glori', 'trite', 'vicious', 'unsavori', 'grossest', 'sinist', 'cheeri', 'dismay', 'easier', 'flatter', 'virtuous', 'egot', 'generos', 'cherish', 'heartwarm', 'advers', 'tough', 'adorn', 'overwhelm', 'devast', 'egotist', 'sociabl', 'lazier', 'affection', 'sarcasm', 'humor', 'funniest', 'startl', 'innoc', 'vomit', 'shyli', 'kiss', 'burdensom', 'melancholi', 'weakest', 'worthless', 'gentlest', 'proud', 'alarmist', 'pervert', 'demeanor', 'loneli', 'reassur', 'weird', 'passionless', 'loyalti', 'frustrat', 'weak', 'immor', 'hater', 'libertin', 'flirt', 'terribl', 'threaten', 'fatalist', 'melancholia', 'avers', 'prick', 'sucker', 'meanest', 'virtuos', 'benefici', 'neurot', 'disappoint', 'restless', 'romant', 'bastard', 'angri', 'flatteri', 'stun', 'bolder', 'amor', 'forbidden', 'hug', 'demean', 'disgust', 'truer', 'benefic', 'teas', 'stinki', 'condemnatori', 'turmoil', 'poison', 'cunt', 'gorgeous', 'harm', 'lousi', 'glorifi', 'piti', 'wimpish', 'impression', 'freakish', 'happiest', 'apathet', 'pitiabl', 'unpleas', 'suffer', 'miseri', 'toler', 'mess', 'casual', 'anxieti', 'stale', 'fieri', 'grudg', 'arrog', 'outrag', 'thrill', 'faithless', 'popular', 'vaniti', 'palat', 'fantasi', 'decay', 'domin', 'mood', 'proudest', 'scream', 'stress', 'calm', 'fatigu', 'dumber', 'benign', 'rude', 'steal', 'tension', 'enthusiast', 'intimid', 'sweetheart', 'prettier', 'perv', 'pathet', 'flawless', 'contradictori', 'distress', 'moron', 'gratifi', 'frighten', 'anguish', 'snob', 'defenseless', 'fantast', 'sad', 'thief', 'worsen', 'tear', 'handsom', 'attract', 'messi', 'smother', 'scarier', 'afraid', 'insult', 'disreput', 'wept', 'victim', 'traumat', 'prejudic', 'amus', 'decent', 'joyous', 'masochist', 'abandon', 'foe', 'pleasantri', 'shitless', 'strengthen', 'antagonist', 'liar', 'romantic', 'violenc', 'sentimentalist', 'gratif', 'decept', 'witchcraft', 'liveli', 'impressionist', 'guilt', 'griev', 'rotten', 'disgrac', 'insincer', 'pushi', 'enjoy', 'freaki', 'ador', 'grate', 'homesick', 'braver', 'battlefield', 'niceti', 'hooray', 'incompet', 'thiev', 'timid', 'adventur', 'needi', 'joyless', 'grimac', 'gloom', 'dump', 'horrid', 'magnific', 'unlucki', 'killer', 'uncontrol', 'funnier', 'ruin', 'weepi', 'play', 'encourag', 'fantas', 'irrit', 'sincer', 'rage', 'despis', 'unhappi', 'hate', 'bravest', 'harmless', 'hero', 'vigor', 'pervers', 'dumb', 'degrad', 'shameless', 'villaini', 'envi', 'aggress', 'fucker', 'courag', 'craziest', 'delici', 'scare', 'finer', 'bittersweet', 'trivial', 'wealthi', 'heroic', 'ecstat', 'irrat', 'prejudici', 'smug', 'stammer', 'feroci', 'phobic', 'villain', 'anger', 'fond', 'obsession', 'awesom', 'wors', 'compassion', 'weakl', 'calmer', 'inhibit', 'bother', 'rancid', 'hatr', 'beaten', 'opportunist', 'offens', 'haunt', 'appal', 'painless', 'pessimist', 'molest', 'lol', 'wrongdo', 'frantic', 'heroin', 'rejoic', 'woe', 'miser', 'meaner', 'saddest', 'mooch', 'attack', 'enthusiasm', 'joke', 'meritocraci', 'angrier', 'emot', 'weirdest', 'silliest', 'kisser', 'hurrah', 'crueler', 'nervous', 'dishonor', 'gracious', 'sicker', 'hopeless', 'delect', 'compliment', 'petrifi', 'cuter', 'reek', 'loveless', 'daze', 'weirder', 'triumphant', 'impolit', 'offend', 'richer', 'violent', 'crazier', 'comedown', 'heroism', 'stubborn', 'maniac', 'strain', 'broke', 'jerk', 'unkind', 'asshol', 'reveng', 'jealousi', 'damnabl', 'unimport', 'careless', 'gentler', 'bitch', 'shi', 'terrorist', 'rigid', 'harsh', 'passion', 'annoy', 'compass', 'geek', 'weirdo', 'agre', 'whore', 'splendor', 'suck', 'fun', 'chuckl', 'snobberi', 'mock', 'doom', 'reluct', 'mourn', 'boldest', 'helpless', 'trembl', 'pestil', 'curs', 'desper', 'harass', 'complain', 'joker', 'obsess', 'uneasi', 'nasti', 'greedi', 'belov', 'grim', 'upset', 'greed', 'honesti', 'phoni', 'feudal', 'loveliest', 'trauma', 'dwell', 'ecstasi', 'blame', 'legit', 'calmest', 'sickest', 'scariest', 'sarcast', 'cheat', 'fatal', 'stronger', 'energet', 'feroc', 'healthi', 'goddam', 'disast', 'flirtati', 'defeatist', 'glorious', 'terrifi', 'gossip', 'forbad', 'sicken', 'poorer', 'strangest', 'cute', 'struggl', 'woebegon', 'worri', 'gratitud', 'cheer', 'distraught', 'ok', 'vulner', 'blessed', 'fairest', 'lame', 'optim', 'grouch', 'fright', 'pessim', 'sadder', 'worst', 'despair', 'bash', 'intellectu', 'promis', 'prais', 'heartfelt', 'horror', 'crap', 'repress', 'prouder', 'fought', 'discourag', 'heal', 'lazi', 'precious', 'melanchol', 'isol', 'aggressor', 'giggl', 'idiot', 'twitchi', 'glorif', 'kid', 'selfish', 'smile', 'grief', 'hellish', 'shit', 'fume', 'bitchi', 'fool', 'trustworthi', 'optimist', 'dumpi', 'unwelcom', 'neat', 'nicest', 'furi', 'unfriend', 'ridicul', 'arguabl', 'shamefac', 'pervi', 'upbeat', 'wow', 'easygo', 'blameworthi', 'charmer', 'stink', 'phobia', 'adversari', 'sillier', 'distrust', 'dread', 'happier', 'vital', 'dissatisfact', 'insecur', 'assault', 'ineffect', 'panic', 'powerless', 'nightmar', 'wealth', 'enrag', 'splendid', 'foolish', 'ugliest', 'contradict', 'worrier', 'freak']\n",
      "Number of words in affect dictionary: 629\n",
      "\n",
      "Contents of cognition dictionary:\n",
      "['analyt', 'guess', 'suspect', 'coz', 'mysteri', 'unawar', 'proverbi', 'dissimilar', 'consequenti', 'stimul', 'occasion', 'rethink', 'purest', 'entireti', 'abnorm', 'referenti', 'differenti', 'diagnos', 'ambigu', 'concentr', 'undon', 'undecid', 'disclosur', 'fundamentalist', 'blur', 'reaction', 'variabl', 'recogniz', 'meant', 'reconstruct', 'supposit', 'acknowledg', 'choosi', 'accuraci', 'mislead', 'perceiv', 'hypothesi', 'unknow', 'cuz', 'reveal', 'potenti', 'puriti', 'persuad', 'indirect', 'generat', 'presumpt', 'clue', 'justif', 'evidenti', 'realize', 'deduct', 'causal', 'inevit', 'thinker', 'evidentiari', 'factor', 'undeni', 'unclear', 'pick', 'induc', 'accur', 'absolut', 'everytim', 'reconsider', 'diagnost', 'solut', 'diagnosi', 'discoveri', 'ration', 'unambigu', 'explicit', 'decis', 'insight', 'confess', 'fuzzi', 'led', 'lesson', 'theoriz', 'analyz', 'learner', 'split', 'enlighten', 'cos', 'respons', 'dubious', 'react', 'guarante', 'someday', 'blatant', 'undo', 'perceptu', 'infal', 'afterthought', 'inequ', 'feasibl', 'knowabl', 'approxim', 'logic', 'meaning', 'unknown', 'categor', 'trigger', 'incomplet', 'curious', 'attribut', 'pretend', 'reactionari', 'prove', 'meaningless', 'reactiv', 'proof', 'bet', 'purpos', 'suppos', 'discern', 'imagin', 'kinda', 'comprehens', 'sens', 'percept', 'everyday', 'rational', 'unquestion', 'random', 'implicit', 'clarif', 'theori', 'dunno', 'analysi', 'obedi', 'memor', 'genuin', 'evalu', 'explor', 'linkag', 'grasp', 'curios', 'methink', 'somewher', 'deduc', 'unwant', 'induct', 'unrel', 'outstand', 'discov', 'unresolv', 'exploratori', 'choos', 'perspect', 'inferenti', 'disclos', 'factual', 'arbitrari', 'conting', 'vagu', 'persuas', 'manipul', 'impli', 'somehow', 'sorta', 'borderlin', 'reconcil', 'anyhow', 'conscious', 'probabilist', 'precis', 'rearrang', 'referenc', 'obscur']\n",
      "Number of words in cognition dictionary: 169\n",
      "Unique affect words in text: 424\n",
      "Unique cognition words in text: 147\n",
      "Top 50 words by weighted frequency:\n",
      "winch: 0.9977554187558108\n",
      "infantil: 0.9977554187558108\n",
      "sojourn: 0.9977554187558108\n",
      "nurnberg: 0.9977554187558108\n",
      "unredeem: 0.9977554187558108\n",
      "attle: 0.9977554187558108\n",
      "unitedn: 0.9977554187558108\n",
      "masaryk: 0.9977554187558108\n",
      "valueless: 0.9977554187558108\n",
      "vouch: 0.9977554187558108\n",
      "tot: 0.9977554187558108\n",
      "furnitur: 0.9977554187558108\n",
      "stepp: 0.9977554187558108\n",
      "shiver: 0.9977554187558108\n",
      "handsom: 0.9977554187558108\n",
      "chasten: 0.9977554187558108\n",
      "dishonesti: 0.9977554187558108\n",
      "qui: 0.9977554187558108\n",
      "drunk: 0.9977554187558108\n",
      "amplif: 0.9977554187558108\n",
      "cheek: 0.9977554187558108\n",
      "madman: 0.9977554187558108\n",
      "unorthodox: 0.9977554187558108\n",
      "aac: 0.9977554187558108\n",
      "wang: 0.9977554187558108\n",
      "rasmussen: 0.9977554187558108\n",
      "foretast: 0.9977554187558108\n",
      "uninform: 0.9977554187558108\n",
      "nought: 0.9977554187558108\n",
      "pitiabl: 0.9977554187558108\n",
      "reassum: 0.9977554187558108\n",
      "extenu: 0.9977554187558108\n",
      "frankfurt: 0.9977554187558108\n",
      "kardelj: 0.9977554187558108\n",
      "vall: 0.9977554187558108\n",
      "lexicon: 0.9977554187558108\n",
      "goliath: 0.9977554187558108\n",
      "yasin: 0.9977554187558108\n",
      "primev: 0.9977554187558108\n",
      "locarno: 0.9977554187558108\n",
      "utilitarian: 0.9977554187558108\n",
      "genial: 0.9977554187558108\n",
      "rat: 0.9977554187558108\n",
      "fiddl: 0.9977554187558108\n",
      "hanker: 0.9977554187558108\n",
      "utrecht: 0.9977554187558108\n",
      "anecdot: 0.9977554187558108\n",
      "picturesqu: 0.9977554187558108\n",
      "sneer: 0.9977554187558108\n",
      "shirt: 0.9977554187558108\n"
     ]
    }
   ],
   "source": [
    "affect_path = os.path.join(data_dict, 'dictionary_affect.pkl')\n",
    "cognition_path = os.path.join(data_dict, 'dictionary_cognition.pkl')\n",
    "\n",
    "affect = joblib.load(affect_path)\n",
    "cognition = joblib.load(cognition_path)\n",
    "\n",
    "# == Count dictionary words\n",
    "\n",
    "print(\"Contents of affect dictionary:\")\n",
    "print(affect)\n",
    "print(\"Number of words in affect dictionary:\", len(affect))\n",
    "\n",
    "print(\"\\nContents of cognition dictionary:\")\n",
    "print(cognition)\n",
    "print(\"Number of words in cognition dictionary:\", len(cognition))\n",
    "\n",
    "a_list = [[i, removed_lowfreq_words_word_counts[i]] for i in affect if i in removed_lowfreq_words_word_counts]\n",
    "c_list = [[i, removed_lowfreq_words_word_counts[i]] for i in cognition if i in removed_lowfreq_words_word_counts]\n",
    "\n",
    "a_list = sorted(a_list, key=lambda x: x[1], reverse=True)\n",
    "c_list = sorted(c_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "a = [[i[0], f\"({i[1]}),\"] for i in a_list]\n",
    "c = [[i[0], f\"({i[1]}),\"] for i in c_list]\n",
    "\n",
    "a1 = ' '.join(str(r) for v in a for r in v)\n",
    "c1 = ' '.join(str(r) for v in c for r in v)\n",
    "\n",
    "affect_out_path = os.path.join(data_freq, \"removed_lowfreq_words_affect_words.txt\")\n",
    "cog_out_path = os.path.join(data_freq, \"removed_lowfreq_words_cog_words.txt\")\n",
    "\n",
    "with open(affect_out_path, \"w\") as output:\n",
    "    output.write(a1)\n",
    "\n",
    "with open(cog_out_path, \"w\") as output:\n",
    "    output.write(c1)\n",
    "\n",
    "# number of affect/cognitive words that appear in word_counts\n",
    "num_affect_words = len(a_list)\n",
    "num_cog_words = len(c_list)\n",
    "\n",
    "print(f\"Unique affect words in text: {num_affect_words}\")\n",
    "print(f\"Unique cognition words in text: {num_cog_words}\")\n",
    "\n",
    "# == Calculate weighted frequencies for all words\n",
    "\n",
    "l = sum(removed_lowfreq_words_word_counts.values())\n",
    "\n",
    "a = 0.001 # Method to downweight with a smoothing parameter: For frequent words (large v/1), weight approaches 0; for rare words (small v/1) closer to 1\n",
    "removed_lowfreq_words_word_counts_weighted = {k: a / (a + (v / l)) for k, v in removed_lowfreq_words_word_counts.items()}\n",
    "\n",
    "joblib.dump(removed_lowfreq_words_word_counts_weighted, os.path.join(data_freq, 'removed_lowfreq_words_word_counts_weighted.pkl'))\n",
    "\n",
    "# To print top 50 by weighted values, sort the dictionary by value descending:\n",
    "top_50_weighted = sorted(removed_lowfreq_words_word_counts_weighted.items(), key=lambda x: x[1], reverse=True)[:50]\n",
    "\n",
    "print(\"Top 50 words by weighted frequency:\")\n",
    "for word, weight in top_50_weighted:\n",
    "    print(f\"{word}: {weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a7ec60-2d73-4c68-948c-592970c4ad93",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df626ded-5e49-4a34-a3d1-315ba370226f",
   "metadata": {},
   "source": [
    "## Final Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0d0856a2-44f1-4304-b221-09a9ab72ad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_freq)\n",
    "\n",
    "removed_lowfreq_words_counts = joblib.load('removed_lowfreq_words_word_counts.pkl')  \n",
    "\n",
    "def select(lista):\n",
    "    for i in range(len(lista)):\n",
    "        x = lista[i][0]\n",
    "        y = lista[i][1]\n",
    "        y = [w for w in y if removed_lowfreq_words_counts.get(w, 0) >= 10]\n",
    "        lista[i] = [x, y]\n",
    "    return lista\n",
    "\n",
    "for data_path in preprocessed_files:\n",
    "    data = joblib.load(data_path)\n",
    "    data = select(data)\n",
    "    # Extract the filename\n",
    "    fname = os.path.basename(data_path)  # e.g., 'preprocessed_speeches_indexed1.pkl'\n",
    "    \n",
    "    # Add prefix and '_final' before '.pkl'\n",
    "    cleaned_name = f\"removed_lowfreq_words_{fname.replace('.pkl', '_final.pkl')}\"\n",
    "    \n",
    "    # Full path\n",
    "    cleaned_path = os.path.join(data_preprocessed, cleaned_name)\n",
    "    \n",
    "    joblib.dump(data, cleaned_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "605ff6af-7346-4c8e-9729-f7fcbda0f927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          filename  speech_length_final_removed_lowfreq_words\n",
      "0  ARG_01_1946.txt                                        339\n",
      "1  AUS_01_1946.txt                                        360\n",
      "2  BEL_01_1946.txt                                        269\n",
      "3  BLR_01_1946.txt                                        380\n",
      "4  BOL_01_1946.txt                                        142\n",
      "Total unique tokens across all final speeches: 9473\n",
      "Average tokens per final speech: 405.87783053323597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit\\\\data\\\\un_corpus_merged_removed_lowfreq_words.pkl']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(data_preprocessed)\n",
    "\n",
    "final_files = [\n",
    "    os.path.join(data_preprocessed, 'removed_lowfreq_words_preprocessed_speeches_indexed1_final.pkl'),\n",
    "    os.path.join(data_preprocessed, 'removed_lowfreq_words_preprocessed_speeches_indexed2_final.pkl'),\n",
    "    os.path.join(data_preprocessed, 'removed_lowfreq_words_preprocessed_speeches_indexed3_final.pkl'),\n",
    "    os.path.join(data_preprocessed, 'removed_lowfreq_words_preprocessed_speeches_indexed4_final.pkl')\n",
    "]\n",
    "\n",
    "final_data = []\n",
    "for fname in final_files:\n",
    "    final_data.extend(joblib.load(fname))\n",
    "\n",
    "df_final = pd.DataFrame(final_data, columns=[\"filename\", \"speech_final_removed_lowfreq_words\"])\n",
    "df_merged_removed_lowfreq_words = df_merged.merge(df_final, on=\"filename\", how=\"left\")\n",
    "\n",
    "\n",
    "df_merged_removed_lowfreq_words[\"speech_length_final_removed_lowfreq_words\"] = df_merged_removed_lowfreq_words[\"speech_final_removed_lowfreq_words\"].apply(\n",
    "    lambda x: len(x) if isinstance(x, list) else 0\n",
    ")\n",
    "\n",
    "print(df_merged_removed_lowfreq_words[[\"filename\", \"speech_length_final_removed_lowfreq_words\"]].head())\n",
    "\n",
    "all_tokens_final = [token for speech in df_merged_removed_lowfreq_words[\"speech_final_removed_lowfreq_words\"].dropna() for token in speech]\n",
    "unique_tokens_final = set(all_tokens_final)\n",
    "print(\"Total unique tokens across all final speeches:\", len(unique_tokens_final))\n",
    "\n",
    "print(\"Average tokens per final speech:\", df_merged_removed_lowfreq_words[\"speech_length_final_removed_lowfreq_words\"].mean())\n",
    "\n",
    "# Save final merged DataFrame\n",
    "joblib.dump(df_merged_removed_lowfreq_words, os.path.join(data_c, \"un_corpus_merged_removed_lowfreq_words.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cdf361-e904-4a69-a0e4-bea63a524dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSV\n",
    "df_merged_removed_lowfreq_words.to_csv(\n",
    "    os.path.join(data_c, \"un_corpus_merged_removed_lowfreq_words.csv\"),\n",
    "    sep=';',\n",
    "    index=False,\n",
    "    encoding='utf-8'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c268f5-4277-4321-89a5-436c67f729ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
