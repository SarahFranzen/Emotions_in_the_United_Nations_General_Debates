{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a39bd0b5-2ac1-499d-8938-2fdfe4c3d93d",
   "metadata": {},
   "source": [
    "# Development of Emotion and Reasoning in the General Speeches of the United Nations: A text-based machine learning approach\n",
    "## Script 1: Model Traning, Calculation Centroids & Speech Scoring\n",
    "## Author: Sarah Franzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6ca6f721-48ad-495c-8e5f-e60790225709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "from pathlib import Path\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "# from gensim.summarization.textcleaner import get_sentences\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from random import shuffle\n",
    "import nltk\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import joblib\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from string import punctuation\n",
    "from scipy.spatial.distance import cosine\n",
    "import glob\n",
    "import spacy\n",
    "from multiprocessing import Pool, freeze_support\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# === Initialize NLP Tools ===\n",
    "\n",
    "translator = str.maketrans('', '', punctuation) \n",
    "tagger = nltk.perceptron.PerceptronTagger()\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "# === Set Working Directory ===\n",
    "\n",
    "# Set your working directory (adjust this as needed)\n",
    "wd = Path(r\"C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\")\n",
    "wd_models = wd / \"models\"\n",
    "wd_results = wd / \"results\"\n",
    "\n",
    "\n",
    "# === Define Folder Paths ===\n",
    "\n",
    "# Make sure that you have these folders in your working directory\n",
    "data_c = wd / \"data\"\n",
    "data_temp = data_c / \"temp\"\n",
    "data_freq = data_c / \"freq\"\n",
    "data_dict = data_c / \"dictionaries\"\n",
    "data_sent = data_c / \"sentences\"\n",
    "data_preprocessed = data_c / \"preprocessed\"\n",
    "fig_dir = wd /\"fig\"\n",
    "\n",
    "# Upload ressources\n",
    "stopwords = joblib.load(data_c / \"stopwords.pkl\")\n",
    "word_counts = joblib.load(data_freq / \"word_counts.pkl\")\n",
    "word_counts_weighted = joblib.load(data_freq / \"word_counts_weighted.pkl\")\n",
    "affect_dic = joblib.load(data_dict / 'dictionary_affect.pkl')\n",
    "cognition_dic = joblib.load(data_dict / 'dictionary_cognition.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "125227ca-a0b5-43d7-956d-7d49a66f81b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_temp)\n",
    "cleaned_files = [\n",
    "    str(data_temp / 'clean_speeches_indexed1.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed2.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed3.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed4.pkl')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "af0997e8-9da3-45c3-bdd4-7d38a708a474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl processed\n",
      "Unique tokens: 10778\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed1.pkl saved\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl processed\n",
      "Unique tokens: 10793\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed2.pkl saved\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl processed\n",
      "Unique tokens: 10797\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed3.pkl saved\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl processed\n",
      "Unique tokens: 10803\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed4.pkl saved\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_sent)\n",
    "def extract_sentences(dataname):\n",
    "    data = joblib.load(dataname)\n",
    "    data = [a[1] for a in data]  # keep only text, no id\n",
    "\n",
    "    sentences = []\n",
    "    for doc in data:\n",
    "        sentences += sent_tokenize(doc)  # use nltk's sent_tokenize here\n",
    "\n",
    "    sentences = [item for item in sentences if len(item.split()) > 1]\n",
    "    sentences = [gensim.utils.simple_preprocess(item) for item in sentences]\n",
    "\n",
    "    sentences = [[a for a in s if not a.isdigit()] for s in sentences]\n",
    "    sentences = [[a for a in s if len(a) > 2] for s in sentences]\n",
    "\n",
    "    #texts = [\" \".join(s) for s in sentences if len(s) > 0]\n",
    "    #docs = list(nlp.pipe(texts, batch_size=50, n_process=1))\n",
    "    #sentences = [\n",
    "     #   [tok.text for tok in doc if tok.tag_.startswith((\"N\", \"V\", \"J\"))]\n",
    "       # for doc in docs\n",
    "   # ]\n",
    "\n",
    "    sentences = [tagger.tag(s) for s in sentences]\n",
    "    sentences = [[i[0] for i in s if i[1].startswith(('N', 'V', 'J'))] for s in sentences]\n",
    "\n",
    "    sentences = [[stemmer.stem(i) for i in s] for s in sentences]\n",
    "    sentences = [[a for a in s if a not in stopwords] for s in sentences]\n",
    "    sentences = [[a for a in s if word_counts[a] >= 10] for s in sentences]\n",
    "\n",
    "    sentences = [s for s in sentences if len(s) > 1]  # eliminate empty\n",
    "    shuffle(sentences)\n",
    "\n",
    "    lab = dataname.replace('clean_speeches_', 'sentences_').replace('_.pkl', '.pkl')\n",
    "    print(f'{dataname} processed')\n",
    "    joblib.dump(sentences, lab)\n",
    "    \n",
    "    unique_tokens = set(token for s in sentences for token in s)\n",
    "    print(f\"Unique tokens: {len(unique_tokens)}\")\n",
    "\n",
    "    print(f'{lab} saved')\n",
    "\n",
    "# Run for all your files\n",
    "for fname in cleaned_files:\n",
    "    extract_sentences(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fddaea83-7d1f-4b14-85b9-401b2e727c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_files = [\n",
    "    os.path.join(data_temp, 'sentences_indexed1.pkl'),\n",
    "    os.path.join(data_temp, 'sentences_indexed2.pkl'),\n",
    "    os.path.join(data_temp, 'sentences_indexed3.pkl'),\n",
    "    os.path.join(data_temp, 'sentences_indexed4.pkl')\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c41580ba-2954-4d84-a574-e9c3d4c8ef1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens across all files: 10932\n"
     ]
    }
   ],
   "source": [
    "# == Get sum of unique tokens\n",
    "all_unique_tokens = set()\n",
    "\n",
    "for dataname in sentences_files:\n",
    "    data = joblib.load(dataname)  # load list of tokenized sentences\n",
    "    for sentence in data:\n",
    "        all_unique_tokens.update(sentence)  # add tokens to the set\n",
    "\n",
    "print(f\"Total unique tokens across all files: {len(all_unique_tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4f1a2e83-e5f4-436b-84c9-0d9d51f3b761",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_files = [\n",
    "    'sentences_indexed1.pkl',\n",
    "    'sentences_indexed2.pkl',\n",
    "    'sentences_indexed3.pkl',\n",
    "    'sentences_indexed4.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "465bc73c-70b6-4dbf-a379-8c5235f83a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for dataname in sentences_files:  # <-- your list of sentence files\n",
    "    data = joblib.load(dataname)\n",
    "    dataset.extend(data)  # extend instead of append if you want all sentences in a single list\n",
    "\n",
    "# === Model training ===\n",
    "w2v = Word2Vec(\n",
    "    sentences=dataset,    # iterator that loops over tokenized sentences\n",
    "    vector_size=300,      # Word vector dimensionality (use `vector_size` in newer gensim)\n",
    "    window=8,             # Context window size\n",
    "    min_count=10,         # Minimum word count\n",
    "    workers=8,            # Number of threads\n",
    "    sample=1e-3,          # Downsample setting for frequent words\n",
    "    epochs=10             # Number of iterations over the corpus\n",
    ")\n",
    "\n",
    "# Optimize memory usage (optional)\n",
    "w2v.wv.fill_norms()  # only works in older gensim versions\n",
    "\n",
    "# Save model\n",
    "wd_models.mkdir(parents=True, exist_ok=True)  # create folder if it doesn't exist\n",
    "w2v.save(str(wd_models / 'w2v-vectors_8_300.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9d94142c-1e7f-4b0b-bfec-98604fc84304",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec.load(str(wd_models / \"w2v-vectors_8_300.pkl\"))\n",
    "word_vectors = w2v.wv # Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5b75bd3b-2957-42d1-a89c-89e5b3ea44f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['centroids/cog_centroid.pkl']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Calculate centroids ===\n",
    "\n",
    "def findcentroid(text, model):\n",
    "    vecs = [model.wv[w] * word_counts_weighted[w] for w in text if w in model.wv]\n",
    "    vecs = [v for v in vecs if len(v) > 0]\n",
    "    centroid = np.mean(vecs, axis=0)\n",
    "    #centroid = centroid.reshape(1, -1)\n",
    "    return centroid\n",
    "\n",
    "\n",
    "affect_centroid = findcentroid(affect_dic, w2v)\n",
    "cog_centroid = findcentroid(cognition_dic, w2v)\n",
    "\n",
    "os.chdir(data_c)\n",
    "joblib.dump(affect_centroid, 'centroids/affect_centroid.pkl')\n",
    "joblib.dump(cog_centroid, 'centroids/cog_centroid.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bcab4536-fca7-468d-ae93-6e8da17cbfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affect centroid vector:\n",
      " [-1.15326650e-01  8.53892416e-02  6.89750463e-02  3.35071951e-01\n",
      "  1.81380063e-01  2.68096685e-01  3.98819707e-02  1.96546853e-01\n",
      " -9.14333984e-02 -2.41713122e-01  1.56426936e-01 -7.17874244e-02\n",
      " -4.57372889e-03  2.90141664e-02 -1.69361264e-01  4.43692505e-02\n",
      "  1.97134987e-01 -9.84080583e-02  2.52814472e-01  1.23981386e-01\n",
      " -2.11398065e-01  2.80269414e-01  1.02628767e-01 -8.45284089e-02\n",
      "  5.95363736e-01 -1.14517286e-01 -2.69610167e-01  9.41117033e-02\n",
      "  8.64352360e-02  3.87301557e-02 -1.90139443e-01  3.38734873e-02\n",
      " -2.83255130e-01  1.48426712e-01  4.06959057e-02  2.75360882e-01\n",
      " -3.97043601e-02 -4.20348316e-01  3.19102794e-01 -4.82898485e-03\n",
      "  6.35465607e-02  1.95765793e-01  2.40678415e-01  3.07683907e-02\n",
      "  2.62121528e-01  2.93513566e-01 -2.47121364e-01 -2.03618541e-01\n",
      "  4.83783260e-02  3.70993376e-01 -6.17285147e-02  1.95195347e-01\n",
      " -8.31152871e-02  1.55983269e-01  7.87725970e-02  2.36506969e-01\n",
      "  2.99298167e-01  1.23046607e-01  2.30309129e-01  1.92827985e-01\n",
      " -9.40576494e-02  1.22534327e-01 -2.95517836e-02 -1.21202422e-02\n",
      "  1.70207143e-01  4.78992090e-02 -5.12165837e-02  2.52866238e-01\n",
      "  1.95341542e-01  1.39433980e-01 -5.47445603e-02  1.02291390e-01\n",
      "  2.30517313e-01 -1.70463458e-01  1.69800341e-01 -1.35856599e-01\n",
      " -2.55832285e-01  1.53982770e-02 -1.06617231e-02 -2.17319503e-02\n",
      " -2.49431059e-01 -1.25946820e-01 -2.44641602e-01  1.59104988e-02\n",
      "  2.05978453e-01  3.42838168e-02 -3.85969207e-02  1.31307751e-01\n",
      "  1.82811528e-01 -1.28378451e-01 -2.39805356e-02 -1.99138552e-01\n",
      "  7.28948638e-02 -7.60898069e-02  1.84571326e-01  1.74907833e-01\n",
      "  1.84409320e-01 -8.02419037e-02 -2.25781426e-01  4.07256514e-01\n",
      "  1.19390070e-01 -2.70215631e-01 -7.50576258e-02 -7.17256516e-02\n",
      "  8.79935399e-02 -1.81510791e-01  2.53924914e-02 -9.09256339e-02\n",
      " -1.75053865e-01  2.48277858e-02 -1.41725808e-01 -3.46298933e-01\n",
      "  2.03493044e-01  2.08202004e-01 -1.85001642e-01  2.09324241e-01\n",
      "  1.39466673e-01 -1.04864746e-01 -2.98457723e-02 -1.79461598e-01\n",
      " -7.14908764e-02  2.28713900e-02 -1.56011283e-01  2.63357401e-01\n",
      " -1.15112970e-02  5.50648831e-02  1.60956249e-01 -1.41940400e-01\n",
      " -1.34159001e-02 -3.89095657e-02 -2.51465559e-01 -4.80697542e-01\n",
      "  2.17660647e-02 -3.80046549e-03  7.17837587e-02 -1.71103194e-01\n",
      "  7.89752826e-02  1.07090421e-01  8.17973912e-02 -2.37896740e-01\n",
      "  5.22748902e-02 -6.44538701e-02 -3.72424535e-02  3.50133628e-01\n",
      " -6.06604591e-02 -6.16919994e-02 -3.51923466e-01 -7.31191933e-02\n",
      " -7.54560679e-02  5.92179224e-02 -2.63329335e-02 -6.70651495e-02\n",
      " -2.23660216e-01 -5.98345958e-02 -8.64336267e-02  1.39386738e-02\n",
      " -5.50539158e-02 -4.74875778e-01  2.47418076e-01 -2.17372850e-01\n",
      "  6.38099834e-02  1.46087974e-01 -1.03610970e-01 -3.00998867e-01\n",
      " -3.65088224e-01  1.43271312e-01 -1.23102672e-01 -1.21406175e-01\n",
      " -1.71056613e-02  6.38041049e-02 -5.87299429e-02  2.89656103e-01\n",
      "  2.16764212e-01  1.03716224e-01  3.12847108e-01 -1.34404227e-01\n",
      " -1.00719661e-01 -5.91693297e-02 -2.87229359e-01 -1.84687033e-01\n",
      " -9.56656262e-02  6.76897615e-02  2.82629058e-02  1.29663348e-01\n",
      " -1.22167394e-01 -1.04454637e-01 -1.74359739e-01 -1.11755617e-01\n",
      "  2.32209876e-01 -2.25849181e-01 -7.69850463e-02  3.06101263e-01\n",
      "  4.15448219e-01  2.60186046e-01 -1.46637231e-01 -9.53956917e-02\n",
      " -4.99192160e-03 -1.14134990e-01  7.32597411e-02  7.05042258e-02\n",
      " -2.03010589e-01  1.81022212e-01  1.32499158e-03  2.86469549e-01\n",
      "  3.56774703e-02 -2.01328516e-01 -1.05033405e-01  8.34161490e-02\n",
      " -2.39076391e-02 -4.34995294e-01 -3.17351907e-01 -5.83128911e-03\n",
      "  3.70582216e-03  4.28363532e-02  6.44645328e-03 -3.17459494e-01\n",
      "  5.69865219e-02  9.26302224e-02 -3.20910990e-01 -4.11130004e-02\n",
      " -5.60707692e-03 -8.38544592e-02 -2.61744112e-01  3.67454556e-03\n",
      " -1.14050947e-01 -1.13669550e-02  2.28586376e-01  2.03663364e-01\n",
      " -3.84104520e-01  1.81445628e-01  9.23760533e-02 -1.90871149e-01\n",
      " -2.51611710e-01 -6.37604157e-04 -2.15477377e-01  1.21251889e-01\n",
      " -2.78834607e-02 -1.51485786e-01  1.00043923e-01 -6.72362894e-02\n",
      " -3.89173776e-01  4.10921983e-02  6.03940524e-02  2.54087627e-01\n",
      " -1.74959615e-01 -7.84900263e-02 -3.18845585e-02 -1.66618988e-01\n",
      "  8.52521136e-02  2.93241799e-01  4.90326136e-01  1.33900687e-01\n",
      "  1.67300478e-02  3.85701098e-02 -2.99104899e-01  2.05782354e-01\n",
      "  1.73744470e-01  7.69966096e-02 -1.79590970e-01 -2.75497645e-01\n",
      " -8.79053771e-02  3.75238240e-01 -1.01374984e-01  1.32453904e-01\n",
      " -1.29795447e-01 -5.69147840e-02 -2.18173116e-01 -5.17089032e-02\n",
      " -1.00859344e-01 -1.32980242e-01  1.52923074e-02  2.09840328e-01\n",
      " -3.00817359e-02  1.16667718e-01  8.25100942e-05  3.89074057e-01\n",
      " -4.18747105e-02  4.41753715e-01  1.83326304e-01  1.04414634e-01\n",
      "  1.59714252e-01 -1.23861460e-02 -4.91661299e-03  6.66862950e-02\n",
      "  1.88490644e-01  2.90018439e-01 -1.99861228e-01  1.31533951e-01\n",
      " -1.59380715e-02  1.16470434e-01 -2.30356053e-01  3.06016624e-01\n",
      " -9.26478282e-02  2.77416021e-01  2.54579067e-01  1.73179477e-01\n",
      " -2.88053662e-01 -3.37962687e-01 -2.76749372e-01  1.27783149e-01]\n",
      "\n",
      "Cognition centroid vector:\n",
      " [-5.88319525e-02  1.50912568e-01  1.05234429e-01  2.13846937e-01\n",
      "  5.76002151e-03  1.60374038e-04  7.60983676e-02  2.46842861e-01\n",
      "  3.15387361e-02 -1.85984761e-01  2.25134362e-02  1.21964976e-01\n",
      "  2.42110968e-01  1.37160961e-02 -5.11092283e-02 -5.10991849e-02\n",
      " -1.75570741e-01 -1.10852078e-01  1.22875780e-01  2.17586890e-01\n",
      " -3.66435610e-02 -3.86376753e-02  6.02309145e-02 -2.27332842e-02\n",
      "  1.14946708e-01 -1.43809825e-01  3.63612063e-02 -1.58858046e-01\n",
      "  4.58003432e-02 -7.98035488e-02 -1.64329279e-02 -1.00082785e-01\n",
      " -2.89592240e-02 -1.60012811e-01 -1.26948699e-01  1.81150422e-01\n",
      "  7.27350190e-02 -1.69116035e-01  5.86291887e-02  1.49466619e-01\n",
      " -1.28986567e-01  9.08189639e-02  8.48838091e-02 -8.87328237e-02\n",
      "  3.93932052e-02 -7.61405677e-02  7.62432395e-03 -5.51429987e-02\n",
      " -1.39817931e-02 -1.39609519e-02  6.08476773e-02 -1.99725362e-03\n",
      "  7.45189609e-03 -7.53068328e-02  1.02732375e-01  1.05039358e-01\n",
      "  3.11965525e-01 -1.52724758e-01 -1.46273106e-01  2.09933624e-01\n",
      " -4.66697693e-01  5.57387322e-02  6.94873631e-02  1.72937885e-01\n",
      "  2.10350096e-01  2.53019094e-01 -8.24733227e-02  2.23485790e-02\n",
      "  2.22443074e-01 -2.11401321e-02 -7.85282552e-02  1.10950470e-01\n",
      "  1.68118700e-01  4.69990708e-02  4.24070545e-02  5.14744148e-02\n",
      " -2.09394470e-01 -1.23372331e-01  1.38899967e-01  1.68224171e-01\n",
      " -9.52683687e-02 -5.90498596e-02 -1.51080608e-01 -1.41240418e-01\n",
      "  8.31052847e-03  9.15852413e-02  7.91029632e-03 -3.45935598e-02\n",
      "  2.17927366e-01  4.38572693e-04 -6.03594445e-03 -1.32237002e-01\n",
      "  1.72062144e-01  2.55513698e-01  4.29230183e-02  5.16380891e-02\n",
      "  7.53768608e-02  9.10004377e-02 -2.64072359e-01  8.71171281e-02\n",
      " -2.02648848e-01 -1.62337542e-01  9.12292451e-02 -9.32637304e-02\n",
      " -6.65518716e-02  1.13526903e-01 -1.71032980e-01  5.86694758e-03\n",
      "  1.54725075e-01  9.82111171e-02  4.49533910e-02 -3.02038759e-01\n",
      " -3.38135883e-02  2.33165056e-01 -1.76353514e-01 -9.64908302e-02\n",
      " -1.16847523e-01 -1.37579337e-01  8.90052691e-02 -2.12354325e-02\n",
      " -4.39972319e-02 -8.91320815e-04  8.58771652e-02  2.74676263e-01\n",
      " -3.06745060e-02  2.41079107e-01  5.69716729e-02  1.21994846e-01\n",
      " -5.73544428e-02 -1.09561630e-01 -1.55120343e-01 -5.15427627e-02\n",
      " -2.20652632e-02  2.07543686e-01  3.59116271e-02 -2.83172578e-01\n",
      " -5.51986769e-02  1.24446843e-02 -5.00308909e-02 -1.63951844e-01\n",
      " -1.01026379e-01  4.18441817e-02  1.67154986e-02  2.45183930e-02\n",
      " -2.92618237e-02  5.42988814e-02 -5.08440658e-02  2.49119028e-01\n",
      "  1.22427687e-01  8.68309811e-02 -5.91386482e-02 -5.56416102e-02\n",
      "  2.84627117e-02 -2.66205873e-02 -1.34759113e-01  1.44547716e-01\n",
      "  7.81852081e-02 -1.86115995e-01  2.36067981e-01  1.53744454e-02\n",
      " -8.92883614e-02  8.19946602e-02 -1.21710233e-01 -2.04801172e-01\n",
      " -3.72316062e-01 -3.11648175e-02 -7.51587600e-02 -3.25112194e-02\n",
      "  1.47348568e-01  1.53187856e-01  6.68422654e-02 -1.20292865e-01\n",
      "  2.55550861e-01 -4.73745726e-02  5.12557179e-02 -5.67079009e-03\n",
      "  4.15139757e-02 -2.00858325e-01 -4.44622599e-02  1.58292606e-01\n",
      " -5.12735806e-02 -1.67727336e-01  5.09603648e-04 -3.35444398e-02\n",
      "  1.60584658e-01 -8.96427557e-02 -1.99414119e-01  1.40284330e-01\n",
      " -1.97622068e-02 -1.60498276e-01 -1.75801769e-01 -1.99141294e-01\n",
      " -2.48363730e-03 -2.95239687e-02  1.99499160e-01  6.44048676e-02\n",
      " -1.83616951e-01  6.96329102e-02  1.00032531e-01 -1.24057732e-03\n",
      "  3.24303173e-02  9.62794274e-02  2.10489649e-02  1.83144242e-01\n",
      " -4.13752645e-02  1.73305236e-02 -2.19701841e-01 -1.18116349e-01\n",
      "  6.73872381e-02 -2.35080019e-01 -1.50893498e-02  1.77346975e-01\n",
      "  7.99185485e-02  2.14774206e-01  5.21430895e-02  5.52496240e-02\n",
      " -7.29686543e-02  1.85277671e-01 -1.21790163e-01  6.78786188e-02\n",
      "  4.52279411e-02  7.87835568e-02 -4.12271954e-02  1.51287600e-01\n",
      " -3.30988884e-01  9.72311348e-02  9.98274833e-02 -2.99296379e-02\n",
      " -2.41209287e-02  2.52589852e-01  9.72909927e-02 -1.29692465e-01\n",
      " -1.11186728e-01  3.17023434e-02 -7.41513744e-02 -4.31519933e-02\n",
      "  8.48471094e-03 -1.82019398e-01  5.85858375e-02 -1.21416301e-02\n",
      "  2.15202607e-02  4.61425111e-02 -7.76963979e-02 -9.18032601e-02\n",
      " -3.12280715e-01  1.37126431e-01 -2.78524607e-01  6.95924158e-04\n",
      "  9.67091024e-02  1.40853291e-02  1.10088304e-01  1.42711908e-01\n",
      "  4.26320098e-02  1.13213724e-02  1.13259286e-01  2.13465929e-01\n",
      " -1.58278003e-01  7.07122386e-02 -8.32316950e-02 -9.13710222e-02\n",
      " -1.05392046e-01  1.97419554e-01 -2.47860640e-01 -1.30083770e-01\n",
      "  4.88671698e-02 -1.18265510e-01 -1.80094078e-01  2.95774284e-04\n",
      " -9.90717486e-02  7.40707433e-03 -9.27614979e-03  1.52153671e-02\n",
      " -6.07058443e-02 -7.09732622e-02 -7.67645950e-04 -1.33466691e-01\n",
      "  5.16034197e-03  7.00125024e-02  1.24222226e-01  1.36739463e-01\n",
      "  1.17551953e-01 -1.28403172e-01  1.08156487e-01  5.02968654e-02\n",
      "  8.90146941e-02  1.14634939e-01 -4.55125198e-02  6.27790913e-02\n",
      "  1.45042744e-02 -6.14795536e-02 -1.53316841e-01 -7.84190837e-03\n",
      " -1.26907736e-01 -1.39436703e-02  1.53954431e-01  2.52507210e-01\n",
      " -1.29403889e-01 -1.62682787e-01 -4.91647385e-02  2.45504633e-01]\n",
      "\n",
      "Shape of affect centroid: (300,)\n",
      "Shape of cognition centroid: (300,)\n",
      "\n",
      "Affect centroid summary: min, max, mean: -0.48069754 0.59536374 0.0058688787\n",
      "Cognition centroid summary: min, max, mean: -0.4666977 0.31196553 0.0021264204\n"
     ]
    }
   ],
   "source": [
    "# Print the vectors\n",
    "print(\"Affect centroid vector:\\n\", affect_centroid)\n",
    "print(\"\\nCognition centroid vector:\\n\", cog_centroid)\n",
    "\n",
    "# Optional: shape and stats\n",
    "print(\"\\nShape of affect centroid:\", affect_centroid.shape)\n",
    "print(\"Shape of cognition centroid:\", cog_centroid.shape)\n",
    "\n",
    "print(\"\\nAffect centroid summary: min, max, mean:\", \n",
    "      np.min(affect_centroid), np.max(affect_centroid), np.mean(affect_centroid))\n",
    "print(\"Cognition centroid summary: min, max, mean:\", \n",
    "      np.min(cog_centroid), np.max(cog_centroid), np.mean(cog_centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c8316f-f794-4bcd-87c9-578aa82f88b0",
   "metadata": {},
   "source": [
    "## Emotionality Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3376a4f6-a002-4179-8614-dfef96113180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISSUE why doesnt this work with the sentences?!\n",
    "\n",
    "# Set wd to data_preprocessed\n",
    "os.chdir(data_preprocessed)\n",
    "\n",
    "# === Load preprocessed speech data ===\n",
    "\n",
    "preprocessed_final_files = [\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed1_final.pkl')),\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed2_final.pkl')),\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed3_final.pkl')),\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed4_final.pkl'))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1cbd1f95-c7f4-41f1-a292-542ee34ae882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit\\\\data\\\\distances_10epochs.pkl']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################################\n",
    "# Define Functions              ###\n",
    "###################################\n",
    "\n",
    "# apparently is missing deleting intermediate files\n",
    "\n",
    "def documentvecweight(lista):\n",
    "    out = []\n",
    "    lista = [i for i in lista if len(i[1]) > 0]\n",
    "    for s in lista:\n",
    "        vecs = [w2v.wv[w] * word_counts_weighted[w] for w in s[1] if w in w2v.wv]\n",
    "        if len(vecs) == 0:\n",
    "            a = np.nan\n",
    "            c = np.nan\n",
    "            score = np.nan\n",
    "        else:\n",
    "            v = np.mean(vecs, axis=0)\n",
    "            a = cosine(v, affect_centroid)\n",
    "            c = cosine(v, cog_centroid)\n",
    "            score = (1 + 1 - a) / (1 + 1 - c)\n",
    "        out.append([s[0], a, c, score])\n",
    "    return out\n",
    "\n",
    "\n",
    "def main_function(file_path, idx):\n",
    "    dataset = joblib.load(file_path)\n",
    "    data = documentvecweight(dataset)\n",
    "    lab = os.path.join(data_c, f'temp_distances_main_{idx}.pkl')\n",
    "    joblib.dump(data, lab)\n",
    "\n",
    "\n",
    "###################################\n",
    "#      Run main directly        ###\n",
    "###################################\n",
    "\n",
    "def main():\n",
    "    files = [\n",
    "        os.path.join(data_preprocessed, f'preprocessed_speeches_indexed{i+1}_final.pkl') #Changed!\n",
    "        for i in range(4)\n",
    "    ]\n",
    "    for i, f in enumerate(files, start=1):\n",
    "        main_function(f, i)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "###################################\n",
    "#      Recompose everything     ###\n",
    "###################################\n",
    "\n",
    "DATA_temp = [os.path.join(data_c, f'temp_distances_main_{i+1}.pkl') for i in range(4)]\n",
    "\n",
    "tot = []\n",
    "for dataname in DATA_temp:\n",
    "    d = joblib.load(dataname)\n",
    "    tot += d\n",
    "\n",
    "tot_df = pd.DataFrame(tot, columns=['filename', 'affect_d', 'cognition_d', 'score'])\n",
    "joblib.dump(tot_df, os.path.join(data_c, 'distances_10epochs.pkl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fb955ab4-243d-470d-a967-e0af1e474c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          filename  affect_d  cognition_d     score\n",
      "0  COL_03_1948.txt  1.159868     0.600979  0.600514\n",
      "1  HTI_47_1992.txt  0.474285     0.886777  1.370539\n",
      "2  LKA_46_1991.txt  1.416345     1.189522  0.720137\n",
      "3  HND_05_1950.txt  0.747536     0.270929  0.724357\n",
      "4  GHA_25_1970.txt  1.368114     0.735999  0.499909\n",
      "Shape: (10760, 4)\n",
      "           affect_d   cognition_d         score\n",
      "count  10760.000000  10760.000000  10760.000000\n",
      "mean       1.196353      0.909197      0.738105\n",
      "std        0.259407      0.201238      0.199796\n",
      "min        0.293323      0.262322      0.310588\n",
      "25%        1.032945      0.769638      0.587802\n",
      "50%        1.235943      0.924176      0.712655\n",
      "75%        1.392101      1.065732      0.868452\n",
      "max        1.741876      1.455069      1.613793\n",
      "              filename  affect_d  cognition_d     score\n",
      "0      COL_03_1948.txt  1.159868     0.600979  0.600514\n",
      "1      HTI_47_1992.txt  0.474285     0.886777  1.370539\n",
      "2      LKA_46_1991.txt  1.416345     1.189522  0.720137\n",
      "3      HND_05_1950.txt  0.747536     0.270929  0.724357\n",
      "4      GHA_25_1970.txt  1.368114     0.735999  0.499909\n",
      "...                ...       ...          ...       ...\n",
      "10755  SLV_66_2011.txt  0.668358     0.701658  1.025648\n",
      "10756  AUT_23_1968.txt  1.509631     0.904342  0.447556\n",
      "10757  CIV_24_1969.txt  0.446327     0.626288  1.131003\n",
      "10758  CYP_68_2013.txt  1.432286     0.850868  0.494037\n",
      "10759  MWI_34_1979.txt  1.512555     0.978339  0.477110\n",
      "\n",
      "[10760 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Print the first few rows\n",
    "print(tot_df.head())\n",
    "\n",
    "# Optionally, print the shape to see how many documents were processed\n",
    "print(\"Shape:\", tot_df.shape)\n",
    "\n",
    "# Print a quick summary\n",
    "print(tot_df.describe())\n",
    "\n",
    "# Or print the full DataFrame (if small)\n",
    "print(tot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8c295cd4-4fa0-470f-b67c-4f584f7a3f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your main corpus CSV \n",
    "un_corpus_merged = pd.read_csv(os.path.join(data_c, \"un_corpus_merged.csv\"), sep=';', encoding='utf-8') \n",
    "# Merge on filename \n",
    "un_corpus_scored = un_corpus_merged.merge(tot_df, on=\"filename\", how=\"left\")\n",
    "\n",
    "# Optionally save back as pickle\n",
    "joblib.dump(un_corpus_scored, os.path.join(data_c, \"un_corpus_scored.pkl\"))\n",
    "\n",
    "un_corpus_scored.to_csv(\n",
    "    os.path.join(data_c, \"un_corpus_scored.csv\"),\n",
    "    sep=';', \n",
    "    index=False, \n",
    "    encoding='utf-8'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d8d13667-1eb3-482e-83ca-5969a229750a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              filename                                             speech  \\\n",
      "0      COL_03_1948.txt  Mr. URDANETA-ARBELAEZ declared that when the s...   \n",
      "1      HTI_47_1992.txt  In the name of the \\npeople of Haiti, I am hap...   \n",
      "2      LKA_46_1991.txt  ﻿I have the honour to convey to you, Sir, and ...   \n",
      "3      HND_05_1950.txt  It is indeed difficult to find words which do ...   \n",
      "4      GHA_25_1970.txt  121.\\t I should like to begin by congratulatin...   \n",
      "...                ...                                                ...   \n",
      "10755  SLV_66_2011.txt  \\nThis new session of the Assembly of the Unit...   \n",
      "10756  AUT_23_1968.txt  1. May I ask you, Sir, to convey to the Presid...   \n",
      "10757  CIV_24_1969.txt  83. Permit me to take this opportunity to pay ...   \n",
      "10758  CYP_68_2013.txt  It is a great honour to \\naddress the General ...   \n",
      "10759  MWI_34_1979.txt  ﻿Allow me, Mr. President, first to join the re...   \n",
      "\n",
      "      country_code  year   country_name  speech_length_words  \\\n",
      "0              COL  1948       Colombia                 1795   \n",
      "1              HTI  1992          Haiti                 3432   \n",
      "2              LKA  1991      Sri Lanka                 2434   \n",
      "3              HND  1950       Honduras                 1111   \n",
      "4              GHA  1970          Ghana                 4031   \n",
      "...            ...   ...            ...                  ...   \n",
      "10755          SLV  2011    El Salvador                 2190   \n",
      "10756          AUT  1968        Austria                 3594   \n",
      "10757          CIV  1969  Côte d'Ivoire                 2746   \n",
      "10758          CYP  2013         Cyprus                 2052   \n",
      "10759          MWI  1979         Malawi                 3190   \n",
      "\n",
      "       english_official_language  security_council_permanent  \\\n",
      "0                              0                           0   \n",
      "1                              0                           0   \n",
      "2                              0                           0   \n",
      "3                              0                           0   \n",
      "4                              1                           0   \n",
      "...                          ...                         ...   \n",
      "10755                          0                           0   \n",
      "10756                          0                           0   \n",
      "10757                          0                           0   \n",
      "10758                          0                           0   \n",
      "10759                          1                           0   \n",
      "\n",
      "                          speaker_name           position  gender_dummy  \\\n",
      "0               Mr. URDANETA-ARBELAEZ                 NaN           0.0   \n",
      "1                             ARISTIDE  (Vice-) President           NaN   \n",
      "2                              Kalpage                NaN           NaN   \n",
      "3                           Mr. CARIAS                NaN           0.0   \n",
      "4                            Mr. OWUSU                NaN           0.0   \n",
      "...                                ...                ...           ...   \n",
      "10755  Carlos Mauricio Funes Cartagena  (Vice-) President           NaN   \n",
      "10756                    Mr. WALDHEIM                 NaN           0.0   \n",
      "10757                        Mr. USHER                NaN           0.0   \n",
      "10758               Nicos Anastasiades  (Vice-) President           NaN   \n",
      "10759                          Matenje                NaN           NaN   \n",
      "\n",
      "               speech_label  affect_d  cognition_d     score  \n",
      "0           Colombia (1948)  1.159868     0.600979  0.600514  \n",
      "1              Haiti (1992)  0.474285     0.886777  1.370539  \n",
      "2          Sri Lanka (1991)  1.416345     1.189522  0.720137  \n",
      "3           Honduras (1950)  0.747536     0.270929  0.724357  \n",
      "4              Ghana (1970)  1.368114     0.735999  0.499909  \n",
      "...                     ...       ...          ...       ...  \n",
      "10755    El Salvador (2011)  0.668358     0.701658  1.025648  \n",
      "10756        Austria (1968)  1.509631     0.904342  0.447556  \n",
      "10757  Côte d'Ivoire (1969)  0.446327     0.626288  1.131003  \n",
      "10758         Cyprus (2013)  1.432286     0.850868  0.494037  \n",
      "10759         Malawi (1979)  1.512555     0.978339  0.477110  \n",
      "\n",
      "[10760 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "print(un_corpus_scored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1210b88f-cf13-4da0-807b-77764cf0d439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count where affect_d is NaN: 0\n",
      "Count where affect_d is not NaN: 10760\n"
     ]
    }
   ],
   "source": [
    "# Count where affect_d is NaN\n",
    "nan_count = un_corpus_scored['affect_d'].isna().sum()\n",
    "\n",
    "# Count where affect_d is not NaN\n",
    "not_nan_count = un_corpus_scored['affect_d'].notna().sum()\n",
    "\n",
    "print(\"Count where affect_d is NaN:\", nan_count)\n",
    "print(\"Count where affect_d is not NaN:\", not_nan_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594b17bd-15b6-4897-ab94-9b12175e7697",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
