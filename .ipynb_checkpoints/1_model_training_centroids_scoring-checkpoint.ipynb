{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ca6f721-48ad-495c-8e5f-e60790225709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "from pathlib import Path\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "# from gensim.summarization.textcleaner import get_sentences\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from random import shuffle\n",
    "import nltk\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import joblib\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from string import punctuation\n",
    "from scipy.spatial.distance import cosine\n",
    "import glob\n",
    "import spacy\n",
    "from multiprocessing import Pool, freeze_support\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# === Initialize NLP Tools ===\n",
    "\n",
    "translator = str.maketrans('', '', punctuation) \n",
    "tagger = nltk.perceptron.PerceptronTagger()\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "# === Set Working Directory ===\n",
    "\n",
    "# Set your working directory (adjust this as needed)\n",
    "wd = Path(r\"C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\")\n",
    "wd_models = wd / \"models\"\n",
    "wd_results = wd / \"results\"\n",
    "\n",
    "\n",
    "# === Define Folder Paths ===\n",
    "\n",
    "# Make sure that you have these folders in your working directory\n",
    "data_c = wd / \"data\"\n",
    "data_temp = data_c / \"temp\"\n",
    "data_freq = data_c / \"freq\"\n",
    "data_dict = data_c / \"dictionaries\"\n",
    "data_sent = data_c / \"sentences\"\n",
    "data_preprocessed = data_c / \"preprocessed\"\n",
    "fig_dir = wd /\"fig\"\n",
    "\n",
    "# Upload ressources\n",
    "stopwords = joblib.load(data_c / \"stopwords.pkl\")\n",
    "word_counts = joblib.load(data_freq / \"word_counts_stemmed.pkl\")\n",
    "word_counts_weighted = joblib.load(data_freq / \"word_counts_weighted.pkl\")\n",
    "affect_dic = joblib.load(data_dict / 'dictionary_affect.pkl')\n",
    "cognition_dic = joblib.load(data_dict / 'dictionary_cognition.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "125227ca-a0b5-43d7-956d-7d49a66f81b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_temp)\n",
    "cleaned_files = [\n",
    "    str(data_temp / 'clean_speeches_indexed1.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed2.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed3.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed4.pkl')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af0997e8-9da3-45c3-bdd4-7d38a708a474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl processed\n",
      "Unique tokens: 4500\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed1.pkl saved\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl processed\n",
      "Unique tokens: 4502\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed2.pkl saved\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl processed\n",
      "Unique tokens: 4484\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed3.pkl saved\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl processed\n",
      "Unique tokens: 4518\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed4.pkl saved\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_sent)\n",
    "def extract_sentences(dataname):\n",
    "    data = joblib.load(dataname)\n",
    "    data = [a[1] for a in data]  # keep only text, no id\n",
    "\n",
    "    sentences = []\n",
    "    for doc in data:\n",
    "        sentences += sent_tokenize(doc)  # use nltk's sent_tokenize here\n",
    "\n",
    "    sentences = [item for item in sentences if len(item.split()) > 1]\n",
    "    sentences = [gensim.utils.simple_preprocess(item) for item in sentences]\n",
    "\n",
    "    sentences = [[a for a in s if not a.isdigit()] for s in sentences]\n",
    "    sentences = [[a for a in s if len(a) > 2] for s in sentences]\n",
    "\n",
    "    #texts = [\" \".join(s) for s in sentences if len(s) > 0]\n",
    "    #docs = list(nlp.pipe(texts, batch_size=50, n_process=1))\n",
    "    #sentences = [\n",
    "     #   [tok.text for tok in doc if tok.tag_.startswith((\"N\", \"V\", \"J\"))]\n",
    "       # for doc in docs\n",
    "   # ]\n",
    "\n",
    "    sentences = [tagger.tag(s) for s in sentences]\n",
    "    sentences = [[i[0] for i in s if i[1].startswith(('N', 'V', 'J'))] for s in sentences]\n",
    "\n",
    "    sentences = [[stemmer.stem(i) for i in s] for s in sentences]\n",
    "    sentences = [[a for a in s if a not in stopwords] for s in sentences]\n",
    "    sentences = [[a for a in s if word_counts[a] >= 10] for s in sentences]\n",
    "\n",
    "    sentences = [s for s in sentences if len(s) > 1]  # eliminate empty\n",
    "    shuffle(sentences)\n",
    "\n",
    "    lab = dataname.replace('clean_speeches_', 'sentences_').replace('_.pkl', '.pkl')\n",
    "    print(f'{dataname} processed')\n",
    "    joblib.dump(sentences, lab)\n",
    "    \n",
    "    unique_tokens = set(token for s in sentences for token in s)\n",
    "    print(f\"Unique tokens: {len(unique_tokens)}\")\n",
    "\n",
    "    print(f'{lab} saved')\n",
    "\n",
    "# Run for all your files\n",
    "for fname in cleaned_files:\n",
    "    extract_sentences(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f1a2e83-e5f4-436b-84c9-0d9d51f3b761",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_files = [\n",
    "    'sentences_indexed1.pkl',\n",
    "    'sentences_indexed2.pkl',\n",
    "    'sentences_indexed3.pkl',\n",
    "    'sentences_indexed4.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c41580ba-2954-4d84-a574-e9c3d4c8ef1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens across all files: 5508\n"
     ]
    }
   ],
   "source": [
    "# == Get sum of unique tokens\n",
    "all_unique_tokens = set()\n",
    "\n",
    "for dataname in sentences_files:\n",
    "    data = joblib.load(dataname)  # load list of tokenized sentences\n",
    "    for sentence in data:\n",
    "        all_unique_tokens.update(sentence)  # add tokens to the set\n",
    "\n",
    "print(f\"Total unique tokens across all files: {len(all_unique_tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "465bc73c-70b6-4dbf-a379-8c5235f83a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for dataname in sentences_files:  # <-- your list of sentence files\n",
    "    data = joblib.load(dataname)\n",
    "    dataset.extend(data)  # extend instead of append if you want all sentences in a single list\n",
    "\n",
    "# === Model training ===\n",
    "w2v = Word2Vec(\n",
    "    sentences=dataset,    # iterator that loops over tokenized sentences\n",
    "    vector_size=300,      # Word vector dimensionality (use `vector_size` in newer gensim)\n",
    "    window=8,             # Context window size\n",
    "    min_count=10,         # Minimum word count\n",
    "    workers=8,            # Number of threads\n",
    "    sample=1e-3,          # Downsample setting for frequent words\n",
    "    epochs=10             # Number of iterations over the corpus\n",
    ")\n",
    "\n",
    "# Optimize memory usage (optional)\n",
    "w2v.wv.fill_norms()  # only works in older gensim versions\n",
    "\n",
    "# Save model\n",
    "wd_models.mkdir(parents=True, exist_ok=True)  # create folder if it doesn't exist\n",
    "w2v.save(str(wd_models / 'w2v-vectors_8_300.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d94142c-1e7f-4b0b-bfec-98604fc84304",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec.load(str(wd_models / \"w2v-vectors_8_300.pkl\"))\n",
    "word_vectors = w2v.wv # Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b75bd3b-2957-42d1-a89c-89e5b3ea44f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['centroids/cog_centroid.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Calculate centroids ===\n",
    "\n",
    "def findcentroid(text, model):\n",
    "    vecs = [model.wv[w] * word_counts_weighted[w] for w in text if w in model.wv]\n",
    "    vecs = [v for v in vecs if len(v) > 0]\n",
    "    centroid = np.mean(vecs, axis=0)\n",
    "    #centroid = centroid.reshape(1, -1)\n",
    "    return centroid\n",
    "\n",
    "\n",
    "affect_centroid = findcentroid(affect_dic, w2v)\n",
    "cog_centroid = findcentroid(cognition_dic, w2v)\n",
    "\n",
    "os.chdir(data_c)\n",
    "joblib.dump(affect_centroid, 'centroids/affect_centroid.pkl')\n",
    "joblib.dump(cog_centroid, 'centroids/cog_centroid.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcab4536-fca7-468d-ae93-6e8da17cbfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affect centroid vector:\n",
      " [-0.27070785  0.07733393  0.09620134  0.35799858  0.2457734   0.16819087\n",
      "  0.09431585  0.17867339 -0.01392951 -0.18173373  0.07389046 -0.05243336\n",
      " -0.08362902 -0.01764814 -0.12524877  0.01050707  0.20518973 -0.13921753\n",
      "  0.22047777  0.20882878 -0.1773565   0.25334916  0.09851619  0.07518107\n",
      "  0.73512375 -0.1585063  -0.33174515  0.16673328  0.09699049  0.04387075\n",
      " -0.24330004  0.00397614 -0.30414858  0.18564765  0.19301413  0.11609907\n",
      " -0.21530004 -0.37899354  0.3393474   0.0266055   0.03752064  0.15998538\n",
      "  0.32920605  0.01876376  0.20658898  0.04452509 -0.33948043 -0.13507654\n",
      " -0.03960928  0.2853234  -0.06559735  0.03378145 -0.02413347  0.16460955\n",
      "  0.04750048  0.21164843  0.276862    0.15080781  0.368788    0.17759691\n",
      " -0.09335481  0.08420185 -0.11471112 -0.08240609  0.06851572  0.08676937\n",
      " -0.13361289  0.29014727  0.07825062  0.2027863  -0.0272716   0.08671182\n",
      "  0.12169847 -0.14189164  0.24791044 -0.10401725 -0.26149753 -0.03902197\n",
      " -0.0797374  -0.04818179 -0.29885593 -0.13045147 -0.19880669  0.18755163\n",
      "  0.18571721 -0.03430157 -0.0048476   0.06053796  0.09855708 -0.08789191\n",
      " -0.19510292 -0.2501613   0.05065754 -0.07757132  0.10722072  0.22829242\n",
      "  0.11303023 -0.14808208 -0.12705094  0.4187861  -0.00699808 -0.14536671\n",
      "  0.02709108 -0.04030658  0.12324943 -0.20538199 -0.04345768 -0.16400065\n",
      " -0.18048997  0.085557   -0.14755058 -0.39580727  0.09126537  0.20125099\n",
      " -0.16722769  0.35384938  0.01875829 -0.14227538 -0.03901339 -0.26440898\n",
      " -0.12318031  0.10169658 -0.06305309  0.24370143 -0.02952132  0.06440838\n",
      "  0.13406053 -0.13809626 -0.08274558 -0.04209099 -0.05229325 -0.5903521\n",
      " -0.00157376 -0.07884777  0.0108875  -0.2100296   0.11779858 -0.07195665\n",
      "  0.02848947 -0.1997467   0.1343914  -0.02478417 -0.2065046   0.35423607\n",
      " -0.07620467  0.04896824 -0.32193103 -0.10892998  0.03772626  0.06951021\n",
      " -0.0353653   0.01859954 -0.3538304   0.02821769  0.01426852  0.04982012\n",
      "  0.02793025 -0.4095211   0.27856845 -0.17767033  0.02688094  0.18616128\n",
      " -0.15244016 -0.16098215 -0.28889358  0.14435157 -0.03948075 -0.15155096\n",
      "  0.22459699  0.19244705 -0.1469849   0.36438394  0.275278    0.2878411\n",
      "  0.31493506 -0.23655805 -0.21680862 -0.03295935 -0.40743154 -0.2830667\n",
      " -0.08337673  0.22106136  0.04336923  0.00759244 -0.15452638 -0.10828503\n",
      " -0.14553407  0.11323106  0.2339146  -0.28536078 -0.23178516  0.19311294\n",
      "  0.2905598   0.20903417 -0.07245297  0.04211364  0.00699321 -0.17761202\n",
      " -0.02451765 -0.00576293 -0.31494534  0.27812326  0.03524575  0.26230934\n",
      "  0.08864592 -0.14218104 -0.09057914 -0.00553277 -0.00782647 -0.36446226\n",
      " -0.14864127  0.00408493  0.06266025  0.14842494  0.07018296 -0.26514474\n",
      " -0.02585776  0.05731732 -0.36529416 -0.02615524  0.00611248 -0.06005111\n",
      " -0.23004748  0.01124792  0.0658135  -0.0838663   0.1607469   0.15257703\n",
      " -0.31208682  0.1338923   0.04323365 -0.04660474 -0.14746432 -0.00617332\n",
      " -0.06980784  0.23867327  0.00095398 -0.13664463  0.07695076 -0.01172001\n",
      " -0.35500216 -0.12237075  0.0969317   0.20362325 -0.18993077 -0.08563889\n",
      " -0.02779604 -0.14523578  0.08686029  0.1493265   0.55076957  0.01811122\n",
      " -0.14470302  0.10378077 -0.23039041  0.26457125  0.09827732  0.21032536\n",
      " -0.23562343 -0.27733693 -0.09570732  0.23600927 -0.10680109  0.12700896\n",
      " -0.05610174 -0.14547633 -0.14642122 -0.15545724 -0.05632706 -0.06476402\n",
      "  0.01591341  0.20703566 -0.08882148  0.16597217 -0.06751803  0.47736138\n",
      "  0.07933765  0.36362857  0.2627654   0.09007989  0.04872708  0.01974122\n",
      "  0.05614417 -0.0274445   0.10332106  0.3467878  -0.24711247  0.02722801\n",
      "  0.07113566  0.2652371  -0.20347083  0.3143981  -0.03281167  0.1652089\n",
      "  0.3120068   0.29988337 -0.2015152  -0.31120268 -0.35680842  0.11173082]\n",
      "\n",
      "Cognition centroid vector:\n",
      " [-0.08128797  0.12895916  0.13447821  0.17686075  0.08968422  0.03670583\n",
      "  0.02300188 -0.05554772  0.18818894 -0.21631536  0.01577253  0.05099783\n",
      "  0.26078722 -0.04081487 -0.08520933  0.01014432 -0.2527215  -0.18958367\n",
      "  0.04443862  0.12275628 -0.13818246 -0.03839194  0.15149507  0.03835746\n",
      "  0.2295591  -0.13337323 -0.01750283 -0.2243015   0.11566497 -0.04676012\n",
      " -0.05662942 -0.00514071 -0.11662114 -0.15990914 -0.05270558  0.1221031\n",
      " -0.01610954 -0.01803606  0.09037773  0.06483485 -0.14055206  0.06523272\n",
      " -0.03535692  0.00333334 -0.09924118 -0.07934181 -0.06991953 -0.13783458\n",
      "  0.06158337 -0.0689598   0.06346314 -0.13192333 -0.03425149 -0.10384401\n",
      " -0.02686705  0.14164563  0.2891897  -0.05467517 -0.11193923  0.16153465\n",
      " -0.39946434  0.12374334  0.0117033   0.0971051   0.30116907  0.20709021\n",
      "  0.04459459  0.0844198   0.13004003 -0.07469486 -0.12263812  0.2083217\n",
      "  0.07634185  0.02270996  0.1607977   0.03376974 -0.19790937 -0.0552286\n",
      "  0.09870686  0.22460197 -0.13699776 -0.10701932 -0.12271749 -0.15266545\n",
      " -0.08160035  0.04720717 -0.07925533  0.00720137  0.06995719  0.03639622\n",
      " -0.04336837 -0.14237076  0.09225917  0.21793213  0.00752476  0.12441512\n",
      " -0.01500452  0.04063994 -0.18661615  0.02067256 -0.18824731 -0.17746189\n",
      "  0.051141   -0.05052818 -0.00265763  0.14542091 -0.22573195  0.03587218\n",
      "  0.14025553  0.08017363 -0.06487955 -0.34892958  0.02405275  0.0265456\n",
      " -0.19594374 -0.00671536 -0.06852232 -0.17250371 -0.01975712 -0.12724409\n",
      "  0.08311096 -0.08119964  0.12923045  0.13884138  0.05463736  0.22841956\n",
      "  0.0421963   0.16433631  0.03648771 -0.06474698 -0.06570081 -0.15824713\n",
      " -0.12909696  0.2189761   0.03065082 -0.2511869  -0.09155325 -0.10659631\n",
      " -0.16288663 -0.10505001 -0.0521297  -0.13496995  0.04939902  0.09885534\n",
      "  0.06945802  0.12732376 -0.13977478  0.24254945  0.13598824  0.10141096\n",
      " -0.07342952  0.03783276 -0.07408802  0.04463686 -0.06513356  0.1059785\n",
      "  0.16196379 -0.23661304  0.16951455  0.00421912 -0.12472562  0.05178541\n",
      " -0.1517539  -0.09841263 -0.39097401 -0.04713274  0.00139015 -0.02202492\n",
      "  0.14298452  0.3034537  -0.04334203 -0.08986747  0.20550467  0.07070141\n",
      "  0.09857401 -0.02633452  0.06889223 -0.11832205 -0.11852523  0.09876675\n",
      " -0.00994979 -0.09406749 -0.0929048  -0.03858995  0.19511189 -0.11444544\n",
      " -0.11949759  0.05632614 -0.0071992  -0.14385025 -0.22236022 -0.16023657\n",
      "  0.08384141 -0.15476269  0.23812246  0.04394739 -0.20748544  0.02993751\n",
      "  0.04990726  0.07049929 -0.06329995  0.15522385 -0.03109637  0.14652726\n",
      " -0.01429435 -0.03075454 -0.1913226  -0.08809948  0.1677023  -0.08680777\n",
      " -0.01476068  0.11471397  0.0449564   0.29076067  0.01574665 -0.04270709\n",
      " -0.15531689  0.19692221 -0.13979779  0.01112601  0.06704571 -0.02243809\n",
      "  0.07681318  0.20956224 -0.10023407  0.06244059  0.16714576 -0.00471888\n",
      " -0.04469812  0.24924697  0.2111732  -0.00753131 -0.04137129  0.11449926\n",
      " -0.05572208 -0.12467683 -0.03872255 -0.23061565  0.07585426 -0.0206696\n",
      " -0.06726541  0.08175654 -0.17489934 -0.24363296 -0.2186328   0.12459406\n",
      " -0.21471646 -0.02853896  0.16137078  0.06800627  0.20165484  0.12005273\n",
      "  0.10094813  0.02833047  0.0367537   0.20108774 -0.13254447  0.19402012\n",
      " -0.07904404 -0.10343191 -0.01378361  0.02810429 -0.13744459 -0.15035178\n",
      "  0.00531649 -0.29197606 -0.09930415 -0.17524388 -0.17208801  0.13781795\n",
      "  0.05230794 -0.04156445 -0.22131744 -0.24577388  0.02016897 -0.04575573\n",
      "  0.21236703  0.12535185  0.14576694  0.15074259  0.03849113 -0.17085281\n",
      "  0.00187858 -0.07373735  0.05943508  0.07194492 -0.01903635 -0.00441401\n",
      "  0.03448394 -0.03818359 -0.16463539  0.0089616  -0.13673702 -0.00440779\n",
      "  0.17758672  0.31446123 -0.03217444 -0.1508328  -0.09406237  0.35807544]\n",
      "\n",
      "Shape of affect centroid: (300,)\n",
      "Shape of cognition centroid: (300,)\n",
      "\n",
      "Affect centroid summary: min, max, mean: -0.5903521 0.73512375 0.005429662\n",
      "Cognition centroid summary: min, max, mean: -0.39946434 0.35807544 -0.0023091196\n"
     ]
    }
   ],
   "source": [
    "# Print the vectors\n",
    "print(\"Affect centroid vector:\\n\", affect_centroid)\n",
    "print(\"\\nCognition centroid vector:\\n\", cog_centroid)\n",
    "\n",
    "# Optional: shape and stats\n",
    "print(\"\\nShape of affect centroid:\", affect_centroid.shape)\n",
    "print(\"Shape of cognition centroid:\", cog_centroid.shape)\n",
    "\n",
    "print(\"\\nAffect centroid summary: min, max, mean:\", \n",
    "      np.min(affect_centroid), np.max(affect_centroid), np.mean(affect_centroid))\n",
    "print(\"Cognition centroid summary: min, max, mean:\", \n",
    "      np.min(cog_centroid), np.max(cog_centroid), np.mean(cog_centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c8316f-f794-4bcd-87c9-578aa82f88b0",
   "metadata": {},
   "source": [
    "## Emotionality Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3376a4f6-a002-4179-8614-dfef96113180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISSUE why doesnt this work with the sentences?!\n",
    "\n",
    "# Set wd to data_preprocessed\n",
    "os.chdir(data_preprocessed)\n",
    "\n",
    "# === Load preprocessed speech data ===\n",
    "\n",
    "preprocessed_final_files = [\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed1_final.pkl')),\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed2_final.pkl')),\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed3_final.pkl')),\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed4_final.pkl'))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cbd1f95-c7f4-41f1-a292-542ee34ae882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit\\\\data\\\\distances_10epochs.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################################\n",
    "# Define Functions              ###\n",
    "###################################\n",
    "\n",
    "# apparently is missing deleting intermediate files\n",
    "\n",
    "def documentvecweight(lista):\n",
    "    out = []\n",
    "    lista = [i for i in lista if len(i[1]) > 0]\n",
    "    for s in lista:\n",
    "        vecs = [w2v.wv[w] * word_counts_weighted[w] for w in s[1] if w in w2v.wv]\n",
    "        if len(vecs) == 0:\n",
    "            a = np.nan\n",
    "            c = np.nan\n",
    "            score = np.nan\n",
    "        else:\n",
    "            v = np.mean(vecs, axis=0)\n",
    "            a = cosine(v, affect_centroid)\n",
    "            c = cosine(v, cog_centroid)\n",
    "            score = (1 + 1 - a) / (1 + 1 - c)\n",
    "        out.append([s[0], a, c, score])\n",
    "    return out\n",
    "\n",
    "\n",
    "def main_function(file_path, idx):\n",
    "    dataset = joblib.load(file_path)\n",
    "    data = documentvecweight(dataset)\n",
    "    lab = os.path.join(data_c, f'temp_distances_main_{idx}.pkl')\n",
    "    joblib.dump(data, lab)\n",
    "\n",
    "\n",
    "###################################\n",
    "#      Run main directly        ###\n",
    "###################################\n",
    "\n",
    "def main():\n",
    "    files = [\n",
    "        os.path.join(data_preprocessed, f'preprocessed_speeches_indexed{i+1}_final.pkl') #Changed!\n",
    "        for i in range(4)\n",
    "    ]\n",
    "    for i, f in enumerate(files, start=1):\n",
    "        main_function(f, i)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "###################################\n",
    "#      Recompose everything     ###\n",
    "###################################\n",
    "\n",
    "DATA_temp = [os.path.join(data_c, f'temp_distances_main_{i+1}.pkl') for i in range(4)]\n",
    "\n",
    "tot = []\n",
    "for dataname in DATA_temp:\n",
    "    d = joblib.load(dataname)\n",
    "    tot += d\n",
    "\n",
    "tot_df = pd.DataFrame(tot, columns=['filename', 'affect_d', 'cognition_d', 'score'])\n",
    "joblib.dump(tot_df, os.path.join(data_c, 'distances_10epochs.pkl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb955ab4-243d-470d-a967-e0af1e474c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          filename  affect_d  cognition_d     score\n",
      "0  SEN_63_2008.txt  1.133518     1.111065  0.974741\n",
      "1  KWT_73_2018.txt  1.478596     1.203444  0.654573\n",
      "2  AFG_53_1998.txt  1.147844     1.066845  0.913199\n",
      "3  ARG_12_1957.txt  1.207251     0.721205  0.619919\n",
      "4  SYR_23_1968.txt  0.978242     0.874406  0.907750\n",
      "Shape: (10760, 4)\n",
      "           affect_d   cognition_d         score\n",
      "count  10760.000000  10760.000000  10760.000000\n",
      "mean       1.253685      0.980612      0.730884\n",
      "std        0.262114      0.195785      0.209001\n",
      "min        0.334058      0.326138      0.289035\n",
      "25%        1.090293      0.848361      0.573664\n",
      "50%        1.299281      0.998405      0.703619\n",
      "75%        1.453219      1.129200      0.865534\n",
      "max        1.759300      1.494848      1.682891\n",
      "              filename  affect_d  cognition_d     score\n",
      "0      SEN_63_2008.txt  1.133518     1.111065  0.974741\n",
      "1      KWT_73_2018.txt  1.478596     1.203444  0.654573\n",
      "2      AFG_53_1998.txt  1.147844     1.066845  0.913199\n",
      "3      ARG_12_1957.txt  1.207251     0.721205  0.619919\n",
      "4      SYR_23_1968.txt  0.978242     0.874406  0.907750\n",
      "...                ...       ...          ...       ...\n",
      "10755  ERI_65_2010.txt  1.374684     1.092240  0.688857\n",
      "10756  ESP_75_2020.txt  0.799298     0.796564  0.997728\n",
      "10757  AZE_62_2007.txt  1.529428     1.084912  0.514237\n",
      "10758  OMN_75_2020.txt  1.406405     1.219972  0.760992\n",
      "10759  NOR_74_2019.txt  1.372289     1.160859  0.748040\n",
      "\n",
      "[10760 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Print the first few rows\n",
    "print(tot_df.head())\n",
    "\n",
    "# Optionally, print the shape to see how many documents were processed\n",
    "print(\"Shape:\", tot_df.shape)\n",
    "\n",
    "# Print a quick summary\n",
    "print(tot_df.describe())\n",
    "\n",
    "# Or print the full DataFrame (if small)\n",
    "print(tot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c295cd4-4fa0-470f-b67c-4f584f7a3f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your main corpus CSV \n",
    "un_corpus_merged = pd.read_csv(os.path.join(data_c, \"un_corpus_merged.csv\"), sep=';', encoding='utf-8') \n",
    "# Merge on filename \n",
    "un_corpus_scored = un_corpus_merged.merge(tot_df, on=\"filename\", how=\"left\")\n",
    "\n",
    "# Optionally save back as pickle\n",
    "joblib.dump(un_corpus_scored, os.path.join(data_c, \"un_corpus_scored.pkl\"))\n",
    "\n",
    "un_corpus_merged.to_csv(\n",
    "    os.path.join(data_c, \"un_corpus_scored.csv\"),\n",
    "    sep=';', \n",
    "    index=False, \n",
    "    encoding='utf-8'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8d13667-1eb3-482e-83ca-5969a229750a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              filename                                             speech  \\\n",
      "0      SEN_63_2008.txt  Mr. Miguel \\ndâ€™Escoto, in electing you to the ...   \n",
      "1      KWT_73_2018.txt  At the outset, it is my pleasure, on behalf of...   \n",
      "2      AFG_53_1998.txt  The urgency of the\\nalarming situation in Afgh...   \n",
      "3      ARG_12_1957.txt  First of all, I should like to join my congrat...   \n",
      "4      SYR_23_1968.txt  1. When I, as the then Chairman of the Asian G...   \n",
      "...                ...                                                ...   \n",
      "10755  ERI_65_2010.txt  Let me start by extending my \\nsincere congrat...   \n",
      "10756  ESP_75_2020.txt  Mr. President, Mr. Secretary-General, Ladies a...   \n",
      "10757  AZE_62_2007.txt  I would like to \\nadd my voice to those of pre...   \n",
      "10758  OMN_75_2020.txt  In the name of God the Merciful and the Compas...   \n",
      "10759  NOR_74_2019.txt  Norway is a firm supporter of a rules-based mu...   \n",
      "\n",
      "      country_code  year country_name  speech_length_words  \\\n",
      "0              SEN  2008      Senegal                 2269   \n",
      "1              KWT  2018       Kuwait                 2997   \n",
      "2              AFG  1998  Afghanistan                 2385   \n",
      "3              ARG  1957    Argentina                 2081   \n",
      "4              SYR  1968        Syria                 6902   \n",
      "...            ...   ...          ...                  ...   \n",
      "10755          ERI  2010      Eritrea                  997   \n",
      "10756          ESP  2020        Spain                 3277   \n",
      "10757          AZE  2007   Azerbaijan                 1573   \n",
      "10758          OMN  2020         Oman                 1137   \n",
      "10759          NOR  2019       Norway                 2205   \n",
      "\n",
      "       english_official_language  security_council_permanent  \\\n",
      "0                              0                           0   \n",
      "1                              0                           0   \n",
      "2                              0                           0   \n",
      "3                              0                           0   \n",
      "4                              0                           0   \n",
      "...                          ...                         ...   \n",
      "10755                          0                           0   \n",
      "10756                          0                           0   \n",
      "10757                          0                           0   \n",
      "10758                          0                           0   \n",
      "10759                          0                           0   \n",
      "\n",
      "                                         speaker_name  \\\n",
      "0                                      Abdoulaye Wade   \n",
      "1           Sheikh Jaber Al-Mubarak Al-Hamed Al Sabah   \n",
      "2                                   Abdullah Abdullah   \n",
      "3                                          Mr. DRAGO    \n",
      "4                                           Mr. TOMEH   \n",
      "...                                               ...   \n",
      "10755                            Osman Mohammed Saleh   \n",
      "10756                Mr. Pedro Sanchez Perez-Castejon   \n",
      "10757                               Elmar Mammadyarov   \n",
      "10758  Mr. Sayyid Badr bin Hamad bin Hamood Albusaidi   \n",
      "10759                                Ms. Erna Solberg   \n",
      "\n",
      "                                    position  gender_dummy  \\\n",
      "0                          (Vice-) President           NaN   \n",
      "1                    (Deputy) Prime Minister           NaN   \n",
      "2      (Deputy) Minister for Foreign Affairs           NaN   \n",
      "3                                        NaN           0.0   \n",
      "4                                        NaN           0.0   \n",
      "...                                      ...           ...   \n",
      "10755  (Deputy) Minister for Foreign Affairs           NaN   \n",
      "10756                     Head of Government           0.0   \n",
      "10757  (Deputy) Minister for Foreign Affairs           NaN   \n",
      "10758  (Deputy) Minister for Foreign Affairs           0.0   \n",
      "10759                (Deputy) Prime Minister           1.0   \n",
      "\n",
      "             speech_label  affect_d  cognition_d     score  \n",
      "0          Senegal (2008)  1.133518     1.111065  0.974741  \n",
      "1           Kuwait (2018)  1.478596     1.203444  0.654573  \n",
      "2      Afghanistan (1998)  1.147844     1.066845  0.913199  \n",
      "3        Argentina (1957)  1.207251     0.721205  0.619919  \n",
      "4            Syria (1968)  0.978242     0.874406  0.907750  \n",
      "...                   ...       ...          ...       ...  \n",
      "10755      Eritrea (2010)  1.374684     1.092240  0.688857  \n",
      "10756        Spain (2020)  0.799298     0.796564  0.997728  \n",
      "10757   Azerbaijan (2007)  1.529428     1.084912  0.514237  \n",
      "10758         Oman (2020)  1.406405     1.219972  0.760992  \n",
      "10759       Norway (2019)  1.372289     1.160859  0.748040  \n",
      "\n",
      "[10760 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "print(un_corpus_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1210b88f-cf13-4da0-807b-77764cf0d439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count where affect_d is NaN: 0\n",
      "Count where affect_d is not NaN: 10811\n"
     ]
    }
   ],
   "source": [
    "# Count where affect_d is NaN\n",
    "nan_count = un_corpus_merged['affect_d'].isna().sum()\n",
    "\n",
    "# Count where affect_d is not NaN\n",
    "not_nan_count = un_corpus_merged['affect_d'].notna().sum()\n",
    "\n",
    "print(\"Count where affect_d is NaN:\", nan_count)\n",
    "print(\"Count where affect_d is not NaN:\", not_nan_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594b17bd-15b6-4897-ab94-9b12175e7697",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
