{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ca6f721-48ad-495c-8e5f-e60790225709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "from pathlib import Path\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "# from gensim.summarization.textcleaner import get_sentences\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from random import shuffle\n",
    "import nltk\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import joblib\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from string import punctuation\n",
    "from scipy.spatial.distance import cosine\n",
    "import glob\n",
    "import spacy\n",
    "from multiprocessing import Pool, freeze_support\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# === Initialize NLP Tools ===\n",
    "\n",
    "translator = str.maketrans('', '', punctuation) \n",
    "tagger = nltk.perceptron.PerceptronTagger()\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "# === Set Working Directory ===\n",
    "\n",
    "# Set your working directory (adjust this as needed)\n",
    "wd = Path(r\"C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\")\n",
    "wd_models = wd / \"models\"\n",
    "wd_results = wd / \"results\"\n",
    "\n",
    "\n",
    "# === Define Folder Paths ===\n",
    "\n",
    "# Make sure that you have these folders in your working directory\n",
    "data_c = wd / \"data\"\n",
    "data_temp = data_c / \"temp\"\n",
    "data_freq = data_c / \"freq\"\n",
    "data_dict = data_c / \"dictionaries\"\n",
    "data_sent = data_c / \"sentences\"\n",
    "data_preprocessed = data_c / \"preprocessed\"\n",
    "fig_dir = wd /\"fig\"\n",
    "\n",
    "# Upload ressources\n",
    "stopwords = joblib.load(data_c / \"stopwords.pkl\")\n",
    "word_counts = joblib.load(data_freq / \"word_counts.pkl\")\n",
    "word_counts_weighted = joblib.load(data_freq / \"word_counts_weighted.pkl\")\n",
    "affect_dic = joblib.load(data_dict / 'dictionary_affect.pkl')\n",
    "cognition_dic = joblib.load(data_dict / 'dictionary_cognition.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "125227ca-a0b5-43d7-956d-7d49a66f81b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_temp)\n",
    "cleaned_files = [\n",
    "    str(data_temp / 'clean_speeches_indexed1.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed2.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed3.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed4.pkl')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af0997e8-9da3-45c3-bdd4-7d38a708a474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl processed\n",
      "Unique tokens: 8400\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed1.pkl saved\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl processed\n",
      "Unique tokens: 8405\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed2.pkl saved\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl processed\n",
      "Unique tokens: 8401\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed3.pkl saved\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl processed\n",
      "Unique tokens: 8391\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed4.pkl saved\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_sent)\n",
    "def extract_sentences(dataname):\n",
    "    data = joblib.load(dataname)\n",
    "    data = [a[1] for a in data]  # keep only text, no id\n",
    "\n",
    "    sentences = []\n",
    "    for doc in data:\n",
    "        sentences += sent_tokenize(doc)  # use nltk's sent_tokenize here\n",
    "\n",
    "    sentences = [item for item in sentences if len(item.split()) > 1]\n",
    "    sentences = [gensim.utils.simple_preprocess(item) for item in sentences]\n",
    "\n",
    "    sentences = [[a for a in s if not a.isdigit()] for s in sentences]\n",
    "    sentences = [[a for a in s if len(a) > 2] for s in sentences]\n",
    "\n",
    "    #texts = [\" \".join(s) for s in sentences if len(s) > 0]\n",
    "    #docs = list(nlp.pipe(texts, batch_size=50, n_process=1))\n",
    "    #sentences = [\n",
    "     #   [tok.text for tok in doc if tok.tag_.startswith((\"N\", \"V\", \"J\"))]\n",
    "       # for doc in docs\n",
    "   # ]\n",
    "\n",
    "    sentences = [tagger.tag(s) for s in sentences]\n",
    "    sentences = [[i[0] for i in s if i[1].startswith(('N', 'V', 'J'))] for s in sentences]\n",
    "\n",
    "    sentences = [[stemmer.stem(i) for i in s] for s in sentences]\n",
    "    sentences = [[a for a in s if a not in stopwords] for s in sentences]\n",
    "    sentences = [[a for a in s if word_counts[a] >= 10] for s in sentences]\n",
    "\n",
    "    sentences = [s for s in sentences if len(s) > 1]  # eliminate empty\n",
    "    shuffle(sentences)\n",
    "\n",
    "    lab = dataname.replace('clean_speeches_', 'sentences_').replace('_.pkl', '.pkl')\n",
    "    print(f'{dataname} processed')\n",
    "    joblib.dump(sentences, lab)\n",
    "    \n",
    "    unique_tokens = set(token for s in sentences for token in s)\n",
    "    print(f\"Unique tokens: {len(unique_tokens)}\")\n",
    "\n",
    "    print(f'{lab} saved')\n",
    "\n",
    "# Run for all your files\n",
    "for fname in cleaned_files:\n",
    "    extract_sentences(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fddaea83-7d1f-4b14-85b9-401b2e727c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_files = [\n",
    "    os.path.join(data_temp, 'sentences_indexed1.pkl'),\n",
    "    os.path.join(data_temp, 'sentences_indexed2.pkl'),\n",
    "    os.path.join(data_temp, 'sentences_indexed3.pkl'),\n",
    "    os.path.join(data_temp, 'sentences_indexed4.pkl')\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c41580ba-2954-4d84-a574-e9c3d4c8ef1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens across all files: 8508\n"
     ]
    }
   ],
   "source": [
    "# == Get sum of unique tokens\n",
    "all_unique_tokens = set()\n",
    "\n",
    "for dataname in sentences_files:\n",
    "    data = joblib.load(dataname)  # load list of tokenized sentences\n",
    "    for sentence in data:\n",
    "        all_unique_tokens.update(sentence)  # add tokens to the set\n",
    "\n",
    "print(f\"Total unique tokens across all files: {len(all_unique_tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f1a2e83-e5f4-436b-84c9-0d9d51f3b761",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_files = [\n",
    "    'sentences_indexed1.pkl',\n",
    "    'sentences_indexed2.pkl',\n",
    "    'sentences_indexed3.pkl',\n",
    "    'sentences_indexed4.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "465bc73c-70b6-4dbf-a379-8c5235f83a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for dataname in sentences_files:  # <-- your list of sentence files\n",
    "    data = joblib.load(dataname)\n",
    "    dataset.extend(data)  # extend instead of append if you want all sentences in a single list\n",
    "\n",
    "# === Model training ===\n",
    "w2v = Word2Vec(\n",
    "    sentences=dataset,    # iterator that loops over tokenized sentences\n",
    "    vector_size=300,      # Word vector dimensionality (use `vector_size` in newer gensim)\n",
    "    window=8,             # Context window size\n",
    "    min_count=10,         # Minimum word count\n",
    "    workers=8,            # Number of threads\n",
    "    sample=1e-3,          # Downsample setting for frequent words\n",
    "    epochs=10             # Number of iterations over the corpus\n",
    ")\n",
    "\n",
    "# Optimize memory usage (optional)\n",
    "w2v.wv.fill_norms()  # only works in older gensim versions\n",
    "\n",
    "# Save model\n",
    "wd_models.mkdir(parents=True, exist_ok=True)  # create folder if it doesn't exist\n",
    "w2v.save(str(wd_models / 'w2v-vectors_8_300.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d94142c-1e7f-4b0b-bfec-98604fc84304",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec.load(str(wd_models / \"w2v-vectors_8_300.pkl\"))\n",
    "word_vectors = w2v.wv # Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b75bd3b-2957-42d1-a89c-89e5b3ea44f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['centroids/cog_centroid.pkl']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Calculate centroids ===\n",
    "\n",
    "def findcentroid(text, model):\n",
    "    vecs = [model.wv[w] * word_counts_weighted[w] for w in text if w in model.wv]\n",
    "    vecs = [v for v in vecs if len(v) > 0]\n",
    "    centroid = np.mean(vecs, axis=0)\n",
    "    #centroid = centroid.reshape(1, -1)\n",
    "    return centroid\n",
    "\n",
    "\n",
    "affect_centroid = findcentroid(affect_dic, w2v)\n",
    "cog_centroid = findcentroid(cognition_dic, w2v)\n",
    "\n",
    "os.chdir(data_c)\n",
    "joblib.dump(affect_centroid, 'centroids/affect_centroid.pkl')\n",
    "joblib.dump(cog_centroid, 'centroids/cog_centroid.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bcab4536-fca7-468d-ae93-6e8da17cbfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affect centroid vector:\n",
      " [-0.00102843 -0.04505273  0.2686238   0.04995833  0.12448414 -0.22024421\n",
      "  0.02002718  0.13428923  0.05709088 -0.201109   -0.09748005  0.0465512\n",
      " -0.0594778  -0.09207746 -0.13474967 -0.13982649  0.01046207  0.31272015\n",
      "  0.19611344 -0.04461817  0.07295036  0.0667109   0.02109417 -0.05929884\n",
      "  0.25985494 -0.07108637 -0.4274462   0.04899051  0.14073169 -0.22614689\n",
      "  0.07533123 -0.0112998  -0.04141686  0.04214694 -0.01623218 -0.27087027\n",
      "  0.08074587 -0.05814374  0.18459216 -0.13962166 -0.02706708  0.10797511\n",
      " -0.00773254 -0.23971277  0.07589351  0.28366277  0.06687983  0.01656772\n",
      "  0.16070996  0.19863772  0.08499901 -0.24409768  0.24698676 -0.03492389\n",
      " -0.12952498  0.12853302  0.19065093 -0.17242715 -0.07315606  0.08943436\n",
      " -0.0538155  -0.06167968  0.13589464  0.10902671 -0.13337298  0.14933337\n",
      "  0.01390664 -0.08452251 -0.08017185  0.11252235 -0.13780881  0.10866585\n",
      "  0.17758091 -0.12374552 -0.06560096  0.01818446 -0.25835025  0.07482362\n",
      " -0.18389069  0.20401467  0.17832932 -0.14878437 -0.1400679   0.30318585\n",
      "  0.05434011  0.03214268 -0.07392681 -0.10092343  0.19391541 -0.0713184\n",
      " -0.07875132 -0.0089947   0.00575016  0.05930267  0.3300728   0.20094731\n",
      "  0.07783126 -0.1447857   0.07966428  0.07957852 -0.09072399  0.03193255\n",
      "  0.1742157   0.20296009  0.32672966  0.09098526 -0.04924873 -0.18054098\n",
      " -0.04668159  0.2212801  -0.13833056 -0.02038404  0.00460234  0.02344929\n",
      " -0.01036575  0.23726656  0.04721921  0.00316751  0.01508089 -0.01759364\n",
      "  0.11045974  0.15711664  0.30477917  0.02226036 -0.11830691  0.02556348\n",
      "  0.00257598 -0.15105766 -0.13046645 -0.13113245 -0.06374594  0.09710931\n",
      "  0.1570321  -0.09840355  0.15145281 -0.17712182  0.04603352  0.11330558\n",
      " -0.01359012 -0.25912738 -0.02707861 -0.19374897  0.05283843  0.02589999\n",
      "  0.07417866 -0.02995498 -0.06718495 -0.13890827  0.10865299  0.09129309\n",
      " -0.00344319 -0.25269133 -0.1080911   0.03485056  0.04983766  0.07792177\n",
      " -0.26995966  0.01457008 -0.17952144  0.44853818 -0.04694615 -0.12857206\n",
      " -0.14048141  0.23894762 -0.16629362 -0.08133786  0.08333499 -0.25780994\n",
      "  0.19278356  0.01601254  0.07202202 -0.01322611 -0.08649696 -0.07939845\n",
      " -0.06508907  0.05289429 -0.0392956  -0.06873548  0.08415955 -0.05434256\n",
      " -0.05056017 -0.14215384  0.06809073 -0.0550991  -0.02987971 -0.01119631\n",
      "  0.09555603  0.25635228 -0.040206   -0.01601915 -0.05753687 -0.11587267\n",
      " -0.04252565 -0.13477968 -0.02661361  0.03265932  0.05368304  0.22584294\n",
      "  0.0201091  -0.09177642 -0.00055902  0.12535971  0.00945049 -0.0404246\n",
      "  0.22402057 -0.07766594 -0.07282744  0.09358305  0.13887107  0.27451727\n",
      "  0.10170979 -0.05785197  0.04934258  0.01246891  0.18962443 -0.0042947\n",
      " -0.11061833 -0.382403   -0.41589105 -0.12828082  0.14297451 -0.22018056\n",
      " -0.32736415 -0.26009002  0.10112468 -0.07225885 -0.19387557  0.10939155\n",
      " -0.13561358 -0.18263254 -0.12180131  0.02660337 -0.09063801  0.30581433\n",
      " -0.22170486 -0.02644416 -0.14271864 -0.04536005  0.26484087 -0.42889988\n",
      "  0.06121191  0.02458156 -0.1626358   0.15197551 -0.15448765  0.0271529\n",
      " -0.03634817  0.07786057 -0.01630452  0.10149139 -0.06602852  0.0642729\n",
      "  0.12878068  0.02608788 -0.09913365 -0.24247032  0.15508151  0.15163574\n",
      " -0.07241388 -0.05584592  0.07306009 -0.01358707 -0.02992113 -0.2770954\n",
      " -0.00304173 -0.07489154 -0.00836854  0.13205694 -0.02011813 -0.09187114\n",
      " -0.0520941   0.06643239 -0.05439058 -0.15444604  0.19714527 -0.16334097\n",
      "  0.02133937 -0.06544177 -0.09100726  0.00537138  0.12109591  0.08160517\n",
      " -0.13577287  0.1317042   0.04689266  0.0816129  -0.20989548 -0.17644376\n",
      "  0.02186886 -0.22561525 -0.04618759  0.16280761  0.24878962 -0.02722019\n",
      "  0.05742127  0.2464884   0.14281395 -0.22706997  0.25508943 -0.06360892]\n",
      "\n",
      "Cognition centroid vector:\n",
      " [-5.11513054e-02  3.57989781e-02  1.47535786e-01  2.69898679e-02\n",
      "  1.38050541e-01  3.90473637e-03  5.84501848e-02 -1.94093764e-01\n",
      "  1.00431003e-01 -5.42897768e-02  1.63645986e-02  6.27187714e-02\n",
      "  6.13144524e-02  2.94615682e-02 -1.79280937e-01  3.28443572e-03\n",
      " -1.30776092e-01  4.19701412e-02  3.65375243e-02  4.55822870e-02\n",
      " -4.23550121e-02 -1.23416945e-01  3.27490680e-02 -8.04147795e-02\n",
      "  2.10918561e-01 -4.29008016e-03 -9.85037833e-02 -8.02686587e-02\n",
      " -1.01962253e-01 -1.00897342e-01  6.48834780e-02  2.80514156e-04\n",
      "  1.15607142e-01  1.55453399e-01  3.03716771e-02 -1.07650019e-01\n",
      "  2.92088836e-02 -1.21217612e-02  7.27367625e-02 -1.82717621e-01\n",
      " -1.33172810e-01  7.35164732e-02 -5.88971414e-02 -2.12678060e-01\n",
      "  1.86908338e-02  6.95074946e-02  6.01396486e-02  4.91533205e-02\n",
      " -4.91371192e-02  1.28130570e-01  1.88279808e-01 -2.79057007e-02\n",
      "  1.13593325e-01  8.58766064e-02 -2.20174119e-02  1.67252108e-01\n",
      "  2.20191002e-01 -1.25419348e-01 -6.80202767e-02  1.06910005e-01\n",
      " -1.33941323e-01 -1.21904112e-01 -1.38902832e-02 -1.21683769e-01\n",
      " -8.84470195e-02  8.39512646e-02  2.92297993e-02 -8.88019800e-04\n",
      " -4.26815785e-02  3.31565156e-03 -1.13642797e-01 -1.31783560e-01\n",
      " -4.60767075e-02 -1.16395866e-02  6.13007434e-02 -6.20531328e-02\n",
      " -6.06813654e-02  1.67391330e-01 -3.91567461e-02  1.18054345e-01\n",
      "  4.41127345e-02 -3.50038968e-02  7.79392421e-02  2.26077344e-02\n",
      "  6.70876503e-02  4.51932736e-02 -1.98779069e-02  6.30945042e-02\n",
      "  2.68172801e-01 -9.66249406e-02 -1.01004966e-01 -5.20611331e-02\n",
      " -1.74557026e-02 -1.10130697e-01  7.44675696e-02  1.44597754e-01\n",
      " -1.28849655e-01 -3.45601141e-02 -3.30569297e-01  2.18668297e-01\n",
      "  3.87843288e-02 -3.54025029e-02  4.45253029e-02 -1.01741530e-01\n",
      "  1.88563824e-01  1.58481151e-01  1.22138567e-01 -1.84226975e-01\n",
      " -3.62528078e-02  9.71797481e-02  1.42404605e-02 -6.22098371e-02\n",
      "  1.11859366e-01 -1.36351585e-01  1.50274737e-02  2.99704913e-02\n",
      " -1.27863944e-01  5.34552895e-02 -9.26334760e-04 -7.00474204e-03\n",
      " -8.88271555e-02  1.73472732e-01 -1.77563410e-02  5.65546453e-02\n",
      "  1.30133718e-01 -7.92557597e-02  9.18070897e-02 -1.08981065e-01\n",
      " -1.24012008e-02 -8.93244818e-02  3.55747677e-02  9.21651945e-02\n",
      "  2.01083254e-02  1.17723890e-01  1.40194759e-01 -4.82214615e-02\n",
      "  1.59348816e-01  5.24413772e-02  2.99532060e-02  1.25545368e-01\n",
      " -6.61162063e-02 -1.07388496e-01  5.03775626e-02 -4.33683358e-02\n",
      " -7.15272548e-03  1.05614159e-02 -2.21989062e-02 -3.01430449e-02\n",
      " -2.69080531e-02  1.52210698e-01  1.57168824e-02 -5.13060316e-02\n",
      " -7.51730576e-02 -7.08199963e-02  9.15048644e-02  1.98144138e-01\n",
      " -6.00777306e-02  4.94768955e-02 -5.13007678e-02  1.54552221e-01\n",
      " -3.88090424e-02  4.23491448e-02 -1.13154627e-01  3.55422646e-02\n",
      " -1.96353361e-01 -1.97633550e-01  5.23357689e-02 -1.81877598e-01\n",
      "  1.61811054e-01  1.78631749e-02  2.06271946e-01 -3.93014774e-02\n",
      " -7.72827044e-02 -1.38699234e-01 -2.34567281e-02  3.31014208e-02\n",
      " -1.58857871e-02  4.28929627e-02 -9.66693554e-03 -4.96089906e-02\n",
      "  9.26480070e-03 -1.65770248e-01 -1.15246614e-02 -8.02973434e-02\n",
      "  9.13281552e-03  1.38122827e-01  9.83282179e-03 -3.27161141e-02\n",
      "  2.02459283e-03  1.71635218e-03 -1.73665717e-01 -2.03066871e-01\n",
      "  1.22933201e-01 -7.96214417e-02  1.10618763e-01 -3.90268974e-02\n",
      "  1.23987295e-01 -3.63820605e-02  4.96464372e-02 -5.51987886e-02\n",
      " -2.55345941e-01 -4.96193692e-02  2.28807367e-02 -4.78413962e-02\n",
      "  1.14430323e-01  3.27892639e-02 -9.26010981e-02  3.55298445e-02\n",
      "  4.11170833e-02  1.17776804e-01 -3.95744992e-03 -1.09650686e-01\n",
      " -1.15164079e-01  2.38466442e-01 -1.63303763e-02  5.62657323e-03\n",
      " -6.52533844e-02  1.80133898e-02 -1.31407514e-01  1.07940562e-01\n",
      " -4.26563360e-02  1.92498323e-02 -2.19819754e-01 -5.48258014e-02\n",
      "  1.31781757e-01  2.94390507e-02 -1.23975985e-01  1.09453700e-01\n",
      "  8.69908091e-03  1.19644865e-01  1.26969256e-02  3.81288901e-02\n",
      " -1.29383244e-02  6.60421327e-02 -1.65581286e-01 -8.33006799e-02\n",
      "  1.21232092e-01 -1.15889169e-01  2.06349254e-01 -5.03723435e-02\n",
      "  6.16675653e-02 -2.67907046e-02 -3.14488001e-02 -3.18490155e-02\n",
      " -3.99253368e-02  7.51566067e-02  1.86740421e-02  3.14528495e-02\n",
      "  4.45397757e-02 -4.53003757e-02 -1.17407821e-01  9.71219444e-04\n",
      " -1.23839468e-01  8.84406790e-02 -9.05427895e-03 -1.31582478e-02\n",
      "  1.11831553e-01 -7.38062933e-02  6.90881386e-02 -2.55522728e-02\n",
      "  1.68273821e-01  2.57910322e-02 -7.88254142e-02  1.30387833e-02\n",
      "  7.86067247e-02 -2.44203378e-02 -7.96060041e-02 -5.26380772e-03\n",
      "  7.55763650e-02 -5.48731443e-03 -3.98578532e-02  7.57027864e-02\n",
      " -5.50676957e-02 -1.23203032e-01 -2.31900979e-02 -9.00255144e-02\n",
      " -7.74344951e-02  1.08004129e-02  4.85180952e-02 -2.48324070e-02\n",
      "  2.90377259e-01 -1.44052161e-02 -1.08855903e-01  1.30285084e-01\n",
      "  1.78805937e-03  1.42871827e-01 -6.23862743e-02  2.09743027e-02\n",
      " -1.05678707e-01 -1.35739401e-01 -6.71411529e-02  3.73438448e-02\n",
      " -4.77740131e-02 -3.25077549e-02  5.54013774e-02  2.06779733e-01\n",
      "  1.14236539e-02 -2.03664631e-01  8.15097168e-02  3.32877636e-02]\n",
      "\n",
      "Shape of affect centroid: (300,)\n",
      "Shape of cognition centroid: (300,)\n",
      "\n",
      "Affect centroid summary: min, max, mean: -0.42889988 0.44853818 -0.0009635989\n",
      "Cognition centroid summary: min, max, mean: -0.3305693 0.29037726 0.0010865201\n"
     ]
    }
   ],
   "source": [
    "# Print the vectors\n",
    "print(\"Affect centroid vector:\\n\", affect_centroid)\n",
    "print(\"\\nCognition centroid vector:\\n\", cog_centroid)\n",
    "\n",
    "# Optional: shape and stats\n",
    "print(\"\\nShape of affect centroid:\", affect_centroid.shape)\n",
    "print(\"Shape of cognition centroid:\", cog_centroid.shape)\n",
    "\n",
    "print(\"\\nAffect centroid summary: min, max, mean:\", \n",
    "      np.min(affect_centroid), np.max(affect_centroid), np.mean(affect_centroid))\n",
    "print(\"Cognition centroid summary: min, max, mean:\", \n",
    "      np.min(cog_centroid), np.max(cog_centroid), np.mean(cog_centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c8316f-f794-4bcd-87c9-578aa82f88b0",
   "metadata": {},
   "source": [
    "## Emotionality Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3376a4f6-a002-4179-8614-dfef96113180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISSUE why doesnt this work with the sentences?!\n",
    "\n",
    "# Set wd to data_preprocessed\n",
    "os.chdir(data_preprocessed)\n",
    "\n",
    "# === Load preprocessed speech data ===\n",
    "\n",
    "preprocessed_final_files = [\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed1_final.pkl')),\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed2_final.pkl')),\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed3_final.pkl')),\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed4_final.pkl'))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1cbd1f95-c7f4-41f1-a292-542ee34ae882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit\\\\data\\\\distances_10epochs.pkl']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################################\n",
    "# Define Functions              ###\n",
    "###################################\n",
    "\n",
    "# apparently is missing deleting intermediate files\n",
    "\n",
    "def documentvecweight(lista):\n",
    "    out = []\n",
    "    lista = [i for i in lista if len(i[1]) > 0]\n",
    "    for s in lista:\n",
    "        vecs = [w2v.wv[w] * word_counts_weighted[w] for w in s[1] if w in w2v.wv]\n",
    "        if len(vecs) == 0:\n",
    "            a = np.nan\n",
    "            c = np.nan\n",
    "            score = np.nan\n",
    "        else:\n",
    "            v = np.mean(vecs, axis=0)\n",
    "            a = cosine(v, affect_centroid)\n",
    "            c = cosine(v, cog_centroid)\n",
    "            score = (1 + 1 - a) / (1 + 1 - c)\n",
    "        out.append([s[0], a, c, score])\n",
    "    return out\n",
    "\n",
    "\n",
    "def main_function(file_path, idx):\n",
    "    dataset = joblib.load(file_path)\n",
    "    data = documentvecweight(dataset)\n",
    "    lab = os.path.join(data_c, f'temp_distances_main_{idx}.pkl')\n",
    "    joblib.dump(data, lab)\n",
    "\n",
    "\n",
    "###################################\n",
    "#      Run main directly        ###\n",
    "###################################\n",
    "\n",
    "def main():\n",
    "    files = [\n",
    "        os.path.join(data_preprocessed, f'preprocessed_speeches_indexed{i+1}_final.pkl') #Changed!\n",
    "        for i in range(4)\n",
    "    ]\n",
    "    for i, f in enumerate(files, start=1):\n",
    "        main_function(f, i)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "###################################\n",
    "#      Recompose everything     ###\n",
    "###################################\n",
    "\n",
    "DATA_temp = [os.path.join(data_c, f'temp_distances_main_{i+1}.pkl') for i in range(4)]\n",
    "\n",
    "tot = []\n",
    "for dataname in DATA_temp:\n",
    "    d = joblib.load(dataname)\n",
    "    tot += d\n",
    "\n",
    "tot_df = pd.DataFrame(tot, columns=['filename', 'affect_d', 'cognition_d', 'score'])\n",
    "joblib.dump(tot_df, os.path.join(data_c, 'distances_10epochs.pkl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb955ab4-243d-470d-a967-e0af1e474c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          filename  affect_d  cognition_d     score\n",
      "0  URY_10_1955.txt  1.403596     0.978834  0.584043\n",
      "1  NOR_71_2016.txt  1.383179     1.168100  0.741460\n",
      "2  BHS_57_2002.txt  1.461827     1.302409  0.771474\n",
      "3  CHN_54_1999.txt  1.375474     0.966137  0.604070\n",
      "4  GMB_41_1986.txt  1.427820     1.020183  0.583966\n",
      "Shape: (4999, 4)\n",
      "          affect_d  cognition_d        score\n",
      "count  4999.000000  4999.000000  4999.000000\n",
      "mean      1.340100     1.065684     0.702335\n",
      "std       0.248605     0.191818     0.200435\n",
      "min       0.344712     0.364131     0.302257\n",
      "25%       1.202359     0.939051     0.549704\n",
      "50%       1.391068     1.093530     0.674553\n",
      "75%       1.525907     1.213055     0.831995\n",
      "max       1.768527     1.517948     1.543633\n",
      "             filename  affect_d  cognition_d     score\n",
      "0     URY_10_1955.txt  1.403596     0.978834  0.584043\n",
      "1     NOR_71_2016.txt  1.383179     1.168100  0.741460\n",
      "2     BHS_57_2002.txt  1.461827     1.302409  0.771474\n",
      "3     CHN_54_1999.txt  1.375474     0.966137  0.604070\n",
      "4     GMB_41_1986.txt  1.427820     1.020183  0.583966\n",
      "...               ...       ...          ...       ...\n",
      "4994  GAB_24_1969.txt  0.782717     0.637575  0.893468\n",
      "4995  IDN_78_2023.txt  1.427714     1.187334  0.704208\n",
      "4996  SYC_71_2016.txt  1.291772     1.167904  0.851137\n",
      "4997  TUR_58_2003.txt  1.502007     1.212588  0.632443\n",
      "4998  JAM_20_1965.txt  1.667997     1.041882  0.346516\n",
      "\n",
      "[4999 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Print the first few rows\n",
    "print(tot_df.head())\n",
    "\n",
    "# Optionally, print the shape to see how many documents were processed\n",
    "print(\"Shape:\", tot_df.shape)\n",
    "\n",
    "# Print a quick summary\n",
    "print(tot_df.describe())\n",
    "\n",
    "# Or print the full DataFrame (if small)\n",
    "print(tot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8c295cd4-4fa0-470f-b67c-4f584f7a3f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your main corpus CSV \n",
    "un_corpus_merged = pd.read_csv(os.path.join(data_c, \"un_corpus_merged.csv\"), sep=';', encoding='utf-8') \n",
    "# Merge on filename \n",
    "un_corpus_scored = un_corpus_merged.merge(tot_df, on=\"filename\", how=\"left\")\n",
    "\n",
    "# Optionally save back as pickle\n",
    "joblib.dump(un_corpus_scored, os.path.join(data_c, \"un_corpus_scored.pkl\"))\n",
    "\n",
    "un_corpus_scored.to_csv(\n",
    "    os.path.join(data_c, \"un_corpus_scored.csv\"),\n",
    "    sep=';', \n",
    "    index=False, \n",
    "    encoding='utf-8'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d8d13667-1eb3-482e-83ca-5969a229750a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             filename                                             speech  \\\n",
      "0     URY_10_1955.txt  97.\\tFrom this august rostrum, I wish to reaff...   \n",
      "1     NOR_71_2016.txt  This year’s session of the General Assembly co...   \n",
      "2     BHS_57_2002.txt  ﻿On behalf of Prime\\nMinister Perry G. Christi...   \n",
      "3     CHN_54_1999.txt  Please allow me to warmly congratulate you, Si...   \n",
      "4     GMB_41_1986.txt  It is gratifying to see at the helm of this im...   \n",
      "...               ...                                                ...   \n",
      "4994  GAB_24_1969.txt  77. Madam President, I should like, in my turn...   \n",
      "4995  IDN_78_2023.txt  Today I am wearing a traditional fabric from E...   \n",
      "4996  SYC_71_2016.txt  Allow me to add Seychelles’ voice to those who...   \n",
      "4997  TUR_58_2003.txt  ﻿I should like to start by\\nextending our warm...   \n",
      "4998  JAM_20_1965.txt  85. Mr. President, on behalf of the Jamaican d...   \n",
      "\n",
      "     country_code  year country_name  speech_length_words  \\\n",
      "0             URY  1955      Uruguay                 2228   \n",
      "1             NOR  2016       Norway                 1610   \n",
      "2             BHS  2002      Bahamas                 2036   \n",
      "3             CHN  1999        China                 3137   \n",
      "4             GMB  1986       Gambia                 2971   \n",
      "...           ...   ...          ...                  ...   \n",
      "4994          GAB  1969        Gabon                 6369   \n",
      "4995          IDN  2023    Indonesia                 1203   \n",
      "4996          SYC  2016   Seychelles                 1221   \n",
      "4997          TUR  2003      Türkiye                 1769   \n",
      "4998          JAM  1965      Jamaica                 3831   \n",
      "\n",
      "      english_official_language  security_council_permanent  \\\n",
      "0                             0                           0   \n",
      "1                             0                           0   \n",
      "2                             1                           0   \n",
      "3                             0                           1   \n",
      "4                             1                           0   \n",
      "...                         ...                         ...   \n",
      "4994                          0                           0   \n",
      "4995                          0                           0   \n",
      "4996                          1                           0   \n",
      "4997                          0                           0   \n",
      "4998                          0                           0   \n",
      "\n",
      "                         speaker_name                               position  \\\n",
      "0                       Mr. Basagoiti                                    NaN   \n",
      "1                    Ms. Erna Solberg                (Deputy) Prime Minister   \n",
      "2               Frederick A. Mitchell  (Deputy) Minister for Foreign Affairs   \n",
      "3                        Tang Jiaxuan  (Deputy) Minister for Foreign Affairs   \n",
      "4                          Mr. JABANG                                    NaN   \n",
      "...                               ...                                    ...   \n",
      "4994                       Mr. AYOUNE                                    NaN   \n",
      "4995  Retno Lestari Priansari Marsudi  (Deputy) Minister for Foreign Affairs   \n",
      "4996          Ms. Marie-Louise Potter              Diplomatic Representative   \n",
      "4997                     Abdullah Gül  (Deputy) Minister for Foreign Affairs   \n",
      "4998                     Mr. SHEARER                                     NaN   \n",
      "\n",
      "      gender_dummy       speech_label  affect_d  cognition_d     score  \n",
      "0              0.0     Uruguay (1955)  1.403596     0.978834  0.584043  \n",
      "1              1.0      Norway (2016)  1.383179     1.168100  0.741460  \n",
      "2              NaN     Bahamas (2002)  1.461827     1.302409  0.771474  \n",
      "3              NaN       China (1999)  1.375474     0.966137  0.604070  \n",
      "4              0.0      Gambia (1986)  1.427820     1.020183  0.583966  \n",
      "...            ...                ...       ...          ...       ...  \n",
      "4994           0.0       Gabon (1969)  0.782717     0.637575  0.893468  \n",
      "4995           NaN   Indonesia (2023)  1.427714     1.187334  0.704208  \n",
      "4996           1.0  Seychelles (2016)  1.291772     1.167904  0.851137  \n",
      "4997           NaN     Türkiye (2003)  1.502007     1.212588  0.632443  \n",
      "4998           0.0     Jamaica (1965)  1.667997     1.041882  0.346516  \n",
      "\n",
      "[4999 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "print(un_corpus_scored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1210b88f-cf13-4da0-807b-77764cf0d439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count where affect_d is NaN: 0\n",
      "Count where affect_d is not NaN: 4999\n"
     ]
    }
   ],
   "source": [
    "# Count where affect_d is NaN\n",
    "nan_count = un_corpus_scored['affect_d'].isna().sum()\n",
    "\n",
    "# Count where affect_d is not NaN\n",
    "not_nan_count = un_corpus_scored['affect_d'].notna().sum()\n",
    "\n",
    "print(\"Count where affect_d is NaN:\", nan_count)\n",
    "print(\"Count where affect_d is not NaN:\", not_nan_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594b17bd-15b6-4897-ab94-9b12175e7697",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
