{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6ca6f721-48ad-495c-8e5f-e60790225709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "from pathlib import Path\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "# from gensim.summarization.textcleaner import get_sentences\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from random import shuffle\n",
    "import nltk\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.tokenize import sent_tokenize\n",
    "tagger = nltk.perceptron.PerceptronTagger()\n",
    "import joblib\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from string import punctuation\n",
    "from scipy.spatial.distance import cosine\n",
    "import glob\n",
    "from multiprocessing import Pool, freeze_support\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# === Initialize NLP Tools ===\n",
    "\n",
    "translator = str.maketrans('', '', punctuation) \n",
    "tagger = nltk.perceptron.PerceptronTagger()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "# === Set Working Directory ===\n",
    "\n",
    "# Set your working directory (adjust this as needed)\n",
    "wd = Path(r\"C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\")\n",
    "wd_models = wd / \"models\"\n",
    "wd_results = wd / \"results\"\n",
    "\n",
    "\n",
    "# === Define Folder Paths ===\n",
    "\n",
    "# Make sure that you have these folders in your working directory\n",
    "data_c = wd / \"data\"\n",
    "data_temp = data_c / \"temp\"\n",
    "data_freq = data_c / \"freq\"\n",
    "data_dict = data_c / \"dictionaries\"\n",
    "data_sent = data_c / \"sentences\"\n",
    "data_preprocessed = data_c / \"preprocessed\"\n",
    "fig_dir = wd /\"fig\"\n",
    "\n",
    "# Upload ressources\n",
    "stopwords = joblib.load(data_c / \"stopwords.pkl\")\n",
    "word_counts = joblib.load(data_freq / \"word_counts_stemmed.pkl\")\n",
    "word_counts_weighted = joblib.load(data_freq / \"word_counts_weighted.pkl\")\n",
    "affect_dic = joblib.load(data_dict / 'dictionary_affect.pkl')\n",
    "cognition_dic = joblib.load(data_dict / 'dictionary_cognition.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "125227ca-a0b5-43d7-956d-7d49a66f81b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_temp)\n",
    "cleaned_files = [\n",
    "    str(data_temp / 'clean_speeches_indexed1.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed2.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed3.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed4.pkl')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "af0997e8-9da3-45c3-bdd4-7d38a708a474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl processed\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed1.pkl saved\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl processed\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed2.pkl saved\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl processed\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed3.pkl saved\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl processed\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed4.pkl saved\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_sent)\n",
    "def extract_sentences(dataname):\n",
    "    data = joblib.load(dataname)\n",
    "    data = [a[1] for a in data]  # keep only text, no id\n",
    "\n",
    "    sentences = []\n",
    "    for doc in data:\n",
    "        sentences += sent_tokenize(doc)  # use nltk's sent_tokenize here\n",
    "\n",
    "    sentences = [item for item in sentences if len(item.split()) > 1]\n",
    "    sentences = [gensim.utils.simple_preprocess(item) for item in sentences]\n",
    "\n",
    "    sentences = [[a for a in s if not a.isdigit()] for s in sentences]\n",
    "    sentences = [[a for a in s if len(a) > 2] for s in sentences]\n",
    "\n",
    "    sentences = [tagger.tag(s) for s in sentences]\n",
    "    sentences = [[i[0] for i in s if i[1].startswith(('N', 'V', 'J'))] for s in sentences]\n",
    "\n",
    "    sentences = [[stemmer.stem(i) for i in s] for s in sentences]\n",
    "    sentences = [[a for a in s if a not in stopwords] for s in sentences]\n",
    "    sentences = [[a for a in s if word_counts[a] >= 10] for s in sentences]\n",
    "\n",
    "    sentences = [s for s in sentences if len(s) > 1]  # eliminate empty\n",
    "    shuffle(sentences)\n",
    "\n",
    "    lab = dataname.replace('clean_speeches_', 'sentences_').replace('_.pkl', '.pkl')\n",
    "    print(f'{dataname} processed')\n",
    "    joblib.dump(sentences, lab)\n",
    "    print(f'{lab} saved')\n",
    "\n",
    "# Run for all your files\n",
    "for fname in cleaned_files:\n",
    "    extract_sentences(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4f1a2e83-e5f4-436b-84c9-0d9d51f3b761",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_files = [\n",
    "    'sentences_indexed1.pkl',\n",
    "    'sentences_indexed2.pkl',\n",
    "    'sentences_indexed3.pkl',\n",
    "    'sentences_indexed4.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "465bc73c-70b6-4dbf-a379-8c5235f83a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for dataname in sentences_files:  # <-- your list of sentence files\n",
    "    data = joblib.load(dataname)\n",
    "    dataset.extend(data)  # extend instead of append if you want all sentences in a single list\n",
    "\n",
    "# === Model training ===\n",
    "w2v = Word2Vec(\n",
    "    sentences=dataset,    # iterator that loops over tokenized sentences\n",
    "    vector_size=300,      # Word vector dimensionality (use `vector_size` in newer gensim)\n",
    "    window=8,             # Context window size\n",
    "    min_count=10,         # Minimum word count\n",
    "    workers=8,            # Number of threads\n",
    "    sample=1e-3,          # Downsample setting for frequent words\n",
    "    epochs=10             # Number of iterations over the corpus\n",
    ")\n",
    "\n",
    "# Optimize memory usage (optional)\n",
    "w2v.wv.fill_norms()  # only works in older gensim versions\n",
    "\n",
    "# Save model\n",
    "wd_models.mkdir(parents=True, exist_ok=True)  # create folder if it doesn't exist\n",
    "w2v.save(str(wd_models / 'w2v-vectors_8_300.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9d94142c-1e7f-4b0b-bfec-98604fc84304",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec.load(str(wd_models / \"w2v-vectors_8_300.pkl\"))\n",
    "word_vectors = w2v.wv # Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5b75bd3b-2957-42d1-a89c-89e5b3ea44f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['centroids/cog_centroid.pkl']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Calculate centroids ===\n",
    "\n",
    "def findcentroid(text, model):\n",
    "    vecs = [model.wv[w] * word_counts_weighted[w] for w in text if w in model.wv]\n",
    "    vecs = [v for v in vecs if len(v) > 0]\n",
    "    centroid = np.mean(vecs, axis=0)\n",
    "    #centroid = centroid.reshape(1, -1)\n",
    "    return centroid\n",
    "\n",
    "\n",
    "affect_centroid = findcentroid(affect_dic, w2v)\n",
    "cog_centroid = findcentroid(cognition_dic, w2v)\n",
    "\n",
    "os.chdir(data_c)\n",
    "joblib.dump(affect_centroid, 'centroids/affect_centroid.pkl')\n",
    "joblib.dump(cog_centroid, 'centroids/cog_centroid.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bcab4536-fca7-468d-ae93-6e8da17cbfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affect centroid vector:\n",
      " [-1.80915624e-01  4.30551469e-02  4.86425944e-02  2.65579313e-01\n",
      "  1.73561484e-01  2.01928109e-01  2.59091765e-01  2.36184999e-01\n",
      "  3.98101518e-03 -2.57320940e-01  3.30538414e-02 -4.66788746e-02\n",
      " -2.68821884e-02 -1.46668795e-02 -9.18277130e-02  4.52306643e-02\n",
      "  1.75294057e-01 -2.10680008e-01  1.29888400e-01  1.53037488e-01\n",
      " -2.45522588e-01  2.53033429e-01  1.37193590e-01 -4.95398790e-02\n",
      "  7.25664914e-01 -8.10661539e-02 -3.53843033e-01  1.57792762e-01\n",
      "  8.46496150e-02 -7.38068372e-02 -3.12771946e-01  5.49141020e-02\n",
      " -2.04309061e-01  1.51313365e-01  1.33174509e-01  1.24550983e-01\n",
      "  6.05256902e-03 -3.79772633e-01  2.44276762e-01 -9.23716836e-03\n",
      "  3.13754403e-03  2.22832456e-01  1.93224162e-01  7.57742450e-02\n",
      "  1.42783597e-01  6.40479922e-02 -2.33480632e-01 -1.22875564e-01\n",
      "  7.92843774e-02  3.89629900e-01 -6.71419799e-02  2.23763511e-01\n",
      " -9.55797508e-02  2.05186456e-01  3.33965905e-02  1.99110731e-01\n",
      "  3.10475737e-01  1.28718331e-01  3.44972521e-01  2.34461591e-01\n",
      " -1.23477317e-01  2.13530049e-01  5.69710620e-02 -1.08717561e-01\n",
      "  7.55079165e-02  1.06706373e-01 -3.85497697e-03  2.69491136e-01\n",
      "  1.12570517e-01  2.40660697e-01 -1.72032937e-02  2.52427578e-01\n",
      "  1.40481919e-01 -1.99293271e-01  1.78248793e-01 -1.65404916e-01\n",
      " -1.27066761e-01  4.26906860e-03  2.25693937e-02 -6.20057099e-02\n",
      " -2.07735941e-01 -6.64213523e-02 -2.39039585e-01 -5.50880469e-02\n",
      "  1.47760734e-01 -4.16651741e-02 -3.63793597e-03  2.04856277e-01\n",
      "  7.92842805e-02 -7.89860860e-02 -1.51567757e-01 -1.32508874e-01\n",
      "  2.75703380e-03 -1.23685531e-01  2.87566990e-01  1.02893665e-01\n",
      "  2.00154722e-01 -4.17750627e-02  5.44385379e-03  4.98195678e-01\n",
      "  2.00351432e-01 -1.64328039e-01 -5.34973033e-02 -9.26775336e-02\n",
      "  2.46356115e-01 -1.68911412e-01  5.06539568e-02 -5.02169691e-03\n",
      " -1.58990011e-01  4.95634452e-02 -8.58196840e-02 -2.44232073e-01\n",
      "  1.59825459e-01  2.44394422e-01 -1.20554775e-01  4.62343872e-01\n",
      "  1.21297188e-01 -1.94517583e-01 -6.10820167e-02 -2.51493663e-01\n",
      " -4.85286675e-02 -3.20635922e-02 -3.06267850e-02  2.04512358e-01\n",
      " -1.05276249e-01  3.98439616e-02  1.38895810e-01  3.19489231e-03\n",
      " -6.41083121e-02 -4.95081581e-02 -4.58312258e-02 -5.73703289e-01\n",
      "  3.31278220e-02  8.26915056e-02 -3.83493118e-02 -2.30495393e-01\n",
      "  9.37731415e-02  5.72440140e-02 -7.66411126e-02 -1.37690440e-01\n",
      "  6.91753998e-02 -7.97108337e-02 -7.35261589e-02  3.29088032e-01\n",
      "  1.11882232e-01 -9.86582879e-03 -3.57143193e-01 -2.00884640e-02\n",
      " -7.61331990e-02  3.44276242e-02  7.25190789e-02 -5.31032383e-02\n",
      " -2.66760111e-01  1.25571042e-01 -1.59138236e-02  1.30245999e-01\n",
      "  9.89418849e-02 -4.09314066e-01  2.51556277e-01 -3.29144508e-01\n",
      " -2.34743804e-02  1.62215009e-01 -1.28832996e-01 -1.42040640e-01\n",
      " -3.41340959e-01  2.12881342e-01  4.29793149e-02 -1.19487293e-01\n",
      "  1.17802620e-01  2.18177274e-01 -1.21656500e-01  2.16564238e-01\n",
      "  1.60609484e-01  1.94230944e-01  2.84384012e-01 -1.40942663e-01\n",
      " -7.60614574e-02 -1.35346606e-01 -1.86218768e-01 -1.21665321e-01\n",
      " -7.63691366e-02  3.06716859e-01  8.49606097e-02  2.19271202e-02\n",
      " -1.33892685e-01 -1.18122265e-01 -6.49348125e-02  1.80258691e-01\n",
      "  1.75020456e-01 -3.93849492e-01 -2.28168681e-01  2.25088015e-01\n",
      "  3.19361210e-01  2.47900158e-01 -9.00891516e-03 -1.17928781e-01\n",
      "  2.08800728e-03 -2.73608476e-01  1.92031991e-02  5.55379353e-02\n",
      " -2.13193715e-01  2.26124808e-01 -3.15643959e-02  2.26974785e-01\n",
      "  3.97069082e-02 -2.13517874e-01 -1.11288801e-01  7.80686270e-03\n",
      "  3.95582765e-02 -3.91383380e-01 -1.38440847e-01  2.14809645e-02\n",
      " -8.40196386e-03  4.73751053e-02 -5.71342036e-02 -2.89739698e-01\n",
      "  7.31997862e-02  1.12421073e-01 -5.23660123e-01 -4.50476743e-02\n",
      " -2.83765867e-02  3.07092555e-02 -2.49689475e-01  8.11417252e-02\n",
      " -9.04008839e-03  4.91937660e-02  2.15396836e-01  1.02766044e-01\n",
      " -3.65327537e-01  1.69250697e-01 -4.90325876e-02 -1.96686193e-01\n",
      " -1.71003982e-01 -1.73428580e-02 -8.70791301e-02  2.17983887e-01\n",
      "  4.98113558e-02 -1.25807106e-01  9.27243084e-02 -4.84696478e-02\n",
      " -4.44931567e-01 -8.28099810e-03  1.37664169e-01  2.13539615e-01\n",
      " -2.60035872e-01  8.97546951e-03 -2.65863091e-02 -1.28733903e-01\n",
      "  3.61909568e-02  1.42359704e-01  5.70402026e-01 -6.54937327e-02\n",
      " -1.15831792e-01  1.22419871e-01 -2.33691022e-01  2.50931412e-01\n",
      "  1.88870266e-01  1.00164168e-01 -2.39869952e-01 -2.24839523e-01\n",
      " -1.11993305e-01  3.63301992e-01  8.72067641e-03  1.08783551e-01\n",
      " -5.10471761e-02  3.58971879e-02 -2.42222071e-01 -5.55803552e-02\n",
      " -1.07040159e-01 -1.85556471e-01  2.46840194e-02  2.60976940e-01\n",
      "  9.20674764e-03  1.75204650e-01  4.04757299e-02  5.48271298e-01\n",
      "  1.15020029e-01  3.73143971e-01  2.32030988e-01  1.21593982e-01\n",
      "  1.17772937e-01 -3.27447169e-02 -3.44376117e-02 -5.88120334e-03\n",
      "  2.25938037e-01  2.84659714e-01 -3.00460815e-01  2.29730699e-02\n",
      " -7.71068782e-02  1.38225257e-01 -1.06387429e-01  2.99748600e-01\n",
      " -5.67008567e-04  1.86533511e-01  2.30687723e-01  2.08834141e-01\n",
      " -2.26690158e-01 -4.03873622e-01 -2.92340308e-01  4.00367379e-02]\n",
      "\n",
      "Cognition centroid vector:\n",
      " [-0.0305301   0.17588273  0.170422    0.09489924 -0.01386344 -0.05115236\n",
      "  0.0723794   0.03496026  0.13661131 -0.23045072  0.01391608  0.02060167\n",
      "  0.26148283 -0.02912643 -0.04854375 -0.05081615 -0.17875913 -0.26052704\n",
      "  0.06208301  0.2589503  -0.1492652   0.10454159  0.1776359  -0.00285094\n",
      "  0.27660236 -0.12296186 -0.0135588  -0.10823444  0.06466636 -0.04425308\n",
      "  0.04867446 -0.01947575 -0.09024028  0.04378852 -0.04379233  0.23066267\n",
      "  0.06481634 -0.13119666 -0.00957186  0.11566469 -0.1815374   0.04266826\n",
      "  0.05853227 -0.01020019  0.04658272 -0.14384009 -0.00900155 -0.12296761\n",
      "  0.03354463  0.07176846 -0.00376495 -0.01276327 -0.01270005 -0.04533552\n",
      "  0.03865964  0.16561899  0.25198367 -0.23038758 -0.06457898  0.11054149\n",
      " -0.35791618  0.12187406  0.04220784  0.04210257  0.23167862  0.25245196\n",
      "  0.00487841  0.05012687  0.27890497 -0.03620638 -0.05249434  0.21695463\n",
      "  0.08264954 -0.02016968 -0.00726106  0.06865475 -0.05988801 -0.08213919\n",
      "  0.19633736  0.2749086  -0.06752913 -0.168428   -0.1606722  -0.12864488\n",
      "  0.00484517  0.14395864 -0.0974763   0.03969178  0.18104753  0.05644602\n",
      "  0.01263402 -0.18640837  0.16401269  0.24472949  0.18814881  0.00303304\n",
      " -0.00180003  0.08050353 -0.18879725  0.011926   -0.16211137 -0.17288782\n",
      "  0.20747519 -0.05840045 -0.05422379  0.09340271 -0.15214914  0.03605088\n",
      "  0.13992086  0.08855584 -0.06981234 -0.28198624 -0.02256832  0.06363516\n",
      " -0.15783206  0.06604355 -0.02963239 -0.15737873  0.06211825 -0.06565984\n",
      " -0.04940044 -0.05860874  0.18521045  0.20063901 -0.03878368  0.27933308\n",
      "  0.05228554  0.12079893 -0.05554134 -0.19283904  0.00470704 -0.16613303\n",
      " -0.06127292  0.28275135 -0.05767753 -0.25915152 -0.08820657  0.06181393\n",
      " -0.10031064 -0.04259045 -0.17171271 -0.01587176  0.07651481  0.03843568\n",
      "  0.11715344 -0.00332999 -0.06852731  0.174277    0.06101495  0.12284876\n",
      " -0.16534294 -0.11982048  0.05382289  0.04225681 -0.12024987  0.2155188\n",
      "  0.07855798 -0.19867475  0.14664058 -0.00445694 -0.07836436  0.0930922\n",
      " -0.09559067 -0.10019538 -0.3184734  -0.07353313 -0.00475357 -0.00891581\n",
      "  0.04355659  0.31067422  0.03539255 -0.16356008  0.16100398 -0.05089959\n",
      "  0.10315651  0.0204384   0.12306218 -0.07787956 -0.14572653  0.1561065\n",
      "  0.07217604 -0.10338365 -0.06411885 -0.11554919  0.1776866  -0.20528008\n",
      " -0.16147901  0.19002722 -0.06824971 -0.18237494 -0.19636515 -0.14173056\n",
      "  0.15930049 -0.12006439  0.2159531   0.00360672 -0.05630982  0.04553832\n",
      "  0.14993188 -0.04763605 -0.05274466  0.18164307 -0.03704785  0.21687515\n",
      " -0.11326642 -0.0709912  -0.15067366 -0.1521607   0.14018153 -0.16028792\n",
      "  0.04657858  0.20274442  0.08392735  0.2976548   0.06838542  0.00231466\n",
      " -0.09330261  0.10232286 -0.11466058 -0.01772376  0.02618781 -0.04959559\n",
      "  0.07101455  0.22170305 -0.0768441   0.00371758  0.14171137 -0.06745917\n",
      " -0.03778668  0.20307857  0.22633862 -0.09056835 -0.06801838  0.03063743\n",
      " -0.04969559 -0.09980629  0.0014034  -0.23127423  0.10580628  0.03679391\n",
      "  0.02066747  0.13691452 -0.08061925 -0.15573184 -0.39397585  0.16553801\n",
      " -0.21572521 -0.08404234  0.20277402  0.06405102  0.11814504  0.14261113\n",
      " -0.00535728 -0.04828534  0.06715102  0.26673272 -0.06874548  0.04458723\n",
      " -0.11058382 -0.12714769 -0.1355832   0.09764821 -0.20458785 -0.03737286\n",
      " -0.00085119 -0.22122137 -0.18833987 -0.09551759 -0.11225434  0.15390097\n",
      "  0.07005639 -0.05627325 -0.17951526 -0.08904679  0.13965657 -0.02946933\n",
      "  0.03389514 -0.00843831  0.14624356  0.07508969 -0.03888934 -0.11284024\n",
      " -0.00860921 -0.0936632   0.13863057  0.08166675 -0.104987    0.1164788\n",
      " -0.00937179 -0.12485331 -0.18946907 -0.05175663 -0.08032056  0.00545954\n",
      "  0.07445845  0.19972873 -0.08123317 -0.1003222  -0.01218343  0.23244429]\n",
      "\n",
      "Shape of affect centroid: (300,)\n",
      "Shape of cognition centroid: (300,)\n",
      "\n",
      "Affect centroid summary: min, max, mean: -0.5737033 0.7256649 0.014211443\n",
      "Cognition centroid summary: min, max, mean: -0.39397585 0.31067422 0.0036837896\n"
     ]
    }
   ],
   "source": [
    "# Print the vectors\n",
    "print(\"Affect centroid vector:\\n\", affect_centroid)\n",
    "print(\"\\nCognition centroid vector:\\n\", cog_centroid)\n",
    "\n",
    "# Optional: shape and stats\n",
    "print(\"\\nShape of affect centroid:\", affect_centroid.shape)\n",
    "print(\"Shape of cognition centroid:\", cog_centroid.shape)\n",
    "\n",
    "print(\"\\nAffect centroid summary: min, max, mean:\", \n",
    "      np.min(affect_centroid), np.max(affect_centroid), np.mean(affect_centroid))\n",
    "print(\"Cognition centroid summary: min, max, mean:\", \n",
    "      np.min(cog_centroid), np.max(cog_centroid), np.mean(cog_centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c8316f-f794-4bcd-87c9-578aa82f88b0",
   "metadata": {},
   "source": [
    "## Emotionality Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3376a4f6-a002-4179-8614-dfef96113180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISSUE why doesnt this work with the sentences?!\n",
    "\n",
    "# Set wd to data_preprocessed\n",
    "os.chdir(data_preprocessed)\n",
    "\n",
    "# === Load preprocessed speech data ===\n",
    "\n",
    "preprocessed_final_files = [\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed1_final.pkl')),\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed2_final.pkl')),\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed3_final.pkl')),\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed4_final.pkl'))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1cbd1f95-c7f4-41f1-a292-542ee34ae882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit\\\\data\\\\distances_10epochs.pkl']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################################\n",
    "# Define Functions              ###\n",
    "###################################\n",
    "\n",
    "# apparently is missing deleting intermediate files\n",
    "\n",
    "def documentvecweight(lista):\n",
    "    out = []\n",
    "    lista = [i for i in lista if len(i[1]) > 0]\n",
    "    for s in lista:\n",
    "        vecs = [w2v.wv[w] * word_counts_weighted[w] for w in s[1] if w in w2v.wv]\n",
    "        if len(vecs) == 0:\n",
    "            a = np.nan\n",
    "            c = np.nan\n",
    "            score = np.nan\n",
    "        else:\n",
    "            v = np.mean(vecs, axis=0)\n",
    "            a = cosine(v, affect_centroid)\n",
    "            c = cosine(v, cog_centroid)\n",
    "            score = (1 + 1 - a) / (1 + 1 - c)\n",
    "        out.append([s[0], a, c, score])\n",
    "    return out\n",
    "\n",
    "\n",
    "def main_function(file_path, idx):\n",
    "    dataset = joblib.load(file_path)\n",
    "    data = documentvecweight(dataset)\n",
    "    lab = os.path.join(data_c, f'temp_distances_main_{idx}.pkl')\n",
    "    joblib.dump(data, lab)\n",
    "\n",
    "\n",
    "###################################\n",
    "#      Run main directly        ###\n",
    "###################################\n",
    "\n",
    "def main():\n",
    "    files = [\n",
    "        os.path.join(data_preprocessed, f'preprocessed_speeches_indexed{i+1}_final.pkl') #Changed!\n",
    "        for i in range(4)\n",
    "    ]\n",
    "    for i, f in enumerate(files, start=1):\n",
    "        main_function(f, i)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "###################################\n",
    "#      Recompose everything     ###\n",
    "###################################\n",
    "\n",
    "DATA_temp = [os.path.join(data_c, f'temp_distances_main_{i+1}.pkl') for i in range(4)]\n",
    "\n",
    "tot = []\n",
    "for dataname in DATA_temp:\n",
    "    d = joblib.load(dataname)\n",
    "    tot += d\n",
    "\n",
    "tot_df = pd.DataFrame(tot, columns=['filename', 'affect_d', 'cognition_d', 'score'])\n",
    "joblib.dump(tot_df, os.path.join(data_c, 'distances_10epochs.pkl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fb955ab4-243d-470d-a967-e0af1e474c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          filename  affect_d  cognition_d     score\n",
      "0  MYS_14_1959.txt  1.238613     1.034314  0.788441\n",
      "1  SAU_56_2001.txt  1.023923     0.974944  0.952219\n",
      "2  BDI_56_2001.txt  1.285262     1.238824  0.938992\n",
      "3  TGO_65_2010.txt  1.279652     1.159825  0.857378\n",
      "4  BGR_42_1987.txt  1.433111     0.901496  0.516056\n",
      "Shape: (10777, 4)\n",
      "           affect_d   cognition_d         score\n",
      "count  10777.000000  10777.000000  10777.000000\n",
      "mean       1.251250      0.972559      0.728025\n",
      "std        0.261643      0.194183      0.210453\n",
      "min        0.330084      0.318983      0.290318\n",
      "25%        1.087340      0.841552      0.569493\n",
      "50%        1.295868      0.989703      0.699832\n",
      "75%        1.450793      1.119933      0.863281\n",
      "max        1.761331      1.477002      1.691517\n",
      "              filename  affect_d  cognition_d     score\n",
      "0      MYS_14_1959.txt  1.238613     1.034314  0.788441\n",
      "1      SAU_56_2001.txt  1.023923     0.974944  0.952219\n",
      "2      BDI_56_2001.txt  1.285262     1.238824  0.938992\n",
      "3      TGO_65_2010.txt  1.279652     1.159825  0.857378\n",
      "4      BGR_42_1987.txt  1.433111     0.901496  0.516056\n",
      "...                ...       ...          ...       ...\n",
      "10772  CSK_30_1975.txt  1.474145     1.017436  0.535187\n",
      "10773  AFG_61_2006.txt  1.016798     1.203017  1.233655\n",
      "10774  NER_48_1993.txt  1.297108     1.198048  0.876477\n",
      "10775  AND_54_1999.txt  1.166677     0.837366  0.716754\n",
      "10776  EGY_74_2019.txt  1.532991     1.122895  0.532443\n",
      "\n",
      "[10777 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Print the first few rows\n",
    "print(tot_df.head())\n",
    "\n",
    "# Optionally, print the shape to see how many documents were processed\n",
    "print(\"Shape:\", tot_df.shape)\n",
    "\n",
    "# Print a quick summary\n",
    "print(tot_df.describe())\n",
    "\n",
    "# Or print the full DataFrame (if small)\n",
    "print(tot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8c295cd4-4fa0-470f-b67c-4f584f7a3f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your main corpus CSV \n",
    "un_corpus_merged = pd.read_csv(os.path.join(data_c, \"un_corpus_merged.csv\"), sep=';', encoding='utf-8') \n",
    "# Merge on filename \n",
    "un_corpus_merged = un_corpus_merged.merge(tot_df, on=\"filename\", how=\"left\")\n",
    "\n",
    "# Optionally save back as pickle\n",
    "joblib.dump(un_corpus_merged, os.path.join(data_c, \"un_corpus_merged_with_scores.pkl\"))\n",
    "\n",
    "un_corpus_merged.to_csv(\n",
    "    os.path.join(data_c, \"un_corpus_merged_with_scores.csv\"),\n",
    "    sep=';', \n",
    "    index=False, \n",
    "    encoding='utf-8'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d8d13667-1eb3-482e-83ca-5969a229750a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              filename                                             speech  \\\n",
      "0      MYS_14_1959.txt  1.\\tMr. President, it was my intention to cong...   \n",
      "1      SAU_56_2001.txt  In the name of Allah, most compassionate, most...   \n",
      "2      BDI_56_2001.txt  ﻿It\\nis for me a signal honour to take the flo...   \n",
      "3      TGO_65_2010.txt  At the \\noutset, allow me to extend my congrat...   \n",
      "4      BGR_42_1987.txt  ﻿\\nComrade president, allow me to congratulate...   \n",
      "...                ...                                                ...   \n",
      "10806  CSK_30_1975.txt  42.\\t  Mr. President, may I first of all congr...   \n",
      "10807  AFG_61_2006.txt  Every year our gathering \\nunder this roof doe...   \n",
      "10808  NER_48_1993.txt  On behalf of the Niger and its people, I wish ...   \n",
      "10809  AND_54_1999.txt  We are coming to\\nthe end of 1999, a date that...   \n",
      "10810  EGY_74_2019.txt  At the outset, it gives me pleasure to sincere...   \n",
      "\n",
      "      country_code  year    country_name  speech_length_words  \\\n",
      "0              MYS  1959        Malaysia                 4004   \n",
      "1              SAU  2001    Saudi Arabia                 1934   \n",
      "2              BDI  2001         Burundi                 2676   \n",
      "3              TGO  2010            Togo                 1579   \n",
      "4              BGR  1987        Bulgaria                 3584   \n",
      "...            ...   ...             ...                  ...   \n",
      "10806          CSK  1975  Czechoslovakia                 3458   \n",
      "10807          AFG  2006     Afghanistan                 1186   \n",
      "10808          NER  1993           Niger                 3049   \n",
      "10809          AND  1999         Andorra                 2685   \n",
      "10810          EGY  2019           Egypt                 1891   \n",
      "\n",
      "       english_official_language  security_council_permanent  \\\n",
      "0                              0                           0   \n",
      "1                              0                           0   \n",
      "2                              1                           0   \n",
      "3                              0                           0   \n",
      "4                              0                           0   \n",
      "...                          ...                         ...   \n",
      "10806                          0                           0   \n",
      "10807                          0                           0   \n",
      "10808                          0                           0   \n",
      "10809                          0                           0   \n",
      "10810                          0                           0   \n",
      "\n",
      "                   speaker_name                               position  \\\n",
      "0                  Dato' Ismail                                    NaN   \n",
      "1               Fawzi Shobokshi              Diplomatic Representative   \n",
      "2          Thérence Sinunguruza  (Deputy) Minister for Foreign Affairs   \n",
      "3       Gilbert Fossoun Houngbo                (Deputy) Prime Minister   \n",
      "4                     MIADENOV                                     NaN   \n",
      "...                         ...                                    ...   \n",
      "10806              Mr. CHNOUPEK                                    NaN   \n",
      "10807          Mr. Hamid Karzai                      (Vice-) President   \n",
      "10808      MR. MAHAMANE OUSMANE                      (Vice-) President   \n",
      "10809          Marc Forné Molné                (Deputy) Prime Minister   \n",
      "10810  Mr. Abdel Fattah Al Sisi                      (Vice-) President   \n",
      "\n",
      "       gender_dummy           speech_label  affect_d  cognition_d     score  \n",
      "0               NaN        Malaysia (1959)  1.238613     1.034314  0.788441  \n",
      "1               NaN    Saudi Arabia (2001)  1.023923     0.974944  0.952219  \n",
      "2               NaN         Burundi (2001)  1.285262     1.238824  0.938992  \n",
      "3               NaN            Togo (2010)  1.279652     1.159825  0.857378  \n",
      "4               NaN        Bulgaria (1987)  1.433111     0.901496  0.516056  \n",
      "...             ...                    ...       ...          ...       ...  \n",
      "10806           0.0  Czechoslovakia (1975)  1.474145     1.017436  0.535187  \n",
      "10807           0.0     Afghanistan (2006)  1.016798     1.203017  1.233655  \n",
      "10808           0.0           Niger (1993)  1.297108     1.198048  0.876477  \n",
      "10809           NaN         Andorra (1999)  1.166677     0.837366  0.716754  \n",
      "10810           0.0           Egypt (2019)  1.532991     1.122895  0.532443  \n",
      "\n",
      "[10811 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "print(un_corpus_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1210b88f-cf13-4da0-807b-77764cf0d439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count where affect_d is NaN: 0\n",
      "Count where affect_d is not NaN: 10811\n"
     ]
    }
   ],
   "source": [
    "# Count where affect_d is NaN\n",
    "nan_count = un_corpus_merged['affect_d'].isna().sum()\n",
    "\n",
    "# Count where affect_d is not NaN\n",
    "not_nan_count = un_corpus_merged['affect_d'].notna().sum()\n",
    "\n",
    "print(\"Count where affect_d is NaN:\", nan_count)\n",
    "print(\"Count where affect_d is not NaN:\", not_nan_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594b17bd-15b6-4897-ab94-9b12175e7697",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
