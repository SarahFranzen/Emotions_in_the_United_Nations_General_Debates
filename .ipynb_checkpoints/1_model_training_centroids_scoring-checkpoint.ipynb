{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a39bd0b5-2ac1-499d-8938-2fdfe4c3d93d",
   "metadata": {},
   "source": [
    "# Development of Emotion and Reasoning in the General Speeches of the United Nations: A text-based machine learning approach\n",
    "## Script 1: Model Traning, Calculation Centroids & Speech Scoring\n",
    "### Author: Sarah Franzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ca6f721-48ad-495c-8e5f-e60790225709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your working directory path (e.g., C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit):  C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory set to: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\n"
     ]
    }
   ],
   "source": [
    "# == Import libraries for data processing and NLP ==\n",
    "\n",
    "import os\n",
    "import gensim\n",
    "from pathlib import Path\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from random import shuffle\n",
    "import nltk\n",
    "from gensim.utils import simple_preprocess\n",
    "import joblib\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from string import punctuation\n",
    "from scipy.spatial.distance import cosine\n",
    "import glob\n",
    "import spacy\n",
    "from multiprocessing import Pool, freeze_support\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# === Initialize NLP Tools ===\n",
    "\n",
    "translator = str.maketrans('', '', punctuation) \n",
    "tagger = nltk.perceptron.PerceptronTagger()\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# === Set Working Directory ===\n",
    "\n",
    "# Prompt user to enter working directory path\n",
    "wd = Path(input(\"Please enter your working directory path (e.g., C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit): \").strip())\n",
    "\n",
    "# Change to the entered working directory\n",
    "try:\n",
    "    os.chdir(wd)\n",
    "    print(f\"Working directory set to: {os.getcwd()}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: The directory you entered does not exist. Please restart and enter a valid path.\")\n",
    "    exit(1)\n",
    "\n",
    "# === Define Folder Paths ===\n",
    "\n",
    "# Folders were already created in the script 0_data_creation\n",
    "data_c = wd / \"data\"\n",
    "data_temp = data_c / \"temp\"\n",
    "data_freq = data_c / \"freq\"\n",
    "data_dict = data_c / \"dictionaries\"\n",
    "data_sent = data_c / \"sentences\"\n",
    "data_preprocessed = data_c / \"preprocessed\"\n",
    "data_results = data_c / \"results\"\n",
    "data_models = data_c / \"models\" \n",
    "\n",
    "# Upload ressources\n",
    "stopwords = joblib.load(data_c / \"stopwords.pkl\")         \n",
    "word_counts = joblib.load(data_freq / \"word_counts.pkl\")\n",
    "word_counts_weighted = joblib.load(data_freq / \"word_counts_weighted.pkl\")\n",
    "affect_dic = joblib.load(data_dict / 'dictionary_affect.pkl')\n",
    "cognition_dic = joblib.load(data_dict / 'dictionary_cognition.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "125227ca-a0b5-43d7-956d-7d49a66f81b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_temp)\n",
    "cleaned_files = [\n",
    "    str(data_temp / 'clean_speeches_indexed1.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed2.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed3.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed4.pkl')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855a7f37-bbf9-4259-8a07-7e2a589f1de4",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2645f36-e935-4b0e-b5e6-7461f398d887",
   "metadata": {},
   "source": [
    "### Sentence Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af0997e8-9da3-45c3-bdd4-7d38a708a474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of very short sentences being dropped: 86798\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl processed\n",
      "Unique tokens: 8157\n",
      "Example sentences (first 5):\n",
      "['propos', 'ecuador']\n",
      "['solut', 'distress', 'problem', 'lao', 'congo', 'compromis', 'solut', 'halt', 'spare']\n",
      "['gratifi', 'algeria']\n",
      "['connexion', 'reactionari', 'negoti', 'mask']\n",
      "['ensur', 'solut']\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed1.pkl saved\n",
      "Number of very short sentences being dropped: 65694\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl processed\n",
      "Unique tokens: 8613\n",
      "Example sentences (first 5):\n",
      "['bueno', 'bueno', 'econom', 'ministeri', 'arusha']\n",
      "['susten', 'flame', 'region']\n",
      "['nowaday', 'premis', 'human']\n",
      "['social', 'econom', 'transform']\n",
      "['indigen', 'african', 'racial', 'discrimin', 'practis', 'african', 'regim']\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed2.pkl saved\n",
      "Number of very short sentences being dropped: 48615\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl processed\n",
      "Unique tokens: 8228\n",
      "Example sentences (first 5):\n",
      "['gratitud', 'guinea', 'non']\n",
      "['underlin', 'indonesia', 'timor', 'toward']\n",
      "['albanian', 'european', 'albanian', 'enjoy', 'european']\n",
      "['flame', 'ignit', 'area', 'potenti', 'transform', 'conflagr', 'magnitud', 'engulf', 'hemispher']\n",
      "['sincer', 'afghanistan']\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed3.pkl saved\n",
      "Number of very short sentences being dropped: 56143\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl processed\n",
      "Unique tokens: 8179\n",
      "Example sentences (first 5):\n",
      "['strengthen', 'win', 'win', 'built', 'framework', 'forum']\n",
      "['devast', 'global', 'achiev', 'goal', 'remot']\n",
      "['meant', 'degrad', 'plastic', 'bag', 'erad', 'predat', 'biodivers']\n",
      "['papua', 'guinea', 'modest', 'financi', 'contribut', 'fiji', 'niue', 'kiribati', 'tonga', 'tuvalu', 'vanuatu', 'region', 'entiti', 'forum', 'secretariat', 'region', 'melanesian', 'spearhead', 'region', 'partnership']\n",
      "['migrat', 'violent', 'scourg', 'terror', 'threat', 'violent', 'diseas', 'resurg', 'ebola', 'measl', 'transcend', 'border']\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed4.pkl saved\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_sent)\n",
    "\n",
    "# Function to split cleaned speeches (clean_speeches) into sentences, tokenize, clean, tag, stem, filter, and save them.\n",
    "\n",
    "def extract_sentences(dataname):\n",
    "    data = joblib.load(dataname)\n",
    "    data = [a[1] for a in data]  # keep only text, no id\n",
    "\n",
    "    sentences = []\n",
    "    for doc in data:\n",
    "        sentences += sent_tokenize(doc)  # use nltk's sent_tokenize here\n",
    "\n",
    "    sentences = [item for item in sentences if len(item.split()) > 1]\n",
    "    sentences = [gensim.utils.simple_preprocess(item) for item in sentences]\n",
    "\n",
    "    sentences = [[a for a in s if not a.isdigit()] for s in sentences]\n",
    "    sentences = [[a for a in s if len(a) > 2] for s in sentences]\n",
    "\n",
    "    sentences = [tagger.tag(s) for s in sentences]\n",
    "    sentences = [[i[0] for i in s if i[1].startswith(('N', 'V', 'J'))] for s in sentences]\n",
    "\n",
    "    sentences = [[stemmer.stem(i) for i in s] for s in sentences]\n",
    "    sentences = [[a for a in s if a not in stopwords] for s in sentences]\n",
    "    sentences = [[a for a in s if word_counts[a] >= 10] for s in sentences]\n",
    "\n",
    "    dropped_count = sum(1 for s in sentences if len(s) <= 1)\n",
    "    print(f\"Number of very short sentences being dropped: {dropped_count}\")\n",
    "\n",
    "    sentences = [s for s in sentences if len(s) > 1]\n",
    "    shuffle(sentences)\n",
    "\n",
    "    lab = dataname.replace('clean_speeches_', 'sentences_').replace('_.pkl', '.pkl')\n",
    "    print(f'{dataname} processed')\n",
    "    joblib.dump(sentences, lab)\n",
    "    \n",
    "    unique_tokens = set(token for s in sentences for token in s)\n",
    "    print(f\"Unique tokens: {len(unique_tokens)}\")\n",
    "\n",
    "    # Print preview of first 5 processed sentences\n",
    "    print(\"Example sentences (first 5):\")\n",
    "    for s in sentences[:5]:\n",
    "        print(s)\n",
    "\n",
    "\n",
    "    print(f'{lab} saved')\n",
    "\n",
    "for fname in cleaned_files:\n",
    "    extract_sentences(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9ae013e-bb3f-43d5-98e1-cd113108f0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentences (first 5):\n",
      "['propos', 'ecuador']\n",
      "['solut', 'distress', 'problem', 'lao', 'congo', 'compromis', 'solut', 'halt', 'spare']\n",
      "['gratifi', 'algeria']\n",
      "['connexion', 'reactionari', 'negoti', 'mask']\n",
      "['ensur', 'solut']\n"
     ]
    }
   ],
   "source": [
    "# Pick the first file to see how the sentence split looks like\n",
    "file_path = os.path.join(data_temp, 'sentences_indexed1.pkl')\n",
    "\n",
    "sentences = joblib.load(file_path)\n",
    "\n",
    "print(\"Example sentences (first 5):\")\n",
    "for s in sentences[:5]:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fddaea83-7d1f-4b14-85b9-401b2e727c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_files = [\n",
    "    os.path.join(data_temp, 'sentences_indexed1.pkl'),\n",
    "    os.path.join(data_temp, 'sentences_indexed2.pkl'),\n",
    "    os.path.join(data_temp, 'sentences_indexed3.pkl'),\n",
    "    os.path.join(data_temp, 'sentences_indexed4.pkl')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c41580ba-2954-4d84-a574-e9c3d4c8ef1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens across all files: 9453\n",
      "Total number of tokens (including repeats): 4286666\n"
     ]
    }
   ],
   "source": [
    "# == Get sum of unique tokens ==\n",
    "all_unique_tokens = set()\n",
    "total_token_count = 0\n",
    "\n",
    "for dataname in sentences_files:\n",
    "    data = joblib.load(dataname)\n",
    "    for sentence in data:\n",
    "        all_unique_tokens.update(sentence)\n",
    "        total_token_count += len(sentence)\n",
    "\n",
    "print(f\"Total unique tokens across all files: {len(all_unique_tokens)}\")\n",
    "print(f\"Total number of tokens): {total_token_count}\")\n",
    "\n",
    "sentences_files = [\n",
    "    'sentences_indexed1.pkl',\n",
    "    'sentences_indexed2.pkl',\n",
    "    'sentences_indexed3.pkl',\n",
    "    'sentences_indexed4.pkl']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28938326-25d1-43cc-b0a8-4400dc138584",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d8d0e2-3d81-41ea-90c4-6d36bc9fa11c",
   "metadata": {},
   "source": [
    "### Train Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "465bc73c-70b6-4dbf-a379-8c5235f83a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for dataname in sentences_files:  \n",
    "    data = joblib.load(dataname)\n",
    "    dataset.extend(data) \n",
    "\n",
    "# === Model training ===\n",
    "w2v = Word2Vec(\n",
    "    sentences=dataset,    \n",
    "    vector_size=300,      # Dimension of the vector\n",
    "    window=8,             # Context window size\n",
    "    min_count=10,         # Minimum word count\n",
    "    workers=8,            # Number of threads\n",
    "    sample=1e-3,          # Downsample setting for frequent words\n",
    "    epochs=10,            # Number of iterations over the corpus\n",
    "    seed=12\n",
    ")\n",
    "\n",
    "\n",
    "w2v.wv.fill_norms() \n",
    "\n",
    "# Save model\n",
    "data_models.mkdir(parents=True, exist_ok=True) \n",
    "w2v.save(str(data_models / 'w2v-vectors_8_300.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d94142c-1e7f-4b0b-bfec-98604fc84304",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec.load(str(data_models / \"w2v-vectors_8_300.pkl\"))\n",
    "word_vectors = w2v.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aa347e-aad5-477c-a52a-40a84a7684c6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420620dc-4af4-4a55-a7b6-1827949d1b17",
   "metadata": {},
   "source": [
    "### Calculate Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5b75bd3b-2957-42d1-a89c-89e5b3ea44f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affect centroid vector:\n",
      " [-0.07808653  0.04432783  0.15128663  0.02482613 -0.20148386  0.17362411\n",
      "  0.3059602  -0.00671935  0.20959945 -0.41386503  0.02208215  0.01422188\n",
      " -0.11857989  0.1683662   0.15100546 -0.1452995  -0.11277463 -0.13173348\n",
      "  0.04783015 -0.14632805  0.02243414  0.03817457  0.09147596 -0.04292862\n",
      "  0.19472788  0.21312277  0.26943254 -0.06671417  0.07640403  0.00585344\n",
      " -0.14568348  0.13248236  0.11544412  0.08892977 -0.08234151  0.07010842\n",
      "  0.12160216  0.02064304 -0.04536121  0.03285974 -0.12188496  0.04357501\n",
      " -0.16517885 -0.29768786 -0.33015186 -0.304141   -0.01710525  0.17722869\n",
      " -0.03105955 -0.10664435 -0.08749358  0.05592329  0.18274833 -0.10353719\n",
      "  0.270673    0.39321417  0.03746451 -0.12544352 -0.00845609  0.23214205\n",
      "  0.06738508  0.25743803  0.0380513   0.13371372 -0.01917457  0.01217712\n",
      "  0.02778401  0.0079133  -0.1642968  -0.23810627  0.26865909  0.01259481\n",
      "  0.09213922 -0.2445543  -0.02018768  0.03525403  0.22437744 -0.2530975\n",
      "  0.20459649  0.29143894 -0.15213835  0.00624318 -0.01365623  0.16128023\n",
      " -0.3601372  -0.33116236  0.04277676 -0.10687678 -0.05088457 -0.13788547\n",
      "  0.04278688 -0.09726215 -0.09475895 -0.24461117 -0.02381112  0.4553049\n",
      "  0.01970967 -0.03748587  0.1256437  -0.14824001 -0.04952343  0.04117807\n",
      "  0.38339537 -0.1571493   0.06019346  0.10575294 -0.15591782 -0.12425679\n",
      " -0.01346163  0.3372347   0.04861049  0.2521934  -0.05723038  0.04115647\n",
      "  0.09670592 -0.0435617  -0.13845429 -0.14215226  0.06695306 -0.27152702\n",
      " -0.03131763  0.08304295 -0.21494344  0.44570127 -0.07993623 -0.1771511\n",
      "  0.05234001 -0.45369837  0.03453526  0.07499295  0.09820322 -0.07717545\n",
      " -0.06042391  0.22978081 -0.07488304 -0.0334705  -0.12651558  0.09701802\n",
      "  0.12697992 -0.06835189 -0.16667897  0.10038184  0.03243879 -0.03498676\n",
      "  0.01561819  0.21918698  0.23655811  0.2913986  -0.18547076 -0.0951466\n",
      " -0.25284433 -0.21218835  0.10638137  0.21069793  0.39202195 -0.10409483\n",
      "  0.08234527  0.22150493 -0.07846031 -0.4501208   0.04978739 -0.00562111\n",
      "  0.15352654 -0.20745361  0.1122778  -0.0322227  -0.09579805 -0.00510876\n",
      "  0.27022532  0.10186162  0.04721802  0.06792529  0.15417373  0.01920053\n",
      " -0.27963316  0.30245417  0.05568648 -0.11738587  0.0079299  -0.1815662\n",
      " -0.09489159 -0.07955278 -0.1713155  -0.12170253  0.19414797 -0.19318524\n",
      "  0.3405537  -0.2831554  -0.0551802  -0.1086603   0.04371757 -0.06438933\n",
      " -0.05490102  0.15825418  0.00313275  0.17724459  0.19383629  0.13930987\n",
      "  0.01736031  0.13691735 -0.20394571  0.29820034  0.12016863 -0.16586508\n",
      "  0.37024245  0.15495637  0.06083256  0.09342234 -0.0709232  -0.13464496\n",
      " -0.35463178 -0.26928008  0.18861209  0.01747525  0.11501049  0.2474881\n",
      " -0.02900635  0.1809978  -0.04381289  0.02958644  0.02582604  0.20122461\n",
      " -0.02952659  0.04454014 -0.1812448  -0.17755608 -0.01713375 -0.12735966\n",
      "  0.19534077 -0.27932116  0.05456059  0.18309544 -0.01598965  0.04068817\n",
      " -0.01453718  0.18299924 -0.24245863  0.2896659   0.1338389  -0.07852411\n",
      " -0.03796688  0.15742047  0.1909538   0.25707155 -0.01036181  0.13610515\n",
      "  0.07295027  0.04257736 -0.00145555 -0.07037853  0.01054018  0.03867938\n",
      "  0.19720556  0.12799361  0.04565347  0.06146434 -0.17471917 -0.10939693\n",
      "  0.15175359  0.11212084 -0.5066227  -0.4650985   0.0016854   0.08373627\n",
      "  0.10043716  0.07687293 -0.18410514 -0.05868629  0.10088392 -0.09833616\n",
      "  0.07002403 -0.33932862 -0.00842032 -0.06254883 -0.03008784 -0.02138581\n",
      "  0.05275811  0.04715985  0.12077482 -0.04486506  0.16135153 -0.07852849\n",
      "  0.0219493   0.21489456 -0.10908438  0.06535805  0.04568575 -0.00584793\n",
      "  0.00627348  0.05973382  0.07007244 -0.0612823   0.2823514  -0.05184939\n",
      "  0.26304698  0.08224571  0.16684653 -0.24383461  0.04452109 -0.44909266]\n",
      "\n",
      "Cognition centroid vector:\n",
      " [ 0.01045784  0.03849173  0.14518525  0.03429212 -0.21410912  0.12309743\n",
      "  0.12571378  0.0034587   0.11721969  0.12183855  0.15585183  0.01124101\n",
      "  0.07489875  0.26813513  0.1446679   0.17287754 -0.10100634 -0.0066912\n",
      "  0.08848366  0.04335545  0.03494743 -0.11061896 -0.04981208  0.01852531\n",
      "  0.11147325 -0.09177792  0.25041467 -0.3159461  -0.02472603  0.05400014\n",
      " -0.17686483  0.15726791  0.15964876 -0.04794312 -0.10162641 -0.23925516\n",
      " -0.0466855  -0.08762161 -0.08213515  0.12263495 -0.11929424  0.0697298\n",
      " -0.04253595 -0.10671873 -0.00850237 -0.24486485 -0.1414263   0.05064918\n",
      " -0.06110954  0.1254194   0.00646234 -0.08310407  0.24146882  0.08707355\n",
      "  0.12065041  0.14973423 -0.05336889 -0.23640338  0.00508804 -0.04499372\n",
      " -0.08232454 -0.00443319 -0.16561884 -0.06303869  0.16746114 -0.12860242\n",
      " -0.09486888 -0.08818744 -0.06402979 -0.11567641 -0.0223216   0.14126253\n",
      "  0.0015597  -0.30538332 -0.0463167   0.19570461  0.0807229  -0.05564471\n",
      "  0.0473591  -0.02578371 -0.00945527  0.27724627  0.01537083  0.20827784\n",
      " -0.19079839 -0.17728956  0.11152661 -0.28256327 -0.01718403 -0.03505164\n",
      "  0.0287152  -0.03624825 -0.04876773 -0.08919764 -0.05659487  0.07085654\n",
      "  0.07456411  0.04512692  0.01587177 -0.05719982  0.00141605  0.05313639\n",
      "  0.190682   -0.23391166 -0.13958374  0.14197865 -0.04300638  0.14686498\n",
      " -0.07232361  0.05121155 -0.00749286 -0.0261545  -0.1624528  -0.08522772\n",
      " -0.01970668 -0.12903848  0.00607162 -0.07873494 -0.11966247  0.02516364\n",
      " -0.04965686 -0.08213118 -0.02198121  0.19609329 -0.04837393 -0.16338944\n",
      " -0.09554967 -0.14716993  0.10201921  0.08455952 -0.03660655 -0.0942883\n",
      " -0.02482625  0.11975534  0.04094014 -0.23071    -0.05911421  0.14475773\n",
      "  0.09072489  0.18473455 -0.19191077 -0.19201134  0.06179317 -0.20968975\n",
      "  0.02838315  0.03843476 -0.13054198  0.21327728 -0.19967908  0.02155942\n",
      "  0.01083736 -0.08770169 -0.08810568  0.02520737 -0.10905531  0.06427871\n",
      " -0.1032528   0.0667745  -0.08231433 -0.00233786  0.05483271  0.03513454\n",
      "  0.06547066  0.03403161  0.05466965  0.15359516 -0.12020637 -0.0623909\n",
      "  0.01121562  0.03502095  0.18332331  0.07473797  0.15472367  0.04355111\n",
      "  0.06032512  0.23501149 -0.0442268  -0.08835014  0.04906833 -0.08522169\n",
      " -0.10103133 -0.03136037  0.09355205 -0.0446846   0.07226774  0.14592512\n",
      "  0.09974342  0.09754153 -0.10385461  0.00083306 -0.11268528  0.09187295\n",
      " -0.04713535  0.07321627  0.02335203  0.01395348  0.05693438 -0.06620816\n",
      "  0.040419   -0.04103656 -0.101752    0.15331501 -0.05445966 -0.12281169\n",
      "  0.3169371  -0.01944624 -0.2147784  -0.01469178  0.00072651 -0.06667705\n",
      " -0.06899559 -0.09485916 -0.12333545  0.13501571  0.00651648  0.27854124\n",
      " -0.08187338  0.03092608 -0.18268727 -0.12640105 -0.01820401  0.17218764\n",
      " -0.14765552 -0.02870273 -0.12331757 -0.06569696  0.0148772  -0.1516813\n",
      " -0.06717288 -0.22471465  0.02386822  0.04736657 -0.06980686  0.05337962\n",
      "  0.04245921 -0.06276509 -0.13397893 -0.0094339  -0.02530576  0.09617683\n",
      "  0.01246658  0.00420742  0.15805659 -0.07371081  0.01549448 -0.04715483\n",
      " -0.02458646  0.08145402 -0.05487184 -0.01142913 -0.0415384   0.02026158\n",
      "  0.01608787 -0.166449   -0.12076079 -0.11200412 -0.11695816  0.06383262\n",
      "  0.08178197 -0.06437496 -0.1727185  -0.20292674 -0.09844729  0.1339151\n",
      "  0.18520357  0.00111749  0.04943512  0.00888223 -0.17400318 -0.10589273\n",
      " -0.15334685  0.01007402  0.07422997 -0.1400323   0.05665236  0.08755822\n",
      "  0.13938566  0.10799626  0.16600586 -0.19727324 -0.06300043 -0.02244155\n",
      " -0.06353398  0.12098375 -0.03429766  0.19879979 -0.2067957   0.05903663\n",
      "  0.19984573 -0.05892245 -0.00659409 -0.03797487  0.22729419 -0.04100949\n",
      "  0.06834874  0.07912605 -0.08043861 -0.02136861  0.00873029 -0.2680972 ]\n",
      "\n",
      "Shape of affect centroid: (300,)\n",
      "Shape of cognition centroid: (300,)\n"
     ]
    }
   ],
   "source": [
    "# == Calculation ==\n",
    "def findcentroid(text, model):\n",
    "    vecs = [model.wv[w] * word_counts_weighted[w] \n",
    "        for w in text \n",
    "        if w in model.wv and w in word_counts_weighted]\n",
    "    vecs = [v for v in vecs if len(v) > 0]\n",
    "    centroid = np.mean(vecs, axis=0)\n",
    "    #centroid = centroid.reshape(1, -1)\n",
    "    return centroid\n",
    "\n",
    "\n",
    "affect_centroid = findcentroid(affect_dic, w2v)\n",
    "cog_centroid = findcentroid(cognition_dic, w2v)\n",
    "\n",
    "os.chdir(data_c)\n",
    "joblib.dump(affect_centroid, 'centroids/affect_centroid.pkl')\n",
    "joblib.dump(cog_centroid, 'centroids/cog_centroid.pkl')\n",
    "\n",
    "# == Overview Vectors ==\n",
    "print(\"Affect centroid vector:\\n\", affect_centroid)\n",
    "print(\"\\nCognition centroid vector:\\n\", cog_centroid)\n",
    "\n",
    "# Shape and stats\n",
    "print(\"\\nShape of affect centroid:\", affect_centroid.shape)\n",
    "print(\"Shape of cognition centroid:\", cog_centroid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af347b1-4f5f-401a-8ca0-1c33eb7776b1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c8316f-f794-4bcd-87c9-578aa82f88b0",
   "metadata": {},
   "source": [
    "### Emotionality Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3376a4f6-a002-4179-8614-dfef96113180",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_preprocessed)\n",
    "\n",
    "# Load preprocessed speech data \n",
    "\n",
    "preprocessed_final_files = [\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed1_final.pkl')),\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed2_final.pkl')),\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed3_final.pkl')),\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed4_final.pkl'))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1cbd1f95-c7f4-41f1-a292-542ee34ae882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit\\\\data\\\\results\\\\distances_10epochs.pkl']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to compute weighted document vectors and derive affective/cognitive distances and scores\n",
    "\n",
    "def documentvecweight(lista):\n",
    "    out = []\n",
    "    lista = [i for i in lista if len(i[1]) > 0]\n",
    "    for s in lista:\n",
    "        # Compute weighted word vectors for each token present in the Word2Vec model\n",
    "        vecs = [w2v.wv[w] * word_counts_weighted[w] for w in s[1] if w in w2v.wv]\n",
    "        if len(vecs) == 0:\n",
    "            a = np.nan\n",
    "            c = np.nan\n",
    "            score = np.nan\n",
    "        else:\n",
    "            # Compute mean vector for each speech\n",
    "            v = np.mean(vecs, axis=0)\n",
    "             # Cosine distance to affective centroid\n",
    "            a = cosine(v, affect_centroid)\n",
    "            # Cosine distance to cognitive centroid\n",
    "            c = cosine(v, cog_centroid)\n",
    "            score = (1 + 1 - a) / (1 + 1 - c)\n",
    "        out.append([s[0], a, c, score])\n",
    "    return out\n",
    "\n",
    "\n",
    "def main_function(file_path, idx):\n",
    "    dataset = joblib.load(file_path)\n",
    "    data = documentvecweight(dataset)\n",
    "    lab = os.path.join(data_results, f'distances_main_{idx}.pkl')\n",
    "    joblib.dump(data, lab)\n",
    "\n",
    "# Main loop: process all preprocessed speech files\n",
    "def main():\n",
    "    files = [\n",
    "        os.path.join(data_preprocessed, f'preprocessed_speeches_indexed{i+1}_final.pkl') #Changed!\n",
    "        for i in range(4)\n",
    "    ]\n",
    "    for i, f in enumerate(files, start=1):\n",
    "        main_function(f, i)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# Merge all distance files into one df\n",
    "DATA_temp = [os.path.join(data_results, f'distances_main_{i+1}.pkl') for i in range(4)]\n",
    "\n",
    "tot = []\n",
    "for dataname in DATA_temp:\n",
    "    d = joblib.load(dataname)\n",
    "    tot += d\n",
    "\n",
    "tot_df = pd.DataFrame(tot, columns=['filename', 'affect_d', 'cognition_d', 'score'])\n",
    "joblib.dump(tot_df, os.path.join(data_results, 'distances_10epochs.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fb955ab4-243d-470d-a967-e0af1e474c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          filename  affect_d  cognition_d     score\n",
      "0  ARG_01_1946.txt  0.768864     0.704282  0.950157\n",
      "1  AUS_01_1946.txt  1.227998     0.799491  0.643062\n",
      "2  BEL_01_1946.txt  1.057272     0.813437  0.794503\n",
      "3  BLR_01_1946.txt  0.710122     0.867815  1.139282\n",
      "4  BOL_01_1946.txt  0.405746     0.707858  1.233807\n"
     ]
    }
   ],
   "source": [
    "print(tot_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8c295cd4-4fa0-470f-b67c-4f584f7a3f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load df_merged and merge with tot_df by filename \n",
    "un_corpus_merged = pd.read_csv(os.path.join(data_c, \"un_corpus_merged.csv\"), sep=';', encoding='utf-8') \n",
    "un_corpus_scored = un_corpus_merged.merge(tot_df, on=\"filename\", how=\"left\")\n",
    "\n",
    "joblib.dump(un_corpus_scored, os.path.join(data_results, \"un_corpus_scored.pkl\"))\n",
    "\n",
    "un_corpus_scored.to_csv(\n",
    "    os.path.join(data_results, \"un_corpus_scored.csv\"),\n",
    "    sep=';', \n",
    "    index=False, \n",
    "    encoding='utf-8'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d8d13667-1eb3-482e-83ca-5969a229750a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              filename                                             speech  \\\n",
      "0      ARG_01_1946.txt  At the resumption of the first session of the ...   \n",
      "1      AUS_01_1946.txt  The General Assembly of the United Nations is ...   \n",
      "2      BEL_01_1946.txt  The\\tprincipal organs of the United Nations ha...   \n",
      "3      BLR_01_1946.txt  As more than a year has elapsed since the Unit...   \n",
      "4      BOL_01_1946.txt  Coming to this platform where so many distingu...   \n",
      "...                ...                                                ...   \n",
      "10947  WSM_79_2024.txt  Excellencies,  \\nI extend my congratulations t...   \n",
      "10948  YEM_79_2024.txt  Your Majesties, Excellencies, and Highnesses, ...   \n",
      "10949  ZAF_79_2024.txt  President of the 79th Session of the UN Genera...   \n",
      "10950  ZMB_79_2024.txt  \\n  YOUR EXCELLENCY PHILEMON YANG, PRESIDENT O...   \n",
      "10951  ZWE_79_2024.txt  Your Excellency, Mr. Philemon Yang, President ...   \n",
      "\n",
      "      country_code  year  country_name  speech_length_words  \\\n",
      "0              ARG  1946     Argentina                 3364   \n",
      "1              AUS  1946     Australia                 4531   \n",
      "2              BEL  1946       Belgium                 2501   \n",
      "3              BLR  1946       Belarus                 3055   \n",
      "4              BOL  1946       Bolivia                 1501   \n",
      "...            ...   ...           ...                  ...   \n",
      "10947          WSM  2024         Samoa                 1426   \n",
      "10948          YEM  2024         Yemen                 1662   \n",
      "10949          ZAF  2024  South Africa                 1685   \n",
      "10950          ZMB  2024        Zambia                 2111   \n",
      "10951          ZWE  2024      Zimbabwe                 1652   \n",
      "\n",
      "       english_official_language  security_council_permanent  \\\n",
      "0                              0                           0   \n",
      "1                              0                           0   \n",
      "2                              1                           0   \n",
      "3                              0                           0   \n",
      "4                              0                           0   \n",
      "...                          ...                         ...   \n",
      "10947                          1                           0   \n",
      "10948                          0                           0   \n",
      "10949                          1                           0   \n",
      "10950                          1                           0   \n",
      "10951                          1                           0   \n",
      "\n",
      "                   speaker_name                               position  \\\n",
      "0                      Mr. Arce                                    NaN   \n",
      "1                     Mr. Makin                                    NaN   \n",
      "2            Mr. Van Langenhove                                    NaN   \n",
      "3                   Mr. Kiselev                                    NaN   \n",
      "4             Mr. Costa du Rels                                    NaN   \n",
      "...                         ...                                    ...   \n",
      "10947      FiamÄ“ Naomi Mata'afa                (Deputy) Prime Minister   \n",
      "10948  Rashad Mohammed Al-Alimi                      (Vice-) President   \n",
      "10949  Matamela Cyril Ramaphosa                      (Vice-) President   \n",
      "10950   Mulambo Hamakuni Haimbe  (Deputy) Minister for Foreign Affairs   \n",
      "10951  Frederick Makamure Shava  (Deputy) Minister for Foreign Affairs   \n",
      "\n",
      "       gender_dummy         speech_label  affect_d  cognition_d     score  \n",
      "0               0.0     Argentina (1946)  0.768864     0.704282  0.950157  \n",
      "1               0.0     Australia (1946)  1.227998     0.799491  0.643062  \n",
      "2               0.0       Belgium (1946)  1.057272     0.813437  0.794503  \n",
      "3               0.0       Belarus (1946)  0.710122     0.867815  1.139282  \n",
      "4               0.0       Bolivia (1946)  0.405746     0.707858  1.233807  \n",
      "...             ...                  ...       ...          ...       ...  \n",
      "10947           NaN         Samoa (2024)  1.029091     1.100458  1.079336  \n",
      "10948           NaN         Yemen (2024)  0.632176     1.057673  1.451539  \n",
      "10949           NaN  South Africa (2024)  0.698619     1.082416  1.418268  \n",
      "10950           NaN        Zambia (2024)  1.121411     1.222208  1.129594  \n",
      "10951           NaN      Zimbabwe (2024)  1.047807     1.022449  0.974060  \n",
      "\n",
      "[10952 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "print(un_corpus_scored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1210b88f-cf13-4da0-807b-77764cf0d439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count where affect_d is NaN: 0\n"
     ]
    }
   ],
   "source": [
    "# Count where affect_d is NaN\n",
    "nan_count = un_corpus_scored['affect_d'].isna().sum()\n",
    "\n",
    "print(\"Count where affect_d is NaN:\", nan_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
