{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a39bd0b5-2ac1-499d-8938-2fdfe4c3d93d",
   "metadata": {},
   "source": [
    "# Development of Emotion and Reasoning in the General Speeches of the United Nations: A text-based machine learning approach\n",
    "## Script 1: Model Traning, Calculation Centroids & Speech Scoring\n",
    "### Author: Sarah Franzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ca6f721-48ad-495c-8e5f-e60790225709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder checked/created: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\results\n",
      "Folder checked/created: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\models\n"
     ]
    }
   ],
   "source": [
    "# == Import libraries for data processing and NLP ==\n",
    "\n",
    "import os\n",
    "import gensim\n",
    "from pathlib import Path\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from random import shuffle\n",
    "import nltk\n",
    "from gensim.utils import simple_preprocess\n",
    "import joblib\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from string import punctuation\n",
    "from scipy.spatial.distance import cosine\n",
    "import glob\n",
    "import spacy\n",
    "from multiprocessing import Pool, freeze_support\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# === Initialize NLP Tools ===\n",
    "\n",
    "translator = str.maketrans('', '', punctuation) \n",
    "tagger = nltk.perceptron.PerceptronTagger()\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# === Set Working Directory ===\n",
    "\n",
    "# Prompt user to enter working directory path\n",
    "#wd = input(\"Please enter your working directory path (e.g., C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit): \").strip()\n",
    "\n",
    "# Change to the entered working directory\n",
    "#try:\n",
    "   # os.chdir(wd)\n",
    "    #print(f\"Working directory set to: {os.getcwd()}\")\n",
    "#except FileNotFoundError:\n",
    "   # print(\"ERROR: The directory you entered does not exist. Please restart and enter a valid path.\")\n",
    "    #exit(1)\n",
    "\n",
    "# Set your working directory (adjust this as needed)\n",
    "#wd = r\"C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\"\n",
    "#os.chdir(wd)\n",
    "\n",
    "# Set your working directory (adjust this as needed)\n",
    "wd = Path(r\"C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\")\n",
    "\n",
    "# === Create new Folders ===\n",
    "additional_folders = [\"results\", \"models\"]\n",
    "\n",
    "# Create/check folders directly in wd\n",
    "for folder in additional_folders:\n",
    "    folder_path = Path(wd) / folder\n",
    "    folder_path.mkdir(exist_ok=True)\n",
    "    print(f\"Folder checked/created: {folder_path}\")\n",
    "\n",
    "# === Define Folder Paths ===\n",
    "\n",
    "# Folders were already created in the script 0_data_creation\n",
    "data_c = wd / \"data\"\n",
    "data_temp = data_c / \"temp\"\n",
    "data_freq = data_c / \"freq\"\n",
    "data_dict = data_c / \"dictionaries\"\n",
    "data_sent = data_c / \"sentences\"\n",
    "data_preprocessed = data_c / \"preprocessed\"\n",
    "fig_dir = wd /\"fig\"\n",
    "\n",
    "wd_results = wd / \"results\" # THESE FOLDERS NEED TO BE CREATED\n",
    "wd_models = wd / \"models\" # THESE FOLDERS NEED TO BE CREATED\n",
    "\n",
    "# Upload ressources\n",
    "#stopwords = joblib.load(data_c / \"stopwords.pkl\")              #### this is from the replication package! Issue for replication\n",
    "stopwords = joblib.load(data_c / \"spacy_stopwords_stemmed.pkl\")\n",
    "word_counts = joblib.load(data_freq / \"word_counts.pkl\")\n",
    "word_counts_weighted = joblib.load(data_freq / \"word_counts_weighted.pkl\")\n",
    "affect_dic = joblib.load(data_dict / 'dictionary_affect.pkl')  ### same issue here!\n",
    "cognition_dic = joblib.load(data_dict / 'dictionary_cognition.pkl') ### same issue here! must also be fixed in 0_data_creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "125227ca-a0b5-43d7-956d-7d49a66f81b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_temp)\n",
    "cleaned_files = [\n",
    "    str(data_temp / 'clean_speeches_indexed1.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed2.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed3.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed4.pkl')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855a7f37-bbf9-4259-8a07-7e2a589f1de4",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2645f36-e935-4b0e-b5e6-7461f398d887",
   "metadata": {},
   "source": [
    "### Sentence Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af0997e8-9da3-45c3-bdd4-7d38a708a474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of very short sentences being dropped: 4244\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl processed\n",
      "Unique tokens: 12330\n",
      "Example sentences (first 5):\n",
      "['excel', 'report', 'work', 'organ', 'submit', 'open', 'session', 'eloqu', 'point', 'view', 'congratul']\n",
      "['deploy', 'montenegrin', 'soldier', 'polic', 'offic', 'intern', 'mission', 'testifi', 'readi', 'fulfil', 'intern', 'oblig', 'develop', 'relat', 'base', 'partnership']\n",
      "['nepal', 'continu', 'believ', 'new', 'regim', 'meaning', 'recogn', 'entir', 'resourc', 'sea', 'belong', 'human', 'constitut', 'common', 'heritag', 'mankind']\n",
      "['exacerb', 'destruct', 'ecosystem', 'biodivers', 'climat', 'chang', 'like', 'increas', 'poverti', 'diseas', 'lead', 'upsurg', 'climat', 'relat', 'migrat', 'compromis', 'futur', 'futur', 'generat']\n",
      "['chines', 'threat', 'fact', 'alibi', 'western', 'colonialist', 'imperialist', 'enterpris', 'african', 'asian', 'latin', 'american', 'countri', 'pay', 'price']\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed1.pkl saved\n",
      "Number of very short sentences being dropped: 4226\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl processed\n",
      "Unique tokens: 12344\n",
      "Example sentences (first 5):\n",
      "['unit', 'nation', 'power', 'prestig', 'speed', 'self', 'determin', 'independ', 'territori']\n",
      "['let', 'turn', 'intern', 'peac']\n",
      "['promis', 'redress', 'set', 'declar', 'human', 'right', 'absolut', 'equal', 'treatment', 'issu', 'harold', 'wilson', 'clark', 'street', 'august']\n",
      "['newer', 'democraci', 'mechan', 'institut', 'protect', 'preserv', 'essenc', 'democrat', 'process', 'tendenc', 'respons', 'moral', 'imper', 'radic', 'measur', 'intend', 'abus', 'polit', 'power', 'misus', 'execut', 'author', 'interest', 'societi']\n",
      "['administr', 'essenti', 'element', 'concern', 'project', 'simplifi', 'limit', 'absolut', 'minimum', 'requir']\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed2.pkl saved\n",
      "Number of very short sentences being dropped: 4385\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl processed\n",
      "Unique tokens: 12369\n",
      "Example sentences (first 5):\n",
      "['intern', 'life', 'mark', 'sort', 'grow', 'tension', 'conflict']\n",
      "['tie', 'aid', 'specif', 'project', 'high', 'prioriti', 'develop', 'plan', 'recipi', 'nation', 'tie', 'loan', 'aid', 'purchas', 'capit', 'good', 'uncompetit', 'price', 'donor', 'countri', 'correspond', 'undertak', 'purchas', 'good', 'repay', 'receiv', 'countri', 'impos', 'high', 'interest', 'rate', 'net', 'valu', 'extern', 'aid', 'reduc', 'point', 'intern', 'bank', 'reconstruct', 'develop', 'assess', 'inflow', 'offset', 'year']\n",
      "['proceed', 'wish', 'reiter', 'clear', 'south', 'african', 'deleg', 'realiz', 'need', 'intern', 'organ', 'achiev', 'end', 'set', 'charter', 'articl', 'satisfi', 'unit', 'nation', 'oper', 'today', 'spirit', 'prevail', 'organ', 'hope', 'purpos', 'undergo', 'radic', 'chang']\n",
      "['rwandan', 'govern', 'determin', 'spare', 'effort', 'establish', 'necessari', 'condit', 'resumpt', 'econom', 'activ', 'deep', 'nation', 'reconcili', 'mention', 'moment', 'democrat', 'process']\n",
      "['simultan', 'develop', 'self', 'administr', 'social', 'matter', 'system', 'actual', 'produc', 'control', 'mean', 'product', 'exercis', 'decis', 'influenc', 'distribut', 'import', 'factor', 'mobil', 'domest', 'resourc', 'countri', 'fire', 'enthusiasm', 'worker']\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed3.pkl saved\n",
      "Number of very short sentences being dropped: 4452\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl processed\n",
      "Unique tokens: 12342\n",
      "Example sentences (first 5):\n",
      "['relat', 'balkan', 'countri', 'develop']\n",
      "['author', 'venezuela', 'honour', 'oblig', 'negoti', 'restructur', 'debt', 'framework', 'moder', 'understand', 'warn', 'face', 'crisi', 'postpon', 'effort', 'deal', 'root', 'problem', 'assum', 'acut', 'form']\n",
      "['decis', 'seat', 'intern', 'seab', 'author', 'law', 'sea', 'tribun', 'consid', 'major', 'step', 'opportun', 'congratul', 'jamaica', 'feder', 'republ', 'germani']\n",
      "['struggl', 'peopl', 'south', 'africa', 'leadership', 'african', 'nation', 'congress', 'anc', 'struggl', 'namibian', 'peopl', 'leadership', 'south', 'west', 'africa', 'peopl', 'organ', 'swapo', 'exemplari', 'term', 'achiev', 'scale', 'sacrific', 'repres', 'key', 'lesson', 'late', 'twentieth', 'centuri', 'end', 'racial', 'oppress']\n",
      "['protect', 'posit', 'element', 'origin', 'doctrin', 'organ']\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed4.pkl saved\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_sent)\n",
    "\n",
    "# Function to split cleaned speeches (clean_speeches) into sentences, tokenize, clean, tag, stem, filter, and save them.\n",
    "\n",
    "def extract_sentences(dataname):\n",
    "    data = joblib.load(dataname)\n",
    "    data = [a[1] for a in data]  # keep only text, no id\n",
    "\n",
    "    sentences = []\n",
    "    for doc in data:\n",
    "        sentences += sent_tokenize(doc)  # use nltk's sent_tokenize here\n",
    "\n",
    "    sentences = [item for item in sentences if len(item.split()) > 1]\n",
    "    sentences = [gensim.utils.simple_preprocess(item) for item in sentences]\n",
    "\n",
    "    sentences = [[a for a in s if not a.isdigit()] for s in sentences]\n",
    "    sentences = [[a for a in s if len(a) > 2] for s in sentences]\n",
    "\n",
    "    sentences = [tagger.tag(s) for s in sentences]\n",
    "    sentences = [[i[0] for i in s if i[1].startswith(('N', 'V', 'J'))] for s in sentences]\n",
    "\n",
    "    sentences = [[stemmer.stem(i) for i in s] for s in sentences]\n",
    "    sentences = [[a for a in s if a not in stopwords] for s in sentences]\n",
    "    sentences = [[a for a in s if word_counts[a] >= 10] for s in sentences]\n",
    "\n",
    "    dropped_count = sum(1 for s in sentences if len(s) <= 1)\n",
    "    print(f\"Number of very short sentences being dropped: {dropped_count}\")\n",
    "\n",
    "    sentences = [s for s in sentences if len(s) > 1]\n",
    "    shuffle(sentences)\n",
    "\n",
    "    lab = dataname.replace('clean_speeches_', 'sentences_').replace('_.pkl', '.pkl')\n",
    "    print(f'{dataname} processed')\n",
    "    joblib.dump(sentences, lab)\n",
    "    \n",
    "    unique_tokens = set(token for s in sentences for token in s)\n",
    "    print(f\"Unique tokens: {len(unique_tokens)}\")\n",
    "\n",
    "    # Print preview of first 5 processed sentences\n",
    "    print(\"Example sentences (first 5):\")\n",
    "    for s in sentences[:5]:\n",
    "        print(s)\n",
    "\n",
    "\n",
    "    print(f'{lab} saved')\n",
    "\n",
    "for fname in cleaned_files:\n",
    "    extract_sentences(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d9ae013e-bb3f-43d5-98e1-cd113108f0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentences (first 5):\n",
      "['excel', 'report', 'work', 'organ', 'submit', 'open', 'session', 'eloqu', 'point', 'view', 'congratul']\n",
      "['deploy', 'montenegrin', 'soldier', 'polic', 'offic', 'intern', 'mission', 'testifi', 'readi', 'fulfil', 'intern', 'oblig', 'develop', 'relat', 'base', 'partnership']\n",
      "['nepal', 'continu', 'believ', 'new', 'regim', 'meaning', 'recogn', 'entir', 'resourc', 'sea', 'belong', 'human', 'constitut', 'common', 'heritag', 'mankind']\n",
      "['exacerb', 'destruct', 'ecosystem', 'biodivers', 'climat', 'chang', 'like', 'increas', 'poverti', 'diseas', 'lead', 'upsurg', 'climat', 'relat', 'migrat', 'compromis', 'futur', 'futur', 'generat']\n",
      "['chines', 'threat', 'fact', 'alibi', 'western', 'colonialist', 'imperialist', 'enterpris', 'african', 'asian', 'latin', 'american', 'countri', 'pay', 'price']\n"
     ]
    }
   ],
   "source": [
    "# Example: pick the first file to see how the sentence split looks like\n",
    "file_path = os.path.join(data_temp, 'sentences_indexed1.pkl')\n",
    "\n",
    "# Load the pickle\n",
    "sentences = joblib.load(file_path)\n",
    "\n",
    "print(\"Example sentences (first 5):\")\n",
    "for s in sentences[:5]:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fddaea83-7d1f-4b14-85b9-401b2e727c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_files = [\n",
    "    os.path.join(data_temp, 'sentences_indexed1.pkl'),\n",
    "    os.path.join(data_temp, 'sentences_indexed2.pkl'),\n",
    "    os.path.join(data_temp, 'sentences_indexed3.pkl'),\n",
    "    os.path.join(data_temp, 'sentences_indexed4.pkl')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c41580ba-2954-4d84-a574-e9c3d4c8ef1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens across all files: 12498\n"
     ]
    }
   ],
   "source": [
    "# == Get sum of unique tokens ==\n",
    "all_unique_tokens = set()\n",
    "\n",
    "for dataname in sentences_files:\n",
    "    data = joblib.load(dataname)  # load list of tokenized sentences\n",
    "    for sentence in data:\n",
    "        all_unique_tokens.update(sentence)  # add tokens to the set\n",
    "\n",
    "print(f\"Total unique tokens across all files: {len(all_unique_tokens)}\")\n",
    "\n",
    "sentences_files = [\n",
    "    'sentences_indexed1.pkl',\n",
    "    'sentences_indexed2.pkl',\n",
    "    'sentences_indexed3.pkl',\n",
    "    'sentences_indexed4.pkl']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28938326-25d1-43cc-b0a8-4400dc138584",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d8d0e2-3d81-41ea-90c4-6d36bc9fa11c",
   "metadata": {},
   "source": [
    "### Train Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "465bc73c-70b6-4dbf-a379-8c5235f83a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for dataname in sentences_files:  \n",
    "    data = joblib.load(dataname)\n",
    "    dataset.extend(data) \n",
    "\n",
    "# === Model training ===\n",
    "w2v = Word2Vec(\n",
    "    sentences=dataset,    \n",
    "    vector_size=300,      # Dimension of the vector\n",
    "    window=8,             # Context window size\n",
    "    min_count=10,         # Minimum word count\n",
    "    workers=8,            # Number of threads\n",
    "    sample=1e-3,          # Downsample setting for frequent words\n",
    "    epochs=10             # Number of iterations over the corpus\n",
    ")\n",
    "\n",
    "# Optimize memory usage (optional)\n",
    "w2v.wv.fill_norms() \n",
    "\n",
    "# Save model\n",
    "wd_models.mkdir(parents=True, exist_ok=True) \n",
    "w2v.save(str(wd_models / 'w2v-vectors_8_300.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d94142c-1e7f-4b0b-bfec-98604fc84304",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec.load(str(wd_models / \"w2v-vectors_8_300.pkl\"))\n",
    "word_vectors = w2v.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aa347e-aad5-477c-a52a-40a84a7684c6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420620dc-4af4-4a55-a7b6-1827949d1b17",
   "metadata": {},
   "source": [
    "### Calculate Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b75bd3b-2957-42d1-a89c-89e5b3ea44f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affect centroid vector:\n",
      " [-1.02785610e-01 -3.07402238e-02 -8.21711794e-02  2.37924665e-01\n",
      "  1.66343763e-01  9.10846964e-02  2.84846932e-01  3.62272322e-01\n",
      " -1.21231005e-02 -2.37350658e-01  1.68229178e-01  2.27351151e-02\n",
      " -2.01144174e-01 -5.34671731e-02 -1.88858196e-01  5.76852411e-02\n",
      "  1.32371739e-01 -1.18452549e-01  2.54693478e-01  6.14628419e-02\n",
      " -9.50220898e-02  1.36210412e-01  1.69262066e-01 -1.75394211e-02\n",
      "  7.08577633e-01 -7.31074139e-02 -4.02246922e-01  1.69142440e-01\n",
      " -1.37794316e-01 -1.69568956e-02 -2.75227338e-01 -6.70964569e-02\n",
      " -2.91324228e-01  1.91873133e-01  7.30456039e-02  1.27081797e-01\n",
      " -4.53694351e-02 -3.80070716e-01  2.32767358e-01 -1.37659488e-02\n",
      " -8.08558799e-03  1.33415192e-01  2.34713301e-01 -1.43392488e-01\n",
      "  2.69387811e-01  4.90580872e-02 -2.19094664e-01 -1.67150795e-03\n",
      "  5.07661700e-02  3.63590121e-01  4.79958160e-03 -1.55586442e-02\n",
      " -7.94477463e-02  2.31407508e-01  1.23839326e-01  2.73110747e-01\n",
      "  3.45264286e-01  4.84697483e-02  2.22315222e-01  3.27853709e-01\n",
      " -1.72252715e-01  1.69384271e-01  1.54607613e-02 -1.76302835e-01\n",
      "  2.75904946e-02  1.46101892e-01 -6.82402924e-02  2.28630304e-01\n",
      "  2.95078248e-01  1.68967366e-01 -1.76830053e-01  2.27201376e-02\n",
      "  1.63419053e-01 -1.90206155e-01  1.86483964e-01 -9.87915844e-02\n",
      " -5.17291650e-02  1.19692329e-02 -8.75580870e-03 -1.36095762e-01\n",
      " -2.30906740e-01 -5.81824183e-02 -2.49898732e-01  1.22893147e-01\n",
      "  1.17668450e-01 -1.66155268e-02 -1.84741050e-01  1.06233016e-01\n",
      "  5.29602282e-02 -1.18315153e-01 -2.01580718e-01 -2.16164947e-01\n",
      " -7.95826912e-02 -1.28077999e-01  1.89847201e-01  2.42601126e-01\n",
      "  1.71744332e-01 -1.87344804e-01 -9.61660519e-02  4.69121188e-01\n",
      "  1.86339803e-02 -2.24769041e-01 -1.19646430e-01 -6.73382208e-02\n",
      "  8.60300884e-02 -2.07516924e-01 -1.09623885e-02 -9.61635038e-02\n",
      " -1.93803608e-01  3.50292176e-02 -1.11860365e-01 -2.11956710e-01\n",
      " -2.80564791e-03  3.30093056e-01 -1.45057276e-01  3.40315402e-01\n",
      "  1.49433762e-01 -2.15172768e-02 -1.32017165e-01 -1.77347735e-01\n",
      " -1.46160051e-01 -3.57039878e-03 -1.22502521e-02  1.92397118e-01\n",
      "  1.07791811e-01  5.75613603e-02 -5.20177223e-02 -6.18088432e-02\n",
      " -2.48950589e-02  1.45244077e-01 -1.27031267e-01 -6.66845500e-01\n",
      "  1.47584863e-02 -6.32642061e-02  4.69715782e-02 -1.63942099e-01\n",
      "  1.12994291e-01  2.68627238e-02  5.03879488e-02 -1.43907860e-01\n",
      "  2.46512033e-02 -2.13593051e-01 -4.91114780e-02  3.09016079e-01\n",
      "  8.60139951e-02 -7.54215941e-02 -1.75129145e-01 -1.41215444e-01\n",
      " -2.98532075e-03  1.23523153e-01  2.86364220e-02  3.60337901e-03\n",
      " -3.34924966e-01  9.45803616e-03 -9.66798663e-02  9.47916731e-02\n",
      "  7.76323723e-03 -3.20851505e-01  2.83750594e-01 -1.88430130e-01\n",
      " -1.87658295e-02  2.24626914e-01 -1.78337172e-01 -2.54941672e-01\n",
      " -2.98743486e-01  2.32687846e-01 -7.38694295e-02 -1.50198475e-01\n",
      "  1.15888134e-01  1.80686712e-01 -1.09988496e-01  3.97286683e-01\n",
      "  1.83873966e-01  2.72329360e-01  3.37971866e-01 -3.39861929e-01\n",
      " -6.63344786e-02 -6.60243258e-02 -4.66471285e-01 -3.07992995e-01\n",
      " -8.99818540e-02  1.48966089e-01  2.00275239e-02  1.69851677e-03\n",
      " -5.86620383e-02 -1.43864140e-01 -1.03304319e-01  1.32208392e-02\n",
      "  1.61895618e-01 -3.01285475e-01 -2.94633865e-01  1.45696536e-01\n",
      "  4.03659493e-01  2.03810200e-01 -1.44978911e-01 -3.02251697e-01\n",
      "  2.56832093e-01 -2.05483750e-01 -2.75047943e-02  9.05948132e-02\n",
      " -2.54128575e-01  1.30003825e-01  3.50980856e-03  1.98384359e-01\n",
      "  2.32904777e-01 -1.70080110e-01 -5.16546406e-02  1.06501430e-02\n",
      " -2.06698035e-03 -2.26088896e-01 -3.03879648e-01  3.69238555e-02\n",
      " -1.05881386e-01  1.27915919e-01  3.44917588e-02 -2.04693973e-01\n",
      "  4.71143704e-03  1.88771427e-01 -4.42733586e-01 -4.03010752e-03\n",
      "  7.43201654e-03  6.00375272e-02 -2.24433169e-01  1.96170896e-01\n",
      " -1.84257850e-02  1.53788537e-01  2.01872841e-01  1.48256883e-01\n",
      " -2.58206785e-01  1.38704494e-01 -1.38474777e-01 -4.89715859e-02\n",
      " -1.25882104e-01 -1.20699428e-01 -2.77553141e-01  1.93464398e-01\n",
      " -1.05151543e-02  2.56695524e-02 -2.89780237e-02 -7.15784132e-02\n",
      " -4.47084486e-01  2.32021920e-02  9.94690657e-02  2.38370553e-01\n",
      " -2.20894024e-01 -4.40210775e-02  6.22391105e-02 -2.08222643e-02\n",
      " -1.79003216e-02  1.83550656e-01  4.76202190e-01  5.80750927e-02\n",
      " -4.72734869e-02  4.02014330e-02 -2.59101123e-01  1.63641289e-01\n",
      "  2.18264833e-01  2.06693131e-02 -2.38672897e-01 -2.42853627e-01\n",
      " -1.47043630e-01  1.74693912e-01 -1.27499476e-01  1.65090963e-01\n",
      " -1.37742192e-01  8.56605992e-02 -2.44416147e-01 -2.60993792e-03\n",
      "  1.82660371e-02 -1.01119854e-01 -4.76315729e-02  1.63158327e-01\n",
      "  6.25827862e-03  1.07980534e-01 -4.31261444e-03  5.08970201e-01\n",
      "  1.01851285e-01  3.08457583e-01  2.28573173e-01  7.10774213e-02\n",
      "  1.26830354e-01 -6.54456764e-02  1.37136608e-01 -1.11963432e-02\n",
      "  8.82915854e-02  2.16175571e-01 -3.34543109e-01 -5.51204905e-02\n",
      "  5.37350425e-04  3.70443523e-01 -1.58666790e-01  2.86669761e-01\n",
      " -7.29131922e-02  1.67036071e-01  1.25385135e-01  1.60031363e-01\n",
      " -1.56663880e-01 -2.84931600e-01 -2.84579843e-01  4.10835594e-02]\n",
      "\n",
      "Cognition centroid vector:\n",
      " [-5.07371090e-02  1.31839320e-01  6.94089085e-02  1.16282873e-01\n",
      "  8.79142955e-02  3.14267911e-02  2.31894657e-01  1.34918436e-01\n",
      "  6.00690022e-02 -1.48669019e-01 -1.45824142e-02 -9.64236632e-02\n",
      "  2.73935139e-01 -1.44364974e-02 -1.34145826e-01 -8.25469345e-02\n",
      " -1.92977726e-01 -1.93394721e-01  9.24657881e-02  1.45117939e-01\n",
      " -9.06541720e-02 -5.63679002e-02  2.84064740e-01 -3.22583877e-02\n",
      "  3.19670856e-01 -1.85155228e-01 -1.60625532e-01 -1.15993030e-01\n",
      " -3.98328230e-02 -7.96059072e-02  5.84645569e-03 -1.40304357e-01\n",
      " -1.99831754e-01 -8.02495629e-02 -9.60439369e-02  2.27196783e-01\n",
      "  2.52603609e-02 -1.96604118e-01 -5.69539592e-02  1.50080591e-01\n",
      " -2.36283779e-01  1.30841127e-02  5.20534720e-03 -2.10780144e-01\n",
      " -1.14772480e-03  8.69712885e-03 -1.72495525e-02 -7.86623061e-02\n",
      " -3.63917053e-02  5.81295565e-02  1.75732613e-01 -1.40069323e-02\n",
      " -5.28879613e-02 -4.48811203e-02  1.20144552e-02  2.30559960e-01\n",
      "  3.19304913e-01 -1.77988455e-01 -5.56023158e-02  2.11561188e-01\n",
      " -3.10114086e-01  8.25892761e-02 -7.36673027e-02  1.07775815e-01\n",
      "  1.33959800e-01  2.58223474e-01  3.07348296e-02  5.87143265e-02\n",
      "  3.00842911e-01  2.33828351e-02 -1.37050837e-01  1.63970843e-01\n",
      "  1.53359443e-01  4.92711887e-02  2.89417077e-02 -2.13851426e-02\n",
      " -1.88016489e-01  1.83568001e-02  8.59338343e-02  1.98291376e-01\n",
      " -1.51126698e-01 -1.24491602e-01 -9.07520354e-02 -1.11706443e-01\n",
      "  1.22020096e-01  6.84760138e-02 -1.60938635e-01  2.01779790e-02\n",
      "  1.21316381e-01  1.11124152e-02 -4.52082418e-02 -1.20180659e-01\n",
      "  1.30554646e-01  1.71495751e-01 -1.02543151e-02  1.70727342e-01\n",
      "  1.45381674e-01  3.02518215e-02 -1.80965319e-01 -6.15877844e-02\n",
      " -1.30438447e-01 -1.96354553e-01  1.24720059e-01 -6.87844083e-02\n",
      "  3.78953689e-03  3.64283584e-02 -1.24152943e-01  3.79593149e-02\n",
      "  5.68804070e-02  8.51097777e-02 -3.39281522e-02 -1.96503341e-01\n",
      " -5.37793897e-02  1.23967223e-01 -2.31267065e-01  1.57461673e-01\n",
      " -4.31990847e-02 -1.24836050e-01  1.78823583e-02  1.03129400e-02\n",
      "  3.07736825e-02 -1.97757520e-02  2.42022306e-01  2.46342406e-01\n",
      " -4.73954044e-02  1.22593768e-01  5.34416102e-02  2.07105249e-01\n",
      "  8.78867693e-03  1.11710750e-01  1.39833093e-02 -1.98787615e-01\n",
      " -8.46696198e-02  2.33654127e-01  1.34448959e-02 -1.77723110e-01\n",
      " -4.65273634e-02  9.05921608e-02  1.71540100e-02 -7.48663694e-02\n",
      " -1.40314639e-01 -7.21492916e-02  2.22743284e-02 -8.42080042e-02\n",
      "  1.19198278e-01  5.27273212e-03 -1.09151527e-01  1.20777994e-01\n",
      "  3.68033275e-02  2.87637979e-01 -3.22156213e-02 -3.33579779e-02\n",
      " -1.83482736e-01  2.35697664e-02 -1.61658779e-01  1.55864388e-01\n",
      "  2.27816235e-02 -1.07365325e-01  1.23269349e-01 -6.09593429e-02\n",
      " -1.59870476e-01 -4.43554558e-02 -1.76291075e-02 -2.30753049e-01\n",
      " -3.21492612e-01 -5.29082045e-02 -9.09886882e-02  7.28607252e-02\n",
      "  9.16520506e-02  2.48864010e-01  9.67579894e-03 -1.13973022e-02\n",
      "  1.17033117e-01 -3.20593186e-04  4.07109857e-02  4.07559089e-02\n",
      "  1.07955165e-01 -5.27269989e-02 -1.16625533e-01  1.44160196e-01\n",
      " -1.49049945e-02 -5.35566583e-02  5.36102019e-02  1.78750772e-02\n",
      "  9.25124586e-02 -1.06210269e-01 -9.74678323e-02  9.82079878e-02\n",
      " -1.53786287e-01 -1.49133950e-01 -2.66640782e-01 -2.37025410e-01\n",
      "  9.18096229e-02 -1.67570844e-01  2.54344910e-01  5.13301305e-02\n",
      " -1.52871832e-01  1.20385490e-01  1.39357343e-01 -1.90833006e-02\n",
      " -1.10758752e-01  8.78379866e-03  5.72624393e-02  1.27173066e-01\n",
      " -2.96746194e-02 -1.23286523e-01 -2.12552726e-01 -1.89223394e-01\n",
      "  6.80754110e-02 -1.21302538e-01 -6.99866116e-02  2.19829887e-01\n",
      "  1.81920938e-02  2.49817789e-01  1.77146215e-02  4.14682887e-02\n",
      " -4.80345823e-03  2.82274038e-01 -1.11785144e-01  4.63142805e-02\n",
      "  1.54038832e-01  9.17692333e-02  1.33103848e-01  3.20141464e-01\n",
      " -9.54682529e-02  1.93494797e-01  1.28483668e-01 -4.78015393e-02\n",
      "  4.39956412e-02  1.92353398e-01  6.69572279e-02  5.11446781e-02\n",
      " -8.57288465e-02  2.80095432e-02 -8.23369026e-02 -1.57481775e-01\n",
      "  6.26037046e-02 -2.44591236e-01  2.54911054e-02  5.31530119e-02\n",
      " -4.86408547e-02  1.74844906e-01 -2.31249612e-02 -1.12723142e-01\n",
      " -3.22573185e-01  2.25821957e-01 -2.72715777e-01 -3.49441692e-02\n",
      "  1.52364478e-01  6.97280979e-04  1.49370506e-01  1.46958185e-02\n",
      " -2.40193140e-02  2.07042880e-02  5.26273400e-02  1.78136379e-01\n",
      " -1.71632782e-01  2.75536943e-02 -5.23965769e-02 -9.48600471e-02\n",
      " -9.93610620e-02 -2.09615547e-02 -1.36811152e-01 -1.65678430e-02\n",
      "  1.02687195e-01 -3.50278169e-01 -1.35670394e-01 -1.51791304e-01\n",
      " -2.31433623e-02  9.88601074e-02  5.07912561e-02 -3.04933805e-02\n",
      " -7.12111890e-02 -1.36759609e-01  6.52638376e-02 -9.63419676e-02\n",
      "  4.94920351e-02  6.13881163e-02  2.03254372e-01  1.47127777e-01\n",
      " -7.17551187e-02 -2.03274667e-01  5.77330515e-02 -1.07551500e-01\n",
      "  7.36319572e-02 -2.07029358e-02 -1.47213459e-01  1.23268645e-02\n",
      " -5.97006939e-02  1.69996507e-02 -5.44100814e-03  3.72358435e-03\n",
      " -1.04956783e-01 -1.46272266e-02  9.93232355e-02  3.74734432e-01\n",
      "  2.49987133e-02 -2.07595572e-01 -7.01074898e-02  2.19731718e-01]\n",
      "\n",
      "Shape of affect centroid: (300,)\n",
      "Shape of cognition centroid: (300,)\n"
     ]
    }
   ],
   "source": [
    "# == Calculation ==\n",
    "def findcentroid(text, model):\n",
    "    vecs = [model.wv[w] * word_counts_weighted[w] for w in text if w in model.wv]\n",
    "    vecs = [v for v in vecs if len(v) > 0]\n",
    "    centroid = np.mean(vecs, axis=0)\n",
    "    #centroid = centroid.reshape(1, -1)\n",
    "    return centroid\n",
    "\n",
    "\n",
    "affect_centroid = findcentroid(affect_dic, w2v)\n",
    "cog_centroid = findcentroid(cognition_dic, w2v)\n",
    "\n",
    "os.chdir(data_c)\n",
    "joblib.dump(affect_centroid, 'centroids/affect_centroid.pkl')\n",
    "joblib.dump(cog_centroid, 'centroids/cog_centroid.pkl')\n",
    "\n",
    "# == Overview Vectors ==\n",
    "print(\"Affect centroid vector:\\n\", affect_centroid)\n",
    "print(\"\\nCognition centroid vector:\\n\", cog_centroid)\n",
    "\n",
    "# Shape and stats\n",
    "print(\"\\nShape of affect centroid:\", affect_centroid.shape)\n",
    "print(\"Shape of cognition centroid:\", cog_centroid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af347b1-4f5f-401a-8ca0-1c33eb7776b1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c8316f-f794-4bcd-87c9-578aa82f88b0",
   "metadata": {},
   "source": [
    "### Emotionality Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3376a4f6-a002-4179-8614-dfef96113180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set wd to data_preprocessed\n",
    "os.chdir(data_preprocessed)\n",
    "\n",
    "# Load preprocessed speech data # NOR SURE IF USING THIS FILE IS CORRECT\n",
    "\n",
    "preprocessed_final_files = [\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed1_final.pkl')),\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed2_final.pkl')),\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed3_final.pkl')),\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed4_final.pkl'))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cbd1f95-c7f4-41f1-a292-542ee34ae882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit\\\\results\\\\distances_10epochs.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to compute weighted document vectors and derive affective/cognitive distances and scores\n",
    "\n",
    "def documentvecweight(lista):\n",
    "    out = []\n",
    "    lista = [i for i in lista if len(i[1]) > 0]\n",
    "    for s in lista:\n",
    "        # Compute weighted word vectors for each token present in the Word2Vec model\n",
    "        vecs = [w2v.wv[w] * word_counts_weighted[w] for w in s[1] if w in w2v.wv]\n",
    "        if len(vecs) == 0:\n",
    "            a = np.nan\n",
    "            c = np.nan\n",
    "            score = np.nan\n",
    "        else:\n",
    "            # Compute mean vector for each speech\n",
    "            v = np.mean(vecs, axis=0)\n",
    "             # Cosine distance to affective centroid\n",
    "            a = cosine(v, affect_centroid)\n",
    "            # Cosine distance to cognitive centroid\n",
    "            c = cosine(v, cog_centroid)\n",
    "            score = (1 + 1 - a) / (1 + 1 - c)\n",
    "        out.append([s[0], a, c, score])\n",
    "    return out\n",
    "\n",
    "\n",
    "def main_function(file_path, idx):\n",
    "    dataset = joblib.load(file_path)\n",
    "    data = documentvecweight(dataset)\n",
    "    lab = os.path.join(wd_results, f'distances_main_{idx}.pkl')\n",
    "    joblib.dump(data, lab)\n",
    "\n",
    "# Main loop: process all preprocessed speech files\n",
    "def main():\n",
    "    files = [\n",
    "        os.path.join(data_preprocessed, f'preprocessed_speeches_indexed{i+1}_final.pkl') #Changed!\n",
    "        for i in range(4)\n",
    "    ]\n",
    "    for i, f in enumerate(files, start=1):\n",
    "        main_function(f, i)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# Merge all distance files into a single DataFrame\n",
    "DATA_temp = [os.path.join(wd_results, f'distances_main_{i+1}.pkl') for i in range(4)]\n",
    "\n",
    "tot = []\n",
    "for dataname in DATA_temp:\n",
    "    d = joblib.load(dataname)\n",
    "    tot += d\n",
    "\n",
    "tot_df = pd.DataFrame(tot, columns=['filename', 'affect_d', 'cognition_d', 'score'])\n",
    "joblib.dump(tot_df, os.path.join(wd_results, 'distances_10epochs.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb955ab4-243d-470d-a967-e0af1e474c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          filename  affect_d  cognition_d     score\n",
      "0  FIN_68_2013.txt  1.437249     1.099765  0.625116\n",
      "1  JAM_30_1975.txt  1.154664     0.829715  0.722333\n",
      "2  CRI_36_1981.txt  1.403022     1.015460  0.606352\n",
      "3  IND_28_1973.txt  1.506429     1.039613  0.513929\n",
      "4  COG_70_2015.txt  1.225038     1.215136  0.987384\n"
     ]
    }
   ],
   "source": [
    "# Print the first few rows\n",
    "print(tot_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c295cd4-4fa0-470f-b67c-4f584f7a3f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load df_merged and merge with tot_df by filename \n",
    "un_corpus_merged = pd.read_csv(os.path.join(data_c, \"un_corpus_merged.csv\"), sep=';', encoding='utf-8') \n",
    "un_corpus_scored = un_corpus_merged.merge(tot_df, on=\"filename\", how=\"left\")\n",
    "\n",
    "joblib.dump(un_corpus_scored, os.path.join(wd_results, \"un_corpus_scored.pkl\"))\n",
    "\n",
    "un_corpus_scored.to_csv(\n",
    "    os.path.join(wd_results, \"un_corpus_scored.csv\"),\n",
    "    sep=';', \n",
    "    index=False, \n",
    "    encoding='utf-8'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8d13667-1eb3-482e-83ca-5969a229750a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              filename                                             speech  \\\n",
      "0      FIN_68_2013.txt  We have convened here \\nin New York at a time ...   \n",
      "1      JAM_30_1975.txt  152.\\t Let me begin by expressing, on behalf o...   \n",
      "2      CRI_36_1981.txt  I am happy to join previous speakers in congra...   \n",
      "3      IND_28_1973.txt  ï»¿122.\\tMr. President, I bring to you and to al...   \n",
      "4      COG_70_2015.txt  His Excellency Mr. Denis Sassou Nguesso, Presi...   \n",
      "...                ...                                                ...   \n",
      "10947  LKA_72_2017.txt  I am very pleased to be able to congratulate t...   \n",
      "10948  PNG_40_1985.txt  My delegation and I would like to congratulate...   \n",
      "10949  COL_79_2024.txt  *\"It is the hour of the peoples. If government...   \n",
      "10950  CHN_04_1949.txt  Mr. Tsiang observed that the general debate at...   \n",
      "10951  SGP_41_1986.txt   We meet this year under inauspicious circumst...   \n",
      "\n",
      "      country_code  year      country_name  speech_length_words  \\\n",
      "0              FIN  2013           Finland                 2071   \n",
      "1              JAM  1975           Jamaica                 2864   \n",
      "2              CRI  1981        Costa Rica                 4144   \n",
      "3              IND  1973             India                 5341   \n",
      "4              COG  2015             Congo                 1584   \n",
      "...            ...   ...               ...                  ...   \n",
      "10947          LKA  2017         Sri Lanka                 1465   \n",
      "10948          PNG  1985  Papua New Guinea                 3349   \n",
      "10949          COL  2024          Colombia                 1957   \n",
      "10950          CHN  1949             China                 1700   \n",
      "10951          SGP  1986         Singapore                 1919   \n",
      "\n",
      "       english_official_language  security_council_permanent  \\\n",
      "0                              0                           0   \n",
      "1                              0                           0   \n",
      "2                              0                           0   \n",
      "3                              1                           0   \n",
      "4                              0                           0   \n",
      "...                          ...                         ...   \n",
      "10947                          0                           0   \n",
      "10948                          1                           0   \n",
      "10949                          0                           0   \n",
      "10950                          0                           1   \n",
      "10951                          1                           0   \n",
      "\n",
      "                   speaker_name                               position  \\\n",
      "0                Erkki Tuomioja  (Deputy) Minister for Foreign Affairs   \n",
      "1                  Mr. THOMPSON                                    NaN   \n",
      "2            Mr. NIEHAUSQUESADA                                    NaN   \n",
      "3                  Swaran Singh                                    NaN   \n",
      "4       Mr. Jean-Claude Gakosso  (Deputy) Minister for Foreign Affairs   \n",
      "...                         ...                                    ...   \n",
      "10947  Mr. Maithripala Sirisena                      (Vice-) President   \n",
      "10948                Mr. Giheno                                    NaN   \n",
      "10949      Gustavo Petro Urrego                      (Vice-) President   \n",
      "10950                Mr. Tsiang                                    NaN   \n",
      "10951            Mr. DHANABALAN                                    NaN   \n",
      "\n",
      "       gender_dummy             speech_label  affect_d  cognition_d     score  \n",
      "0               NaN           Finland (2013)  1.437249     1.099765  0.625116  \n",
      "1               0.0           Jamaica (1975)  1.154664     0.829715  0.722333  \n",
      "2               0.0        Costa Rica (1981)  1.403022     1.015460  0.606352  \n",
      "3               NaN             India (1973)  1.506429     1.039613  0.513929  \n",
      "4               0.0             Congo (2015)  1.225038     1.215136  0.987384  \n",
      "...             ...                      ...       ...          ...       ...  \n",
      "10947           0.0         Sri Lanka (2017)  1.230667     1.013796  0.780096  \n",
      "10948           0.0  Papua New Guinea (1985)  1.521862     1.211805  0.606623  \n",
      "10949           NaN          Colombia (2024)  0.350812     0.663744  1.234186  \n",
      "10950           0.0             China (1949)  0.809748     0.996112  1.185642  \n",
      "10951           0.0         Singapore (1986)  1.215336     0.799170  0.653435  \n",
      "\n",
      "[10952 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "print(un_corpus_scored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1210b88f-cf13-4da0-807b-77764cf0d439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count where affect_d is NaN: 0\n",
      "Count where affect_d is not NaN: 10952\n"
     ]
    }
   ],
   "source": [
    "# Count where affect_d is NaN\n",
    "nan_count = un_corpus_scored['affect_d'].isna().sum()\n",
    "\n",
    "# Count where affect_d is not NaN\n",
    "not_nan_count = un_corpus_scored['affect_d'].notna().sum()\n",
    "\n",
    "print(\"Count where affect_d is NaN:\", nan_count)\n",
    "print(\"Count where affect_d is not NaN:\", not_nan_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751788c2-eb58-421a-91c4-bcdb0b325cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051890b7-921f-4bc1-92f9-cea6ee3d5aed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91380369-f1c0-4f97-9553-d8dd0a9c229c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
