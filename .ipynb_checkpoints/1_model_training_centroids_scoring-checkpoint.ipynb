{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a39bd0b5-2ac1-499d-8938-2fdfe4c3d93d",
   "metadata": {},
   "source": [
    "# Development of Emotion and Reasoning in the General Speeches of the United Nations: A text-based machine learning approach\n",
    "## Script 1: Model Traning, Calculation Centroids & Speech Scoring\n",
    "### Author: Sarah Franzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ca6f721-48ad-495c-8e5f-e60790225709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder checked/created: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\results\n",
      "Folder checked/created: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\models\n"
     ]
    }
   ],
   "source": [
    "# == Import libraries for data processing and NLP ==\n",
    "\n",
    "import os\n",
    "import gensim\n",
    "from pathlib import Path\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from random import shuffle\n",
    "import nltk\n",
    "from gensim.utils import simple_preprocess\n",
    "import joblib\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from string import punctuation\n",
    "from scipy.spatial.distance import cosine\n",
    "import glob\n",
    "import spacy\n",
    "from multiprocessing import Pool, freeze_support\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# === Initialize NLP Tools ===\n",
    "\n",
    "translator = str.maketrans('', '', punctuation) \n",
    "tagger = nltk.perceptron.PerceptronTagger()\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# === Set Working Directory ===\n",
    "\n",
    "# Prompt user to enter working directory path\n",
    "#wd = input(\"Please enter your working directory path (e.g., C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit): \").strip()\n",
    "\n",
    "# Change to the entered working directory\n",
    "#try:\n",
    "   # os.chdir(wd)\n",
    "    #print(f\"Working directory set to: {os.getcwd()}\")\n",
    "#except FileNotFoundError:\n",
    "   # print(\"ERROR: The directory you entered does not exist. Please restart and enter a valid path.\")\n",
    "    #exit(1)\n",
    "\n",
    "# Set your working directory (adjust this as needed)\n",
    "#wd = r\"C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\"\n",
    "#os.chdir(wd)\n",
    "\n",
    "# Set your working directory (adjust this as needed)\n",
    "wd = Path(r\"C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\")\n",
    "\n",
    "# === Create new Folders ===\n",
    "additional_folders = [\"results\", \"models\"]\n",
    "\n",
    "# Create/check folders directly in wd\n",
    "for folder in additional_folders:\n",
    "    folder_path = Path(wd) / folder\n",
    "    folder_path.mkdir(exist_ok=True)\n",
    "    print(f\"Folder checked/created: {folder_path}\")\n",
    "\n",
    "# === Define Folder Paths ===\n",
    "\n",
    "# Folders were already created in the script 0_data_creation\n",
    "data_c = wd / \"data\"\n",
    "data_temp = data_c / \"temp\"\n",
    "data_freq = data_c / \"freq\"\n",
    "data_dict = data_c / \"dictionaries\"\n",
    "data_sent = data_c / \"sentences\"\n",
    "data_preprocessed = data_c / \"preprocessed\"\n",
    "fig_dir = wd /\"fig\"\n",
    "\n",
    "wd_results = wd / \"results\" # THESE FOLDERS NEED TO BE CREATED\n",
    "wd_models = wd / \"models\" # THESE FOLDERS NEED TO BE CREATED\n",
    "\n",
    "# Upload ressources\n",
    "stopwords = joblib.load(data_c / \"stopwords.pkl\")              #### this is from the replication package! Issue for replication\n",
    "word_counts = joblib.load(data_freq / \"word_counts.pkl\")\n",
    "word_counts_weighted = joblib.load(data_freq / \"word_counts_weighted.pkl\")\n",
    "affect_dic = joblib.load(data_dict / 'dictionary_affect.pkl')  ### same issue here!\n",
    "cognition_dic = joblib.load(data_dict / 'dictionary_cognition.pkl') ### same issue here! must also be fixed in 0_data_creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "125227ca-a0b5-43d7-956d-7d49a66f81b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_temp)\n",
    "cleaned_files = [\n",
    "    str(data_temp / 'clean_speeches_indexed1.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed2.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed3.pkl'),\n",
    "    str(data_temp / 'clean_speeches_indexed4.pkl')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855a7f37-bbf9-4259-8a07-7e2a589f1de4",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2645f36-e935-4b0e-b5e6-7461f398d887",
   "metadata": {},
   "source": [
    "### Sentence Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af0997e8-9da3-45c3-bdd4-7d38a708a474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of very short sentences being dropped: 6493\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl processed\n",
      "Unique tokens: 10856\n",
      "Example sentences (first 5):\n",
      "['measur', 'solv', 'problem', 'cannot', 'dictat', 'tactic', 'strateg', 'consider', 'great', 'concern']\n",
      "['huge', 'damag', 'caus', 'katrina', 'coast']\n",
      "['idea', 'thing', 'line', 'current', 'approach']\n",
      "['confer', 'support', 'declar']\n",
      "['injunct', 'observ', 'pursu', 'polici', 'polit', 'adventur', 'harm', 'intern', 'oper', 'welfar', 'peopl']\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed1.pkl saved\n",
      "Number of very short sentences being dropped: 6651\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl processed\n",
      "Unique tokens: 10873\n",
      "Example sentences (first 5):\n",
      "['achiev', 'modern', 'public', 'elimin', 'insecur', 'joint', 'societi', 'terror']\n",
      "['year', 'histori', 'end', 'cold', 'event', 'taken', 'place', 'half', 'process', 'technolog', 'explor', 'atom', 'scienc', 'remot', 'sens', 'progress', 'incalcul', 'valu', 'futur', 'mankind', 'note', 'concern', 'guarante', 'secur', 'nation']\n",
      "['presid', 'kenya', 'becom', 'countri']\n",
      "['general', 'assembl', 'vote', 'year', 'year', 'right', 'remain', 'usurp', 'violat']\n",
      "['sweep', 'bind', 'commit', 'limit', 'veto', 'secur']\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed2.pkl saved\n",
      "Number of very short sentences being dropped: 6522\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl processed\n",
      "Unique tokens: 10852\n",
      "Example sentences (first 5):\n",
      "['histori', 'shown', 'privileg', 'come', 'great', 'strength', 'limit']\n",
      "['econom', 'front', 'deleg', 'view', 'slow', 'progress', 'africa', 'attain', 'sustain', 'econom', 'growth', 'develop', 'relat', 'failur', 'intern', 'communiti', 'adequ', 'resourc', 'develop']\n",
      "['confid', 'abil', 'resolv', 'conflict', 'shaken', 'somalia', 'afghanistan', 'azerbaijan', 'tajikistan', 'liberia', 'rwanda', 'burundi']\n",
      "['predecessor', 'made', 'contribut', 'general', 'assembl']\n",
      "['cogniz', 'fact', 'structur', 'transform', 'drive', 'focus', 'infrastructur', 'develop', 'road', 'play', 'critic', 'part']\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed3.pkl saved\n",
      "Number of very short sentences being dropped: 6510\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl processed\n",
      "Unique tokens: 10866\n",
      "Example sentences (first 5):\n",
      "['fact', 'popul', 'irian', 'particip', 'defenc', 'proclam']\n",
      "['stress', 'need', 'credibl', 'practic', 'step', 'comprehens', 'resolut', 'palestinian', 'base', 'intern', 'legitimaci', 'initi', 'guarante', 'palestinian', 'peopl', 'establish', 'border', 'capit']\n",
      "['global', 'resourc', 'fisheri']\n",
      "['reconstruct', 'societi', 'valu', 'renew', 'acknowledg', 'valu', 'famili', 'role', 'societi', 'basic', 'cell']\n",
      "['various', 'promulg', 'encourag', 'foreign', 'capit', 'invest', 'sign', 'contract', 'european', 'compani', 'explor', 'exploit', 'resourc', 'sub', 'soil']\n",
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\sentences_indexed4.pkl saved\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_sent)\n",
    "\n",
    "# Function to split cleaned speeches (clean_speeches) into sentences, tokenize, clean, tag, stem, filter, and save them.\n",
    "\n",
    "def extract_sentences(dataname):\n",
    "    data = joblib.load(dataname)\n",
    "    data = [a[1] for a in data]  # keep only text, no id\n",
    "\n",
    "    sentences = []\n",
    "    for doc in data:\n",
    "        sentences += sent_tokenize(doc)  # use nltk's sent_tokenize here\n",
    "\n",
    "    sentences = [item for item in sentences if len(item.split()) > 1]\n",
    "    sentences = [gensim.utils.simple_preprocess(item) for item in sentences]\n",
    "\n",
    "    sentences = [[a for a in s if not a.isdigit()] for s in sentences]\n",
    "    sentences = [[a for a in s if len(a) > 2] for s in sentences]\n",
    "\n",
    "    sentences = [tagger.tag(s) for s in sentences]\n",
    "    sentences = [[i[0] for i in s if i[1].startswith(('N', 'V', 'J'))] for s in sentences]\n",
    "\n",
    "    sentences = [[stemmer.stem(i) for i in s] for s in sentences]\n",
    "    sentences = [[a for a in s if a not in stopwords] for s in sentences]\n",
    "    sentences = [[a for a in s if word_counts[a] >= 10] for s in sentences]\n",
    "\n",
    "    dropped_count = sum(1 for s in sentences if len(s) <= 1)\n",
    "    print(f\"Number of very short sentences being dropped: {dropped_count}\")\n",
    "\n",
    "    sentences = [s for s in sentences if len(s) > 1]\n",
    "    shuffle(sentences)\n",
    "\n",
    "    lab = dataname.replace('clean_speeches_', 'sentences_').replace('_.pkl', '.pkl')\n",
    "    print(f'{dataname} processed')\n",
    "    joblib.dump(sentences, lab)\n",
    "    \n",
    "    unique_tokens = set(token for s in sentences for token in s)\n",
    "    print(f\"Unique tokens: {len(unique_tokens)}\")\n",
    "\n",
    "    # Print preview of first 5 processed sentences\n",
    "    print(\"Example sentences (first 5):\")\n",
    "    for s in sentences[:5]:\n",
    "        print(s)\n",
    "\n",
    "\n",
    "    print(f'{lab} saved')\n",
    "\n",
    "for fname in cleaned_files:\n",
    "    extract_sentences(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fddaea83-7d1f-4b14-85b9-401b2e727c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_files = [\n",
    "    os.path.join(data_temp, 'sentences_indexed1.pkl'),\n",
    "    os.path.join(data_temp, 'sentences_indexed2.pkl'),\n",
    "    os.path.join(data_temp, 'sentences_indexed3.pkl'),\n",
    "    os.path.join(data_temp, 'sentences_indexed4.pkl')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c41580ba-2954-4d84-a574-e9c3d4c8ef1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens across all files: 11015\n"
     ]
    }
   ],
   "source": [
    "# == Get sum of unique tokens ==\n",
    "all_unique_tokens = set()\n",
    "\n",
    "for dataname in sentences_files:\n",
    "    data = joblib.load(dataname)  # load list of tokenized sentences\n",
    "    for sentence in data:\n",
    "        all_unique_tokens.update(sentence)  # add tokens to the set\n",
    "\n",
    "print(f\"Total unique tokens across all files: {len(all_unique_tokens)}\")\n",
    "\n",
    "sentences_files = [\n",
    "    'sentences_indexed1.pkl',\n",
    "    'sentences_indexed2.pkl',\n",
    "    'sentences_indexed3.pkl',\n",
    "    'sentences_indexed4.pkl']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28938326-25d1-43cc-b0a8-4400dc138584",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d8d0e2-3d81-41ea-90c4-6d36bc9fa11c",
   "metadata": {},
   "source": [
    "### Train Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "465bc73c-70b6-4dbf-a379-8c5235f83a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for dataname in sentences_files:  \n",
    "    data = joblib.load(dataname)\n",
    "    dataset.extend(data) \n",
    "\n",
    "# === Model training ===\n",
    "w2v = Word2Vec(\n",
    "    sentences=dataset,    \n",
    "    vector_size=300,      # Dimension of the vector\n",
    "    window=8,             # Context window size\n",
    "    min_count=10,         # Minimum word count\n",
    "    workers=8,            # Number of threads\n",
    "    sample=1e-3,          # Downsample setting for frequent words\n",
    "    epochs=10             # Number of iterations over the corpus\n",
    ")\n",
    "\n",
    "# Optimize memory usage (optional)\n",
    "w2v.wv.fill_norms() \n",
    "\n",
    "# Save model\n",
    "wd_models.mkdir(parents=True, exist_ok=True) \n",
    "w2v.save(str(wd_models / 'w2v-vectors_8_300.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d94142c-1e7f-4b0b-bfec-98604fc84304",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec.load(str(wd_models / \"w2v-vectors_8_300.pkl\"))\n",
    "word_vectors = w2v.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aa347e-aad5-477c-a52a-40a84a7684c6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420620dc-4af4-4a55-a7b6-1827949d1b17",
   "metadata": {},
   "source": [
    "### Calculate Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b75bd3b-2957-42d1-a89c-89e5b3ea44f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affect centroid vector:\n",
      " [-1.43235356e-01 -1.13797188e-01  5.68300709e-02  1.21403314e-01\n",
      "  2.54126400e-01  1.70395553e-01  1.93211466e-01  3.81024301e-01\n",
      " -5.01806644e-05 -2.15641439e-01  1.66493475e-01 -1.89212412e-02\n",
      " -9.61072743e-02 -7.16939867e-02 -1.42261282e-01  1.91305757e-01\n",
      "  2.29738161e-01 -9.00678635e-02  1.83804631e-01  1.19748503e-01\n",
      " -1.47012293e-01  2.28954732e-01  1.08761445e-01 -1.08681805e-01\n",
      "  6.93900645e-01 -1.70672148e-01 -2.68425018e-01  5.02407067e-02\n",
      "  2.25024205e-02 -8.01452026e-02 -1.35954082e-01 -2.93332431e-03\n",
      " -3.25143039e-01  1.97603971e-01  8.95463005e-02  1.48656860e-01\n",
      " -7.85482004e-02 -3.79747987e-01  3.09812933e-01 -1.56153768e-01\n",
      " -8.34607258e-02  9.08261016e-02  2.72220910e-01 -1.88786119e-01\n",
      "  4.85519804e-02  5.00045717e-02 -1.35693789e-01 -7.02909902e-02\n",
      "  1.25538245e-01  3.29549193e-01 -3.39466855e-02  6.36829287e-02\n",
      " -1.08764827e-01  1.81328014e-01  1.10385552e-01  1.40552327e-01\n",
      "  4.04010564e-01  6.57801852e-02  2.21811041e-01  1.74902916e-01\n",
      " -2.09510982e-01  8.92454386e-02  4.57955338e-02 -6.57918826e-02\n",
      "  1.97181284e-01  1.39650062e-01 -1.25527114e-01  8.56344327e-02\n",
      "  1.79199830e-01  9.81034786e-02 -1.97390258e-01  1.54369801e-01\n",
      "  2.57880509e-01 -2.00510010e-01  2.05673426e-01 -8.23364705e-02\n",
      " -2.86490530e-01 -3.69153619e-02 -2.51182057e-02  8.30699399e-04\n",
      " -8.46948177e-02 -1.55644655e-01 -2.89099306e-01  1.97727278e-01\n",
      "  9.17034820e-02  2.70468509e-03 -2.17165962e-01  1.75357878e-01\n",
      " -9.13944915e-02  3.46010141e-02 -1.18067510e-01 -2.04675406e-01\n",
      "  2.82137888e-04 -1.05650261e-01  2.92865157e-01  1.62621960e-01\n",
      "  1.35778621e-01 -1.24348916e-01 -1.57022074e-01  5.02565622e-01\n",
      "  6.19333871e-02 -3.03097874e-01 -6.92831650e-02 -1.10007256e-01\n",
      "  1.42514899e-01 -1.28746495e-01 -8.19799826e-02 -1.42965559e-02\n",
      " -2.17405871e-01  2.31703501e-02 -1.56750396e-01 -3.52862984e-01\n",
      "  5.43977804e-02  2.47908741e-01 -1.18908346e-01  4.27362293e-01\n",
      "  5.70804961e-02 -1.50687963e-01 -6.36522621e-02 -3.77674699e-01\n",
      " -9.24145952e-02 -9.18295234e-03  1.67408332e-01  3.46548021e-01\n",
      " -1.23565227e-01 -1.15442552e-01  1.54962763e-01 -2.67611444e-01\n",
      "  1.11606754e-02  6.80569783e-02 -1.54171094e-01 -5.82697690e-01\n",
      "  2.14625970e-02  1.10775912e-02  2.70036776e-02 -1.78857476e-01\n",
      "  2.09940448e-02 -8.00870806e-02 -1.93188805e-02 -2.56212831e-01\n",
      "  4.34704497e-02 -1.01428106e-01 -4.39811833e-02  4.50869918e-01\n",
      "  7.84609746e-03 -9.88863409e-02 -2.81599015e-01 -8.78138542e-02\n",
      " -3.98968831e-02  1.93077922e-01 -7.75858806e-03 -2.42003605e-01\n",
      " -4.54935700e-01  3.10024023e-02 -6.57668188e-02  3.43302935e-02\n",
      "  6.69822395e-02 -4.03848857e-01  3.34664106e-01 -2.69380361e-01\n",
      " -5.90777621e-02  1.86955988e-01 -1.09989271e-01 -1.46724120e-01\n",
      " -3.90285373e-01  2.31822148e-01 -1.43247182e-02 -1.37072191e-01\n",
      "  1.69318900e-01  1.24957554e-01 -9.37620848e-02  2.92894632e-01\n",
      "  1.78709313e-01  2.54061669e-01  2.21853927e-01 -2.55179822e-01\n",
      " -8.97990167e-02 -7.88735896e-02 -4.67292458e-01 -3.12140942e-01\n",
      " -8.66782293e-02  2.12831989e-01 -8.69626738e-03  1.06468737e-01\n",
      "  6.28409758e-02  1.89701859e-02 -9.94763151e-02 -8.27428177e-02\n",
      "  2.01856032e-01 -3.48598480e-01 -1.22565582e-01  1.38987318e-01\n",
      "  3.54662716e-01  2.34632850e-01 -7.42259249e-02 -1.13038830e-01\n",
      "  1.16734222e-01 -2.54475534e-01 -6.65725693e-02  1.13446772e-01\n",
      " -2.66750753e-01  1.60649240e-01 -3.84645280e-03  1.55205995e-01\n",
      "  1.52131945e-01 -5.58465011e-02 -1.17628932e-01 -2.01990269e-02\n",
      " -9.89497676e-02 -2.63101161e-01 -2.40570918e-01 -2.44032759e-02\n",
      "  8.20090026e-02  9.89766344e-02  1.15539074e-01 -3.74283105e-01\n",
      "  1.29921529e-02  8.80993009e-02 -4.64163542e-01  8.93587023e-02\n",
      " -7.96624869e-02  1.37444697e-02 -2.19328105e-01  1.90473065e-01\n",
      " -2.94007380e-02  7.84961507e-02  1.37627736e-01  7.53208026e-02\n",
      " -2.66239941e-01  6.89093247e-02 -4.33200151e-02 -1.60824433e-01\n",
      " -1.90759581e-02  5.94427763e-03 -8.58643278e-02  1.25653848e-01\n",
      " -9.86011419e-03 -1.67739660e-01 -4.61000167e-02  5.39732985e-02\n",
      " -4.32089329e-01  2.17422973e-02  1.52968705e-01  1.80633515e-01\n",
      " -2.75793582e-01 -1.03272729e-01  7.69907609e-02 -9.17507038e-02\n",
      "  3.13019590e-03  2.07888409e-01  4.51923221e-01 -1.00566791e-02\n",
      " -7.42060840e-02 -3.34834009e-02 -2.35894486e-01  2.31219336e-01\n",
      "  1.13747843e-01  8.22327733e-02 -1.04114443e-01 -1.13523297e-01\n",
      " -1.47581726e-01  1.43932536e-01 -7.20478520e-02  1.99157178e-01\n",
      " -1.43339962e-01  1.05429571e-02 -2.45322332e-01 -1.05285242e-01\n",
      "  4.85255709e-03 -1.14174753e-01  1.20212964e-03  2.13298440e-01\n",
      "  7.00269779e-03  1.90843761e-01 -1.23437911e-01  4.42893773e-01\n",
      "  2.81198770e-02  2.45648220e-01  3.40813428e-01  1.03616104e-01\n",
      "  5.22148535e-02 -9.89655033e-02  3.93512361e-02 -3.90171781e-02\n",
      "  5.74684143e-02  2.24649802e-01 -1.12136886e-01  1.48112979e-02\n",
      "  2.17499211e-02  1.74671948e-01 -2.74138719e-01  4.44190651e-02\n",
      " -9.83074605e-02  1.17878802e-01  9.70896706e-02  1.34102553e-01\n",
      " -5.44257499e-02 -1.74805030e-01 -3.03226590e-01  6.09597154e-02]\n",
      "\n",
      "Cognition centroid vector:\n",
      " [ 3.13893007e-03 -2.09384621e-03  9.69309285e-02  8.07601027e-03\n",
      "  7.79056400e-02  5.04433773e-02  1.66220829e-01 -1.36052638e-01\n",
      "  1.08869128e-01 -2.66366363e-01  7.67147392e-02 -1.74961537e-02\n",
      "  2.48539865e-01  1.64841525e-02 -1.41546726e-01  2.16826275e-02\n",
      " -2.03460589e-01 -1.57529667e-01  7.91620389e-02  2.89801478e-01\n",
      " -6.04574271e-02 -6.36016652e-02  1.19759828e-01 -5.53064197e-02\n",
      "  2.28764951e-01 -7.99517781e-02  3.37574631e-02 -1.86493158e-01\n",
      " -9.01931897e-03 -1.08231287e-02  5.04511446e-02 -3.13172713e-02\n",
      " -1.50451407e-01 -1.43109784e-01  2.37738378e-02  2.48729199e-01\n",
      " -2.96574645e-02 -1.08133130e-01  8.20624828e-02  9.56478938e-02\n",
      " -1.85619205e-01  2.44253986e-02  1.07189007e-02 -6.86342865e-02\n",
      " -3.68959419e-02 -1.49321318e-01 -1.02972418e-01 -7.95139223e-02\n",
      "  6.36502653e-02  7.05629494e-03  1.07425183e-01 -1.19006604e-01\n",
      "  6.04434460e-02 -9.36032161e-02  9.54031944e-03  1.67591661e-01\n",
      "  3.04807693e-01 -1.29450083e-01 -7.62936100e-02  2.58844286e-01\n",
      " -2.46826857e-01  1.37923330e-01 -1.40265664e-02  1.32425576e-01\n",
      "  1.98154196e-01  1.47584707e-01 -7.45647475e-02  6.71626776e-02\n",
      "  3.20211262e-01 -3.28798480e-02 -1.34902224e-01  1.70223072e-01\n",
      "  1.95128262e-01  4.29803096e-02  1.08997636e-01 -5.90302199e-02\n",
      " -1.55881196e-01  3.90862301e-02  6.77286163e-02  1.13229409e-01\n",
      " -1.78434715e-01 -1.35068670e-01 -1.84003323e-01 -5.96673898e-02\n",
      "  2.64592208e-02  1.72595844e-01 -1.20419316e-01  5.62096573e-02\n",
      "  8.23960975e-02  6.20622002e-02 -1.07337110e-01 -1.34780169e-01\n",
      "  8.42951536e-02  1.70943797e-01  3.35358158e-02  9.03753191e-02\n",
      "  2.56756153e-02  1.77302137e-01 -1.94088116e-01 -1.84622128e-02\n",
      " -2.22151741e-01 -2.18636245e-01  1.79077655e-01 -7.46821612e-02\n",
      " -3.19133117e-03  9.37619358e-02 -2.02932119e-01  3.05549726e-02\n",
      "  2.04738781e-01  7.30707794e-02 -1.64434202e-02 -2.54580319e-01\n",
      " -7.57813305e-02  1.01416953e-01 -2.76737869e-01  3.27174142e-02\n",
      " -7.50405341e-02 -1.00866273e-01 -7.19922781e-02 -8.55199546e-02\n",
      " -5.91218099e-02 -1.03375323e-01  7.02761561e-02  2.01758727e-01\n",
      "  1.92559995e-02  1.61315441e-01  6.60713576e-03  1.06236503e-01\n",
      "  4.73105870e-02  8.06894246e-03 -1.29598230e-01 -1.28997222e-01\n",
      "  1.14471680e-02  1.95414394e-01 -6.30683303e-02 -2.48509094e-01\n",
      " -5.22603914e-02  9.18491036e-02 -1.17866464e-01 -9.73320305e-02\n",
      " -1.21572018e-01  9.97346547e-03  3.96249741e-02  5.21742776e-02\n",
      "  1.64623290e-01  1.25517040e-01 -1.27504826e-01  1.80977181e-01\n",
      " -1.14745402e-03  2.02189431e-01 -1.70088395e-01 -7.89318085e-02\n",
      " -2.45247018e-02  1.37587681e-01 -1.35933712e-01  7.94596672e-02\n",
      "  1.03757091e-01 -1.10700279e-01  9.52980891e-02  3.24412510e-02\n",
      " -1.49996340e-01  8.25432464e-02 -4.77721021e-02 -1.86018795e-01\n",
      " -2.72020757e-01 -8.49350989e-02 -5.92679642e-02  4.92626219e-04\n",
      "  5.53651862e-02  1.92522198e-01  7.28646070e-02 -8.67479146e-02\n",
      "  1.39874592e-01 -3.76612209e-02  2.78546400e-02 -5.15188798e-02\n",
      "  1.16354421e-01 -1.24164866e-02 -1.16502129e-01  7.37179294e-02\n",
      "  8.93885922e-03 -1.39003947e-01 -4.94942144e-02 -1.36865694e-02\n",
      "  7.43361637e-02 -1.46718889e-01 -1.45841047e-01  1.40863940e-01\n",
      " -8.76376927e-02 -1.30760357e-01 -1.93875164e-01 -1.39634892e-01\n",
      "  1.06281526e-01 -8.36574584e-02  2.25412041e-01 -1.42462561e-02\n",
      " -1.87730297e-01 -8.97694305e-02  4.10761237e-02  6.15895912e-02\n",
      " -1.46668166e-01  7.42019666e-03  5.58449402e-02  1.13294885e-01\n",
      " -1.59414589e-01  9.65398940e-05 -2.67524153e-01 -2.80767024e-01\n",
      "  9.18974727e-02 -2.09003076e-01 -7.94425383e-02  1.18391953e-01\n",
      "  1.01137953e-03  2.64771700e-01  9.71628726e-02 -7.79720396e-02\n",
      " -3.13141719e-02  1.50550589e-01 -1.90919667e-01  2.82641836e-02\n",
      "  1.10213161e-01  5.21399565e-02  2.22063288e-02  2.27624118e-01\n",
      " -1.54945552e-01  4.65428084e-02  1.50808632e-01 -1.34265035e-01\n",
      " -1.28793344e-01  1.64884627e-01  8.98475721e-02 -1.28723130e-01\n",
      " -1.80168733e-01  1.20137967e-01 -8.27716887e-02 -2.00174958e-01\n",
      "  3.28707583e-02 -1.64653569e-01  7.85541013e-02  4.34850231e-02\n",
      "  3.12878266e-02  7.91958869e-02 -1.95477501e-01 -1.92123786e-01\n",
      " -3.03780913e-01  1.89279720e-01 -2.04273924e-01  4.50162254e-02\n",
      "  2.07056850e-01  1.04226634e-01  2.84928352e-01  1.12824723e-01\n",
      "  1.06400266e-01 -1.49916083e-01 -5.57948872e-02  1.76944226e-01\n",
      " -1.12536632e-01  1.19416401e-01 -2.03766137e-01 -1.16084442e-01\n",
      " -3.93343233e-02  1.05444081e-01 -1.76319152e-01 -1.26587585e-01\n",
      " -6.08866960e-02 -1.87606722e-01 -1.49460524e-01 -1.39062107e-01\n",
      " -8.13479573e-02  8.81061852e-02  3.22392024e-02  1.55776730e-02\n",
      " -1.28998131e-01 -1.35621414e-01  1.18593328e-01 -1.29490599e-01\n",
      "  2.67090261e-01  1.47299990e-01  7.06579089e-02  7.26881325e-02\n",
      "  8.19547251e-02 -2.08659187e-01 -4.54874225e-02  4.73835273e-03\n",
      "  2.71488335e-02  1.11490376e-01 -9.58540589e-02 -6.81386441e-02\n",
      "  6.36138096e-02  5.03843948e-02 -1.50324211e-01  7.18456358e-02\n",
      " -3.36876772e-02 -1.68248355e-01  8.73956829e-02  2.21381694e-01\n",
      "  1.92857906e-02 -1.66095018e-01 -6.07597865e-02  1.59857899e-01]\n",
      "\n",
      "Shape of affect centroid: (300,)\n",
      "Shape of cognition centroid: (300,)\n"
     ]
    }
   ],
   "source": [
    "# == Calculation ==\n",
    "def findcentroid(text, model):\n",
    "    vecs = [model.wv[w] * word_counts_weighted[w] for w in text if w in model.wv]\n",
    "    vecs = [v for v in vecs if len(v) > 0]\n",
    "    centroid = np.mean(vecs, axis=0)\n",
    "    #centroid = centroid.reshape(1, -1)\n",
    "    return centroid\n",
    "\n",
    "\n",
    "affect_centroid = findcentroid(affect_dic, w2v)\n",
    "cog_centroid = findcentroid(cognition_dic, w2v)\n",
    "\n",
    "os.chdir(data_c)\n",
    "joblib.dump(affect_centroid, 'centroids/affect_centroid.pkl')\n",
    "joblib.dump(cog_centroid, 'centroids/cog_centroid.pkl')\n",
    "\n",
    "# == Overview Vectors ==\n",
    "print(\"Affect centroid vector:\\n\", affect_centroid)\n",
    "print(\"\\nCognition centroid vector:\\n\", cog_centroid)\n",
    "\n",
    "# Shape and stats\n",
    "print(\"\\nShape of affect centroid:\", affect_centroid.shape)\n",
    "print(\"Shape of cognition centroid:\", cog_centroid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af347b1-4f5f-401a-8ca0-1c33eb7776b1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c8316f-f794-4bcd-87c9-578aa82f88b0",
   "metadata": {},
   "source": [
    "### Emotionality Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3376a4f6-a002-4179-8614-dfef96113180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set wd to data_preprocessed\n",
    "os.chdir(data_preprocessed)\n",
    "\n",
    "# Load preprocessed speech data # NOR SURE IF USING THIS FILE IS CORRECT\n",
    "\n",
    "preprocessed_final_files = [\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed1_final.pkl')),\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed2_final.pkl')),\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed3_final.pkl')),\n",
    "   joblib.load(os.path.join(data_preprocessed, 'preprocessed_speeches_indexed4_final.pkl'))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1cbd1f95-c7f4-41f1-a292-542ee34ae882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit\\\\results\\\\distances_10epochs.pkl']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to compute weighted document vectors and derive affective/cognitive distances and scores\n",
    "\n",
    "def documentvecweight(lista):\n",
    "    out = []\n",
    "    lista = [i for i in lista if len(i[1]) > 0]\n",
    "    for s in lista:\n",
    "        # Compute weighted word vectors for each token present in the Word2Vec model\n",
    "        vecs = [w2v.wv[w] * word_counts_weighted[w] for w in s[1] if w in w2v.wv]\n",
    "        if len(vecs) == 0:\n",
    "            a = np.nan\n",
    "            c = np.nan\n",
    "            score = np.nan\n",
    "        else:\n",
    "            # Compute mean vector for each speech\n",
    "            v = np.mean(vecs, axis=0)\n",
    "             # Cosine distance to affective centroid\n",
    "            a = cosine(v, affect_centroid)\n",
    "            # Cosine distance to cognitive centroid\n",
    "            c = cosine(v, cog_centroid)\n",
    "            score = (1 + 1 - a) / (1 + 1 - c)\n",
    "        out.append([s[0], a, c, score])\n",
    "    return out\n",
    "\n",
    "\n",
    "def main_function(file_path, idx):\n",
    "    dataset = joblib.load(file_path)\n",
    "    data = documentvecweight(dataset)\n",
    "    lab = os.path.join(wd_results, f'distances_main_{idx}.pkl')\n",
    "    joblib.dump(data, lab)\n",
    "\n",
    "# Main loop: process all preprocessed speech files\n",
    "def main():\n",
    "    files = [\n",
    "        os.path.join(data_preprocessed, f'preprocessed_speeches_indexed{i+1}_final.pkl') #Changed!\n",
    "        for i in range(4)\n",
    "    ]\n",
    "    for i, f in enumerate(files, start=1):\n",
    "        main_function(f, i)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# Merge all distance files into a single DataFrame\n",
    "DATA_temp = [os.path.join(wd_results, f'distances_main_{i+1}.pkl') for i in range(4)]\n",
    "\n",
    "tot = []\n",
    "for dataname in DATA_temp:\n",
    "    d = joblib.load(dataname)\n",
    "    tot += d\n",
    "\n",
    "tot_df = pd.DataFrame(tot, columns=['filename', 'affect_d', 'cognition_d', 'score'])\n",
    "joblib.dump(tot_df, os.path.join(wd_results, 'distances_10epochs.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fb955ab4-243d-470d-a967-e0af1e474c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          filename  affect_d  cognition_d     score\n",
      "0  CHL_32_1977.txt  1.393397     0.695997  0.465185\n",
      "1  PRT_76_2021.txt  1.279130     1.106764  0.807032\n",
      "2  TZA_35_1980.txt  1.327479     0.831486  0.575535\n",
      "3  HND_49_1994.txt  1.191035     0.753466  0.648972\n",
      "4  MHL_76_2021.txt  1.264608     0.952535  0.702068\n"
     ]
    }
   ],
   "source": [
    "# Print the first few rows\n",
    "print(tot_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8c295cd4-4fa0-470f-b67c-4f584f7a3f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load df_merged and merge with tot_df by filename \n",
    "un_corpus_merged = pd.read_csv(os.path.join(data_c, \"un_corpus_merged.csv\"), sep=';', encoding='utf-8') \n",
    "un_corpus_scored = un_corpus_merged.merge(tot_df, on=\"filename\", how=\"left\")\n",
    "\n",
    "joblib.dump(un_corpus_scored, os.path.join(wd_results, \"un_corpus_scored.pkl\"))\n",
    "\n",
    "un_corpus_scored.to_csv(\n",
    "    os.path.join(wd_results, \"un_corpus_scored.csv\"),\n",
    "    sep=';', \n",
    "    index=False, \n",
    "    encoding='utf-8'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d8d13667-1eb3-482e-83ca-5969a229750a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              filename                                             speech  \\\n",
      "0      CHL_32_1977.txt  ﻿78.\\t  I am pleased to congratulate Mr. Lazar...   \n",
      "1      PRT_76_2021.txt  I congratulate you on your election, Mr. Presi...   \n",
      "2      TZA_35_1980.txt  ﻿Permit me, Sir, on my own personal behalf and...   \n",
      "3      HND_49_1994.txt  First of all, I am happy to express to you, Si...   \n",
      "4      MHL_76_2021.txt  Mr. President, Mr. Secretary-General, Excellen...   \n",
      "...                ...                                                ...   \n",
      "10947  AUS_42_1987.txt  ﻿It had been the intention of the Minister for...   \n",
      "10948  GAB_70_2015.txt  This session is being held at a time when huma...   \n",
      "10949  IDN_79_2024.txt  Bismillahirrahmanirrahim,  Mr. President,  \\nT...   \n",
      "10950  MDV_52_1997.txt  ﻿First of all, Sir, let me\\ncongratulate you o...   \n",
      "10951  CIV_47_1992.txt  We grieved \\nto learn of the aviation disaster...   \n",
      "\n",
      "      country_code  year      country_name  speech_length_words  \\\n",
      "0              CHL  1977             Chile                 3657   \n",
      "1              PRT  2021          Portugal                  887   \n",
      "2              TZA  1980          Tanzania                 4051   \n",
      "3              HND  1994          Honduras                 3277   \n",
      "4              MHL  2021  Marshall Islands                 1915   \n",
      "...            ...   ...               ...                  ...   \n",
      "10947          AUS  1987         Australia                 4226   \n",
      "10948          GAB  2015             Gabon                 2391   \n",
      "10949          IDN  2024         Indonesia                 1287   \n",
      "10950          MDV  1997          Maldives                 2858   \n",
      "10951          CIV  1992     Côte d'Ivoire                 3633   \n",
      "\n",
      "       english_official_language  security_council_permanent  \\\n",
      "0                              0                           0   \n",
      "1                              0                           0   \n",
      "2                              1                           0   \n",
      "3                              0                           0   \n",
      "4                              1                           0   \n",
      "...                          ...                         ...   \n",
      "10947                          0                           0   \n",
      "10948                          0                           0   \n",
      "10949                          0                           0   \n",
      "10950                          0                           0   \n",
      "10951                          0                           0   \n",
      "\n",
      "                            speaker_name  \\\n",
      "0                               Carvajal   \n",
      "1                Marcelo Rebelo de Sousa   \n",
      "2                                  Mkapa   \n",
      "3      Mr. Carlos Roberto Reina Idiáquez   \n",
      "4                            David Kabua   \n",
      "...                                  ...   \n",
      "10947                          WOOLCOTT    \n",
      "10948                  Ali Bongo Ondimba   \n",
      "10949    Retno Lestari Priansari Marsudi   \n",
      "10950                    Fathulla Jameel   \n",
      "10951                            Mr Essy   \n",
      "\n",
      "                                    position  gender_dummy  \\\n",
      "0                                        NaN           NaN   \n",
      "1                          (Vice-) President           NaN   \n",
      "2                                        NaN           NaN   \n",
      "3                          (Vice-) President           0.0   \n",
      "4                          (Vice-) President           NaN   \n",
      "...                                      ...           ...   \n",
      "10947                                    NaN           NaN   \n",
      "10948                      (Vice-) President           NaN   \n",
      "10949  (Deputy) Minister for Foreign Affairs           NaN   \n",
      "10950  (Deputy) Minister for Foreign Affairs           NaN   \n",
      "10951                                    NaN           0.0   \n",
      "\n",
      "                  speech_label  affect_d  cognition_d     score  \n",
      "0                 Chile (1977)  1.393397     0.695997  0.465185  \n",
      "1              Portugal (2021)  1.279130     1.106764  0.807032  \n",
      "2              Tanzania (1980)  1.327479     0.831486  0.575535  \n",
      "3              Honduras (1994)  1.191035     0.753466  0.648972  \n",
      "4      Marshall Islands (2021)  1.264608     0.952535  0.702068  \n",
      "...                        ...       ...          ...       ...  \n",
      "10947         Australia (1987)  1.287956     0.810602  0.598659  \n",
      "10948             Gabon (2015)  1.451188     1.063538  0.586049  \n",
      "10949         Indonesia (2024)  1.216818     1.187323  0.963706  \n",
      "10950          Maldives (1997)  1.328209     0.954697  0.642676  \n",
      "10951     Côte d'Ivoire (1992)  1.296458     0.924025  0.653865  \n",
      "\n",
      "[10952 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "print(un_corpus_scored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1210b88f-cf13-4da0-807b-77764cf0d439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count where affect_d is NaN: 0\n",
      "Count where affect_d is not NaN: 10952\n"
     ]
    }
   ],
   "source": [
    "# Count where affect_d is NaN\n",
    "nan_count = un_corpus_scored['affect_d'].isna().sum()\n",
    "\n",
    "# Count where affect_d is not NaN\n",
    "not_nan_count = un_corpus_scored['affect_d'].notna().sum()\n",
    "\n",
    "print(\"Count where affect_d is NaN:\", nan_count)\n",
    "print(\"Count where affect_d is not NaN:\", not_nan_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751788c2-eb58-421a-91c4-bcdb0b325cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051890b7-921f-4bc1-92f9-cea6ee3d5aed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91380369-f1c0-4f97-9553-d8dd0a9c229c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
