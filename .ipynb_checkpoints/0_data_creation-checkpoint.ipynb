{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b41cb22-e190-48d5-9926-7ac6fac8310d",
   "metadata": {},
   "source": [
    "# Emotion and Reason in Political Language: Examining the UN General speeches\n",
    "## Script 1: Preprocessing & Token Frequencies\n",
    "## Author: Sarah Franzen\n",
    "\n",
    "### Instructions BEFORE running this script:\n",
    "- Ensure all required packages are installed. If not, set `InstallPackages = TRUE` (see code cells below).  \n",
    "- Set your working directory appropriately.  \n",
    "- The script will automatically create the required folder structure.  \n",
    "- Later, you will be asked to download the data from:  \n",
    "  https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/0TJX8Y  \n",
    "  and store it **unzipped** inside the created folder *data_original*\n",
    "\n",
    "\n",
    "### Description: \n",
    "- Extract documents from their original txt documents and store them as one csv\n",
    "- Data Cleaning and Pre-Processing\n",
    "- Count word frequencies and weight themhemnal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5588afc-105a-426e-9348-742a830a2732",
   "metadata": {},
   "source": [
    "## Setup, Installation and Verification of required Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebb8987a-a0da-453e-97e2-aafa209d0e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "InstallPackages = False # Set this to True to install the following packages \n",
    "\n",
    "if InstallPackages:\n",
    "    import sys\n",
    "\n",
    "    packages = [\n",
    "        \"pandas\",\n",
    "        \"nltk\",\n",
    "        \"spacy\",\n",
    "        \"numpy\",\n",
    "        \"gensim\",\n",
    "        \"pycountry\",\n",
    "        \"wordcloud\",\n",
    "        \"matplotlib\",\n",
    "        \"tqdm\"\n",
    "    ]\n",
    "\n",
    "    for package in packages:\n",
    "        !{sys.executable} -m pip install {package}\n",
    "\n",
    "\n",
    "DownloadAdditions = False # Set this to True to download these additional resources\n",
    "if DownloadAdditions:\n",
    "    nltk.download(\"punkt\")\n",
    "    nltk.download(\"averaged_perceptron_tagger\")\n",
    "    spacy.cli.download('en_core_web_lg')         # Download spaCy English model (large)\n",
    "\n",
    "#########################\n",
    "# Check if all packages are included\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32d7d499-0563-46e0-9730-914c9f106d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Import standard and third-party libraries for data processing, NLP, and visualization ==\n",
    "\n",
    "import gensim\n",
    "import joblib\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import pycountry\n",
    "import random\n",
    "import re\n",
    "import spacy\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "#from matplotlib.colors import ListedColormap\n",
    "from multiprocessing import Pool, freeze_support\n",
    "# from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOPWORDS\n",
    "from string import punctuation\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# === Initialize NLP Tools ===\n",
    "\n",
    "# Translator to remove punctuation\n",
    "translator = str.maketrans('', '', punctuation)\n",
    "\n",
    "# POS tagger (not used by SpaCy, but optionally available via NLTK)\n",
    "tagger = nltk.perceptron.PerceptronTagger()\n",
    "\n",
    "# Load SpaCy English model with unnecessary components disabled\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4d495156-bbb2-403c-86f2-83b47d8cb03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Folder structure created:\n",
      "- C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\n",
      "  - data_original\n",
      "  - dictionaries\n",
      "  - freq\n",
      "  - preprocessed\n",
      "  - temp\n",
      "  - tokenized\n"
     ]
    }
   ],
   "source": [
    "# === Set Working Directory and create folder structure ===\n",
    "\n",
    "# Prompt user to enter working directory path\n",
    "#wd = input(\"Please enter your working directory path (e.g., C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit): \").strip()\n",
    "\n",
    "# Change to the entered working directory\n",
    "#try:\n",
    "   # os.chdir(wd)\n",
    "    #print(f\"Working directory set to: {os.getcwd()}\")\n",
    "#except FileNotFoundError:\n",
    "   # print(\"ERROR: The directory you entered does not exist. Please restart and enter a valid path.\")\n",
    "    #exit(1)\n",
    "\n",
    "# Set your working directory (adjust this as needed)\n",
    "wd = r\"C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\"\n",
    "os.chdir(wd)\n",
    "\n",
    "# Set base path to current working directory\n",
    "base_path = Path.cwd()\n",
    "data_path = base_path / \"data\"\n",
    "\n",
    "# List of subfolders to create inside 'data'\n",
    "subfolders = [\"data_original\", \"dictionaries\", \"freq\", \"preprocessed\", \"temp\", \"tokenized\"]\n",
    "\n",
    "# Create 'data' folder if it doesn't exist\n",
    "data_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Create subfolders\n",
    "for name in subfolders:\n",
    "    (data_path / name).mkdir(exist_ok=True)\n",
    "\n",
    "print(\"\\nFolder structure created:\")\n",
    "print(f\"- {data_path}\")\n",
    "for name in subfolders:\n",
    "    print(f\"  - {name}\")\n",
    "\n",
    "# Prompt user to place raw data files\n",
    "#print(f\"\\nPlease place your raw data files (unzipped) into the folder:\\n  {data_path / 'data_original'}\")\n",
    "#input(\"Press Enter after you have placed the files to continue...\")\n",
    "\n",
    "#print(\"Continuing with the script...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56fa9a21-650f-4d8e-8a21-3d9629655c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define Folder Paths ===\n",
    "\n",
    "# Make sure that you have these folders in your working directory\n",
    "data_c = os.path.join(wd, 'data')\n",
    "data_temp = os.path.join(data_c, 'temp')\n",
    "data_freq = os.path.join(data_c, 'freq')\n",
    "data_dict = os.path.join(data_c, 'dictionaries')\n",
    "data_preprocessed = os.path.join(data_c, 'preprocessed')\n",
    "data_tokenized = os.path.join(data_c, 'tokenized')\n",
    "fig = os.path.join(wd, 'fig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "296fc7d6-a2ac-406e-a322-6c9f2702ee1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\n"
     ]
    }
   ],
   "source": [
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff34b6-cbee-4f8c-8076-5f385f596b58",
   "metadata": {},
   "source": [
    "## Load and Prepare Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18067b9-6337-4fb3-a570-603083a7deae",
   "metadata": {},
   "source": [
    "### This chunk can be skipped at the moment\n",
    "### Think of proper header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93d247a0-5ff2-4062-add9-f3267ae1fe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¾ Total speeches found: 10761\n",
      "\n",
      "âœ… Saved raw data with 400 speeches to '.\\data\\un_corpus_raw.csv'\n"
     ]
    }
   ],
   "source": [
    "# == Load and Save Sample from UN General Debate Corpus ==\n",
    "\n",
    "# Set Folder path containing the original TXT files    \n",
    "base_folder = r\".\\data\\data_original\\UN General Debate Corpus\\UNGDC_1946-2023\\TXT\"\n",
    "\n",
    "#  Gather all relevant txt-files\n",
    "all_txt_files = []\n",
    "for root, dirs, files in os.walk(base_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.txt') and not file.startswith('._'):\n",
    "            all_txt_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"ðŸ§¾ Total speeches found: {len(all_txt_files)}\")\n",
    "\n",
    "# Randomly pick 800 files from the full collection   ###################################################### REMOVE AT LATER POINT\n",
    "sampled_files = random.sample(all_txt_files,400)\n",
    "\n",
    "# Read the selected files into a list\n",
    "raw_data = []\n",
    "for filepath in sampled_files:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        raw_data.append({'filename': os.path.basename(filepath), 'speech': content})\n",
    "\n",
    "# Create DataFrame from the collected speeches\n",
    "df_raw = pd.DataFrame(raw_data)\n",
    "\n",
    "df_raw = df_raw[df_raw['filename'] != '.DS_Store-to-UTF-8.txt'].copy()\n",
    "\n",
    "# Save df_raw as a pickle file for quick future loading\n",
    "raw_pickle_path = r\".\\data\\un_corpus_raw.pkl\"\n",
    "df_raw.to_pickle(raw_pickle_path)\n",
    "\n",
    "# Export df as CSV \n",
    "raw_output_path = r\".\\data\\un_corpus_raw.csv\"\n",
    "df_raw.to_csv(raw_output_path, index=False, sep=';', encoding='utf-8')\n",
    "\n",
    "print(f\"\\nâœ… Saved raw data with {len(df_raw)} speeches to '{raw_output_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6405ff20-92fa-4c97-becb-ac5fff70f84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BEL_15_1960.txt</td>\n",
       "      <td>Having paid in its turn its tribute to the Pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NZL_32_1977.txt</td>\n",
       "      <td>ï»¿74.\\t Mr. President, I offer you my warmest c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BEL_55_2000.txt</td>\n",
       "      <td>Mr. President, I should first of all like to c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QAT_66_2011.txt</td>\n",
       "      <td>It gives me \\ngreat pleasure to address the Ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BRA_10_1955.txt</td>\n",
       "      <td>1.\\tMay I be allowed to come to this rostrum t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech\n",
       "0  BEL_15_1960.txt  Having paid in its turn its tribute to the Pre...\n",
       "1  NZL_32_1977.txt  ï»¿74.\\t Mr. President, I offer you my warmest c...\n",
       "2  BEL_55_2000.txt  Mr. President, I should first of all like to c...\n",
       "3  QAT_66_2011.txt  It gives me \\ngreat pleasure to address the Ge...\n",
       "4  BRA_10_1955.txt  1.\\tMay I be allowed to come to this rostrum t..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# == Check if everything worked & drop empty speeches ==\n",
    "\n",
    "# Load df_raw\n",
    "df_raw = pd.read_pickle(r\".\\data\\un_corpus_raw.pkl\")\n",
    "\n",
    "# Drop empty speeches\n",
    "df_raw['speech'] = df_raw['speech'].astype(str)\n",
    "df_raw = df_raw[df_raw['speech'].str.strip() != ''].copy()\n",
    "\n",
    "# View df to check structure\n",
    "df_raw.head()         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7d3162-ebbd-449b-a2be-1148fceda725",
   "metadata": {},
   "source": [
    "## Create new variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25710e79-5068-4d39-8023-f4f97513f165",
   "metadata": {},
   "source": [
    "### Year, country_code and country_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "162da07c-1112-43e8-a434-2bef0ed32de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min year: 1947\n",
      "Max year: 2023\n",
      "Missing codes: []\n",
      "df_raw saved to C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\un_corpus_raw.pkl\n"
     ]
    }
   ],
   "source": [
    "# Extract country code (first 3 letters) and year (last 4 digits before .txt)\n",
    "df_raw['country_code'] = df_raw['filename'].str.extract(r'^([A-Z]{2,3})')\n",
    "df_raw['year'] = df_raw['filename'].str.extract(r'_(\\d{4})\\.txt$').astype(int)\n",
    "\n",
    "print(\"Min year:\", df_raw['year'].min())\n",
    "print(\"Max year:\", df_raw['year'].max())\n",
    "\n",
    "# Match country codes to country names\n",
    "code_to_name = {country.alpha_3: country.name for country in pycountry.countries}\n",
    "\n",
    "# Add custom short names and legacy codes\n",
    "custom_names = {\n",
    "    \"BOL\": \"Bolivia\",\n",
    "    \"COD\": \"The Democratic Republic of the Congo\",\n",
    "    \"IRN\": \"Iran\",\n",
    "    \"LAO\": \"Laos\",\n",
    "    \"MDA\": \"Moldova\",\n",
    "    \"PRK\": \"North Korea\",\n",
    "    \"PSE\": \"Palestine\",\n",
    "    \"RUS\": \"Russia\",\n",
    "    \"SYR\": \"Syria\",\n",
    "    \"TZA\": \"Tanzania\",\n",
    "    \"VAT\": \"Vatican City State\",\n",
    "    \"VEN\": \"Venezuela\",\n",
    "    \"VNM\": \"Vietnam\",\n",
    "    \"YMD\": \"South Yemen\",\n",
    "    \"YUG\": \"Yugoslavia\",\n",
    "    \"DDR\": \"East Germany\",\n",
    "    # \"SUN\": \"Soviet Union\",\n",
    "    \"EU\": \"European Union\",\n",
    "    \"CSK\": \"Czechoslovakia\",\n",
    "    \"FSM\": \"Micronesia\",\n",
    "    \"KOR\": \"South Korea\"\n",
    "    \n",
    "}\n",
    "\n",
    "# Update the main mapping with custom names\n",
    "code_to_name.update(custom_names)\n",
    "\n",
    "# Map with updated dictionary\n",
    "df_raw['country_name'] = df_raw['country_code'].map(code_to_name)\n",
    "\n",
    "# Check missing mappings\n",
    "missing = df_raw.loc[df_raw['country_name'].isna(), 'country_code'].unique()\n",
    "print(\"Missing codes:\", missing)\n",
    "\n",
    "# Check structure of the df\n",
    "df_raw.head() \n",
    "\n",
    "save_path = os.path.join(data_c, 'un_corpus_raw.pkl')\n",
    "df_raw.to_pickle(save_path)\n",
    "print(f\"df_raw saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "383d2904-f015-4c9d-9aeb-51a94f03f950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    country_code                          country_name\n",
      "0            AFG                           Afghanistan\n",
      "1            AGO                                Angola\n",
      "2            ALB                               Albania\n",
      "3            AND                               Andorra\n",
      "4            ARE                  United Arab Emirates\n",
      "5            ARG                             Argentina\n",
      "6            ARM                               Armenia\n",
      "7            ATG                   Antigua and Barbuda\n",
      "8            AUS                             Australia\n",
      "9            AUT                               Austria\n",
      "10           AZE                            Azerbaijan\n",
      "11           BDI                               Burundi\n",
      "12           BEL                               Belgium\n",
      "13           BEN                                 Benin\n",
      "14           BFA                          Burkina Faso\n",
      "15           BGD                            Bangladesh\n",
      "16           BGR                              Bulgaria\n",
      "17           BHR                               Bahrain\n",
      "18           BHS                               Bahamas\n",
      "19           BIH                Bosnia and Herzegovina\n",
      "20           BLR                               Belarus\n",
      "21           BLZ                                Belize\n",
      "22           BOL                               Bolivia\n",
      "23           BRA                                Brazil\n",
      "24           BRB                              Barbados\n",
      "25           BTN                                Bhutan\n",
      "26           BWA                              Botswana\n",
      "27           CAF              Central African Republic\n",
      "28           CAN                                Canada\n",
      "29           CHL                                 Chile\n",
      "30           CHN                                 China\n",
      "31           CIV                         CÃ´te d'Ivoire\n",
      "32           COD  The Democratic Republic of the Congo\n",
      "33           COG                                 Congo\n",
      "34           COL                              Colombia\n",
      "35           COM                               Comoros\n",
      "36           CPV                            Cabo Verde\n",
      "37           CSK                        Czechoslovakia\n",
      "38           CUB                                  Cuba\n",
      "39           CYP                                Cyprus\n",
      "40           CZE                               Czechia\n",
      "41           DEU                               Germany\n",
      "42           DJI                              Djibouti\n",
      "43           DNK                               Denmark\n",
      "44           DOM                    Dominican Republic\n",
      "45           DZA                               Algeria\n",
      "46           ECU                               Ecuador\n",
      "47           EGY                                 Egypt\n",
      "48           ESP                                 Spain\n",
      "49           EST                               Estonia\n",
      "50           ETH                              Ethiopia\n",
      "51            EU                        European Union\n",
      "52           FIN                               Finland\n",
      "53           FJI                                  Fiji\n",
      "54           FRA                                France\n",
      "55           FSM                            Micronesia\n",
      "56           GAB                                 Gabon\n",
      "57           GBR                        United Kingdom\n",
      "58           GEO                               Georgia\n",
      "59           GHA                                 Ghana\n",
      "60           GIN                                Guinea\n",
      "61           GMB                                Gambia\n",
      "62           GNB                         Guinea-Bissau\n",
      "63           GNQ                     Equatorial Guinea\n",
      "64           GRC                                Greece\n",
      "65           GRD                               Grenada\n",
      "66           GTM                             Guatemala\n",
      "67           GUY                                Guyana\n",
      "68           HRV                               Croatia\n",
      "69           HTI                                 Haiti\n",
      "70           HUN                               Hungary\n",
      "71           IDN                             Indonesia\n",
      "72           IND                                 India\n",
      "73           IRL                               Ireland\n",
      "74           IRN                                  Iran\n",
      "75           IRQ                                  Iraq\n",
      "76           ISR                                Israel\n",
      "77           ITA                                 Italy\n",
      "78           JAM                               Jamaica\n",
      "79           JOR                                Jordan\n",
      "80           JPN                                 Japan\n",
      "81           KAZ                            Kazakhstan\n",
      "82           KEN                                 Kenya\n",
      "83           KHM                              Cambodia\n",
      "84           KNA                 Saint Kitts and Nevis\n",
      "85           LAO                                  Laos\n",
      "86           LBR                               Liberia\n",
      "87           LBY                                 Libya\n",
      "88           LCA                           Saint Lucia\n",
      "89           LIE                         Liechtenstein\n",
      "90           LKA                             Sri Lanka\n",
      "91           LSO                               Lesotho\n",
      "92           LTU                             Lithuania\n",
      "93           LUX                            Luxembourg\n",
      "94           LVA                                Latvia\n",
      "95           MAR                               Morocco\n",
      "96           MCO                                Monaco\n",
      "97           MDA                               Moldova\n",
      "98           MDG                            Madagascar\n",
      "99           MEX                                Mexico\n",
      "100          MLT                                 Malta\n",
      "101          MMR                               Myanmar\n",
      "102          MNG                              Mongolia\n",
      "103          MOZ                            Mozambique\n",
      "104          MRT                            Mauritania\n",
      "105          MUS                             Mauritius\n",
      "106          MWI                                Malawi\n",
      "107          MYS                              Malaysia\n",
      "108          NER                                 Niger\n",
      "109          NGA                               Nigeria\n",
      "110          NIC                             Nicaragua\n",
      "111          NLD                           Netherlands\n",
      "112          NPL                                 Nepal\n",
      "113          NRU                                 Nauru\n",
      "114          NZL                           New Zealand\n",
      "115          OMN                                  Oman\n",
      "116          PAK                              Pakistan\n",
      "117          PAN                                Panama\n",
      "118          PER                                  Peru\n",
      "119          PHL                           Philippines\n",
      "120          PLW                                 Palau\n",
      "121          PNG                      Papua New Guinea\n",
      "122          POL                                Poland\n",
      "123          PRT                              Portugal\n",
      "124          PRY                              Paraguay\n",
      "125          PSE                             Palestine\n",
      "126          QAT                                 Qatar\n",
      "127          ROU                               Romania\n",
      "128          RUS                                Russia\n",
      "129          RWA                                Rwanda\n",
      "130          SAU                          Saudi Arabia\n",
      "131          SDN                                 Sudan\n",
      "132          SEN                               Senegal\n",
      "133          SGP                             Singapore\n",
      "134          SLB                       Solomon Islands\n",
      "135          SLE                          Sierra Leone\n",
      "136          SLV                           El Salvador\n",
      "137          SMR                            San Marino\n",
      "138          SOM                               Somalia\n",
      "139          SRB                                Serbia\n",
      "140          SUR                              Suriname\n",
      "141          SVK                              Slovakia\n",
      "142          SVN                              Slovenia\n",
      "143          SWE                                Sweden\n",
      "144          SWZ                              Eswatini\n",
      "145          SYC                            Seychelles\n",
      "146          SYR                                 Syria\n",
      "147          TCD                                  Chad\n",
      "148          TGO                                  Togo\n",
      "149          THA                              Thailand\n",
      "150          TJK                            Tajikistan\n",
      "151          TKM                          Turkmenistan\n",
      "152          TON                                 Tonga\n",
      "153          TTO                   Trinidad and Tobago\n",
      "154          TUN                               Tunisia\n",
      "155          TUR                               TÃ¼rkiye\n",
      "156          TZA                              Tanzania\n",
      "157          UKR                               Ukraine\n",
      "158          URY                               Uruguay\n",
      "159          USA                         United States\n",
      "160          UZB                            Uzbekistan\n",
      "161          VEN                             Venezuela\n",
      "162          VNM                               Vietnam\n",
      "163          WSM                                 Samoa\n",
      "164          YEM                                 Yemen\n",
      "165          YUG                            Yugoslavia\n",
      "166          ZAF                          South Africa\n",
      "167          ZMB                                Zambia\n"
     ]
    }
   ],
   "source": [
    "# == Check the country names \n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(df_raw[['country_code', 'country_name']].drop_duplicates().sort_values('country_code').reset_index(drop=True))\n",
    "# Reset to default afterward\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5928538b-8519-40e2-bdee-ac72024f9f48",
   "metadata": {},
   "source": [
    "### Length of raw speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9841ec29-b451-419b-957e-7803fbe9401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column: speech length in words\n",
    "df_raw['speech_length_words'] = df_raw['speech'].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "911e2229-337f-4479-984c-3a6ab9dff5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average speech length (words): 2933.24\n",
      "20 shortest speeches:\n",
      "            filename    country_name  year  speech_length_words\n",
      "297  RWA_69_2014.txt          Rwanda  2014                  523\n",
      "235  RWA_74_2019.txt          Rwanda  2019                  611\n",
      "184  IRN_03_1948.txt            Iran  1948                  813\n",
      "3    QAT_66_2011.txt           Qatar  2011                  817\n",
      "338  SVN_62_2007.txt        Slovenia  2007                  857\n",
      "215  BEN_55_2000.txt           Benin  2000                  990\n",
      "270   EU_70_2015.txt  European Union  2015                 1029\n",
      "53   UZB_62_2007.txt      Uzbekistan  2007                 1045\n",
      "54   LTU_59_2004.txt       Lithuania  2004                 1082\n",
      "383  POL_62_2007.txt          Poland  2007                 1084\n",
      "39   ETH_75_2020.txt        Ethiopia  2020                 1086\n",
      "80   LIE_55_2000.txt   Liechtenstein  2000                 1116\n",
      "71   ROU_78_2023.txt         Romania  2023                 1143\n",
      "246  SWZ_44_1989.txt        Eswatini  1989                 1153\n",
      "357  TKM_70_2015.txt    Turkmenistan  2015                 1190\n",
      "296  HRV_56_2001.txt         Croatia  2001                 1205\n",
      "322  EGY_07_1952.txt           Egypt  1952                 1221\n",
      "99   LSO_60_2005.txt         Lesotho  2005                 1256\n",
      "173  BHS_73_2018.txt         Bahamas  2018                 1291\n",
      "370  IND_65_2010.txt           India  2010                 1310\n",
      "\n",
      "20 longest speeches:\n",
      "            filename   country_name  year  speech_length_words\n",
      "183  IDN_15_1960.txt      Indonesia  1960                13463\n",
      "216  KHM_33_1978.txt       Cambodia  1978                 7582\n",
      "69   CUB_17_1962.txt           Cuba  1962                 7417\n",
      "312  IND_26_1971.txt          India  1971                 7108\n",
      "388  IRL_32_1977.txt        Ireland  1977                 7041\n",
      "30   ISR_23_1968.txt         Israel  1968                 6256\n",
      "16   MAR_29_1974.txt        Morocco  1974                 6142\n",
      "301  GTM_40_1985.txt      Guatemala  1985                 6039\n",
      "321  EGY_33_1978.txt          Egypt  1978                 6007\n",
      "396  PER_26_1971.txt           Peru  1971                 5999\n",
      "208  FRA_14_1959.txt         France  1959                 5932\n",
      "330  CUB_54_1999.txt           Cuba  1999                 5643\n",
      "8    USA_14_1959.txt  United States  1959                 5585\n",
      "87   ROU_28_1973.txt        Romania  1973                 5576\n",
      "141  GRD_32_1977.txt        Grenada  1977                 5462\n",
      "13   URY_35_1980.txt        Uruguay  1980                 5422\n",
      "81   MNG_27_1972.txt       Mongolia  1972                 5421\n",
      "290  ISR_19_1964.txt         Israel  1964                 5354\n",
      "34   CHL_21_1966.txt          Chile  1966                 5270\n",
      "394  NER_36_1981.txt          Niger  1981                 5270\n"
     ]
    }
   ],
   "source": [
    "# Calculate average length\n",
    "avg_length = df_raw['speech_length_words'].mean()\n",
    "\n",
    "# Print it\n",
    "print(\"Average speech length (words):\", round(avg_length, 2))\n",
    "\n",
    "# 20 shortest speeches\n",
    "print(\"20 shortest speeches:\")\n",
    "print(df_raw.nsmallest(20, 'speech_length_words')[['filename', 'country_name', 'year', 'speech_length_words']])\n",
    "\n",
    "# 20 longest speeches\n",
    "print(\"\\n20 longest speeches:\")\n",
    "print(df_raw.nlargest(20, 'speech_length_words')[['filename', 'country_name', 'year', 'speech_length_words']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424f152-b786-45bf-bf86-e169a900f628",
   "metadata": {},
   "source": [
    "### New variable: English as Official Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1d2c2a6-23d6-4044-84cc-8785167e6b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries not matched in df_raw['country_name']:\n",
      "Anguilla\n",
      "Bermuda\n",
      "British Virgin Islands\n",
      "Cameroon\n",
      "Cayman Islands\n",
      "Christmas Island\n",
      "Cook Islands\n",
      "Dominica\n",
      "Hong Kong\n",
      "Jersey\n",
      "Marshall Islands\n",
      "Namibia\n",
      "Niue\n",
      "Norfolk Island\n",
      "Northern Mariana Islands\n",
      "Pitcairn Islands\n",
      "Sint Maarten\n",
      "South Sudan\n",
      "Turks and Caicos Islands\n",
      "Tuvalu\n",
      "Uganda\n",
      "Zimbabwe\n"
     ]
    }
   ],
   "source": [
    "# Source for english as official language : https://gradschool.utk.edu/future-students/office-of-graduate-admissions/applying-to-graduate-school/admission-requirements/testing-requirements/countries-with-english-as-official-language/\n",
    "# They are quoting: https://www.cia.gov/the-world-factbook/field/languages/\n",
    "\n",
    "english_countries = [\n",
    "    \"Anguilla\", \"Antigua and Barbuda\", \"Bahamas\", \"Barbados\", \"Belize\", \"Belgium\",\n",
    "    \"Bermuda\", \"Botswana\", \"British Virgin Islands\", \"Burundi\", \"Cameroon\", \"Canada\",\n",
    "    \"Cayman Islands\", \"Christmas Island\", \"Cook Islands\", \"Dominica\", \"Fiji\", \"Gambia\",\n",
    "    \"Ghana\", \"Grenada\", \"Guyana\", \"Hong Kong\", \"India\", \"Ireland\", \"Jersey\", \"Kenya\",\n",
    "    \"Liberia\", \"Malawi\", \"Malta\", \"Marshall Islands\", \"Micronesia\",\n",
    "    \"Namibia\", \"New Zealand\", \"Nigeria\", \"Niue\", \"Norfolk Island\", \"Northern Mariana Islands\",\n",
    "    \"Pakistan\", \"Palau\", \"Papua New Guinea\", \"Philippines\", \"Pitcairn Islands\", \"Rwanda\",\n",
    "    \"Saint Kitts and Nevis\", \"Saint Lucia\", \"Samoa\", \"Seychelles\", \"Sierra Leone\", \"Singapore\",\n",
    "    \"Sint Maarten\", \"Solomon Islands\", \"Somalia\", \"South Africa\", \"South Sudan\", \"Sudan\",\n",
    "    \"Eswatini\", \"Tanzania\", \"Tonga\", \"Trinidad and Tobago\", \"Turks and Caicos Islands\",\n",
    "    \"Tuvalu\", \"Uganda\", \"Zambia\", \"Zimbabwe\"\n",
    "]\n",
    "\n",
    "# 2. Create dummy column\n",
    "df_raw['english_official_language'] = df_raw['country_name'].apply(\n",
    "    lambda x: 1 if x in english_countries else 0\n",
    ")\n",
    "\n",
    "# 3. Find countries in the list that did not match any entry in df_raw\n",
    "matched = set(df_raw['country_name'])\n",
    "unmatched = [country for country in english_countries if country not in matched]\n",
    "\n",
    "# 4. Print unmatched country names\n",
    "print(\"Countries not matched in df_raw['country_name']:\")\n",
    "for country in unmatched:\n",
    "    print(country)\n",
    "\n",
    "# All of these countries are either British Overseas Territories, Australian Territories, self-governing island territories or Special Administrative Regions\n",
    "    # None of the unmatched regions are UN Members\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f094c95f-834e-4454-9442-0d28d4a66090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "      <th>country_code</th>\n",
       "      <th>year</th>\n",
       "      <th>country_name</th>\n",
       "      <th>speech_length_words</th>\n",
       "      <th>english_official_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BEL_15_1960.txt</td>\n",
       "      <td>Having paid in its turn its tribute to the Pre...</td>\n",
       "      <td>BEL</td>\n",
       "      <td>1960</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>4785</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NZL_32_1977.txt</td>\n",
       "      <td>ï»¿74.\\t Mr. President, I offer you my warmest c...</td>\n",
       "      <td>NZL</td>\n",
       "      <td>1977</td>\n",
       "      <td>New Zealand</td>\n",
       "      <td>3801</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BEL_55_2000.txt</td>\n",
       "      <td>Mr. President, I should first of all like to c...</td>\n",
       "      <td>BEL</td>\n",
       "      <td>2000</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>2948</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QAT_66_2011.txt</td>\n",
       "      <td>It gives me \\ngreat pleasure to address the Ge...</td>\n",
       "      <td>QAT</td>\n",
       "      <td>2011</td>\n",
       "      <td>Qatar</td>\n",
       "      <td>817</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BRA_10_1955.txt</td>\n",
       "      <td>1.\\tMay I be allowed to come to this rostrum t...</td>\n",
       "      <td>BRA</td>\n",
       "      <td>1955</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>1632</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech  \\\n",
       "0  BEL_15_1960.txt  Having paid in its turn its tribute to the Pre...   \n",
       "1  NZL_32_1977.txt  ï»¿74.\\t Mr. President, I offer you my warmest c...   \n",
       "2  BEL_55_2000.txt  Mr. President, I should first of all like to c...   \n",
       "3  QAT_66_2011.txt  It gives me \\ngreat pleasure to address the Ge...   \n",
       "4  BRA_10_1955.txt  1.\\tMay I be allowed to come to this rostrum t...   \n",
       "\n",
       "  country_code  year country_name  speech_length_words  \\\n",
       "0          BEL  1960      Belgium                 4785   \n",
       "1          NZL  1977  New Zealand                 3801   \n",
       "2          BEL  2000      Belgium                 2948   \n",
       "3          QAT  2011        Qatar                  817   \n",
       "4          BRA  1955       Brazil                 1632   \n",
       "\n",
       "   english_official_language  \n",
       "0                          1  \n",
       "1                          1  \n",
       "2                          1  \n",
       "3                          0  \n",
       "4                          0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82540d8a-b656-45fb-aeb0-2fd5b4d7108a",
   "metadata": {},
   "source": [
    "### New variable: permanent member security council"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fb92fe4-5c22-43fa-8b48-62ec2240d716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define permanent members of the UN Security Council\n",
    "permanent_members = ['RUS', 'USA', 'FRA', 'GBR', 'CHN']\n",
    "\n",
    "# Create dummy variable\n",
    "df_raw['security_council_permanent'] = df_raw['country_code'].isin(permanent_members).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b7ed94c-7369-4b52-a84e-21bbdccd51ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    country_code    country_name  security_council_permanent  year\n",
      "8            USA   United States                           1  1959\n",
      "26           CHN           China                           1  1999\n",
      "55           GBR  United Kingdom                           1  1975\n",
      "74           USA   United States                           1  2005\n",
      "76           RUS          Russia                           1  2022\n",
      "82           USA   United States                           1  1953\n",
      "88           USA   United States                           1  1986\n",
      "185          USA   United States                           1  1998\n",
      "208          FRA          France                           1  1959\n",
      "230          CHN           China                           1  2005\n",
      "245          CHN           China                           1  2013\n",
      "346          USA   United States                           1  2007\n"
     ]
    }
   ],
   "source": [
    "print(df_raw[df_raw['country_code'].isin(permanent_members)][\n",
    "    ['country_code', 'country_name', 'security_council_permanent', 'year']\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523547d9-9b10-473c-8c85-d1e2fc4fd3e1",
   "metadata": {},
   "source": [
    "### New variables: speaker & position & gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "213e961b-816c-43f1-8ad6-a17bae6cca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_c)\n",
    "df_speakers = pd.read_excel(r\"data_original\\UN General Debate Corpus\\Speakers_by_session.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd158957-391b-4b86-9160-7c606336bacc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Session</th>\n",
       "      <th>ISO Code</th>\n",
       "      <th>Country</th>\n",
       "      <th>Name of Person Speaking</th>\n",
       "      <th>Post</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>78</td>\n",
       "      <td>BRA</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Luiz Inacio Lula da Silva</td>\n",
       "      <td>President</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023</td>\n",
       "      <td>78</td>\n",
       "      <td>USA</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>Joseph R. Biden</td>\n",
       "      <td>President</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023</td>\n",
       "      <td>78</td>\n",
       "      <td>COL</td>\n",
       "      <td>Colombia</td>\n",
       "      <td>Gustavo Petro Urrego</td>\n",
       "      <td>President</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023</td>\n",
       "      <td>78</td>\n",
       "      <td>JOR</td>\n",
       "      <td>Jordan</td>\n",
       "      <td>Abdullah II ibn Al Hussein</td>\n",
       "      <td>King</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023</td>\n",
       "      <td>78</td>\n",
       "      <td>POL</td>\n",
       "      <td>Poland</td>\n",
       "      <td>Andrzej Duda</td>\n",
       "      <td>President</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Session ISO Code                   Country  \\\n",
       "0  2023       78      BRA                   Brazil    \n",
       "1  2023       78      USA  United States of America   \n",
       "2  2023       78      COL                  Colombia   \n",
       "3  2023       78      JOR                    Jordan   \n",
       "4  2023       78      POL                    Poland   \n",
       "\n",
       "      Name of Person Speaking       Post Unnamed: 6  \n",
       "0   Luiz Inacio Lula da Silva  President        NaN  \n",
       "1             Joseph R. Biden  President        NaN  \n",
       "2        Gustavo Petro Urrego  President        NaN  \n",
       "3  Abdullah II ibn Al Hussein       King        NaN  \n",
       "4                Andrzej Duda  President        NaN  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_speakers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18be2d52-b8d1-4cb1-a74d-491a523f26c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "      <th>country_code</th>\n",
       "      <th>year</th>\n",
       "      <th>country_name</th>\n",
       "      <th>speech_length_words</th>\n",
       "      <th>english_official_language</th>\n",
       "      <th>security_council_permanent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BEL_15_1960.txt</td>\n",
       "      <td>Having paid in its turn its tribute to the Pre...</td>\n",
       "      <td>BEL</td>\n",
       "      <td>1960</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>4785</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NZL_32_1977.txt</td>\n",
       "      <td>ï»¿74.\\t Mr. President, I offer you my warmest c...</td>\n",
       "      <td>NZL</td>\n",
       "      <td>1977</td>\n",
       "      <td>New Zealand</td>\n",
       "      <td>3801</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BEL_55_2000.txt</td>\n",
       "      <td>Mr. President, I should first of all like to c...</td>\n",
       "      <td>BEL</td>\n",
       "      <td>2000</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>2948</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QAT_66_2011.txt</td>\n",
       "      <td>It gives me \\ngreat pleasure to address the Ge...</td>\n",
       "      <td>QAT</td>\n",
       "      <td>2011</td>\n",
       "      <td>Qatar</td>\n",
       "      <td>817</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BRA_10_1955.txt</td>\n",
       "      <td>1.\\tMay I be allowed to come to this rostrum t...</td>\n",
       "      <td>BRA</td>\n",
       "      <td>1955</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>1632</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech  \\\n",
       "0  BEL_15_1960.txt  Having paid in its turn its tribute to the Pre...   \n",
       "1  NZL_32_1977.txt  ï»¿74.\\t Mr. President, I offer you my warmest c...   \n",
       "2  BEL_55_2000.txt  Mr. President, I should first of all like to c...   \n",
       "3  QAT_66_2011.txt  It gives me \\ngreat pleasure to address the Ge...   \n",
       "4  BRA_10_1955.txt  1.\\tMay I be allowed to come to this rostrum t...   \n",
       "\n",
       "  country_code  year country_name  speech_length_words  \\\n",
       "0          BEL  1960      Belgium                 4785   \n",
       "1          NZL  1977  New Zealand                 3801   \n",
       "2          BEL  2000      Belgium                 2948   \n",
       "3          QAT  2011        Qatar                  817   \n",
       "4          BRA  1955       Brazil                 1632   \n",
       "\n",
       "   english_official_language  security_council_permanent  \n",
       "0                          1                           0  \n",
       "1                          1                           0  \n",
       "2                          1                           0  \n",
       "3                          0                           0  \n",
       "4                          0                           0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "170a6cc9-fa93-454c-9f81-f489a84afe1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [filename, speech, country_code, year, country_name, speech_length_words, english_official_language, security_council_permanent]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(df_raw[(df_raw['country_code'] == 'MEX') & (df_raw['year'] == 1982)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "672de39e-93ba-4808-82fd-f92b122f0843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "      <th>country_code</th>\n",
       "      <th>year</th>\n",
       "      <th>country_name</th>\n",
       "      <th>speech_length_words</th>\n",
       "      <th>english_official_language</th>\n",
       "      <th>security_council_permanent</th>\n",
       "      <th>Year</th>\n",
       "      <th>ISO Code</th>\n",
       "      <th>Name of Person Speaking</th>\n",
       "      <th>Post</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BEL_15_1960.txt</td>\n",
       "      <td>Having paid in its turn its tribute to the Pre...</td>\n",
       "      <td>BEL</td>\n",
       "      <td>1960</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>4785</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>BEL</td>\n",
       "      <td>Mr. WIGNY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NZL_32_1977.txt</td>\n",
       "      <td>ï»¿74.\\t Mr. President, I offer you my warmest c...</td>\n",
       "      <td>NZL</td>\n",
       "      <td>1977</td>\n",
       "      <td>New Zealand</td>\n",
       "      <td>3801</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1977.0</td>\n",
       "      <td>NZL</td>\n",
       "      <td>Talboys</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BEL_55_2000.txt</td>\n",
       "      <td>Mr. President, I should first of all like to c...</td>\n",
       "      <td>BEL</td>\n",
       "      <td>2000</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>2948</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>BEL</td>\n",
       "      <td>Louis Michel</td>\n",
       "      <td>Deputy Prime Minister</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QAT_66_2011.txt</td>\n",
       "      <td>It gives me \\ngreat pleasure to address the Ge...</td>\n",
       "      <td>QAT</td>\n",
       "      <td>2011</td>\n",
       "      <td>Qatar</td>\n",
       "      <td>817</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>QAT</td>\n",
       "      <td>Hamad bin Khalifa Al-Thani</td>\n",
       "      <td>Head of State</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BRA_10_1955.txt</td>\n",
       "      <td>1.\\tMay I be allowed to come to this rostrum t...</td>\n",
       "      <td>BRA</td>\n",
       "      <td>1955</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>1632</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>BRA</td>\n",
       "      <td>Mr. de Freitas Valle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech  \\\n",
       "0  BEL_15_1960.txt  Having paid in its turn its tribute to the Pre...   \n",
       "1  NZL_32_1977.txt  ï»¿74.\\t Mr. President, I offer you my warmest c...   \n",
       "2  BEL_55_2000.txt  Mr. President, I should first of all like to c...   \n",
       "3  QAT_66_2011.txt  It gives me \\ngreat pleasure to address the Ge...   \n",
       "4  BRA_10_1955.txt  1.\\tMay I be allowed to come to this rostrum t...   \n",
       "\n",
       "  country_code  year country_name  speech_length_words  \\\n",
       "0          BEL  1960      Belgium                 4785   \n",
       "1          NZL  1977  New Zealand                 3801   \n",
       "2          BEL  2000      Belgium                 2948   \n",
       "3          QAT  2011        Qatar                  817   \n",
       "4          BRA  1955       Brazil                 1632   \n",
       "\n",
       "   english_official_language  security_council_permanent    Year ISO Code  \\\n",
       "0                          1                           0  1960.0      BEL   \n",
       "1                          1                           0  1977.0      NZL   \n",
       "2                          1                           0  2000.0      BEL   \n",
       "3                          0                           0  2011.0      QAT   \n",
       "4                          0                           0  1955.0      BRA   \n",
       "\n",
       "      Name of Person Speaking                   Post _merge  \n",
       "0                   Mr. WIGNY                    NaN   both  \n",
       "1                     Talboys                    NaN   both  \n",
       "2                Louis Michel  Deputy Prime Minister   both  \n",
       "3  Hamad bin Khalifa Al-Thani          Head of State   both  \n",
       "4        Mr. de Freitas Valle                    NaN   both  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged = df_raw.merge(\n",
    "    df_speakers[['Year', 'ISO Code', 'Name of Person Speaking', 'Post']],\n",
    "    left_on=['year', 'country_code'],\n",
    "    right_on=['Year', 'ISO Code'],\n",
    "    how='left',\n",
    "    indicator=True)\n",
    "\n",
    "df_merged.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "604ceb1f-a9fc-49ec-aca8-fdc28ad7e73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            filename  year country_code    country_name\n",
      "217  CSK_02_1947.txt  1947          CSK  Czechoslovakia\n"
     ]
    }
   ],
   "source": [
    "# Merge with indicator and set unmatched rows to NA\n",
    "df_merged = df_raw.merge(\n",
    "    df_speakers[['Year', 'ISO Code', 'Name of Person Speaking', 'Post']],\n",
    "    left_on=['year', 'country_code'],\n",
    "    right_on=['Year', 'ISO Code'],\n",
    "    how='left',\n",
    "    indicator=True)\n",
    "\n",
    "# Get rows with no match in df_speakers\n",
    "unmatched = df_merged[df_merged['_merge'] == 'left_only']\n",
    "\n",
    "# Print unmatched rows with selected columns (panda sets them to NA by default)\n",
    "print(unmatched[['filename', 'year', 'country_code', 'country_name']])\n",
    "\n",
    "\n",
    "# Drop the '_merge' column from merged df\n",
    "df_merged = df_merged.drop(columns=['Year', 'ISO Code', '_merge'])\n",
    "\n",
    "# Rename columns\n",
    "df_merged = df_merged.rename(columns={\n",
    "    'Name of Person Speaking': 'speaker_name',\n",
    "    'Post': 'position'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c247f434-ab90-47ce-8c29-f40c65753755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gender_dummy \n",
    "df_merged['gender_dummy'] = df_merged['speaker_name'].apply(\n",
    "    lambda name: 0 if pd.notnull(name) and re.search(r'^(?:Mr|Sir)\\b', name, re.IGNORECASE)\n",
    "    else 1 if pd.notnull(name) and re.search(r'^(?:Mrs|Ms)\\b', name, re.IGNORECASE)\n",
    "    else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "452d80fd-3f71-41d5-9373-cc30ccbe5992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    gender_dummy  count\n",
      "0       0 (male)    180\n",
      "1     1 (female)      5\n",
      "2  NaN (unknown)    216\n"
     ]
    }
   ],
   "source": [
    "# Count all values including NaN\n",
    "counts = df_merged['gender_dummy'].value_counts(dropna=False)\n",
    "\n",
    "# Build summary using .get() to handle missing keys\n",
    "gender_summary = pd.DataFrame({\n",
    "    'gender_dummy': ['0 (male)', '1 (female)', 'NaN (unknown)'],\n",
    "    'count': [\n",
    "        counts.get(0, 0),\n",
    "        counts.get(1, 0),\n",
    "        counts.get(np.nan, 0)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(gender_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f9ba91d-cf5c-43e7-9fae-e35c868fbdf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "      <th>country_code</th>\n",
       "      <th>year</th>\n",
       "      <th>country_name</th>\n",
       "      <th>speech_length_words</th>\n",
       "      <th>english_official_language</th>\n",
       "      <th>security_council_permanent</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>position</th>\n",
       "      <th>gender_dummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BEL_15_1960.txt</td>\n",
       "      <td>Having paid in its turn its tribute to the Pre...</td>\n",
       "      <td>BEL</td>\n",
       "      <td>1960</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>4785</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Mr. WIGNY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NZL_32_1977.txt</td>\n",
       "      <td>ï»¿74.\\t Mr. President, I offer you my warmest c...</td>\n",
       "      <td>NZL</td>\n",
       "      <td>1977</td>\n",
       "      <td>New Zealand</td>\n",
       "      <td>3801</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Talboys</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BEL_55_2000.txt</td>\n",
       "      <td>Mr. President, I should first of all like to c...</td>\n",
       "      <td>BEL</td>\n",
       "      <td>2000</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>2948</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Louis Michel</td>\n",
       "      <td>Deputy Prime Minister</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QAT_66_2011.txt</td>\n",
       "      <td>It gives me \\ngreat pleasure to address the Ge...</td>\n",
       "      <td>QAT</td>\n",
       "      <td>2011</td>\n",
       "      <td>Qatar</td>\n",
       "      <td>817</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hamad bin Khalifa Al-Thani</td>\n",
       "      <td>Head of State</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BRA_10_1955.txt</td>\n",
       "      <td>1.\\tMay I be allowed to come to this rostrum t...</td>\n",
       "      <td>BRA</td>\n",
       "      <td>1955</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>1632</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mr. de Freitas Valle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech  \\\n",
       "0  BEL_15_1960.txt  Having paid in its turn its tribute to the Pre...   \n",
       "1  NZL_32_1977.txt  ï»¿74.\\t Mr. President, I offer you my warmest c...   \n",
       "2  BEL_55_2000.txt  Mr. President, I should first of all like to c...   \n",
       "3  QAT_66_2011.txt  It gives me \\ngreat pleasure to address the Ge...   \n",
       "4  BRA_10_1955.txt  1.\\tMay I be allowed to come to this rostrum t...   \n",
       "\n",
       "  country_code  year country_name  speech_length_words  \\\n",
       "0          BEL  1960      Belgium                 4785   \n",
       "1          NZL  1977  New Zealand                 3801   \n",
       "2          BEL  2000      Belgium                 2948   \n",
       "3          QAT  2011        Qatar                  817   \n",
       "4          BRA  1955       Brazil                 1632   \n",
       "\n",
       "   english_official_language  security_council_permanent  \\\n",
       "0                          1                           0   \n",
       "1                          1                           0   \n",
       "2                          1                           0   \n",
       "3                          0                           0   \n",
       "4                          0                           0   \n",
       "\n",
       "                 speaker_name               position  gender_dummy  \n",
       "0                   Mr. WIGNY                    NaN           0.0  \n",
       "1                     Talboys                    NaN           NaN  \n",
       "2                Louis Michel  Deputy Prime Minister           NaN  \n",
       "3  Hamad bin Khalifa Al-Thani          Head of State           NaN  \n",
       "4        Mr. de Freitas Valle                    NaN           0.0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "94a901c8-3fc4-4b33-a1ff-cede22bd196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store df_merged\n",
    "\n",
    "os.chdir(wd)\n",
    "\n",
    "# Save df_merged as a pickle file for quick future loading\n",
    "merged_pickle_path = r\".\\data\\un_corpus_merged.pkl\"\n",
    "df_merged.to_pickle(merged_pickle_path)\n",
    "\n",
    "# Export df as CSV \n",
    "merged_output_path = r\".\\data\\un_corpus_merged.csv\"\n",
    "df_merged.to_csv(merged_output_path, index=False, sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe0f4af-e939-4100-9618-5b522bdd72a1",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2661f431-a194-47d1-9a1c-89162bf85a91",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd37e08d-b4f2-459e-823c-3816a825ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Clean text by removing empty spaces, line breaks, hyphenation, stray characters, and escape quote ==\n",
    "\n",
    "# Define cleaning function\n",
    "def clean_text(content):\n",
    "    if pd.isna(content):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove line breaks and carriage returns\n",
    "    content = content.replace('\\n', ' ').replace('\\r', ' ')\n",
    "\n",
    "    # Collapse multiple spaces\n",
    "    content = ' '.join(content.split())\n",
    "\n",
    "    # Ensure spacing after punctuation\n",
    "    content = re.sub(r'(?<=[.,])(?=[^\\s])', r' ', content)\n",
    "\n",
    "    # Remove hyphenation at line breaks (e.g. \"inter- national\" â†’ \"international\")\n",
    "    content = re.sub(r'-\\s', '', content)\n",
    "\n",
    "     # Replace hyphen between letters with a space to prevent merging words (e.g. \"russian-and\" â†’ \"russian and\")\n",
    "    content = re.sub(r'(?<=\\w)-(?=\\w)', ' ', content)\n",
    "\n",
    "    # Remove stray backslashes\n",
    "    content = content.replace(\"\\\\\", \"\")\n",
    "\n",
    "    return content\n",
    "\n",
    "# Apply cleaning to each speech\n",
    "df_raw['speech'] = df_raw['speech'].astype(str)  # Ensure column is string type\n",
    "df_clean = df_raw.copy()\n",
    "df_clean['speech'] = df_clean['speech'].apply(clean_text)\n",
    "\n",
    "# Drop rows with empty speeches after cleaning\n",
    "df_clean = df_clean[df_clean['speech'].str.strip().astype(bool)].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7c6bd148-d5cf-4eb3-a66d-fc706e99c13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved clean speeches chunks in 'C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp'\n"
     ]
    }
   ],
   "source": [
    "# == Split cleaned data into chunks and save as separate files ==\n",
    "\n",
    "# Convert cleaned DataFrame to list of lists\n",
    "clean_data = df_clean[['filename', 'speech']].values.tolist()\n",
    "\n",
    "# Split cleaned data into 4 equal chunks\n",
    "data_id1 = clean_data[:int(len(clean_data)/4)]\n",
    "data_id2 = clean_data[int(len(clean_data)/4): int(2*len(clean_data)/4)]\n",
    "data_id3 = clean_data[int(2*len(clean_data)/4): int(3*len(clean_data)/4)]\n",
    "data_id4 = clean_data[int(3*len(clean_data)/4):]\n",
    "\n",
    "# Change directory to the temp folder\n",
    "os.chdir(data_temp)  # make sure `data_temp` exists and is defined\n",
    "\n",
    "# Save each chunk with joblib\n",
    "joblib.dump(data_id1, 'cleanspeeches_indexed1.pkl')\n",
    "joblib.dump(data_id2, 'cleanspeeches_indexed2.pkl')\n",
    "joblib.dump(data_id3, 'cleanspeeches_indexed3.pkl')\n",
    "joblib.dump(data_id4, 'cleanspeeches_indexed4.pkl')\n",
    "\n",
    "# Store list of cleaned data chunk paths to feed into preprocessing function later\n",
    "data_files = [\n",
    "    os.path.join(data_temp, 'cleanspeeches_indexed1.pkl'),\n",
    "    os.path.join(data_temp, 'cleanspeeches_indexed2.pkl'),\n",
    "    os.path.join(data_temp, 'cleanspeeches_indexed3.pkl'),\n",
    "    os.path.join(data_temp, 'cleanspeeches_indexed4.pkl')\n",
    "]\n",
    "\n",
    "print(f\"âœ… Saved clean speeches chunks in '{data_temp}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6e5109-8cf7-41e3-8fb9-8e7dde29b4e0",
   "metadata": {},
   "source": [
    "### Advanced Text Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0aa75944-4b75-4aa4-9430-9aa6bd9ac934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Define function to Tokenize, eliminate digits, remove stopwords, lemmatize, POS-Tagging ==\n",
    "\n",
    "def pro1(lista):\n",
    "    # Remove punctuation\n",
    "    return [[row[0], row[1].translate(translator)] for row in lista]\n",
    "\n",
    "def pro2(lista):\n",
    "    # Tokenize and lowercase with gensim\n",
    "    return [[row[0], gensim.utils.simple_preprocess(row[1])] for row in lista]\n",
    "\n",
    "def pro3(lista):\n",
    "    # Remove tokens that are only digits\n",
    "        a = [[row[0], [w for w in row[1] if not w.isdigit()]] for row in lista]\n",
    "        return a\n",
    "    \n",
    "def pro4(lista):\n",
    "    # Drop short words\n",
    "    return [[row[0], [w for w in row[1] if len(w) > 2]] for row in lista]\n",
    "\n",
    "def tags_spacy(lista):\n",
    "    texts = [' '.join(row[1]) for row in lista]\n",
    "    docs = list(nlp.pipe(texts, batch_size=20, n_process=1))\n",
    "    result = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        filtered_tokens = [token.text for token in doc if token.tag_.startswith(('N', 'V', 'J'))]\n",
    "        result.append([lista[i][0], filtered_tokens])\n",
    "    return result\n",
    "\n",
    "\n",
    "def pro5(lista):\n",
    "    # Remove stopwords using SpaCy stopword list\n",
    "    return [[row[0], [w for w in row[1] if w not in SPACY_STOPWORDS]] for row in lista]\n",
    "\n",
    "def pro6(lista):\n",
    "      return [\n",
    "        [row[0], [stemmer.stem(token) for token in row[1]]]\n",
    "        for row in lista\n",
    "    ]\n",
    "########################## Question for Max: They removed procedural words in the paper\n",
    "\n",
    "def dropnull(lista):\n",
    "    # Drop empty speeches\n",
    "    return [row for row in lista if len(' '.join(row[1])) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "524a274a-004c-4d87-9bb3-5f789a42b90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed1.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed1.pkl] Before tagging: 1.06s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed1.pkl] After tagging: 28.02s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed1.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed1.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed1.pkl] Done. Total time: 33.40s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed2.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed2.pkl] Before tagging: 0.96s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed2.pkl] After tagging: 26.64s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed2.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed2.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed2.pkl] Done. Total time: 31.90s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed3.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed3.pkl] Before tagging: 0.93s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed3.pkl] After tagging: 26.87s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed3.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed3.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed3.pkl] Done. Total time: 31.67s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed4.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed4.pkl] Before tagging: 0.95s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed4.pkl] After tagging: 28.61s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed4.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed4.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed4.pkl] Done. Total time: 33.65s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# == Create full pre-processing function and call it\n",
    "\n",
    "def preprocessing(data_name):\n",
    "    t0 = time.time()\n",
    "    print(f\"Starting preprocessing for {data_name}...\")\n",
    "\n",
    "    data = joblib.load(data_name)\n",
    "    data = pro1(data)\n",
    "    data = pro2(data)\n",
    "    data = pro3(data)\n",
    "    data = pro4(data)\n",
    "\n",
    "    print(f\"[{data_name}] Before tagging: {time.time() - t0:.2f}s\")\n",
    "    data = tags_spacy(data)\n",
    "    print(f\"[{data_name}] After tagging: {time.time() - t0:.2f}s\")\n",
    "\n",
    "    data = pro5(data)\n",
    "   # data = pro6(data)\n",
    "    data = dropnull(data)\n",
    "\n",
    "    # out_name = data_name.replace('cleanspeeches_', 'preprocessed_speeches_').replace('.pkl', '_temp.pkl')\n",
    "\n",
    "    # Store preprocessed corupus (before stemming) for wordcloud\n",
    "    filename_wordcloud = data_name.replace('cleanspeeches_', 'wordcloud_speeches_').replace('.pkl', '.pkl')\n",
    "    out_name_wordcloud = os.path.join(data_preprocessed, os.path.basename(filename_wordcloud))\n",
    "    joblib.dump(data, out_name_wordcloud)\n",
    "\n",
    "    # Apply stemming\n",
    "    data_stemmed = pro6(data)\n",
    "\n",
    "    filename_preprocessed = data_name.replace('cleanspeeches_', 'preprocessed_speeches_').replace('.pkl', '.pkl')\n",
    "    out_preprocessed = os.path.join(data_preprocessed, os.path.basename(filename_preprocessed))\n",
    "    joblib.dump(data_stemmed, out_preprocessed\n",
    "               )\n",
    "    print(f\"[{data_name}] Saved stemmed version: {out_preprocessed}\")\n",
    "\n",
    "    print(f\"[{data_name}] Done. Total time: {time.time() - t0:.2f}s\\n\")\n",
    "\n",
    "def main():\n",
    "    for fname in data_files:\n",
    "        preprocessing(fname)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d3409987-485c-47c9-9430-01fa2c0e650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the pre-processed data\n",
    "preprocessed_files = [\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed1.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed2.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed3.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed4.pkl')\n",
    "]\n",
    "\n",
    "wordcloud_files = [\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed1.pkl'),\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed2.pkl'),\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed3.pkl'),\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed4.pkl')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "134ddbf7-9b95-4697-9d9c-f9c55b52ec80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d006e-6a23-453b-9c91-0e60bd3f8301",
   "metadata": {},
   "source": [
    "## Word-Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf2443e-4c76-41b2-bc16-e72d91c318a8",
   "metadata": {},
   "source": [
    "### Count frequencies of all tokens and display the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "154d978d-cddd-431c-9d53-73c9faf8fdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed1.pkl: removed words with freq < 10\n",
      "Processed C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed2.pkl: removed words with freq < 10\n",
      "Processed C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed3.pkl: removed words with freq < 10\n",
      "Processed C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed4.pkl: removed words with freq < 10\n",
      "\n",
      "[Stemmed] Top 50 most common words:\n",
      "nation: 8913\n",
      "unit: 6946\n",
      "countri: 6652\n",
      "intern: 5884\n",
      "develop: 5540\n",
      "world: 5079\n",
      "peac: 4982\n",
      "peopl: 4565\n",
      "state: 4460\n",
      "secur: 3048\n",
      "govern: 3039\n",
      "general: 2874\n",
      "econom: 2659\n",
      "right: 2495\n",
      "organ: 2429\n",
      "year: 2408\n",
      "assembl: 2406\n",
      "new: 2141\n",
      "problem: 2140\n",
      "human: 2105\n",
      "effort: 2086\n",
      "support: 1991\n",
      "polit: 1901\n",
      "continu: 1898\n",
      "communiti: 1829\n",
      "region: 1819\n",
      "time: 1699\n",
      "need: 1597\n",
      "hope: 1591\n",
      "session: 1540\n",
      "member: 1535\n",
      "achiev: 1512\n",
      "import: 1501\n",
      "council: 1498\n",
      "power: 1470\n",
      "africa: 1458\n",
      "principl: 1455\n",
      "work: 1436\n",
      "war: 1418\n",
      "presid: 1403\n",
      "situat: 1368\n",
      "resolut: 1366\n",
      "global: 1311\n",
      "conflict: 1295\n",
      "great: 1284\n",
      "concern: 1270\n",
      "oper: 1214\n",
      "south: 1201\n",
      "relat: 1190\n",
      "independ: 1185\n",
      "\n",
      "[Stemmed] Top 50 least common words:\n",
      "gandhi: 10\n",
      "philosoph: 10\n",
      "maltes: 10\n",
      "closur: 10\n",
      "injuri: 10\n",
      "secess: 10\n",
      "glori: 10\n",
      "rival: 10\n",
      "bargain: 10\n",
      "unmistak: 10\n",
      "austrian: 10\n",
      "version: 10\n",
      "singapor: 10\n",
      "xxi: 10\n",
      "desol: 10\n",
      "surg: 10\n",
      "oa: 10\n",
      "sanit: 10\n",
      "georgetown: 10\n",
      "purport: 10\n",
      "fallaci: 10\n",
      "languish: 10\n",
      "postur: 10\n",
      "hussein: 10\n",
      "shorter: 10\n",
      "hub: 10\n",
      "bahamian: 10\n",
      "trap: 10\n",
      "inquiri: 10\n",
      "rectifi: 10\n",
      "sixteenth: 10\n",
      "seren: 10\n",
      "memor: 10\n",
      "flare: 10\n",
      "brazzavill: 10\n",
      "normalci: 10\n",
      "jakarta: 10\n",
      "disguis: 10\n",
      "inim: 10\n",
      "unlaw: 10\n",
      "household: 10\n",
      "paralys: 10\n",
      "domain: 10\n",
      "cultiv: 10\n",
      "renamo: 10\n",
      "helm: 10\n",
      "kuwaiti: 10\n",
      "abomin: 10\n",
      "hormuz: 10\n",
      "olymp: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Wordcloud] Top 50 most common words:\n",
      "nations: 6968\n",
      "united: 6858\n",
      "international: 5635\n",
      "world: 4970\n",
      "countries: 4485\n",
      "peace: 4222\n",
      "development: 3122\n",
      "states: 3102\n",
      "people: 2960\n",
      "general: 2825\n",
      "security: 2790\n",
      "economic: 2643\n",
      "assembly: 2353\n",
      "government: 2306\n",
      "new: 2141\n",
      "country: 2085\n",
      "human: 1844\n",
      "organization: 1802\n",
      "political: 1802\n",
      "community: 1692\n",
      "rights: 1680\n",
      "efforts: 1667\n",
      "peoples: 1605\n",
      "support: 1487\n",
      "session: 1482\n",
      "council: 1476\n",
      "africa: 1420\n",
      "time: 1391\n",
      "national: 1345\n",
      "great: 1282\n",
      "developing: 1264\n",
      "war: 1258\n",
      "problems: 1233\n",
      "year: 1206\n",
      "years: 1200\n",
      "south: 1199\n",
      "state: 1189\n",
      "situation: 1186\n",
      "order: 1139\n",
      "hope: 1127\n",
      "global: 1108\n",
      "work: 1089\n",
      "president: 1081\n",
      "charter: 1066\n",
      "social: 1060\n",
      "need: 1037\n",
      "principles: 1033\n",
      "today: 1009\n",
      "progress: 1009\n",
      "republic: 1001\n",
      "\n",
      "[Wordcloud] Top 50 least common words:\n",
      "hgpe: 1\n",
      "trite: 1\n",
      "fiveyear: 1\n",
      "baton: 1\n",
      "personified: 1\n",
      "husbands: 1\n",
      "perished: 1\n",
      "widowers: 1\n",
      "rekindle: 1\n",
      "habyarimana: 1\n",
      "tribe: 1\n",
      "permanency: 1\n",
      "deposed: 1\n",
      "edmund: 1\n",
      "burke: 1\n",
      "subdue: 1\n",
      "subduing: 1\n",
      "overcrowding: 1\n",
      "ngara: 1\n",
      "karagwe: 1\n",
      "firewood: 1\n",
      "wring: 1\n",
      "redeployment: 1\n",
      "stocked: 1\n",
      "dustbin: 1\n",
      "intransigency: 1\n",
      "pamper: 1\n",
      "onumoz: 1\n",
      "masterly: 1\n",
      "brood: 1\n",
      "rhodesias: 1\n",
      "meditated: 1\n",
      "unendowed: 1\n",
      "accommodating: 1\n",
      "primordial: 1\n",
      "weseek: 1\n",
      "superintend: 1\n",
      "bench: 1\n",
      "dieted: 1\n",
      "midpoint: 1\n",
      "unimplemented: 1\n",
      "correlative: 1\n",
      "tatters: 1\n",
      "mounts: 1\n",
      "textual: 1\n",
      "selfreliance: 1\n",
      "concomitants: 1\n",
      "harms: 1\n",
      "maims: 1\n",
      "spawned: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit\\\\data\\\\freq\\\\word_counts_wordcloud.pkl']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#== Count token frequencies ==\n",
    "\n",
    "def count_frequencies(filenames):\n",
    "    total_freq = Counter()\n",
    "    for fname in tqdm(filenames):\n",
    "        data = joblib.load(fname)\n",
    "        tokens = chain.from_iterable(row[1] for row in data if isinstance(row[1], list))\n",
    "        total_freq.update(tokens)\n",
    "    return total_freq\n",
    "\n",
    "def remove_rare_words(filenames, freqs, min_count=10):\n",
    "    for fname in filenames:\n",
    "        data = joblib.load(fname)\n",
    "        filtered_data = []\n",
    "        for doc_id, tokens in data:\n",
    "            filtered_tokens = [w for w in tokens if freqs.get(w, 0) >= min_count]\n",
    "            filtered_data.append([doc_id, filtered_tokens])\n",
    "        joblib.dump(filtered_data, fname)  # overwrite or save as new file\n",
    "        print(f\"Processed {fname}: removed words with freq < {min_count}\")\n",
    "\n",
    "# === Count for preprocessed (stemmed) speeches ===\n",
    "word_counts_stemmed = count_frequencies(preprocessed_files)\n",
    "\n",
    "remove_rare_words(preprocessed_files, word_counts_stemmed, min_count=10)\n",
    "\n",
    "print(\"\\n[Stemmed] Top 50 most common words:\")\n",
    "for word, count in word_counts_stemmed.most_common(50):\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\n[Stemmed] Top 50 least common words:\")\n",
    "for word, count in word_counts_stemmed.most_common()[-50:]:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Save stemmed word counts\n",
    "save_path_stemmed = os.path.join(data_freq, 'word_counts_stemmed.pkl')\n",
    "joblib.dump(word_counts_stemmed, save_path_stemmed)\n",
    "\n",
    "# === Count for wordcloud (unstemmed) speeches ===\n",
    "word_counts_wordcloud = count_frequencies(wordcloud_files)\n",
    "\n",
    "print(\"\\n[Wordcloud] Top 50 most common words:\")\n",
    "for word, count in word_counts_wordcloud.most_common(50):\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\n[Wordcloud] Top 50 least common words:\")\n",
    "for word, count in word_counts_wordcloud.most_common()[-50:]:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Save unstemmed word counts\n",
    "save_path_wordcloud = os.path.join(data_freq, 'word_counts_wordcloud.pkl')\n",
    "joblib.dump(word_counts_wordcloud, save_path_wordcloud)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "932ce36d-d21c-4f1c-aff7-899c321dcfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_c)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39acefa1-c504-478e-98e5-9d465963dc82",
   "metadata": {},
   "source": [
    "### Count the frequency of the dictionary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9e28f481-7e20-486c-a6b3-8729d6aeaad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of affect dictionary:\n",
      "['forbid', 'unattract', 'cruelti', 'crappi', 'apathi', 'scari', 'unimpress', 'sin', 'dumbest', 'eas', 'agit', 'sob', 'shocker', 'tragedi', 'fabul', 'strongest', 'giver', 'sigh', 'aw', 'witch', 'hurtl', 'fucktard', 'cruel', 'glamor', 'funni', 'smarter', 'brillianc', 'irrate', 'alright', 'honest', 'profit', 'fearless', 'grievous', 'relax', 'isolationist', 'hah', 'shyness', 'poorest', 'cruelest', 'troublemak', 'disagre', 'agon', 'terror', 'fight', 'pleas', 'poor', 'crazi', 'hostil', 'stupid', 'damnat', 'vain', 'jade', 'heartless', 'nag', 'gloomi', 'damn', 'dishearten', 'pleaser', 'credit', 'warmth', 'greatest', 'whine', 'shame', 'angriest', 'envious', 'grin', 'blameless', 'sweeter', 'laidback', 'stupidest', 'unprotect', 'whiner', 'unlov', 'shake', 'boredom', 'fairer', 'weaker', 'wellb', 'bold', 'sucki', 'unsuccess', 'mourner', 'liken', 'defens', 'invigor', 'tedious', 'paranoid', 'cynic', 'dignifi', 'paranoia', 'sweetest', 'contented', 'humili', 'crush', 'terrif', 'alarm', 'asham', 'impati', 'fake', 'flirti', 'laziest', 'libertarian', 'empti', 'panicki', 'rape', 'contemptu', 'incent', 'reward', 'brutal', 'ignoramus', 'bereav', 'wimp', 'laugh', 'nicer', 'warm', 'resent', 'threat', 'hell', 'tortur', 'mad', 'worship', 'uncomfort', 'depress', 'unsaf', 'faultless', 'numb', 'humorless', 'murder', 'prettiest', 'jealous', 'tragic', 'discomfort', 'loveli', 'ungrat', 'divin', 'lamest', 'weaken', 'horribl', 'fuck', 'agoni', 'grievanc', 'ach', 'isolation', 'masoch', 'uglier', 'amaz', 'devilish', 'bitter', 'devil', 'smartest', 'glori', 'trite', 'vicious', 'unsavori', 'grossest', 'sinist', 'cheeri', 'dismay', 'easier', 'flatter', 'virtuous', 'egot', 'generos', 'cherish', 'heartwarm', 'advers', 'tough', 'adorn', 'overwhelm', 'devast', 'egotist', 'sociabl', 'lazier', 'affection', 'sarcasm', 'humor', 'funniest', 'startl', 'innoc', 'vomit', 'shyli', 'kiss', 'burdensom', 'melancholi', 'weakest', 'worthless', 'gentlest', 'proud', 'alarmist', 'pervert', 'demeanor', 'loneli', 'reassur', 'weird', 'passionless', 'loyalti', 'frustrat', 'weak', 'immor', 'hater', 'libertin', 'flirt', 'terribl', 'threaten', 'fatalist', 'melancholia', 'avers', 'prick', 'sucker', 'meanest', 'virtuos', 'benefici', 'neurot', 'disappoint', 'restless', 'romant', 'bastard', 'angri', 'flatteri', 'stun', 'bolder', 'amor', 'forbidden', 'hug', 'demean', 'disgust', 'truer', 'benefic', 'teas', 'stinki', 'condemnatori', 'turmoil', 'poison', 'cunt', 'gorgeous', 'harm', 'lousi', 'glorifi', 'piti', 'wimpish', 'impression', 'freakish', 'happiest', 'apathet', 'pitiabl', 'unpleas', 'suffer', 'miseri', 'toler', 'mess', 'casual', 'anxieti', 'stale', 'fieri', 'grudg', 'arrog', 'outrag', 'thrill', 'faithless', 'popular', 'vaniti', 'palat', 'fantasi', 'decay', 'domin', 'mood', 'proudest', 'scream', 'stress', 'calm', 'fatigu', 'dumber', 'benign', 'rude', 'steal', 'tension', 'enthusiast', 'intimid', 'sweetheart', 'prettier', 'perv', 'pathet', 'flawless', 'contradictori', 'distress', 'moron', 'gratifi', 'frighten', 'anguish', 'snob', 'defenseless', 'fantast', 'sad', 'thief', 'worsen', 'tear', 'handsom', 'attract', 'messi', 'smother', 'scarier', 'afraid', 'insult', 'disreput', 'wept', 'victim', 'traumat', 'prejudic', 'amus', 'decent', 'joyous', 'masochist', 'abandon', 'foe', 'pleasantri', 'shitless', 'strengthen', 'antagonist', 'liar', 'romantic', 'violenc', 'sentimentalist', 'gratif', 'decept', 'witchcraft', 'liveli', 'impressionist', 'guilt', 'griev', 'rotten', 'disgrac', 'insincer', 'pushi', 'enjoy', 'freaki', 'ador', 'grate', 'homesick', 'braver', 'battlefield', 'niceti', 'hooray', 'incompet', 'thiev', 'timid', 'adventur', 'needi', 'joyless', 'grimac', 'gloom', 'dump', 'horrid', 'magnific', 'unlucki', 'killer', 'uncontrol', 'funnier', 'ruin', 'weepi', 'play', 'encourag', 'fantas', 'irrit', 'sincer', 'rage', 'despis', 'unhappi', 'hate', 'bravest', 'harmless', 'hero', 'vigor', 'pervers', 'dumb', 'degrad', 'shameless', 'villaini', 'envi', 'aggress', 'fucker', 'courag', 'craziest', 'delici', 'scare', 'finer', 'bittersweet', 'trivial', 'wealthi', 'heroic', 'ecstat', 'irrat', 'prejudici', 'smug', 'stammer', 'feroci', 'phobic', 'villain', 'anger', 'fond', 'obsession', 'awesom', 'wors', 'compassion', 'weakl', 'calmer', 'inhibit', 'bother', 'rancid', 'hatr', 'beaten', 'opportunist', 'offens', 'haunt', 'appal', 'painless', 'pessimist', 'molest', 'lol', 'wrongdo', 'frantic', 'heroin', 'rejoic', 'woe', 'miser', 'meaner', 'saddest', 'mooch', 'attack', 'enthusiasm', 'joke', 'meritocraci', 'angrier', 'emot', 'weirdest', 'silliest', 'kisser', 'hurrah', 'crueler', 'nervous', 'dishonor', 'gracious', 'sicker', 'hopeless', 'delect', 'compliment', 'petrifi', 'cuter', 'reek', 'loveless', 'daze', 'weirder', 'triumphant', 'impolit', 'offend', 'richer', 'violent', 'crazier', 'comedown', 'heroism', 'stubborn', 'maniac', 'strain', 'broke', 'jerk', 'unkind', 'asshol', 'reveng', 'jealousi', 'damnabl', 'unimport', 'careless', 'gentler', 'bitch', 'shi', 'terrorist', 'rigid', 'harsh', 'passion', 'annoy', 'compass', 'geek', 'weirdo', 'agre', 'whore', 'splendor', 'suck', 'fun', 'chuckl', 'snobberi', 'mock', 'doom', 'reluct', 'mourn', 'boldest', 'helpless', 'trembl', 'pestil', 'curs', 'desper', 'harass', 'complain', 'joker', 'obsess', 'uneasi', 'nasti', 'greedi', 'belov', 'grim', 'upset', 'greed', 'honesti', 'phoni', 'feudal', 'loveliest', 'trauma', 'dwell', 'ecstasi', 'blame', 'legit', 'calmest', 'sickest', 'scariest', 'sarcast', 'cheat', 'fatal', 'stronger', 'energet', 'feroc', 'healthi', 'goddam', 'disast', 'flirtati', 'defeatist', 'glorious', 'terrifi', 'gossip', 'forbad', 'sicken', 'poorer', 'strangest', 'cute', 'struggl', 'woebegon', 'worri', 'gratitud', 'cheer', 'distraught', 'ok', 'vulner', 'blessed', 'fairest', 'lame', 'optim', 'grouch', 'fright', 'pessim', 'sadder', 'worst', 'despair', 'bash', 'intellectu', 'promis', 'prais', 'heartfelt', 'horror', 'crap', 'repress', 'prouder', 'fought', 'discourag', 'heal', 'lazi', 'precious', 'melanchol', 'isol', 'aggressor', 'giggl', 'idiot', 'twitchi', 'glorif', 'kid', 'selfish', 'smile', 'grief', 'hellish', 'shit', 'fume', 'bitchi', 'fool', 'trustworthi', 'optimist', 'dumpi', 'unwelcom', 'neat', 'nicest', 'furi', 'unfriend', 'ridicul', 'arguabl', 'shamefac', 'pervi', 'upbeat', 'wow', 'easygo', 'blameworthi', 'charmer', 'stink', 'phobia', 'adversari', 'sillier', 'distrust', 'dread', 'happier', 'vital', 'dissatisfact', 'insecur', 'assault', 'ineffect', 'panic', 'powerless', 'nightmar', 'wealth', 'enrag', 'splendid', 'foolish', 'ugliest', 'contradict', 'worrier', 'freak']\n",
      "Number of words in affect dictionary: 629\n",
      "Contents of cognition dictionary:\n",
      "['analyt', 'guess', 'suspect', 'coz', 'mysteri', 'unawar', 'proverbi', 'dissimilar', 'consequenti', 'stimul', 'occasion', 'rethink', 'purest', 'entireti', 'abnorm', 'referenti', 'differenti', 'diagnos', 'ambigu', 'concentr', 'undon', 'undecid', 'disclosur', 'fundamentalist', 'blur', 'reaction', 'variabl', 'recogniz', 'meant', 'reconstruct', 'supposit', 'acknowledg', 'choosi', 'accuraci', 'mislead', 'perceiv', 'hypothesi', 'unknow', 'cuz', 'reveal', 'potenti', 'puriti', 'persuad', 'indirect', 'generat', 'presumpt', 'clue', 'justif', 'evidenti', 'realize', 'deduct', 'causal', 'inevit', 'thinker', 'evidentiari', 'factor', 'undeni', 'unclear', 'pick', 'induc', 'accur', 'absolut', 'everytim', 'reconsider', 'diagnost', 'solut', 'diagnosi', 'discoveri', 'ration', 'unambigu', 'explicit', 'decis', 'insight', 'confess', 'fuzzi', 'led', 'lesson', 'theoriz', 'analyz', 'learner', 'split', 'enlighten', 'cos', 'respons', 'dubious', 'react', 'guarante', 'someday', 'blatant', 'undo', 'perceptu', 'infal', 'afterthought', 'inequ', 'feasibl', 'knowabl', 'approxim', 'logic', 'meaning', 'unknown', 'categor', 'trigger', 'incomplet', 'curious', 'attribut', 'pretend', 'reactionari', 'prove', 'meaningless', 'reactiv', 'proof', 'bet', 'purpos', 'suppos', 'discern', 'imagin', 'kinda', 'comprehens', 'sens', 'percept', 'everyday', 'rational', 'unquestion', 'random', 'implicit', 'clarif', 'theori', 'dunno', 'analysi', 'obedi', 'memor', 'genuin', 'evalu', 'explor', 'linkag', 'grasp', 'curios', 'methink', 'somewher', 'deduc', 'unwant', 'induct', 'unrel', 'outstand', 'discov', 'unresolv', 'exploratori', 'choos', 'perspect', 'inferenti', 'disclos', 'factual', 'arbitrari', 'conting', 'vagu', 'persuas', 'manipul', 'impli', 'somehow', 'sorta', 'borderlin', 'reconcil', 'anyhow', 'conscious', 'probabilist', 'precis', 'rearrang', 'referenc', 'obscur']\n",
      "Number of words in cognition dictionary: 169\n"
     ]
    }
   ],
   "source": [
    "# == Count dictionary words\n",
    "\n",
    "# Load dictionaries            ##### How did they come up with this dictionary? Why did they exclude words?\n",
    "\n",
    "affect_path = os.path.join(data_dict, 'dictionary_affect.pkl')\n",
    "cognition_path = os.path.join(data_dict, 'dictionary_cognition.pkl')\n",
    "\n",
    "with open(affect_path, 'rb') as f:\n",
    "    affect_dict = pickle.load(f)\n",
    "print(\"Contents of affect dictionary:\")\n",
    "print(affect_dict)\n",
    "print(\"Number of words in affect dictionary:\", len(affect_dict))\n",
    "\n",
    "with open(cognition_path, 'rb') as f:\n",
    "    cognition_dict = pickle.load(f)\n",
    "print(\"Contents of cognition dictionary:\")\n",
    "print(cognition_dict)\n",
    "print(\"Number of words in cognition dictionary:\", len(cognition_dict))\n",
    "\n",
    "affect = joblib.load(affect_path)\n",
    "cognition = joblib.load(cognition_path)\n",
    "\n",
    "a = [[i, word_counts_stemmed[i]] for i in affect if i in word_counts_stemmed]\n",
    "c = [[i, word_counts_stemmed[i]] for i in cognition if i in word_counts_stemmed]\n",
    "\n",
    "a = sorted(a, key=lambda x: x[1], reverse=True)\n",
    "c = sorted(c, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "a = [[i[0], f\"({i[1]}),\"] for i in a]\n",
    "c = [[i[0], f\"({i[1]}),\"] for i in c]\n",
    "\n",
    "a1 = ' '.join(str(r) for v in a for r in v)\n",
    "c1 = ' '.join(str(r) for v in c for r in v)\n",
    "\n",
    "affect_out_path = os.path.join(data_freq, \"affect_words.txt\")\n",
    "cog_out_path = os.path.join(data_freq, \"cog_words.txt\")\n",
    "\n",
    "os.makedirs(data_freq, exist_ok=True)  # ensure directory exists\n",
    "\n",
    "with open(affect_out_path, \"w\") as output:\n",
    "    output.write(a1)\n",
    "\n",
    "with open(cog_out_path, \"w\") as output:\n",
    "    output.write(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0dffe87f-d712-4d71-9305-fb288cf06402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 words by weighted frequency:\n",
      "prescript: 0.9807965218660404\n",
      "unknown: 0.9807965218660404\n",
      "unitari: 0.9807965218660404\n",
      "buy: 0.9807965218660404\n",
      "teacher: 0.9807965218660404\n",
      "bestow: 0.9807965218660404\n",
      "sensibl: 0.9807965218660404\n",
      "falter: 0.9807965218660404\n",
      "greed: 0.9807965218660404\n",
      "vacuum: 0.9807965218660404\n",
      "ear: 0.9807965218660404\n",
      "knew: 0.9807965218660404\n",
      "epoch: 0.9807965218660404\n",
      "almighti: 0.9807965218660404\n",
      "grievanc: 0.9807965218660404\n",
      "guarantor: 0.9807965218660404\n",
      "curtain: 0.9807965218660404\n",
      "bacteriolog: 0.9807965218660404\n",
      "notori: 0.9807965218660404\n",
      "irrig: 0.9807965218660404\n",
      "backdrop: 0.9807965218660404\n",
      "revolt: 0.9807965218660404\n",
      "hungari: 0.9807965218660404\n",
      "governor: 0.9807965218660404\n",
      "lifetim: 0.9807965218660404\n",
      "foresight: 0.9807965218660404\n",
      "dubious: 0.9807965218660404\n",
      "cash: 0.9807965218660404\n",
      "mourn: 0.9807965218660404\n",
      "finland: 0.9807965218660404\n",
      "seventeenth: 0.9807965218660404\n",
      "tonga: 0.9807965218660404\n",
      "nomin: 0.9807965218660404\n",
      "duplic: 0.9807965218660404\n",
      "overlap: 0.9807965218660404\n",
      "exemplifi: 0.9807965218660404\n",
      "forecast: 0.9807965218660404\n",
      "handicap: 0.9807965218660404\n",
      "hindranc: 0.9807965218660404\n",
      "starv: 0.9807965218660404\n",
      "vein: 0.9807965218660404\n",
      "worthwhil: 0.9807965218660404\n",
      "gift: 0.9807965218660404\n",
      "caledonia: 0.9807965218660404\n",
      "pleasant: 0.9807965218660404\n",
      "consecr: 0.9807965218660404\n",
      "sakiet: 0.9807965218660404\n",
      "transmit: 0.9807965218660404\n",
      "parent: 0.9807965218660404\n",
      "sink: 0.9807965218660404\n",
      "golden: 0.9807965218660404\n",
      "synergi: 0.9807965218660404\n",
      "friction: 0.9807965218660404\n",
      "furthermor: 0.9807965218660404\n",
      "mercosur: 0.9807965218660404\n",
      "dissatisfact: 0.9807965218660404\n",
      "outdat: 0.9807965218660404\n",
      "persev: 0.9807965218660404\n",
      "humbl: 0.9807965218660404\n",
      "mexican: 0.9807965218660404\n",
      "unifil: 0.9807965218660404\n",
      "sham: 0.9807965218660404\n",
      "communiqu: 0.9807965218660404\n",
      "persuas: 0.9807965218660404\n",
      "hidden: 0.9807965218660404\n",
      "instinct: 0.9807965218660404\n",
      "czechoslovak: 0.9807965218660404\n",
      "dilemma: 0.9807965218660404\n",
      "propiti: 0.9807965218660404\n",
      "acquiesc: 0.9807965218660404\n",
      "costa: 0.9807965218660404\n",
      "probabl: 0.9807965218660404\n",
      "lisbon: 0.9807965218660404\n",
      "scandal: 0.9807965218660404\n",
      "bosnian: 0.9807965218660404\n",
      "facto: 0.9807965218660404\n",
      "arrear: 0.9807965218660404\n",
      "antarctica: 0.9807965218660404\n",
      "slip: 0.9807965218660404\n",
      "tropic: 0.9807965218660404\n",
      "spill: 0.9807965218660404\n",
      "vice: 0.9807965218660404\n",
      "denomin: 0.9807965218660404\n",
      "verbal: 0.9807965218660404\n",
      "sentenc: 0.9807965218660404\n",
      "politic: 0.9807965218660404\n",
      "intra: 0.9807965218660404\n",
      "occurr: 0.9807965218660404\n",
      "herald: 0.9807965218660404\n",
      "boy: 0.9807965218660404\n",
      "elder: 0.9807965218660404\n",
      "farm: 0.9807965218660404\n",
      "fifteenth: 0.9807965218660404\n",
      "stimulus: 0.9807965218660404\n",
      "driver: 0.9807965218660404\n",
      "realis: 0.9807965218660404\n",
      "airport: 0.9807965218660404\n",
      "season: 0.9807965218660404\n",
      "architect: 0.9807965218660404\n",
      "mar: 0.9807965218660404\n"
     ]
    }
   ],
   "source": [
    "# == Calculate weighted frequencies for all words\n",
    "\n",
    "# - downweights very common words by giving more importance to rare ones\n",
    "word_counts_stemmed = joblib.load(os.path.join(data_freq, 'word_counts_stemmed.pkl'))\n",
    "\n",
    "l = sum(word_counts_stemmed.values())\n",
    "\n",
    "a = 0.001\n",
    "word_counts_weighted = {k: a / (a + (v / l)) for k, v in word_counts_stemmed.items()}\n",
    "#for key in word_counts.keys():\n",
    " #   word_counts[key] = a / (a + (word_counts[key] / l))\n",
    "\n",
    "joblib.dump(word_counts_weighted, os.path.join(data_freq, 'word_counts_weighted.pkl'))\n",
    "\n",
    "################################################################################ ISSUE##################\n",
    "# To print top 100 by weighted values, sort the dictionary by value descending:\n",
    "top_100_weighted = sorted(word_counts_weighted.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "\n",
    "print(\"Top 100 words by weighted frequency:\")\n",
    "for word, weight in top_100_weighted:\n",
    "    print(f\"{word}: {weight}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df626ded-5e49-4a34-a3d1-315ba370226f",
   "metadata": {},
   "source": [
    "## Final Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0d0856a2-44f1-4304-b221-09a9ab72ad17",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'freq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(data_c)\n\u001b[1;32m----> 2\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(freq)\n\u001b[0;32m      3\u001b[0m count \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_counts_stemmed.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# load stemmed counts\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# For each speech only keep tokens that appear at least 10x\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'freq' is not defined"
     ]
    }
   ],
   "source": [
    "os.chdir(data_c)\n",
    "os.chdir(freq)\n",
    "count = joblib.load('word_counts_stemmed.pkl')  # load stemmed counts\n",
    "\n",
    "# For each speech only keep tokens that appear at least 10x\n",
    "\n",
    "def select(lista):\n",
    "    for i in range(len(lista)):\n",
    "        x = lista[i][0]\n",
    "        y = lista[i][1]\n",
    "        y = [w for w in y if count.get(w, 0) >= 10]\n",
    "        lista[i] = [x, y]\n",
    "    return lista\n",
    "\n",
    "for data_path in preprocessed_files:\n",
    "    data = joblib.load(data_path)\n",
    "    data = select(data)\n",
    "    cleaned_path = data_path.replace('.pkl', '_final.pkl')\n",
    "    joblib.dump(data, cleaned_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "001e6aaf-101b-4912-a198-3d79f6d6e828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\n"
     ]
    }
   ],
   "source": [
    "test = os.getcwd()\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5ba7c0-33c7-4420-9cb0-fe54a94ee649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf1db25-1369-45bc-b6c4-ddddb77a3c52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
