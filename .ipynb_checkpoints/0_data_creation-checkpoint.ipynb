{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b41cb22-e190-48d5-9926-7ac6fac8310d",
   "metadata": {},
   "source": [
    "# Emotion and Reason in Political Language: Examining the UN General speeches\n",
    "## Script 1: Preprocessing & Token Frequencies\n",
    "## Author: Sarah Franzen\n",
    "\n",
    "### Instructions BEFORE running this script:\n",
    "- Ensure all required packages are installed. If not, set `InstallPackages = TRUE` (see code cells below).  \n",
    "- Set your working directory appropriately.  \n",
    "- The script will automatically create the required folder structure.  \n",
    "- Later, you will be asked to download the data from:  \n",
    "  https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/0TJX8Y  \n",
    "  and store it **unzipped** inside the created folder *data_original*\n",
    "\n",
    "\n",
    "### Description: \n",
    "- Extract documents from their original txt documents and store them as one csv\n",
    "- Data Cleaning and Pre-Processing\n",
    "- Count word frequencies and weight themh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930ebd56-def9-4a16-91eb-78dbb7d037ae",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5588afc-105a-426e-9348-742a830a2732",
   "metadata": {},
   "source": [
    "## Setup, Installation and Verification of required Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebb8987a-a0da-453e-97e2-aafa209d0e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "InstallPackages = False # Set this to True to install the following packages \n",
    "\n",
    "if InstallPackages:\n",
    "    import sys\n",
    "\n",
    "    packages = [\n",
    "        \"pandas\",\n",
    "        \"nltk\",\n",
    "        \"spacy\",\n",
    "        \"numpy\",\n",
    "        \"gensim\",\n",
    "        \"pycountry\",\n",
    "        \"wordcloud\",\n",
    "        \"matplotlib\",\n",
    "        \"tqdm\"\n",
    "    ]\n",
    "\n",
    "    for package in packages:\n",
    "        !{sys.executable} -m pip install {package}\n",
    "\n",
    "\n",
    "DownloadAdditions = False # Set this to True to download these additional resources\n",
    "if DownloadAdditions:\n",
    "    nltk.download(\"punkt\")\n",
    "    nltk.download(\"averaged_perceptron_tagger\")\n",
    "    spacy.cli.download('en_core_web_lg')         # Download spaCy English model (large)\n",
    "\n",
    "#########################\n",
    "# Check if all packages are included\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32d7d499-0563-46e0-9730-914c9f106d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Import standard and third-party libraries for data processing, NLP, and visualization ==\n",
    "\n",
    "import gensim\n",
    "import joblib\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import pycountry\n",
    "import random\n",
    "import re\n",
    "import spacy\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from multiprocessing import Pool, freeze_support\n",
    "from nltk.stem import SnowballStemmer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOPWORDS\n",
    "from string import punctuation\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# === Initialize NLP Tools ===\n",
    "\n",
    "translator = str.maketrans('', '', punctuation)\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d495156-bbb2-403c-86f2-83b47d8cb03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Folder structure created:\n",
      "- C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\n",
      "  - data_original\n",
      "  - dictionaries\n",
      "  - freq\n",
      "  - preprocessed\n",
      "  - temp\n",
      "  - tokenized\n"
     ]
    }
   ],
   "source": [
    "# === Set Working Directory and create folder structure ===\n",
    "\n",
    "# Prompt user to enter working directory path\n",
    "#wd = input(\"Please enter your working directory path (e.g., C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit): \").strip()\n",
    "\n",
    "# Change to the entered working directory\n",
    "#try:\n",
    "   # os.chdir(wd)\n",
    "    #print(f\"Working directory set to: {os.getcwd()}\")\n",
    "#except FileNotFoundError:\n",
    "   # print(\"ERROR: The directory you entered does not exist. Please restart and enter a valid path.\")\n",
    "    #exit(1)\n",
    "\n",
    "# Set your working directory (adjust this as needed)\n",
    "wd = r\"C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\"\n",
    "os.chdir(wd)\n",
    "\n",
    "# Set base path to current working directory\n",
    "base_path = Path.cwd()\n",
    "data_path = base_path / \"data\"\n",
    "\n",
    "# List of subfolders to create inside 'data'\n",
    "subfolders = [\"data_original\", \"dictionaries\", \"freq\", \"preprocessed\", \"temp\", \"tokenized\"]\n",
    "\n",
    "# Create 'data' folder if it doesn't exist\n",
    "data_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Create subfolders\n",
    "for name in subfolders:\n",
    "    (data_path / name).mkdir(exist_ok=True)\n",
    "\n",
    "print(\"\\nFolder structure created:\")\n",
    "print(f\"- {data_path}\")\n",
    "for name in subfolders:\n",
    "    print(f\"  - {name}\")\n",
    "\n",
    "# Prompt user to place raw data files\n",
    "#print(f\"\\nPlease place your raw data files (unzipped) into the folder:\\n  {data_path / 'data_original'}\")\n",
    "#input(\"Press Enter after you have placed the files to continue...\")\n",
    "\n",
    "#print(\"Continuing with the script...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56fa9a21-650f-4d8e-8a21-3d9629655c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define Folder Paths ===\n",
    "\n",
    "# If an error occurs, make sure that you actually have these folders in your working directory\n",
    "data_c = os.path.join(wd, 'data')\n",
    "data_temp = os.path.join(data_c, 'temp')\n",
    "data_freq = os.path.join(data_c, 'freq')\n",
    "data_dict = os.path.join(data_c, 'dictionaries')\n",
    "data_preprocessed = os.path.join(data_c, 'preprocessed')\n",
    "data_tokenized = os.path.join(data_c, 'tokenized')\n",
    "fig = os.path.join(wd, 'fig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "296fc7d6-a2ac-406e-a322-6c9f2702ee1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\n"
     ]
    }
   ],
   "source": [
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bd410e-bc90-4e0b-aa08-a9bff13fab47",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff34b6-cbee-4f8c-8076-5f385f596b58",
   "metadata": {},
   "source": [
    "## Load and Prepare Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18067b9-6337-4fb3-a570-603083a7deae",
   "metadata": {},
   "source": [
    "### This chunk can be skipped at the moment\n",
    "### Think of proper header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "93d247a0-5ff2-4062-add9-f3267ae1fe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total speeches found: 10761\n",
      "\n",
      " Saved raw data with 100 speeches to '.\\data\\un_corpus_raw.csv'\n"
     ]
    }
   ],
   "source": [
    "# == Load Sample from UN General Debate Corpus ==\n",
    "\n",
    "# Set Folder path containing the original TXT files    \n",
    "base_folder = r\".\\data\\data_original\\UN General Debate Corpus\\UNGDC_1946-2023\\TXT\"\n",
    "\n",
    "# Collect txt-files\n",
    "all_txt_files = []\n",
    "for root, dirs, files in os.walk(base_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.txt') and not file.startswith('._'):\n",
    "            all_txt_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Total speeches found: {len(all_txt_files)}\")\n",
    "\n",
    "# Randomly pick 800 files from the full collection   ###################################################### REMOVE AT LATER POINT\n",
    "sampled_files = random.sample(all_txt_files,100)\n",
    "\n",
    "# Read the selected files into a list\n",
    "raw_data = []\n",
    "for filepath in sampled_files:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        raw_data.append({'filename': os.path.basename(filepath), 'speech': content})\n",
    "\n",
    "df_raw = pd.DataFrame(raw_data)\n",
    "\n",
    "# == Store as csv and pkl ==\n",
    "\n",
    "df_raw = df_raw[df_raw['filename'] != '.DS_Store-to-UTF-8.txt'].copy()\n",
    "\n",
    "raw_pickle_path = r\".\\data\\un_corpus_raw.pkl\"\n",
    "df_raw.to_pickle(raw_pickle_path)\n",
    "\n",
    "raw_output_path = r\".\\data\\un_corpus_raw.csv\"\n",
    "df_raw.to_csv(raw_output_path, index=False, sep=';', encoding='utf-8')\n",
    "\n",
    "print(f\"\\n Saved raw data with {len(df_raw)} speeches to '{raw_output_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6405ff20-92fa-4c97-becb-ac5fff70f84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WSM_32_1977.txt</td>\n",
       "      <td>﻿1.\\tFirst I must extend to you, Sir, the warm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CRI_39_1984.txt</td>\n",
       "      <td>﻿It is an honour and a great pleasure for me t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TUV_65_2010.txt</td>\n",
       "      <td>At the dawn of the new \\nmillennium 10 years a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MHL_59_2004.txt</td>\n",
       "      <td>I am honoured to address the\\nGeneral Assembly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>THA_53_1998.txt</td>\n",
       "      <td>On behalf of the\\nGovernment and the people of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech\n",
       "0  WSM_32_1977.txt  ﻿1.\\tFirst I must extend to you, Sir, the warm...\n",
       "1  CRI_39_1984.txt  ﻿It is an honour and a great pleasure for me t...\n",
       "2  TUV_65_2010.txt  At the dawn of the new \\nmillennium 10 years a...\n",
       "3  MHL_59_2004.txt  I am honoured to address the\\nGeneral Assembly...\n",
       "4  THA_53_1998.txt  On behalf of the\\nGovernment and the people of..."
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# == Load data & drop empty speeches ==\n",
    "\n",
    "df_raw = pd.read_pickle(r\".\\data\\un_corpus_raw.pkl\")\n",
    "\n",
    "# Drop empty speeches\n",
    "df_raw['speech'] = df_raw['speech'].astype(str)\n",
    "df_raw = df_raw[df_raw['speech'].str.strip() != ''].copy()\n",
    "\n",
    "df_raw.head()         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7980e945-9d56-4df3-99e5-acf491617568",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7d3162-ebbd-449b-a2be-1148fceda725",
   "metadata": {},
   "source": [
    "## Create new variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25710e79-5068-4d39-8023-f4f97513f165",
   "metadata": {},
   "source": [
    "#### New Variables: Year, Country Code and Country Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "162da07c-1112-43e8-a434-2bef0ed32de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min year: 1948\n",
      "Max year: 2023\n",
      "Missing codes: []\n"
     ]
    }
   ],
   "source": [
    "# == Create variable: country code & year\n",
    "\n",
    "# Create contry_code and year variable\n",
    "df_raw['country_code'] = df_raw['filename'].str.extract(r'^([A-Z]{2,3})')\n",
    "df_raw['year'] = df_raw['filename'].str.extract(r'_(\\d{4})\\.txt$').astype(int)\n",
    "\n",
    "print(\"Min year:\", df_raw['year'].min())\n",
    "print(\"Max year:\", df_raw['year'].max())\n",
    "# Speeches range from 1946 to 2023\n",
    "\n",
    "# == Create variable: country_name by matching ISO country code \n",
    "code_to_name = {country.alpha_3: country.name for country in pycountry.countries}\n",
    "\n",
    "# Add custom short names and legacy codes\n",
    "custom_names = {\n",
    "    \"BOL\": \"Bolivia\",\n",
    "    \"COD\": \"Democratic Republic of Congo\",\n",
    "    \"IRN\": \"Iran\",\n",
    "    \"LAO\": \"Laos\",\n",
    "    \"MDA\": \"Moldova\",\n",
    "    \"PRK\": \"North Korea\",\n",
    "    \"PSE\": \"Palestine\",\n",
    "    \"RUS\": \"Russia\",\n",
    "    \"SYR\": \"Syria\",\n",
    "    \"TZA\": \"Tanzania\",\n",
    "    \"VAT\": \"Vatican City State\",\n",
    "    \"VEN\": \"Venezuela\",\n",
    "    \"VNM\": \"Vietnam\",\n",
    "    \"YMD\": \"South Yemen\",\n",
    "    \"YUG\": \"Yugoslavia\",\n",
    "    \"DDR\": \"East Germany\",\n",
    "    \"EU\": \"European Union\",\n",
    "    \"CSK\": \"Czechoslovakia\",\n",
    "    \"FSM\": \"Micronesia\",\n",
    "    \"KOR\": \"South Korea\"\n",
    "}\n",
    "\n",
    "code_to_name.update(custom_names)\n",
    "df_raw['country_name'] = df_raw['country_code'].map(code_to_name)\n",
    "\n",
    "# Check missing mappings\n",
    "missing = df_raw.loc[df_raw['country_name'].isna(), 'country_code'].unique()\n",
    "print(\"Missing codes:\", missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "383d2904-f015-4c9d-9aeb-51a94f03f950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   country_code                      country_name\n",
      "0           AFG                       Afghanistan\n",
      "1           ALB                           Albania\n",
      "2           ARE              United Arab Emirates\n",
      "3           ARG                         Argentina\n",
      "4           AUS                         Australia\n",
      "5           AUT                           Austria\n",
      "6           AZE                        Azerbaijan\n",
      "7           BEN                             Benin\n",
      "8           BGD                        Bangladesh\n",
      "9           BGR                          Bulgaria\n",
      "10          BHS                           Bahamas\n",
      "11          BIH            Bosnia and Herzegovina\n",
      "12          BLR                           Belarus\n",
      "13          BRA                            Brazil\n",
      "14          BRB                          Barbados\n",
      "15          CAF          Central African Republic\n",
      "16          CAN                            Canada\n",
      "17          CHL                             Chile\n",
      "18          CMR                          Cameroon\n",
      "19          COD      Democratic Republic of Congo\n",
      "20          CRI                        Costa Rica\n",
      "21          CUB                              Cuba\n",
      "22          CZE                           Czechia\n",
      "23          DDR                      East Germany\n",
      "24          DJI                          Djibouti\n",
      "25          DNK                           Denmark\n",
      "26          DOM                Dominican Republic\n",
      "27          ECU                           Ecuador\n",
      "28          EGY                             Egypt\n",
      "29          ERI                           Eritrea\n",
      "30          EST                           Estonia\n",
      "31          ETH                          Ethiopia\n",
      "32          FIN                           Finland\n",
      "33          FJI                              Fiji\n",
      "34          FRA                            France\n",
      "35          GHA                             Ghana\n",
      "36          GMB                            Gambia\n",
      "37          GRD                           Grenada\n",
      "38          GUY                            Guyana\n",
      "39          HRV                           Croatia\n",
      "40          HUN                           Hungary\n",
      "41          IND                             India\n",
      "42          IRQ                              Iraq\n",
      "43          ISL                           Iceland\n",
      "44          ISR                            Israel\n",
      "45          ITA                             Italy\n",
      "46          JOR                            Jordan\n",
      "47          KEN                             Kenya\n",
      "48          KHM                          Cambodia\n",
      "49          LTU                         Lithuania\n",
      "50          LUX                        Luxembourg\n",
      "51          MEX                            Mexico\n",
      "52          MHL                  Marshall Islands\n",
      "53          MMR                           Myanmar\n",
      "54          MNG                          Mongolia\n",
      "55          MUS                         Mauritius\n",
      "56          MWI                            Malawi\n",
      "57          MYS                          Malaysia\n",
      "58          NER                             Niger\n",
      "59          NGA                           Nigeria\n",
      "60          PAK                          Pakistan\n",
      "61          PER                              Peru\n",
      "62          PHL                       Philippines\n",
      "63          PNG                  Papua New Guinea\n",
      "64          PRT                          Portugal\n",
      "65          PRY                          Paraguay\n",
      "66          SEN                           Senegal\n",
      "67          SLB                   Solomon Islands\n",
      "68          SLV                       El Salvador\n",
      "69          SVK                          Slovakia\n",
      "70          SWE                            Sweden\n",
      "71          TGO                              Togo\n",
      "72          THA                          Thailand\n",
      "73          TJK                        Tajikistan\n",
      "74          TUR                           Türkiye\n",
      "75          TUV                            Tuvalu\n",
      "76          TZA                          Tanzania\n",
      "77          URY                           Uruguay\n",
      "78          USA                     United States\n",
      "79          VCT  Saint Vincent and the Grenadines\n",
      "80          VEN                         Venezuela\n",
      "81          WSM                             Samoa\n",
      "82          ZAF                      South Africa\n",
      "83          ZMB                            Zambia\n"
     ]
    }
   ],
   "source": [
    "# == Check country names and structure\n",
    "\n",
    "df_raw.head() \n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(df_raw[['country_code', 'country_name']].drop_duplicates().sort_values('country_code').reset_index(drop=True))\n",
    "# Reset to default afterward\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5928538b-8519-40e2-bdee-ac72024f9f48",
   "metadata": {},
   "source": [
    "#### New Variable: Length of speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "911e2229-337f-4479-984c-3a6ab9dff5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average speech length (words): 2756.86\n",
      "20 shortest speeches:\n",
      "           filename        country_name  year  speech_length_words\n",
      "65  LTU_57_2002.txt           Lithuania  2002                  767\n",
      "49  TUR_03_1948.txt             Türkiye  1948                  817\n",
      "6   PER_71_2016.txt                Peru  2016                  975\n",
      "26  EST_55_2000.txt             Estonia  2000                  997\n",
      "15  ALB_70_2015.txt             Albania  2015                 1097\n",
      "97  ERI_50_1995.txt             Eritrea  1995                 1129\n",
      "58  PRY_12_1957.txt            Paraguay  1957                 1161\n",
      "89  FIN_78_2023.txt             Finland  2023                 1315\n",
      "2   TUV_65_2010.txt              Tuvalu  2010                 1390\n",
      "42  DOM_60_2005.txt  Dominican Republic  2005                 1409\n",
      "57  MWI_61_2006.txt              Malawi  2006                 1410\n",
      "29  ISL_63_2008.txt             Iceland  2008                 1449\n",
      "92  MMR_65_2010.txt             Myanmar  2010                 1457\n",
      "63  SVK_73_2018.txt            Slovakia  2018                 1532\n",
      "51  ECU_60_2005.txt             Ecuador  2005                 1539\n",
      "30  HUN_57_2002.txt             Hungary  2002                 1548\n",
      "41  TJK_71_2016.txt          Tajikistan  2016                 1551\n",
      "3   MHL_59_2004.txt    Marshall Islands  2004                 1576\n",
      "20  GRD_70_2015.txt             Grenada  2015                 1607\n",
      "52  BHS_78_2023.txt             Bahamas  2023                 1628\n",
      "\n",
      "20 longest speeches:\n",
      "           filename                  country_name  year  speech_length_words\n",
      "50  IND_12_1957.txt                         India  1957                14927\n",
      "83  ETH_21_1966.txt                      Ethiopia  1966                 6548\n",
      "62  AUS_16_1961.txt                     Australia  1961                 6530\n",
      "75  GHA_18_1963.txt                         Ghana  1963                 6352\n",
      "96  BLR_19_1964.txt                       Belarus  1964                 5589\n",
      "71  EGY_35_1980.txt                         Egypt  1980                 5528\n",
      "82  ISR_17_1962.txt                        Israel  1962                 5266\n",
      "93  PAK_03_1948.txt                      Pakistan  1948                 4673\n",
      "18  BGD_34_1979.txt                    Bangladesh  1979                 4614\n",
      "24  COD_44_1989.txt  Democratic Republic of Congo  1989                 4471\n",
      "98  MUS_47_1992.txt                     Mauritius  1992                 4440\n",
      "9   JOR_40_1985.txt                        Jordan  1985                 4311\n",
      "95  BLR_41_1986.txt                       Belarus  1986                 4208\n",
      "64  GMB_50_1995.txt                        Gambia  1995                 4102\n",
      "61  SEN_41_1986.txt                       Senegal  1986                 3948\n",
      "37  DDR_34_1979.txt                  East Germany  1979                 3916\n",
      "74  CHL_05_1950.txt                         Chile  1950                 3874\n",
      "44  BGD_41_1986.txt                    Bangladesh  1986                 3840\n",
      "31  ZAF_50_1995.txt                  South Africa  1995                 3671\n",
      "1   CRI_39_1984.txt                    Costa Rica  1984                 3663\n"
     ]
    }
   ],
   "source": [
    "# Add a new column: speech length in words\n",
    "df_raw['speech_length_words'] = df_raw['speech'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Calculate average length\n",
    "avg_length = df_raw['speech_length_words'].mean()\n",
    "print(\"Average speech length (words):\", round(avg_length, 2))\n",
    "\n",
    "# 20 shortest & longest speeches\n",
    "print(\"20 shortest speeches:\")\n",
    "print(df_raw.nsmallest(20, 'speech_length_words')[['filename', 'country_name', 'year', 'speech_length_words']])\n",
    "\n",
    "print(\"\\n20 longest speeches:\")\n",
    "print(df_raw.nlargest(20, 'speech_length_words')[['filename', 'country_name', 'year', 'speech_length_words']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424f152-b786-45bf-bf86-e169a900f628",
   "metadata": {},
   "source": [
    "#### New variable: English as Official Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d1d2c2a6-23d6-4044-84cc-8785167e6b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries not matched in df_raw['country_name']:\n",
      "Anguilla\n",
      "Antigua and Barbuda\n",
      "Belize\n",
      "Belgium\n",
      "Bermuda\n",
      "Botswana\n",
      "British Virgin Islands\n",
      "Burundi\n",
      "Cayman Islands\n",
      "Christmas Island\n",
      "Cook Islands\n",
      "Dominica\n",
      "Hong Kong\n",
      "Ireland\n",
      "Jersey\n",
      "Liberia\n",
      "Malta\n",
      "Micronesia\n",
      "Namibia\n",
      "New Zealand\n",
      "Niue\n",
      "Norfolk Island\n",
      "Northern Mariana Islands\n",
      "Palau\n",
      "Pitcairn Islands\n",
      "Rwanda\n",
      "Saint Kitts and Nevis\n",
      "Saint Lucia\n",
      "Seychelles\n",
      "Sierra Leone\n",
      "Singapore\n",
      "Sint Maarten\n",
      "Somalia\n",
      "South Sudan\n",
      "Sudan\n",
      "Eswatini\n",
      "Tonga\n",
      "Trinidad and Tobago\n",
      "Turks and Caicos Islands\n",
      "Uganda\n",
      "Zimbabwe\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "      <th>country_code</th>\n",
       "      <th>year</th>\n",
       "      <th>country_name</th>\n",
       "      <th>speech_length_words</th>\n",
       "      <th>english_official_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WSM_32_1977.txt</td>\n",
       "      <td>﻿1.\\tFirst I must extend to you, Sir, the warm...</td>\n",
       "      <td>WSM</td>\n",
       "      <td>1977</td>\n",
       "      <td>Samoa</td>\n",
       "      <td>2362</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CRI_39_1984.txt</td>\n",
       "      <td>﻿It is an honour and a great pleasure for me t...</td>\n",
       "      <td>CRI</td>\n",
       "      <td>1984</td>\n",
       "      <td>Costa Rica</td>\n",
       "      <td>3663</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TUV_65_2010.txt</td>\n",
       "      <td>At the dawn of the new \\nmillennium 10 years a...</td>\n",
       "      <td>TUV</td>\n",
       "      <td>2010</td>\n",
       "      <td>Tuvalu</td>\n",
       "      <td>1390</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MHL_59_2004.txt</td>\n",
       "      <td>I am honoured to address the\\nGeneral Assembly...</td>\n",
       "      <td>MHL</td>\n",
       "      <td>2004</td>\n",
       "      <td>Marshall Islands</td>\n",
       "      <td>1576</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>THA_53_1998.txt</td>\n",
       "      <td>On behalf of the\\nGovernment and the people of...</td>\n",
       "      <td>THA</td>\n",
       "      <td>1998</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>2933</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech  \\\n",
       "0  WSM_32_1977.txt  ﻿1.\\tFirst I must extend to you, Sir, the warm...   \n",
       "1  CRI_39_1984.txt  ﻿It is an honour and a great pleasure for me t...   \n",
       "2  TUV_65_2010.txt  At the dawn of the new \\nmillennium 10 years a...   \n",
       "3  MHL_59_2004.txt  I am honoured to address the\\nGeneral Assembly...   \n",
       "4  THA_53_1998.txt  On behalf of the\\nGovernment and the people of...   \n",
       "\n",
       "  country_code  year      country_name  speech_length_words  \\\n",
       "0          WSM  1977             Samoa                 2362   \n",
       "1          CRI  1984        Costa Rica                 3663   \n",
       "2          TUV  2010            Tuvalu                 1390   \n",
       "3          MHL  2004  Marshall Islands                 1576   \n",
       "4          THA  1998          Thailand                 2933   \n",
       "\n",
       "   english_official_language  \n",
       "0                          1  \n",
       "1                          0  \n",
       "2                          1  \n",
       "3                          1  \n",
       "4                          0  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source for english as official language : https://gradschool.utk.edu/future-students/office-of-graduate-admissions/applying-to-graduate-school/admission-requirements/testing-requirements/countries-with-english-as-official-language/\n",
    "# They are quoting: https://www.cia.gov/the-world-factbook/field/languages/\n",
    "\n",
    "english_countries = [\n",
    "    \"Anguilla\", \"Antigua and Barbuda\", \"Bahamas\", \"Barbados\", \"Belize\", \"Belgium\",\n",
    "    \"Bermuda\", \"Botswana\", \"British Virgin Islands\", \"Burundi\", \"Cameroon\", \"Canada\",\n",
    "    \"Cayman Islands\", \"Christmas Island\", \"Cook Islands\", \"Dominica\", \"Fiji\", \"Gambia\",\n",
    "    \"Ghana\", \"Grenada\", \"Guyana\", \"Hong Kong\", \"India\", \"Ireland\", \"Jersey\", \"Kenya\",\n",
    "    \"Liberia\", \"Malawi\", \"Malta\", \"Marshall Islands\", \"Micronesia\",\n",
    "    \"Namibia\", \"New Zealand\", \"Nigeria\", \"Niue\", \"Norfolk Island\", \"Northern Mariana Islands\",\n",
    "    \"Pakistan\", \"Palau\", \"Papua New Guinea\", \"Philippines\", \"Pitcairn Islands\", \"Rwanda\",\n",
    "    \"Saint Kitts and Nevis\", \"Saint Lucia\", \"Samoa\", \"Seychelles\", \"Sierra Leone\", \"Singapore\",\n",
    "    \"Sint Maarten\", \"Solomon Islands\", \"Somalia\", \"South Africa\", \"South Sudan\", \"Sudan\",\n",
    "    \"Eswatini\", \"Tanzania\", \"Tonga\", \"Trinidad and Tobago\", \"Turks and Caicos Islands\",\n",
    "    \"Tuvalu\", \"Uganda\", \"Zambia\", \"Zimbabwe\"\n",
    "]\n",
    "\n",
    "# Create dummy column\n",
    "df_raw['english_official_language'] = df_raw['country_name'].apply(\n",
    "    lambda x: 1 if x in english_countries else 0\n",
    ")\n",
    "\n",
    "# Detect unmatched countries \n",
    "matched = set(df_raw['country_name'])\n",
    "unmatched = [country for country in english_countries if country not in matched]\n",
    "\n",
    "print(\"Countries not matched in df_raw['country_name']:\")\n",
    "for country in unmatched:\n",
    "    print(country)\n",
    "\n",
    "# All of these countries are either British Overseas Territories, Australian Territories, self-governing island territories or Special Administrative Regions\n",
    "    # None of the unmatched regions are UN Members\n",
    "\n",
    "# Check df with new variable english_official_language\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82540d8a-b656-45fb-aeb0-2fd5b4d7108a",
   "metadata": {},
   "source": [
    "#### New variable: Permanent member security council"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4fb92fe4-5c22-43fa-8b48-62ec2240d716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   country_code   country_name  security_council_permanent  year\n",
      "55          USA  United States                           1  2023\n",
      "99          FRA         France                           1  1996\n"
     ]
    }
   ],
   "source": [
    "# Define permanent members of the UN Security Council and create dummy\n",
    "permanent_members = ['RUS', 'USA', 'FRA', 'GBR', 'CHN']\n",
    "\n",
    "df_raw['security_council_permanent'] = df_raw['country_code'].isin(permanent_members).astype(int)\n",
    "\n",
    "print(df_raw[df_raw['country_code'].isin(permanent_members)][\n",
    "    ['country_code', 'country_name', 'security_council_permanent', 'year']\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523547d9-9b10-473c-8c85-d1e2fc4fd3e1",
   "metadata": {},
   "source": [
    "#### New variables: Speaker, Position & Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "de783a81-efbf-49a6-96f6-ae6a93fa1573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [filename, year, country_code, country_name]\n",
      "Index: []\n",
      "0 rows could not be matched\n",
      "    gender_dummy  count\n",
      "0       0 (male)     46\n",
      "1     1 (female)      4\n",
      "2  NaN (unknown)     50\n"
     ]
    }
   ],
   "source": [
    "# Supplmentary xlsx-file from the UN Dataset provides information on the speaker and their position\n",
    "\n",
    "####### CHECK IF THIS WOULD WORK IN A REPLICATION #####################################\n",
    "\n",
    "# == Create variable speaker_name and position ==\n",
    "df_speakers = pd.read_excel(os.path.join(data_c, \"data_original\", \"UN General Debate Corpus\", \"Speakers_by_session.xlsx\"))\n",
    "\n",
    "df_speakers.head()\n",
    "\n",
    "# Merge new infrormation to dataframe\n",
    "df_merged = df_raw.merge(\n",
    "    df_speakers[['Year', 'ISO Code', 'Name of Person Speaking', 'Post']],\n",
    "    left_on=['year', 'country_code'],\n",
    "    right_on=['Year', 'ISO Code'],\n",
    "    how='left',\n",
    "    indicator=True)\n",
    "\n",
    "# Detect unmatched rows\n",
    "unmatched = df_merged[df_merged['_merge'] == 'left_only']\n",
    "unmatched_count = (df_merged['_merge'] == 'left_only').sum()\n",
    "\n",
    "print(unmatched[['filename', 'year', 'country_code', 'country_name']])\n",
    "print(f\"{unmatched_count} rows could not be matched\")\n",
    "\n",
    "# Clean up \n",
    "df_merged = df_merged.drop(columns=['Year', 'ISO Code', '_merge']).rename(columns={\n",
    "    'Name of Person Speaking': 'speaker_name',\n",
    "    'Post': 'position'\n",
    "})\n",
    "\n",
    "# == Create gender dummy ==\n",
    "df_merged['gender_dummy'] = df_merged['speaker_name'].apply(\n",
    "    lambda name: 0 if pd.notnull(name) and re.search(r'^(?:Mr|Sir)\\b', name, re.IGNORECASE)\n",
    "    else 1 if pd.notnull(name) and re.search(r'^(?:Mrs|Ms)\\b', name, re.IGNORECASE)\n",
    "    else None\n",
    ")\n",
    "\n",
    "# Count all values including NaN\n",
    "counts = df_merged['gender_dummy'].value_counts(dropna=False)\n",
    "\n",
    "# Build summary using .get() to handle missing keys\n",
    "gender_summary = pd.DataFrame({\n",
    "    'gender_dummy': ['0 (male)', '1 (female)', 'NaN (unknown)'],\n",
    "    'count': [\n",
    "        counts.get(0, 0),\n",
    "        counts.get(1, 0),\n",
    "        counts.get(np.nan, 0)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(gender_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3cfdc9-d972-454b-9719-3285b1e5d3e2",
   "metadata": {},
   "source": [
    "Looking at the structure, highest position always seems to be mentioned first --> drop everything else if speaker has more than one position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6a1ce3d9-a03d-46f6-90aa-923660bc4474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmatched position: Chairman of the Presidency\n",
      "Unmatched position: Cabinet Secretary for Foreign Affairs and International Trade\n",
      "Unmatched position: Union Minister for the Office of the State Counsellor\n"
     ]
    }
   ],
   "source": [
    "# == Adjust position variable\n",
    "def normalize_position(pos):\n",
    "    if pd.isna(pos):\n",
    "        return pos\n",
    "\n",
    "    pos = pos.strip()\n",
    "\n",
    "    # --- Fix common typos and extra spaces ---\n",
    "    pos = re.sub(r'\\s+', ' ', pos)  # collapse multiple spaces\n",
    "    pos_lower = pos.lower()\n",
    "\n",
    "     # Turn all ministers that deal with foreign affairs and international relations to \"Minister for Foreign Affairs\n",
    "    foreign_affairs_variants = [\n",
    "        'minister for foregn affairs',\n",
    "        'minister responsible for foreign affairs',\n",
    "        'minsiter for foreign and caricom affairs',\n",
    "        'minister for external affairs',\n",
    "        'minister of external relations',  # <-- added\n",
    "        'foreign minister',\n",
    "        'minister for international affairs and cooperation',\n",
    "        'minister for external relations',\n",
    "        'federal minister for european and international affairs',\n",
    "        'international cooperation',\n",
    "        'federal minister for foreign affairs',\n",
    "        'minister for foreign and caricom affairs',\n",
    "        'minister of foreign affairs and cooperation',\n",
    "        'minister for international relations and cooperation',\n",
    "        'ministry of external relations',\n",
    "        'acting minister for foreign affairs and international cooperation',\n",
    "        'ministry of foreign affairs',\n",
    "        'minister for foreign and political affairs',\n",
    "        'federal minister for europe, integration, and foreign affairs',\n",
    "        'federal minister for europe, integration and foreign affairs',\n",
    "        'minister of foreign and european affaris',\n",
    "        'minister of foreign affairs',\n",
    "        'minister for foreign',\n",
    "        'minister of foreign and european affairs and minister of immigration and asylum',\n",
    "        'minister for foreign affairs and senegalese living abroad',\n",
    "        'minister for foreign affairs with responsibility for brexit',\n",
    "        'minister for foreign affairs and investment promotion'\n",
    "       \n",
    "    ]\n",
    "    if any(variant in pos_lower for variant in foreign_affairs_variants):\n",
    "        return \"Minister for Foreign Affairs\"\n",
    "\n",
    "    # --- Fix \"rime minister\" typo ---\n",
    "    pos = re.sub(r'(?i)\\brime[- ]?minister\\b', 'Prime Minister', pos)\n",
    "\n",
    "    # Normalize different versions of Head of Government, President, Prime Minsiter and Vice-President-\n",
    "    exact_matches = {\n",
    "        r'(?i)^president of (the )?government$': 'Head of Government',\n",
    "        r'(?i)^acting president$': 'President',\n",
    "        r'(?i)^interim president$': 'President',\n",
    "        r'(?i)^constitutional president$': 'President',\n",
    "        r'(?i)^first executive president$': 'President',\n",
    "        r'(?i)^first prime[- ]?minister$': 'Prime Minister',\n",
    "        r'(?i)^head of the goverment$': 'Head of Government',  # <-- catch typo + spaces\n",
    "        r'(?i)^head\\s+of\\s+govern?ment$': 'Head of Government',\n",
    "        r'(?i)^first vice[- ]?president$': 'Vice-President'\n",
    "    }\n",
    "    for pattern, replacement in exact_matches.items():\n",
    "        if re.fullmatch(pattern, pos):\n",
    "            return replacement\n",
    "\n",
    "    # --- Normalize prefixes ---\n",
    "    pos = re.sub(r'(?i)^first vice[- ]?president\\b', 'Vice-President', pos)\n",
    "    pos = re.sub(r'(?i)\\bprime[- ]?minister\\b', 'Prime Minister', pos)\n",
    "    pos = re.sub(r'(?i)\\bpresident\\b', 'President', pos)\n",
    "    pos = re.sub(r'(?i)\\bvice[- ]?president\\b', 'Vice-President', pos)\n",
    "\n",
    "    # --- Collapse primary roles if they appear at start ---\n",
    "    primary_roles = [\n",
    "        (r'(?i)^prime[- ]?minister\\b', 'Prime Minister'),\n",
    "        (r'(?i)^deputy prime[- ]?minister\\b', 'Deputy Prime Minister'),\n",
    "        (r'(?i)^president\\b', 'President'),\n",
    "        (r'(?i)^vice[- ]?president\\b', 'Vice-President'),\n",
    "        (r'(?i)^head of state\\b', 'Head of State'),\n",
    "        (r'(?i)^(crown prince|prince|king|emir|amir)\\b', 'Monarch'),\n",
    "        (r'(?i)^(un representative|permanent representative|delegation|chair of (the )?delegation|chair of diplomatic representative)\\b', 'Diplomatic Representative')\n",
    "    ]\n",
    "    for pattern, replacement in primary_roles:\n",
    "        if re.match(pattern, pos):\n",
    "            return replacement\n",
    "\n",
    "    # --- Monarchs ---\n",
    "    if re.search(r'(?i)\\b(crown prince|prince|king|emir|amir)\\b', pos):\n",
    "        return \"Monarch\"\n",
    "\n",
    "    # --- Head of State ---\n",
    "    if re.search(r'(?i)head of state', pos):\n",
    "        return \"Head of State\"\n",
    "        \n",
    "    # --- Diplomatic Representatives ---\n",
    "    if re.search(r'(?i)(un representative|permanent representative|delegation|chair of (the )?delegation|chair of diplomatic representative)', pos):\n",
    "        return \"Diplomatic Representative\"\n",
    "\n",
    "    # --- Everything else ---\n",
    "    print(\"Unmatched position:\", pos)  # print before assigning Others\n",
    "    return \"Others\"\n",
    "\n",
    "# Apply\n",
    "df_merged[\"position\"] = df_merged[\"position\"].apply(normalize_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7a99b3df-550a-41a3-a751-2692c0f7ea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_positions(pos):\n",
    "    if pd.isna(pos):\n",
    "        return pos  # keep NaN\n",
    "    \n",
    "    if pos in [\"Prime Minister\", \"Deputy Prime Minister\"]:\n",
    "        return \"(Deputy) Prime Minister\"\n",
    "    \n",
    "    if pos in [\"President\", \"Vice-President\"]:\n",
    "        return \"(Vice-) President\"\n",
    "        \n",
    "    if pos in [\"Minister for Foreign Affairs\", \"Deputy Minister for Foreign Affairs\",\n",
    "        \"Deputy Minister Foreign Affairs\",\n",
    "        \"Second Minister for Foreign Affairs\",\n",
    "        \"Second Minister for Foreign Affairs and Trade\",\n",
    "        \"Vice Minister for Foreign Affairs\"]:\n",
    "        return \"(Deputy) Minister for Foreign Affairs\"\n",
    "    \n",
    "    return pos\n",
    "\n",
    "df_merged[\"position\"] = df_merged[\"position\"].apply(merge_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "14ececb8-262d-4270-8d52-5742a864cf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN                                      37\n",
      "(Deputy) Minister for Foreign Affairs    27\n",
      "(Vice-) President                        19\n",
      "(Deputy) Prime Minister                   9\n",
      "Diplomatic Representative                 4\n",
      "Others                                    3\n",
      "Monarch                                   1\n",
      "Name: position, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Pandas so einstellen, dass es alles ausgibt\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "# Alle Positionen mit Häufigkeit\n",
    "position_counts = df_merged['position'].value_counts(dropna=False)\n",
    "\n",
    "print(position_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "df4b9986-4599-46cc-8d2b-c49667f86e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      total_rows  missing  not_missing\n",
      "year                                  \n",
      "1948           2        2            0\n",
      "1950           2        2            0\n",
      "1956           1        1            0\n",
      "1957           2        2            0\n",
      "1958           1        1            0\n",
      "1961           1        1            0\n",
      "1962           1        1            0\n",
      "1963           3        3            0\n",
      "1964           1        1            0\n",
      "1966           1        1            0\n",
      "1968           1        1            0\n",
      "1972           1        1            0\n",
      "1974           1        0            1\n",
      "1975           1        1            0\n",
      "1977           2        1            1\n",
      "1979           4        4            0\n",
      "1980           2        2            0\n",
      "1984           1        1            0\n",
      "1985           2        1            1\n",
      "1986           3        3            0\n",
      "1988           1        1            0\n",
      "1989           3        2            1\n",
      "1990           1        1            0\n",
      "1992           1        1            0\n",
      "1993           2        2            0\n",
      "1994           1        0            1\n",
      "1995           4        0            4\n",
      "1996           2        0            2\n",
      "1997           2        0            2\n",
      "1998           3        0            3\n",
      "2000           1        0            1\n",
      "2001           1        0            1\n",
      "2002           2        0            2\n",
      "2004           2        0            2\n",
      "2005           2        0            2\n",
      "2006           3        0            3\n",
      "2007           1        0            1\n",
      "2008           2        0            2\n",
      "2009           4        0            4\n",
      "2010           3        0            3\n",
      "2011           1        0            1\n",
      "2012           3        0            3\n",
      "2013           1        0            1\n",
      "2015           3        0            3\n",
      "2016           5        0            5\n",
      "2017           3        0            3\n",
      "2018           2        0            2\n",
      "2019           2        0            2\n",
      "2020           2        0            2\n",
      "2021           1        0            1\n",
      "2023           3        0            3\n"
     ]
    }
   ],
   "source": [
    "# Started to document positions properly from 1986 on, before yearly sample size per year mostly less than 20 samples\n",
    "\n",
    "yearly_counts = df_merged.groupby('year')['position'].agg(\n",
    "    total_rows='size',\n",
    "    missing=lambda x: x.isna().sum()\n",
    ")\n",
    "\n",
    "# Add not_missing column\n",
    "yearly_counts['not_missing'] = yearly_counts['total_rows'] - yearly_counts['missing']\n",
    "\n",
    "\n",
    "# Print the entire table\n",
    "pd.set_option('display.max_rows', None)  # show all rows\n",
    "print(yearly_counts)\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41d97e2-840b-40eb-b032-197040a0246c",
   "metadata": {},
   "source": [
    "#### New Variable: Country (Year)\n",
    "\n",
    "This variable is later needed to create clean description plots and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d647e412-7bd4-4190-9c34-39874b4620a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_merged.copy()\n",
    "df_merged['speech_label'] = df_merged['country_name'] + \" (\" + df_merged['year'].astype(str) + \")\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc05d65d-be72-42ae-b753-0940de5df17c",
   "metadata": {},
   "source": [
    "#### Save dataframe with all new variables as un_corpus_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "94a901c8-3fc4-4b33-a1ff-cede22bd196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(wd)\n",
    "\n",
    "# Save df_merged as a pickle file for quick future loading\n",
    "merged_pickle_path = r\".\\data\\un_corpus_merged.pkl\"\n",
    "df_merged.to_pickle(merged_pickle_path)\n",
    "\n",
    "# Export df as CSV \n",
    "merged_output_path = r\".\\data\\un_corpus_merged.csv\"\n",
    "df_merged.to_csv(merged_output_path, index=False, sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea36f19a-789d-4c62-a470-3e73a6d848bf",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe0f4af-e939-4100-9618-5b522bdd72a1",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2661f431-a194-47d1-9a1c-89162bf85a91",
   "metadata": {},
   "source": [
    "#### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cd37e08d-b4f2-459e-823c-3816a825ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Clean text by removing empty spaces, line breaks, hyphenation, stray characters, and escape quote ==\n",
    "\n",
    "def cleaning(content):\n",
    "    if pd.isna(content):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove line breaks and carriage returns\n",
    "    content = content.replace('\\n', ' ').replace('\\r', ' ')\n",
    "\n",
    "    # Collapse multiple spaces\n",
    "    content = ' '.join(content.split())\n",
    "\n",
    "    # Ensure spacing after punctuation\n",
    "    content = re.sub(r'(?<=[.,])(?=[^\\s])', r' ', content)\n",
    "\n",
    "    # Remove hyphenation at line breaks (e.g. \"inter- national\" → \"international\")\n",
    "    content = re.sub(r'-\\s', '', content)\n",
    "\n",
    "     # Replace hyphen between letters with a space to prevent merging words (e.g. \"russian-and\" → \"russian and\")\n",
    "    content = re.sub(r'(?<=\\w)-(?=\\w)', ' ', content)\n",
    "\n",
    "    # Remove stray backslashes\n",
    "    content = content.replace(\"\\\\\", \"\")\n",
    "\n",
    "    return content\n",
    "\n",
    "# Apply cleaning to each speech\n",
    "df_merged['speech'] = df_merged['speech'].astype(str)  # Ensure column is string type\n",
    "df_clean = df_merged.copy()\n",
    "df_clean['speech'] = df_clean['speech'].apply(cleaning)\n",
    "\n",
    "# Drop rows with empty speeches after cleaning\n",
    "df_clean = df_clean[df_clean['speech'].str.strip().astype(bool)].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7c6bd148-d5cf-4eb3-a66d-fc706e99c13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved clean speeches chunks in 'C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp'\n"
     ]
    }
   ],
   "source": [
    "# == Split cleaned data into chunks and save as separate files ==\n",
    "\n",
    "# Convert cleaned DataFrame to list of lists\n",
    "clean_data = df_clean[['filename', 'speech']].values.tolist()\n",
    "\n",
    "# Split cleaned data into 4 equal chunks\n",
    "data_id1 = clean_data[:int(len(clean_data)/4)]\n",
    "data_id2 = clean_data[int(len(clean_data)/4): int(2*len(clean_data)/4)]\n",
    "data_id3 = clean_data[int(2*len(clean_data)/4): int(3*len(clean_data)/4)]\n",
    "data_id4 = clean_data[int(3*len(clean_data)/4):]\n",
    "\n",
    "# Change directory to the temp folder\n",
    "os.chdir(data_temp)  # make sure `data_temp` exists and is defined\n",
    "\n",
    "# Save each chunk with joblib\n",
    "joblib.dump(data_id1, 'clean_speeches_indexed1.pkl')\n",
    "joblib.dump(data_id2, 'clean_speeches_indexed2.pkl')\n",
    "joblib.dump(data_id3, 'clean_speeches_indexed3.pkl')\n",
    "joblib.dump(data_id4, 'clean_speeches_indexed4.pkl')\n",
    "\n",
    "# Store list of cleaned data chunk paths to feed into preprocessing function later\n",
    "clean_files = [\n",
    "    os.path.join(data_temp, 'clean_speeches_indexed1.pkl'),\n",
    "    os.path.join(data_temp, 'clean_speeches_indexed2.pkl'),\n",
    "    os.path.join(data_temp, 'clean_speeches_indexed3.pkl'),\n",
    "    os.path.join(data_temp, 'clean_speeches_indexed4.pkl')\n",
    "]\n",
    "\n",
    "print(f\"Saved clean speeches chunks in '{data_temp}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6e5109-8cf7-41e3-8fb9-8e7dde29b4e0",
   "metadata": {},
   "source": [
    "### Advanced Text Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfca8dd9-c6d7-48c7-96b0-f5a449814336",
   "metadata": {},
   "source": [
    "#### Extend stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "11d056b5-6729-45f8-a233-3e8e6de1f69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 326 stopwords to C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\stopwords.pkl\n"
     ]
    }
   ],
   "source": [
    "# Get SpaCy stopwords\n",
    "SPACY_STOPWORDS = list(nlp.Defaults.stop_words)\n",
    "\n",
    "# Path to save\n",
    "stopwords_path = os.path.join(data_c, \"stopwords.pkl\")\n",
    "\n",
    "# Save stopwords\n",
    "joblib.dump(SPACY_STOPWORDS, stopwords_path)\n",
    "\n",
    "print(f\"Saved {len(SPACY_STOPWORDS)} stopwords to {stopwords_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0aa75944-4b75-4aa4-9430-9aa6bd9ac934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Define function to Tokenize, eliminate digits, remove stopwords, lemmatize, POS-Tagging ==\n",
    "\n",
    "def pro1(lista):\n",
    "    # Remove punctuation\n",
    "    return [[row[0], row[1].translate(translator)] for row in lista]\n",
    "\n",
    "def pro2(lista):\n",
    "    # Tokenize and lowercase with gensim\n",
    "    return [[row[0], gensim.utils.simple_preprocess(row[1])] for row in lista]\n",
    "\n",
    "def pro3(lista):\n",
    "    # Remove tokens that are only digits\n",
    "        a = [[row[0], [w for w in row[1] if not w.isdigit()]] for row in lista]\n",
    "        return a\n",
    "    \n",
    "def pro4(lista):\n",
    "    # Drop short words\n",
    "    return [[row[0], [w for w in row[1] if len(w) > 2]] for row in lista]\n",
    "\n",
    "def tags_spacy(lista):\n",
    "    texts = [' '.join(row[1]) for row in lista]\n",
    "    docs = list(nlp.pipe(texts, batch_size=20, n_process=1))\n",
    "    result = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        filtered_tokens = [token.text for token in doc if token.tag_.startswith(('N', 'V', 'J'))]\n",
    "        result.append([lista[i][0], filtered_tokens])\n",
    "    return result\n",
    "\n",
    "\n",
    "def pro5(lista):\n",
    "    # Remove stopwords using SpaCy stopword list\n",
    "    return [[row[0], [w for w in row[1] if w not in SPACY_STOPWORDS]] for row in lista]\n",
    "\n",
    "def pro6(lista):\n",
    "      return [\n",
    "        [row[0], [stemmer.stem(token) for token in row[1]]]\n",
    "        for row in lista\n",
    "    ]\n",
    "########################## Question for Max: They removed procedural words in the paper\n",
    "\n",
    "def dropnull(lista):\n",
    "    # Drop empty speeches\n",
    "    return [row for row in lista if len(' '.join(row[1])) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "524a274a-004c-4d87-9bb3-5f789a42b90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl] Before tagging: 1.71s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl] After tagging: 71.08s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed1.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl] Done. Total time: 82.56s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl] Before tagging: 2.01s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl] After tagging: 70.45s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed2.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl] Done. Total time: 83.43s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl] Before tagging: 1.93s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl] After tagging: 67.14s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed3.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl] Done. Total time: 79.55s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl] Before tagging: 1.98s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl] After tagging: 83.21s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed4.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl] Done. Total time: 96.78s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# == Create full pre-processing function and call it\n",
    "\n",
    "def preprocessing(data_name):\n",
    "    t0 = time.time()\n",
    "    print(f\"Starting preprocessing for {data_name}...\")\n",
    "\n",
    "    data = joblib.load(data_name)\n",
    "    data = pro1(data)\n",
    "    data = pro2(data)\n",
    "    data = pro3(data)\n",
    "    data = pro4(data)\n",
    "\n",
    "    print(f\"[{data_name}] Before tagging: {time.time() - t0:.2f}s\")\n",
    "    data = tags_spacy(data)\n",
    "    print(f\"[{data_name}] After tagging: {time.time() - t0:.2f}s\")\n",
    "\n",
    "    data = pro5(data)\n",
    "   # data = pro6(data)\n",
    "    data = dropnull(data)\n",
    "\n",
    "    # out_name = data_name.replace('cleanspeeches_', 'preprocessed_speeches_').replace('.pkl', '_temp.pkl')\n",
    "\n",
    "    # Store preprocessed corupus (before stemming) for wordcloud\n",
    "    filename_wordcloud = data_name.replace('clean_speeches_', 'wordcloud_speeches_').replace('.pkl', '.pkl')\n",
    "    out_name_wordcloud = os.path.join(data_preprocessed, os.path.basename(filename_wordcloud))\n",
    "    joblib.dump(data, out_name_wordcloud)\n",
    "\n",
    "    # Apply stemming\n",
    "    data_stemmed = pro6(data)\n",
    "\n",
    "    filename_preprocessed = data_name.replace('clean_speeches_', 'preprocessed_speeches_').replace('.pkl', '.pkl')\n",
    "    out_preprocessed = os.path.join(data_preprocessed, os.path.basename(filename_preprocessed))\n",
    "    joblib.dump(data_stemmed, out_preprocessed\n",
    "               )\n",
    "    print(f\"[{data_name}] Saved stemmed version: {out_preprocessed}\")\n",
    "\n",
    "    print(f\"[{data_name}] Done. Total time: {time.time() - t0:.2f}s\\n\")\n",
    "\n",
    "def main():\n",
    "    for fname in clean_files:\n",
    "        preprocessing(fname)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d3409987-485c-47c9-9430-01fa2c0e650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the pre-processed data\n",
    "preprocessed_files = [\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed1.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed2.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed3.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed4.pkl')\n",
    "]\n",
    "\n",
    "wordcloud_files = [\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed1.pkl'),\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed2.pkl'),\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed3.pkl'),\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed4.pkl')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "134ddbf7-9b95-4697-9d9c-f9c55b52ec80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73db5f7-566d-4276-a631-c1f33fab494d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d006e-6a23-453b-9c91-0e60bd3f8301",
   "metadata": {},
   "source": [
    "## Word-Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf2443e-4c76-41b2-bc16-e72d91c318a8",
   "metadata": {},
   "source": [
    "### Count frequencies of all tokens and display the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "154d978d-cddd-431c-9d53-73c9faf8fdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Stemmed] Top 50 most common words:\n",
      "nation: 21947\n",
      "unit: 18917\n",
      "countri: 16487\n",
      "intern: 15244\n",
      "develop: 13528\n",
      "state: 12937\n",
      "peac: 12471\n",
      "world: 12267\n",
      "peopl: 12013\n",
      "secur: 8200\n",
      "general: 7574\n",
      "govern: 7386\n",
      "econom: 6870\n",
      "organ: 6551\n",
      "year: 6163\n",
      "right: 6139\n",
      "assembl: 5964\n",
      "new: 5573\n",
      "effort: 5522\n",
      "problem: 5257\n",
      "support: 5198\n",
      "human: 5178\n",
      "continu: 5149\n",
      "communiti: 4752\n",
      "region: 4503\n",
      "time: 4475\n",
      "polit: 4447\n",
      "africa: 4264\n",
      "member: 4178\n",
      "session: 4094\n",
      "council: 4066\n",
      "import: 3949\n",
      "achiev: 3900\n",
      "work: 3889\n",
      "need: 3889\n",
      "war: 3857\n",
      "hope: 3619\n",
      "power: 3587\n",
      "presid: 3545\n",
      "situat: 3502\n",
      "principl: 3496\n",
      "republ: 3488\n",
      "conflict: 3409\n",
      "resolut: 3409\n",
      "forc: 3378\n",
      "south: 3287\n",
      "action: 3239\n",
      "relat: 3222\n",
      "global: 3214\n",
      "order: 3162\n",
      "\n",
      "[Stemmed] Top 50 least common words:\n",
      "darkest: 10\n",
      "tripoli: 10\n",
      "venic: 10\n",
      "stick: 10\n",
      "maduro: 10\n",
      "holder: 10\n",
      "outcri: 10\n",
      "reneg: 10\n",
      "dogmat: 10\n",
      "mainstay: 10\n",
      "fallout: 10\n",
      "slate: 10\n",
      "xxi: 10\n",
      "faster: 10\n",
      "mask: 10\n",
      "tor: 10\n",
      "redirect: 10\n",
      "mould: 10\n",
      "spv: 10\n",
      "writer: 10\n",
      "music: 10\n",
      "artist: 10\n",
      "prone: 10\n",
      "allot: 10\n",
      "trauma: 10\n",
      "uruguayan: 10\n",
      "disenchant: 10\n",
      "tacit: 10\n",
      "emissari: 10\n",
      "chairperson: 10\n",
      "angl: 10\n",
      "offshor: 10\n",
      "reforest: 10\n",
      "tour: 10\n",
      "domingo: 10\n",
      "kwame: 10\n",
      "nonnuclear: 10\n",
      "benefic: 10\n",
      "diminut: 10\n",
      "hitlerit: 10\n",
      "tribul: 10\n",
      "relaps: 10\n",
      "extradit: 10\n",
      "purview: 10\n",
      "abraham: 10\n",
      "chasm: 10\n",
      "quaison: 10\n",
      "aosi: 10\n",
      "zelaya: 10\n",
      "intrus: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit\\\\data\\\\freq\\\\word_counts.pkl']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#== Count token frequencies ==\n",
    "\n",
    "def count_frequencies(filenames):\n",
    "    total_freq = Counter()\n",
    "    for fname in tqdm(filenames):\n",
    "        data = joblib.load(fname)\n",
    "        tokens = chain.from_iterable(row[1] for row in data if isinstance(row[1], list))\n",
    "        total_freq.update(tokens)\n",
    "    return total_freq\n",
    "\n",
    "#def remove_rare_words(filenames, freqs, min_count=10):\n",
    "   # for fname in filenames:\n",
    "       # data = joblib.load(fname)\n",
    "       # filtered_data = []\n",
    "        #for doc_id, tokens in data:\n",
    "          #  filtered_tokens = [w for w in tokens if freqs.get(w, 0) >= min_count]\n",
    "          #  filtered_data.append([doc_id, filtered_tokens])\n",
    "       # joblib.dump(filtered_data, fname)  # overwrite or save as new file\n",
    "       # print(f\"Processed {fname}: removed words with freq < {min_count}\")\n",
    "\n",
    "# === Count for preprocessed (stemmed) speeches ===\n",
    "word_counts = count_frequencies(preprocessed_files)\n",
    "\n",
    "#remove_rare_words(preprocessed_files, word_counts, min_count=10)\n",
    "\n",
    "print(\"\\n[Stemmed] Top 50 most common words:\")\n",
    "for word, count in word_counts.most_common(50):\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\n[Stemmed] Top 50 least common words:\")\n",
    "for word, count in word_counts.most_common()[-50:]:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Save stemmed word counts\n",
    "save_path = os.path.join(data_freq, 'word_counts.pkl')\n",
    "joblib.dump(word_counts, save_path)\n",
    "\n",
    "# === Count for wordcloud (unstemmed) speeches ===\n",
    "#word_counts_wordcloud = count_frequencies(wordcloud_files)\n",
    "\n",
    "#print(\"\\n[Wordcloud] Top 50 most common words:\")\n",
    "#for word, count in word_counts_wordcloud.most_common(50):\n",
    "    #print(f\"{word}: {count}\")\n",
    "\n",
    "#print(\"\\n[Wordcloud] Top 50 least common words:\")\n",
    "#for word, count in word_counts_wordcloud.most_common()[-50:]:\n",
    "    #print(f\"{word}: {count}\")\n",
    "\n",
    "# Save unstemmed word counts\n",
    "#save_path_wordcloud = os.path.join(data_freq, 'word_counts_wordcloud.pkl')\n",
    "#joblib.dump(word_counts_wordcloud, save_path_wordcloud)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "932ce36d-d21c-4f1c-aff7-899c321dcfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_c)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39acefa1-c504-478e-98e5-9d465963dc82",
   "metadata": {},
   "source": [
    "### Count the frequency of the dictionary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9e28f481-7e20-486c-a6b3-8729d6aeaad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of affect dictionary:\n",
      "['forbid', 'unattract', 'cruelti', 'crappi', 'apathi', 'scari', 'unimpress', 'sin', 'dumbest', 'eas', 'agit', 'sob', 'shocker', 'tragedi', 'fabul', 'strongest', 'giver', 'sigh', 'aw', 'witch', 'hurtl', 'fucktard', 'cruel', 'glamor', 'funni', 'smarter', 'brillianc', 'irrate', 'alright', 'honest', 'profit', 'fearless', 'grievous', 'relax', 'isolationist', 'hah', 'shyness', 'poorest', 'cruelest', 'troublemak', 'disagre', 'agon', 'terror', 'fight', 'pleas', 'poor', 'crazi', 'hostil', 'stupid', 'damnat', 'vain', 'jade', 'heartless', 'nag', 'gloomi', 'damn', 'dishearten', 'pleaser', 'credit', 'warmth', 'greatest', 'whine', 'shame', 'angriest', 'envious', 'grin', 'blameless', 'sweeter', 'laidback', 'stupidest', 'unprotect', 'whiner', 'unlov', 'shake', 'boredom', 'fairer', 'weaker', 'wellb', 'bold', 'sucki', 'unsuccess', 'mourner', 'liken', 'defens', 'invigor', 'tedious', 'paranoid', 'cynic', 'dignifi', 'paranoia', 'sweetest', 'contented', 'humili', 'crush', 'terrif', 'alarm', 'asham', 'impati', 'fake', 'flirti', 'laziest', 'libertarian', 'empti', 'panicki', 'rape', 'contemptu', 'incent', 'reward', 'brutal', 'ignoramus', 'bereav', 'wimp', 'laugh', 'nicer', 'warm', 'resent', 'threat', 'hell', 'tortur', 'mad', 'worship', 'uncomfort', 'depress', 'unsaf', 'faultless', 'numb', 'humorless', 'murder', 'prettiest', 'jealous', 'tragic', 'discomfort', 'loveli', 'ungrat', 'divin', 'lamest', 'weaken', 'horribl', 'fuck', 'agoni', 'grievanc', 'ach', 'isolation', 'masoch', 'uglier', 'amaz', 'devilish', 'bitter', 'devil', 'smartest', 'glori', 'trite', 'vicious', 'unsavori', 'grossest', 'sinist', 'cheeri', 'dismay', 'easier', 'flatter', 'virtuous', 'egot', 'generos', 'cherish', 'heartwarm', 'advers', 'tough', 'adorn', 'overwhelm', 'devast', 'egotist', 'sociabl', 'lazier', 'affection', 'sarcasm', 'humor', 'funniest', 'startl', 'innoc', 'vomit', 'shyli', 'kiss', 'burdensom', 'melancholi', 'weakest', 'worthless', 'gentlest', 'proud', 'alarmist', 'pervert', 'demeanor', 'loneli', 'reassur', 'weird', 'passionless', 'loyalti', 'frustrat', 'weak', 'immor', 'hater', 'libertin', 'flirt', 'terribl', 'threaten', 'fatalist', 'melancholia', 'avers', 'prick', 'sucker', 'meanest', 'virtuos', 'benefici', 'neurot', 'disappoint', 'restless', 'romant', 'bastard', 'angri', 'flatteri', 'stun', 'bolder', 'amor', 'forbidden', 'hug', 'demean', 'disgust', 'truer', 'benefic', 'teas', 'stinki', 'condemnatori', 'turmoil', 'poison', 'cunt', 'gorgeous', 'harm', 'lousi', 'glorifi', 'piti', 'wimpish', 'impression', 'freakish', 'happiest', 'apathet', 'pitiabl', 'unpleas', 'suffer', 'miseri', 'toler', 'mess', 'casual', 'anxieti', 'stale', 'fieri', 'grudg', 'arrog', 'outrag', 'thrill', 'faithless', 'popular', 'vaniti', 'palat', 'fantasi', 'decay', 'domin', 'mood', 'proudest', 'scream', 'stress', 'calm', 'fatigu', 'dumber', 'benign', 'rude', 'steal', 'tension', 'enthusiast', 'intimid', 'sweetheart', 'prettier', 'perv', 'pathet', 'flawless', 'contradictori', 'distress', 'moron', 'gratifi', 'frighten', 'anguish', 'snob', 'defenseless', 'fantast', 'sad', 'thief', 'worsen', 'tear', 'handsom', 'attract', 'messi', 'smother', 'scarier', 'afraid', 'insult', 'disreput', 'wept', 'victim', 'traumat', 'prejudic', 'amus', 'decent', 'joyous', 'masochist', 'abandon', 'foe', 'pleasantri', 'shitless', 'strengthen', 'antagonist', 'liar', 'romantic', 'violenc', 'sentimentalist', 'gratif', 'decept', 'witchcraft', 'liveli', 'impressionist', 'guilt', 'griev', 'rotten', 'disgrac', 'insincer', 'pushi', 'enjoy', 'freaki', 'ador', 'grate', 'homesick', 'braver', 'battlefield', 'niceti', 'hooray', 'incompet', 'thiev', 'timid', 'adventur', 'needi', 'joyless', 'grimac', 'gloom', 'dump', 'horrid', 'magnific', 'unlucki', 'killer', 'uncontrol', 'funnier', 'ruin', 'weepi', 'play', 'encourag', 'fantas', 'irrit', 'sincer', 'rage', 'despis', 'unhappi', 'hate', 'bravest', 'harmless', 'hero', 'vigor', 'pervers', 'dumb', 'degrad', 'shameless', 'villaini', 'envi', 'aggress', 'fucker', 'courag', 'craziest', 'delici', 'scare', 'finer', 'bittersweet', 'trivial', 'wealthi', 'heroic', 'ecstat', 'irrat', 'prejudici', 'smug', 'stammer', 'feroci', 'phobic', 'villain', 'anger', 'fond', 'obsession', 'awesom', 'wors', 'compassion', 'weakl', 'calmer', 'inhibit', 'bother', 'rancid', 'hatr', 'beaten', 'opportunist', 'offens', 'haunt', 'appal', 'painless', 'pessimist', 'molest', 'lol', 'wrongdo', 'frantic', 'heroin', 'rejoic', 'woe', 'miser', 'meaner', 'saddest', 'mooch', 'attack', 'enthusiasm', 'joke', 'meritocraci', 'angrier', 'emot', 'weirdest', 'silliest', 'kisser', 'hurrah', 'crueler', 'nervous', 'dishonor', 'gracious', 'sicker', 'hopeless', 'delect', 'compliment', 'petrifi', 'cuter', 'reek', 'loveless', 'daze', 'weirder', 'triumphant', 'impolit', 'offend', 'richer', 'violent', 'crazier', 'comedown', 'heroism', 'stubborn', 'maniac', 'strain', 'broke', 'jerk', 'unkind', 'asshol', 'reveng', 'jealousi', 'damnabl', 'unimport', 'careless', 'gentler', 'bitch', 'shi', 'terrorist', 'rigid', 'harsh', 'passion', 'annoy', 'compass', 'geek', 'weirdo', 'agre', 'whore', 'splendor', 'suck', 'fun', 'chuckl', 'snobberi', 'mock', 'doom', 'reluct', 'mourn', 'boldest', 'helpless', 'trembl', 'pestil', 'curs', 'desper', 'harass', 'complain', 'joker', 'obsess', 'uneasi', 'nasti', 'greedi', 'belov', 'grim', 'upset', 'greed', 'honesti', 'phoni', 'feudal', 'loveliest', 'trauma', 'dwell', 'ecstasi', 'blame', 'legit', 'calmest', 'sickest', 'scariest', 'sarcast', 'cheat', 'fatal', 'stronger', 'energet', 'feroc', 'healthi', 'goddam', 'disast', 'flirtati', 'defeatist', 'glorious', 'terrifi', 'gossip', 'forbad', 'sicken', 'poorer', 'strangest', 'cute', 'struggl', 'woebegon', 'worri', 'gratitud', 'cheer', 'distraught', 'ok', 'vulner', 'blessed', 'fairest', 'lame', 'optim', 'grouch', 'fright', 'pessim', 'sadder', 'worst', 'despair', 'bash', 'intellectu', 'promis', 'prais', 'heartfelt', 'horror', 'crap', 'repress', 'prouder', 'fought', 'discourag', 'heal', 'lazi', 'precious', 'melanchol', 'isol', 'aggressor', 'giggl', 'idiot', 'twitchi', 'glorif', 'kid', 'selfish', 'smile', 'grief', 'hellish', 'shit', 'fume', 'bitchi', 'fool', 'trustworthi', 'optimist', 'dumpi', 'unwelcom', 'neat', 'nicest', 'furi', 'unfriend', 'ridicul', 'arguabl', 'shamefac', 'pervi', 'upbeat', 'wow', 'easygo', 'blameworthi', 'charmer', 'stink', 'phobia', 'adversari', 'sillier', 'distrust', 'dread', 'happier', 'vital', 'dissatisfact', 'insecur', 'assault', 'ineffect', 'panic', 'powerless', 'nightmar', 'wealth', 'enrag', 'splendid', 'foolish', 'ugliest', 'contradict', 'worrier', 'freak']\n",
      "Number of words in affect dictionary: 629\n",
      "\n",
      "Contents of cognition dictionary:\n",
      "['analyt', 'guess', 'suspect', 'coz', 'mysteri', 'unawar', 'proverbi', 'dissimilar', 'consequenti', 'stimul', 'occasion', 'rethink', 'purest', 'entireti', 'abnorm', 'referenti', 'differenti', 'diagnos', 'ambigu', 'concentr', 'undon', 'undecid', 'disclosur', 'fundamentalist', 'blur', 'reaction', 'variabl', 'recogniz', 'meant', 'reconstruct', 'supposit', 'acknowledg', 'choosi', 'accuraci', 'mislead', 'perceiv', 'hypothesi', 'unknow', 'cuz', 'reveal', 'potenti', 'puriti', 'persuad', 'indirect', 'generat', 'presumpt', 'clue', 'justif', 'evidenti', 'realize', 'deduct', 'causal', 'inevit', 'thinker', 'evidentiari', 'factor', 'undeni', 'unclear', 'pick', 'induc', 'accur', 'absolut', 'everytim', 'reconsider', 'diagnost', 'solut', 'diagnosi', 'discoveri', 'ration', 'unambigu', 'explicit', 'decis', 'insight', 'confess', 'fuzzi', 'led', 'lesson', 'theoriz', 'analyz', 'learner', 'split', 'enlighten', 'cos', 'respons', 'dubious', 'react', 'guarante', 'someday', 'blatant', 'undo', 'perceptu', 'infal', 'afterthought', 'inequ', 'feasibl', 'knowabl', 'approxim', 'logic', 'meaning', 'unknown', 'categor', 'trigger', 'incomplet', 'curious', 'attribut', 'pretend', 'reactionari', 'prove', 'meaningless', 'reactiv', 'proof', 'bet', 'purpos', 'suppos', 'discern', 'imagin', 'kinda', 'comprehens', 'sens', 'percept', 'everyday', 'rational', 'unquestion', 'random', 'implicit', 'clarif', 'theori', 'dunno', 'analysi', 'obedi', 'memor', 'genuin', 'evalu', 'explor', 'linkag', 'grasp', 'curios', 'methink', 'somewher', 'deduc', 'unwant', 'induct', 'unrel', 'outstand', 'discov', 'unresolv', 'exploratori', 'choos', 'perspect', 'inferenti', 'disclos', 'factual', 'arbitrari', 'conting', 'vagu', 'persuas', 'manipul', 'impli', 'somehow', 'sorta', 'borderlin', 'reconcil', 'anyhow', 'conscious', 'probabilist', 'precis', 'rearrang', 'referenc', 'obscur']\n",
      "Number of words in cognition dictionary: 169\n",
      "Top 100 words by weighted frequency:\n",
      "sown: 0.9925389726762257\n",
      "tyrant: 0.9925389726762257\n",
      "thaw: 0.9925389726762257\n",
      "sizabl: 0.9925389726762257\n",
      "storag: 0.9925389726762257\n",
      "penal: 0.9925389726762257\n",
      "fortitud: 0.9925389726762257\n",
      "stolen: 0.9925389726762257\n",
      "comprehend: 0.9925389726762257\n",
      "fan: 0.9925389726762257\n",
      "candour: 0.9925389726762257\n",
      "irian: 0.9925389726762257\n",
      "gromyko: 0.9925389726762257\n",
      "servitud: 0.9925389726762257\n",
      "uninterrupt: 0.9925389726762257\n",
      "stray: 0.9925389726762257\n",
      "flower: 0.9925389726762257\n",
      "pious: 0.9925389726762257\n",
      "opec: 0.9925389726762257\n",
      "librevill: 0.9925389726762257\n",
      "milk: 0.9925389726762257\n",
      "albert: 0.9925389726762257\n",
      "pervad: 0.9925389726762257\n",
      "hallow: 0.9925389726762257\n",
      "untouch: 0.9925389726762257\n",
      "solicit: 0.9925389726762257\n",
      "farc: 0.9925389726762257\n",
      "ignobl: 0.9925389726762257\n",
      "cotonou: 0.9925389726762257\n",
      "dissid: 0.9925389726762257\n",
      "bore: 0.9925389726762257\n",
      "occasion: 0.9925389726762257\n",
      "piti: 0.9925389726762257\n",
      "discrep: 0.9925389726762257\n",
      "evolutionari: 0.9925389726762257\n",
      "lievano: 0.9925389726762257\n",
      "registr: 0.9925389726762257\n",
      "rational: 0.9925389726762257\n",
      "revitalis: 0.9925389726762257\n",
      "sand: 0.9925389726762257\n",
      "templ: 0.9925389726762257\n",
      "bought: 0.9925389726762257\n",
      "economist: 0.9925389726762257\n",
      "drown: 0.9925389726762257\n",
      "instant: 0.9925389726762257\n",
      "mobutu: 0.9925389726762257\n",
      "kindl: 0.9925389726762257\n",
      "remaind: 0.9925389726762257\n",
      "steep: 0.9925389726762257\n",
      "beg: 0.9925389726762257\n",
      "dog: 0.9925389726762257\n",
      "lure: 0.9925389726762257\n",
      "stormi: 0.9925389726762257\n",
      "exit: 0.9925389726762257\n",
      "phnom: 0.9925389726762257\n",
      "penh: 0.9925389726762257\n",
      "indochines: 0.9925389726762257\n",
      "saigon: 0.9925389726762257\n",
      "indomit: 0.9925389726762257\n",
      "subterfug: 0.9925389726762257\n",
      "pot: 0.9925389726762257\n",
      "tel: 0.9925389726762257\n",
      "aviv: 0.9925389726762257\n",
      "heaviest: 0.9925389726762257\n",
      "travers: 0.9925389726762257\n",
      "epic: 0.9925389726762257\n",
      "spurious: 0.9925389726762257\n",
      "magnanim: 0.9925389726762257\n",
      "phenomen: 0.9925389726762257\n",
      "unep: 0.9925389726762257\n",
      "smile: 0.9925389726762257\n",
      "interwoven: 0.9925389726762257\n",
      "petit: 0.9925389726762257\n",
      "ignit: 0.9925389726762257\n",
      "hollow: 0.9925389726762257\n",
      "amiti: 0.9925389726762257\n",
      "weighti: 0.9925389726762257\n",
      "liaison: 0.9925389726762257\n",
      "antipersonnel: 0.9925389726762257\n",
      "nurs: 0.9925389726762257\n",
      "xviii: 0.9925389726762257\n",
      "insol: 0.9925389726762257\n",
      "interv: 0.9925389726762257\n",
      "anc: 0.9925389726762257\n",
      "bandit: 0.9925389726762257\n",
      "contriv: 0.9925389726762257\n",
      "obey: 0.9925389726762257\n",
      "heap: 0.9925389726762257\n",
      "invinc: 0.9925389726762257\n",
      "monster: 0.9925389726762257\n",
      "tuesday: 0.9925389726762257\n",
      "plata: 0.9925389726762257\n",
      "pawn: 0.9925389726762257\n",
      "befallen: 0.9925389726762257\n",
      "ind: 0.9925389726762257\n",
      "nyerer: 0.9925389726762257\n",
      "longest: 0.9925389726762257\n",
      "acp: 0.9925389726762257\n",
      "inflam: 0.9925389726762257\n",
      "lockerbi: 0.9925389726762257\n"
     ]
    }
   ],
   "source": [
    "# == Count dictionary words\n",
    "\n",
    "# Load dictionaries            ##### How did they come up with this dictionary? Why did they exclude words?\n",
    "\n",
    "affect_path = os.path.join(data_dict, 'dictionary_affect.pkl')\n",
    "cognition_path = os.path.join(data_dict, 'dictionary_cognition.pkl')\n",
    "\n",
    "affect = joblib.load(affect_path)\n",
    "cognition = joblib.load(cognition_path)\n",
    "\n",
    "print(\"Contents of affect dictionary:\")\n",
    "print(affect)\n",
    "print(\"Number of words in affect dictionary:\", len(affect))\n",
    "\n",
    "print(\"\\nContents of cognition dictionary:\")\n",
    "print(cognition)\n",
    "print(\"Number of words in cognition dictionary:\", len(cognition))\n",
    "\n",
    "#with open(affect_path, 'rb') as f:\n",
    "  #  affect_dict = pickle.load(f)\n",
    "#print(\"Contents of affect dictionary:\")\n",
    "#print(affect_dict)\n",
    "#print(\"Number of words in affect dictionary:\", len(affect_dict))\n",
    "\n",
    "#with open(cognition_path, 'rb') as f:\n",
    "  #  cognition_dict = pickle.load(f)\n",
    "#print(\"Contents of cognition dictionary:\")\n",
    "#print(cognition_dict)\n",
    "#print(\"Number of words in cognition dictionary:\", len(cognition_dict))\n",
    "\n",
    "a = [[i, word_counts[i]] for i in affect if i in word_counts]\n",
    "c = [[i, word_counts[i]] for i in cognition if i in word_counts]\n",
    "\n",
    "a = sorted(a, key=lambda x: x[1], reverse=True)\n",
    "c = sorted(c, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "a = [[i[0], f\"({i[1]}),\"] for i in a]\n",
    "c = [[i[0], f\"({i[1]}),\"] for i in c]\n",
    "\n",
    "a1 = ' '.join(str(r) for v in a for r in v)\n",
    "c1 = ' '.join(str(r) for v in c for r in v)\n",
    "\n",
    "affect_out_path = os.path.join(data_freq, \"affect_words.txt\")\n",
    "cog_out_path = os.path.join(data_freq, \"cog_words.txt\")\n",
    "\n",
    "os.makedirs(data_freq, exist_ok=True)  # ensure directory exists\n",
    "\n",
    "with open(affect_out_path, \"w\") as output:\n",
    "    output.write(a1)\n",
    "\n",
    "with open(cog_out_path, \"w\") as output:\n",
    "    output.write(c1)\n",
    "\n",
    "\n",
    "# == Calculate weighted frequencies for all words\n",
    "\n",
    "# - downweights very common words by giving more importance to rare ones\n",
    "#word_counts = joblib.load(os.path.join(data_freq, 'word_counts.pkl'))\n",
    "\n",
    "l = sum(word_counts.values())\n",
    "\n",
    "a = 0.001\n",
    "word_counts_weighted = {k: a / (a + (v / l)) for k, v in word_counts.items()}\n",
    "\n",
    "joblib.dump(word_counts_weighted, os.path.join(data_freq, 'word_counts_weighted.pkl'))\n",
    "\n",
    "################################################################################ ISSUE##################\n",
    "# To print top 100 by weighted values, sort the dictionary by value descending:\n",
    "top_100_weighted = sorted(word_counts_weighted.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "\n",
    "print(\"Top 100 words by weighted frequency:\")\n",
    "for word, weight in top_100_weighted:\n",
    "    print(f\"{word}: {weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a7ec60-2d73-4c68-948c-592970c4ad93",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df626ded-5e49-4a34-a3d1-315ba370226f",
   "metadata": {},
   "source": [
    "## Final Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0d0856a2-44f1-4304-b221-09a9ab72ad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_c)\n",
    "os.chdir(data_freq)\n",
    "\n",
    "count = joblib.load('word_counts.pkl')  # load stemmed counts\n",
    "\n",
    "# For each speech only keep tokens that appear at least 10x\n",
    "\n",
    "def select(lista):\n",
    "    for i in range(len(lista)):\n",
    "        x = lista[i][0]\n",
    "        y = lista[i][1]\n",
    "        y = [w for w in y if count.get(w, 0) >= 10]\n",
    "        lista[i] = [x, y]\n",
    "    return lista\n",
    "\n",
    "for data_path in preprocessed_files:\n",
    "    data = joblib.load(data_path)\n",
    "    data = select(data)\n",
    "    cleaned_path = data_path.replace('.pkl', '_final.pkl')\n",
    "    joblib.dump(data, cleaned_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86873a31-3f1e-4d31-aa16-0f2813161647",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
