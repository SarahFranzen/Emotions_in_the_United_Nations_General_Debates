{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b41cb22-e190-48d5-9926-7ac6fac8310d",
   "metadata": {},
   "source": [
    "# Development of Emotion and Reasoning in the General Speeches of the United Nations: A text-based machine learning approach\n",
    "## Script 0: Data Cleaning, Preprocessing & Token Frequencies\n",
    "### Author: Sarah Franzen\n",
    "\n",
    "### Instructions BEFORE running this script:\n",
    "- Ensure all required packages are installed. If not, set `InstallPackages = TRUE` (see code cells below).  \n",
    "- Set your working directory appropriately.  \n",
    "- The script will automatically create the required folder structure.  \n",
    "- Later, you will be asked to download the folder \"UNGDC_1946_2024.tar.gz\" from:  \n",
    "  https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/0TJX8Y  \n",
    "  and store it **unzipped** inside the created folder *data_original*\n",
    "\n",
    "\n",
    "### Description: \n",
    "- Extract documents from their original txt documents and store them as one csv\n",
    "- Create new variables\n",
    "    - year, country_code, country_name\n",
    "    - speech_length_words\n",
    "    - english_official_language\n",
    "    - security_council_permanent\n",
    "    - gender\n",
    "    - position\n",
    "    - speaker_name\n",
    "    - Country (Year)\n",
    "- Cleaning\n",
    "    - remove line breaks, hypehnation etc.\n",
    "- Preprocessing\n",
    "    - remove punctuation, tokenize, lowercase, pure digit tokens, words shorter than 2 letters, POS-Tagging, stemm, stopword removal\n",
    "    - create new variable: speech_length_preprocessd\n",
    "- Word Frequencies\n",
    "    - word counts of the preprocessed_corpus\n",
    "    - count frequency of the dictionary words\n",
    "    - calculate weighted frequency\n",
    "- Final preprocessing (Not correct, I think)\n",
    "    - Remove words that appear less than 10x times from the preprocessed corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930ebd56-def9-4a16-91eb-78dbb7d037ae",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5588afc-105a-426e-9348-742a830a2732",
   "metadata": {},
   "source": [
    "## Setup, Installation of required Packages and Libraries & Folder Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebb8987a-a0da-453e-97e2-aafa209d0e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "InstallPackages = False # Set this to True to install the following packages \n",
    "\n",
    "if InstallPackages:\n",
    "    import sys\n",
    "\n",
    "    packages = [\n",
    "        \"pandas\",\n",
    "        \"nltk\",\n",
    "        \"spacy\",\n",
    "        \"numpy\",\n",
    "        \"gensim\",\n",
    "        \"pycountry\",\n",
    "        \"matplotlib\",\n",
    "        \"tqdm\",\n",
    "        \"seaborn\",\n",
    "        \"joblib\",\n",
    "        \"scipy\",\n",
    "        \"tabulate\",\n",
    "        \"rapidfuzz\",\n",
    "        \"tableone\"\n",
    "        \n",
    "    ]\n",
    "\n",
    "    for package in packages:\n",
    "        if importlib.util.find_spec(package) is None:\n",
    "            !{sys.executable} -m pip install {package}\n",
    "\n",
    "\n",
    "DownloadAdditions = False # Set this to True to download these additional resources\n",
    "if DownloadAdditions:\n",
    "    nltk.download(\"punkt\")\n",
    "    nltk.download(\"averaged_perceptron_tagger\")\n",
    "    spacy.cli.download('en_core_web_lg') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32d7d499-0563-46e0-9730-914c9f106d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Import libraries for data processing and NLP ==\n",
    "\n",
    "import gensim\n",
    "import joblib\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import pycountry\n",
    "import random\n",
    "import re\n",
    "import spacy\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from multiprocessing import Pool, freeze_support\n",
    "from nltk.stem import SnowballStemmer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOPWORDS\n",
    "from string import punctuation\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "# === Initialize NLP Tools ===\n",
    "\n",
    "translator = str.maketrans('', '', punctuation)\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "tagger = nltk.perceptron.PerceptronTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d495156-bbb2-403c-86f2-83b47d8cb03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Folder structure created:\n",
      "- C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\n",
      "  - data_original\n",
      "  - dictionaries\n",
      "  - freq\n",
      "  - preprocessed\n",
      "  - temp\n",
      "- fig\n",
      "- tables\n"
     ]
    }
   ],
   "source": [
    "# === Set Working Directory and create folder structure ===\n",
    "\n",
    "# Prompt user to enter working directory path\n",
    "#wd = input(\"Please enter your working directory path (e.g., C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit): \").strip()\n",
    "\n",
    "# Change to the entered working directory\n",
    "#try:\n",
    "   # os.chdir(wd)\n",
    "    #print(f\"Working directory set to: {os.getcwd()}\")\n",
    "#except FileNotFoundError:\n",
    "   # print(\"ERROR: The directory you entered does not exist. Please restart and enter a valid path.\")\n",
    "    #exit(1)\n",
    "\n",
    "# Set your working directory (adjust this as needed)\n",
    "\n",
    "wd = Path(r\"C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\")\n",
    "os.chdir(wd)\n",
    "\n",
    "# Set base path to current working directory\n",
    "base_path = Path.cwd()\n",
    "data_path = base_path / \"data\"\n",
    "\n",
    "# List of subfolders to create inside 'data'\n",
    "subfolders = [\"data_original\", \"dictionaries\", \"freq\", \"preprocessed\", \"temp\"]\n",
    "\n",
    "# Create 'data' folder if it doesn't exist\n",
    "data_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Create subfolders\n",
    "for name in subfolders:\n",
    "    (data_path / name).mkdir(exist_ok=True)\n",
    "\n",
    "# Create additional folders in the base directory\n",
    "additional_folders = [\"fig\", \"tables\"]\n",
    "for folder in additional_folders:\n",
    "    (base_path / folder).mkdir(exist_ok=True)\n",
    "\n",
    "print(\"\\nFolder structure created:\")\n",
    "print(f\"- {data_path}\")\n",
    "for name in subfolders:\n",
    "    print(f\"  - {name}\")\n",
    "for folder in additional_folders:\n",
    "    print(f\"- {folder}\")\n",
    "\n",
    "# Prompt user to place raw data files\n",
    "#print(f\"\\nPlease place your raw data files (unzipped) into the folder:\\n  {data_path / 'data_original'}\")\n",
    "#input(\"Press Enter after you have placed the files to continue...\")\n",
    "\n",
    "#print(\"Continuing with the script...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56fa9a21-650f-4d8e-8a21-3d9629655c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define Folder Paths ===\n",
    "\n",
    "# If an error occurs, make sure that you actually have these folders in your working directory\n",
    "\n",
    "data_c = wd / \"data\"\n",
    "data_temp = data_c / \"temp\"\n",
    "data_freq = data_c / \"freq\"\n",
    "data_dict = data_c / \"dictionaries\"\n",
    "data_preprocessed = data_c / \"preprocessed\"\n",
    "#fig_dir = wd /\"fig\"\n",
    "#tables_dir = wd /\"tables\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "296fc7d6-a2ac-406e-a322-6c9f2702ee1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\n"
     ]
    }
   ],
   "source": [
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bd410e-bc90-4e0b-aa08-a9bff13fab47",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff34b6-cbee-4f8c-8076-5f385f596b58",
   "metadata": {},
   "source": [
    "## Load and Prepare Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93d247a0-5ff2-4062-add9-f3267ae1fe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total speeches found: 10953\n",
      "Empty DataFrame\n",
      "Columns: [filename, speech]\n",
      "Index: []\n",
      "\n",
      " Saved raw data with 10952 speeches to '.\\data\\un_corpus_raw.csv'\n"
     ]
    }
   ],
   "source": [
    "# == Load the txt-files from the UN General Debate Corpus ==\n",
    "\n",
    "# Set Folder path containing the original TXT files    \n",
    "base_folder = r\".\\data\\data_original\\TXT\"\n",
    "\n",
    "# Collect txt-files\n",
    "all_txt_files = []\n",
    "for root, dirs, files in os.walk(base_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.txt') and not file.startswith('._'):\n",
    "            all_txt_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Total speeches found: {len(all_txt_files)}\")\n",
    "\n",
    "# Randomly pick 800 files from the full collection   ################################################# REMOVE AT LATER POINT\n",
    "sampled_files = random.sample(all_txt_files,10953)\n",
    "\n",
    "# Read the selected files into a list\n",
    "raw_data = []\n",
    "for filepath in sampled_files:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        raw_data.append({'filename': os.path.basename(filepath), 'speech': content})\n",
    "\n",
    "df_raw = pd.DataFrame(raw_data)\n",
    "\n",
    "# Include only valid filenames\n",
    "df_raw = df_raw[df_raw['filename'] != '.DS_Store-to-UTF-8.txt'].copy()\n",
    "\n",
    "# Drop empty speeches\n",
    "df_raw['speech'] = df_raw['speech'].astype(str)\n",
    "df_raw = df_raw[df_raw['speech'].str.strip() != ''].copy()\n",
    "\n",
    "# Check for duplicates\n",
    "dupe_labels = df_raw[df_raw.duplicated(subset=['filename', 'speech'], keep=False)]\n",
    "print(dupe_labels[['filename', 'speech']].head(20))\n",
    "\n",
    "# == Store as csv and pkl ==\n",
    "\n",
    "raw_pickle_path = r\".\\data\\un_corpus_raw.pkl\"\n",
    "df_raw.to_pickle(raw_pickle_path)\n",
    "\n",
    "raw_output_path = r\".\\data\\un_corpus_raw.csv\"\n",
    "df_raw.to_csv(raw_output_path, index=False, sep=';', encoding='utf-8')\n",
    "\n",
    "print(f\"\\n Saved raw data with {len(df_raw)} speeches to '{raw_output_path}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f056dbf3-a7b0-4a76-9cae-60cfa9007c55",
   "metadata": {},
   "source": [
    "## Is this necessary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6405ff20-92fa-4c97-becb-ac5fff70f84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BFA_74_2019.txt</td>\n",
       "      <td>As Africa’s candidate for the position of Pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VEN_46_1991.txt</td>\n",
       "      <td>﻿Mr. President, I have come before the represe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MDG_51_1996.txt</td>\n",
       "      <td>﻿In common with the speakers preceding me, I\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AGO_48_1993.txt</td>\n",
       "      <td>First of all, Sir,\\non behalf of the Governmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BEN_49_1994.txt</td>\n",
       "      <td>I\\nshould like first of all, Mr. President, to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech\n",
       "0  BFA_74_2019.txt  As Africa’s candidate for the position of Pres...\n",
       "1  VEN_46_1991.txt  ﻿Mr. President, I have come before the represe...\n",
       "2  MDG_51_1996.txt  ﻿In common with the speakers preceding me, I\\n...\n",
       "3  AGO_48_1993.txt  First of all, Sir,\\non behalf of the Governmen...\n",
       "4  BEN_49_1994.txt  I\\nshould like first of all, Mr. President, to..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# == Load data ==\n",
    "\n",
    "df_raw = pd.read_pickle(r\".\\data\\un_corpus_raw.pkl\")\n",
    "df_raw.head()         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7980e945-9d56-4df3-99e5-acf491617568",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7d3162-ebbd-449b-a2be-1148fceda725",
   "metadata": {},
   "source": [
    "## Create new variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25710e79-5068-4d39-8023-f4f97513f165",
   "metadata": {},
   "source": [
    "#### New Variables: Year, Country Code and Country Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "162da07c-1112-43e8-a434-2bef0ed32de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min year: 1946\n",
      "Max year: 2024\n",
      "Missing codes: []\n"
     ]
    }
   ],
   "source": [
    "# == Create variable: country code & year ==\n",
    "\n",
    "# Create contry_code and year variable\n",
    "df_raw['country_code'] = df_raw['filename'].str.extract(r'^([A-Z]{2,3})')\n",
    "df_raw['year'] = df_raw['filename'].str.extract(r'_(\\d{4})\\.txt$').astype(int)\n",
    "\n",
    "print(\"Min year:\", df_raw['year'].min())\n",
    "print(\"Max year:\", df_raw['year'].max())\n",
    "# Speeches range from 1946 to 2023\n",
    "\n",
    "# == Create variable: country_name by matching ISO country code \n",
    "code_to_name = {country.alpha_3: country.name for country in pycountry.countries}\n",
    "\n",
    "# Add custom short names and legacy codes\n",
    "custom_names = {\n",
    "    \"BOL\": \"Bolivia\",\n",
    "    \"COD\": \"Democratic Republic of Congo\",\n",
    "    \"IRN\": \"Iran\",\n",
    "    \"LAO\": \"Laos\",\n",
    "    \"MDA\": \"Moldova\",\n",
    "    \"PRK\": \"North Korea\",\n",
    "    \"PSE\": \"Palestine\",\n",
    "    \"RUS\": \"Russia\",\n",
    "    \"SYR\": \"Syria\",\n",
    "    \"TZA\": \"Tanzania\",\n",
    "    \"VAT\": \"Vatican City State\",\n",
    "    \"VEN\": \"Venezuela\",\n",
    "    \"VNM\": \"Vietnam\",\n",
    "    \"YMD\": \"South Yemen\",\n",
    "    \"YUG\": \"Yugoslavia\",\n",
    "    \"DDR\": \"East Germany\",\n",
    "    \"EU\": \"European Union\",\n",
    "    \"CSK\": \"Czechoslovakia\",\n",
    "    \"FSM\": \"Micronesia\",\n",
    "    \"KOR\": \"South Korea\"\n",
    "}\n",
    "\n",
    "code_to_name.update(custom_names)\n",
    "df_raw['country_name'] = df_raw['country_code'].map(code_to_name)\n",
    "\n",
    "# Check missing mappings\n",
    "missing = df_raw.loc[df_raw['country_name'].isna(), 'country_code'].unique()\n",
    "print(\"Missing codes:\", missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "383d2904-f015-4c9d-9aeb-51a94f03f950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    country_code                      country_name\n",
      "0            AFG                       Afghanistan\n",
      "1            AGO                            Angola\n",
      "2            ALB                           Albania\n",
      "3            AND                           Andorra\n",
      "4            ARE              United Arab Emirates\n",
      "5            ARG                         Argentina\n",
      "6            ARM                           Armenia\n",
      "7            ATG               Antigua and Barbuda\n",
      "8            AUS                         Australia\n",
      "9            AUT                           Austria\n",
      "10           AZE                        Azerbaijan\n",
      "11           BDI                           Burundi\n",
      "12           BEL                           Belgium\n",
      "13           BEN                             Benin\n",
      "14           BFA                      Burkina Faso\n",
      "15           BGD                        Bangladesh\n",
      "16           BGR                          Bulgaria\n",
      "17           BHR                           Bahrain\n",
      "18           BHS                           Bahamas\n",
      "19           BIH            Bosnia and Herzegovina\n",
      "20           BLR                           Belarus\n",
      "21           BLZ                            Belize\n",
      "22           BOL                           Bolivia\n",
      "23           BRA                            Brazil\n",
      "24           BRB                          Barbados\n",
      "25           BRN                 Brunei Darussalam\n",
      "26           BTN                            Bhutan\n",
      "27           BWA                          Botswana\n",
      "28           CAF          Central African Republic\n",
      "29           CAN                            Canada\n",
      "30           CHE                       Switzerland\n",
      "31           CHL                             Chile\n",
      "32           CHN                             China\n",
      "33           CIV                     Côte d'Ivoire\n",
      "34           CMR                          Cameroon\n",
      "35           COD      Democratic Republic of Congo\n",
      "36           COG                             Congo\n",
      "37           COL                          Colombia\n",
      "38           COM                           Comoros\n",
      "39           CPV                        Cabo Verde\n",
      "40           CRI                        Costa Rica\n",
      "41           CSK                    Czechoslovakia\n",
      "42           CUB                              Cuba\n",
      "43           CYP                            Cyprus\n",
      "44           CZE                           Czechia\n",
      "45           DDR                      East Germany\n",
      "46           DEU                           Germany\n",
      "47           DJI                          Djibouti\n",
      "48           DMA                          Dominica\n",
      "49           DNK                           Denmark\n",
      "50           DOM                Dominican Republic\n",
      "51           DZA                           Algeria\n",
      "52           ECU                           Ecuador\n",
      "53           EGY                             Egypt\n",
      "54           ERI                           Eritrea\n",
      "55           ESP                             Spain\n",
      "56           EST                           Estonia\n",
      "57           ETH                          Ethiopia\n",
      "58            EU                    European Union\n",
      "59           FIN                           Finland\n",
      "60           FJI                              Fiji\n",
      "61           FRA                            France\n",
      "62           FSM                        Micronesia\n",
      "63           GAB                             Gabon\n",
      "64           GBR                    United Kingdom\n",
      "65           GEO                           Georgia\n",
      "66           GHA                             Ghana\n",
      "67           GIN                            Guinea\n",
      "68           GMB                            Gambia\n",
      "69           GNB                     Guinea-Bissau\n",
      "70           GNQ                 Equatorial Guinea\n",
      "71           GRC                            Greece\n",
      "72           GRD                           Grenada\n",
      "73           GTM                         Guatemala\n",
      "74           GUY                            Guyana\n",
      "75           HND                          Honduras\n",
      "76           HRV                           Croatia\n",
      "77           HTI                             Haiti\n",
      "78           HUN                           Hungary\n",
      "79           IDN                         Indonesia\n",
      "80           IND                             India\n",
      "81           IRL                           Ireland\n",
      "82           IRN                              Iran\n",
      "83           IRQ                              Iraq\n",
      "84           ISL                           Iceland\n",
      "85           ISR                            Israel\n",
      "86           ITA                             Italy\n",
      "87           JAM                           Jamaica\n",
      "88           JOR                            Jordan\n",
      "89           JPN                             Japan\n",
      "90           KAZ                        Kazakhstan\n",
      "91           KEN                             Kenya\n",
      "92           KGZ                        Kyrgyzstan\n",
      "93           KHM                          Cambodia\n",
      "94           KIR                          Kiribati\n",
      "95           KNA             Saint Kitts and Nevis\n",
      "96           KOR                       South Korea\n",
      "97           KWT                            Kuwait\n",
      "98           LAO                              Laos\n",
      "99           LBN                           Lebanon\n",
      "100          LBR                           Liberia\n",
      "101          LBY                             Libya\n",
      "102          LCA                       Saint Lucia\n",
      "103          LIE                     Liechtenstein\n",
      "104          LKA                         Sri Lanka\n",
      "105          LSO                           Lesotho\n",
      "106          LTU                         Lithuania\n",
      "107          LUX                        Luxembourg\n",
      "108          LVA                            Latvia\n",
      "109          MAR                           Morocco\n",
      "110          MCO                            Monaco\n",
      "111          MDA                           Moldova\n",
      "112          MDG                        Madagascar\n",
      "113          MDV                          Maldives\n",
      "114          MEX                            Mexico\n",
      "115          MHL                  Marshall Islands\n",
      "116          MKD                   North Macedonia\n",
      "117          MLI                              Mali\n",
      "118          MLT                             Malta\n",
      "119          MMR                           Myanmar\n",
      "120          MNE                        Montenegro\n",
      "121          MNG                          Mongolia\n",
      "122          MOZ                        Mozambique\n",
      "123          MRT                        Mauritania\n",
      "124          MUS                         Mauritius\n",
      "125          MWI                            Malawi\n",
      "126          MYS                          Malaysia\n",
      "127          NAM                           Namibia\n",
      "128          NER                             Niger\n",
      "129          NGA                           Nigeria\n",
      "130          NIC                         Nicaragua\n",
      "131          NLD                       Netherlands\n",
      "132          NOR                            Norway\n",
      "133          NPL                             Nepal\n",
      "134          NRU                             Nauru\n",
      "135          NZL                       New Zealand\n",
      "136          OMN                              Oman\n",
      "137          PAK                          Pakistan\n",
      "138          PAN                            Panama\n",
      "139          PER                              Peru\n",
      "140          PHL                       Philippines\n",
      "141          PLW                             Palau\n",
      "142          PNG                  Papua New Guinea\n",
      "143          POL                            Poland\n",
      "144          PRK                       North Korea\n",
      "145          PRT                          Portugal\n",
      "146          PRY                          Paraguay\n",
      "147          PSE                         Palestine\n",
      "148          QAT                             Qatar\n",
      "149          ROU                           Romania\n",
      "150          RUS                            Russia\n",
      "151          RWA                            Rwanda\n",
      "152          SAU                      Saudi Arabia\n",
      "153          SDN                             Sudan\n",
      "154          SEN                           Senegal\n",
      "155          SGP                         Singapore\n",
      "156          SLB                   Solomon Islands\n",
      "157          SLE                      Sierra Leone\n",
      "158          SLV                       El Salvador\n",
      "159          SMR                        San Marino\n",
      "160          SOM                           Somalia\n",
      "161          SRB                            Serbia\n",
      "162          SSD                       South Sudan\n",
      "163          STP             Sao Tome and Principe\n",
      "164          SUR                          Suriname\n",
      "165          SVK                          Slovakia\n",
      "166          SVN                          Slovenia\n",
      "167          SWE                            Sweden\n",
      "168          SWZ                          Eswatini\n",
      "169          SYC                        Seychelles\n",
      "170          SYR                             Syria\n",
      "171          TCD                              Chad\n",
      "172          TGO                              Togo\n",
      "173          THA                          Thailand\n",
      "174          TJK                        Tajikistan\n",
      "175          TKM                      Turkmenistan\n",
      "176          TLS                       Timor-Leste\n",
      "177          TON                             Tonga\n",
      "178          TTO               Trinidad and Tobago\n",
      "179          TUN                           Tunisia\n",
      "180          TUR                           Türkiye\n",
      "181          TUV                            Tuvalu\n",
      "182          TZA                          Tanzania\n",
      "183          UGA                            Uganda\n",
      "184          UKR                           Ukraine\n",
      "185          URY                           Uruguay\n",
      "186          USA                     United States\n",
      "187          UZB                        Uzbekistan\n",
      "188          VAT                Vatican City State\n",
      "189          VCT  Saint Vincent and the Grenadines\n",
      "190          VEN                         Venezuela\n",
      "191          VNM                           Vietnam\n",
      "192          VUT                           Vanuatu\n",
      "193          WSM                             Samoa\n",
      "194          YEM                             Yemen\n",
      "195          YMD                       South Yemen\n",
      "196          YUG                        Yugoslavia\n",
      "197          ZAF                      South Africa\n",
      "198          ZMB                            Zambia\n",
      "199          ZWE                          Zimbabwe\n"
     ]
    }
   ],
   "source": [
    "# == Check country names and structure ==\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(df_raw[['country_code', 'country_name']].drop_duplicates().sort_values('country_code').reset_index(drop=True))\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5928538b-8519-40e2-bdee-ac72024f9f48",
   "metadata": {},
   "source": [
    "#### New Variable: Length of speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "911e2229-337f-4479-984c-3a6ab9dff5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique tokens in the corpus: 225938\n",
      "Total number of tokens in the corpus: 31911386\n",
      "Average speech length (words): 2913.75\n",
      "20 shortest speeches:\n",
      "              filename                      country_name  year  \\\n",
      "5229   EGY_28_1973.txt                             Egypt  1973   \n",
      "6407   VCT_41_1986.txt  Saint Vincent and the Grenadines  1986   \n",
      "1333   BEN_71_2016.txt                             Benin  2016   \n",
      "5711   GNB_76_2021.txt                     Guinea-Bissau  2021   \n",
      "3306   HTI_01_1946.txt                             Haiti  1946   \n",
      "8505   RWA_76_2021.txt                            Rwanda  2021   \n",
      "511    DDR_45_1990.txt                      East Germany  1990   \n",
      "5172   LTU_73_2018.txt                         Lithuania  2018   \n",
      "6739   RWA_69_2014.txt                            Rwanda  2014   \n",
      "7137   IRN_01_1946.txt                              Iran  1946   \n",
      "8184   LTU_72_2017.txt                         Lithuania  2017   \n",
      "172    RWA_70_2015.txt                            Rwanda  2015   \n",
      "1951   ERI_75_2020.txt                           Eritrea  2020   \n",
      "8682   RWA_72_2017.txt                            Rwanda  2017   \n",
      "905    URY_02_1947.txt                           Uruguay  1947   \n",
      "7154   SAU_01_1946.txt                      Saudi Arabia  1946   \n",
      "8458   JOR_65_2010.txt                            Jordan  2010   \n",
      "4571   QAT_63_2008.txt                             Qatar  2008   \n",
      "7762   JOR_61_2006.txt                            Jordan  2006   \n",
      "10856  CUB_01_1946.txt                              Cuba  1946   \n",
      "\n",
      "       speech_length_words  \n",
      "5229                   423  \n",
      "6407                   462  \n",
      "1333                   463  \n",
      "5711                   480  \n",
      "3306                   485  \n",
      "8505                   492  \n",
      "511                    493  \n",
      "5172                   508  \n",
      "6739                   523  \n",
      "7137                   531  \n",
      "8184                   537  \n",
      "172                    539  \n",
      "1951                   540  \n",
      "8682                   540  \n",
      "905                    544  \n",
      "7154                   555  \n",
      "8458                   563  \n",
      "4571                   567  \n",
      "7762                   571  \n",
      "10856                  590  \n",
      "\n",
      "20 longest speeches:\n",
      "              filename country_name  year  speech_length_words\n",
      "8123   IND_15_1960.txt        India  1960                22003\n",
      "8478   CUB_15_1960.txt         Cuba  1960                21777\n",
      "10924  RUS_06_1951.txt       Russia  1951                19045\n",
      "7279   GIN_17_1962.txt       Guinea  1962                18952\n",
      "514    IND_10_1955.txt        India  1955                18053\n",
      "10648  RUS_15_1960.txt       Russia  1960                17943\n",
      "8499   IND_14_1959.txt        India  1959                17526\n",
      "9327   IND_09_1954.txt        India  1954                17178\n",
      "1548   RUS_13_1958.txt       Russia  1958                16872\n",
      "2079   IND_08_1953.txt        India  1953                16080\n",
      "8579   IND_12_1957.txt        India  1957                14927\n",
      "4254   IND_16_1961.txt        India  1961                13951\n",
      "1636   RUS_16_1961.txt       Russia  1961                13941\n",
      "9011   GIN_15_1960.txt       Guinea  1960                13916\n",
      "7126   IND_11_1956.txt        India  1956                13482\n",
      "578    IDN_15_1960.txt    Indonesia  1960                13463\n",
      "7898   IND_13_1958.txt        India  1958                12599\n",
      "10569  RUS_17_1962.txt       Russia  1962                12255\n",
      "3547   RUS_09_1954.txt       Russia  1954                12055\n",
      "3358   RUS_14_1959.txt       Russia  1959                11959\n"
     ]
    }
   ],
   "source": [
    "# Count total number of unique tokens in the corpus\n",
    "all_tokens = set()\n",
    "for speech in df_raw['speech']:\n",
    "    all_tokens.update(str(speech).split())\n",
    "print(\"Total number of unique tokens in the corpus:\", len(all_tokens))\n",
    "\n",
    "# Count total number of tokens in the corpus\n",
    "total_tokens = df_raw['speech'].apply(lambda x: len(str(x).split())).sum()\n",
    "print(\"Total number of tokens in the corpus:\", total_tokens)\n",
    "\n",
    "# Add a new column: speech length in words\n",
    "df_raw['speech_length_words'] = df_raw['speech'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Calculate average length\n",
    "avg_length = df_raw['speech_length_words'].mean()\n",
    "print(\"Average speech length (words):\", round(avg_length, 2))\n",
    "\n",
    "# 20 shortest & longest speeches\n",
    "print(\"20 shortest speeches:\")\n",
    "print(df_raw.nsmallest(20, 'speech_length_words')[['filename', 'country_name', 'year', 'speech_length_words']])\n",
    "\n",
    "print(\"\\n20 longest speeches:\")\n",
    "print(df_raw.nlargest(20, 'speech_length_words')[['filename', 'country_name', 'year', 'speech_length_words']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424f152-b786-45bf-bf86-e169a900f628",
   "metadata": {},
   "source": [
    "#### New variable: English as Official Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1d2c2a6-23d6-4044-84cc-8785167e6b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries not matched in df_raw['country_name']:\n",
      "Anguilla\n",
      "Bermuda\n",
      "British Virgin Islands\n",
      "Cayman Islands\n",
      "Christmas Island\n",
      "Cook Islands\n",
      "Hong Kong\n",
      "Jersey\n",
      "Niue\n",
      "Norfolk Island\n",
      "Northern Mariana Islands\n",
      "Pitcairn Islands\n",
      "Sint Maarten\n",
      "Turks and Caicos Islands\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "      <th>country_code</th>\n",
       "      <th>year</th>\n",
       "      <th>country_name</th>\n",
       "      <th>speech_length_words</th>\n",
       "      <th>english_official_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BFA_74_2019.txt</td>\n",
       "      <td>As Africa’s candidate for the position of Pres...</td>\n",
       "      <td>BFA</td>\n",
       "      <td>2019</td>\n",
       "      <td>Burkina Faso</td>\n",
       "      <td>2788</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VEN_46_1991.txt</td>\n",
       "      <td>﻿Mr. President, I have come before the represe...</td>\n",
       "      <td>VEN</td>\n",
       "      <td>1991</td>\n",
       "      <td>Venezuela</td>\n",
       "      <td>2736</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MDG_51_1996.txt</td>\n",
       "      <td>﻿In common with the speakers preceding me, I\\n...</td>\n",
       "      <td>MDG</td>\n",
       "      <td>1996</td>\n",
       "      <td>Madagascar</td>\n",
       "      <td>1997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AGO_48_1993.txt</td>\n",
       "      <td>First of all, Sir,\\non behalf of the Governmen...</td>\n",
       "      <td>AGO</td>\n",
       "      <td>1993</td>\n",
       "      <td>Angola</td>\n",
       "      <td>3676</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BEN_49_1994.txt</td>\n",
       "      <td>I\\nshould like first of all, Mr. President, to...</td>\n",
       "      <td>BEN</td>\n",
       "      <td>1994</td>\n",
       "      <td>Benin</td>\n",
       "      <td>3856</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech  \\\n",
       "0  BFA_74_2019.txt  As Africa’s candidate for the position of Pres...   \n",
       "1  VEN_46_1991.txt  ﻿Mr. President, I have come before the represe...   \n",
       "2  MDG_51_1996.txt  ﻿In common with the speakers preceding me, I\\n...   \n",
       "3  AGO_48_1993.txt  First of all, Sir,\\non behalf of the Governmen...   \n",
       "4  BEN_49_1994.txt  I\\nshould like first of all, Mr. President, to...   \n",
       "\n",
       "  country_code  year  country_name  speech_length_words  \\\n",
       "0          BFA  2019  Burkina Faso                 2788   \n",
       "1          VEN  1991     Venezuela                 2736   \n",
       "2          MDG  1996    Madagascar                 1997   \n",
       "3          AGO  1993        Angola                 3676   \n",
       "4          BEN  1994         Benin                 3856   \n",
       "\n",
       "   english_official_language  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source for english as official language : https://gradschool.utk.edu/future-students/office-of-graduate-admissions/applying-to-graduate-school/admission-requirements/testing-requirements/countries-with-english-as-official-language/\n",
    "# They are quoting: https://www.cia.gov/the-world-factbook/field/languages/\n",
    "\n",
    "english_countries = [\n",
    "    \"Anguilla\", \"Antigua and Barbuda\", \"Bahamas\", \"Barbados\", \"Belize\", \"Belgium\",\n",
    "    \"Bermuda\", \"Botswana\", \"British Virgin Islands\", \"Burundi\", \"Cameroon\", \"Canada\",\n",
    "    \"Cayman Islands\", \"Christmas Island\", \"Cook Islands\", \"Dominica\", \"Fiji\", \"Gambia\",\n",
    "    \"Ghana\", \"Grenada\", \"Guyana\", \"Hong Kong\", \"India\", \"Ireland\", \"Jersey\", \"Kenya\",\n",
    "    \"Liberia\", \"Malawi\", \"Malta\", \"Marshall Islands\", \"Micronesia\",\n",
    "    \"Namibia\", \"New Zealand\", \"Nigeria\", \"Niue\", \"Norfolk Island\", \"Northern Mariana Islands\",\n",
    "    \"Pakistan\", \"Palau\", \"Papua New Guinea\", \"Philippines\", \"Pitcairn Islands\", \"Rwanda\",\n",
    "    \"Saint Kitts and Nevis\", \"Saint Lucia\", \"Samoa\", \"Seychelles\", \"Sierra Leone\", \"Singapore\",\n",
    "    \"Sint Maarten\", \"Solomon Islands\", \"Somalia\", \"South Africa\", \"South Sudan\", \"Sudan\",\n",
    "    \"Eswatini\", \"Tanzania\", \"Tonga\", \"Trinidad and Tobago\", \"Turks and Caicos Islands\",\n",
    "    \"Tuvalu\", \"Uganda\", \"Zambia\", \"Zimbabwe\"\n",
    "]\n",
    "\n",
    "# Create dummy column for english being the official language\n",
    "df_raw['english_official_language'] = df_raw['country_name'].apply(\n",
    "    lambda x: 1 if x in english_countries else 0\n",
    ")\n",
    "\n",
    "# Detect unmatched countries \n",
    "matched = set(df_raw['country_name'])\n",
    "unmatched = [country for country in english_countries if country not in matched]\n",
    "\n",
    "print(\"Countries not matched in df_raw['country_name']:\")\n",
    "for country in unmatched:\n",
    "    print(country)\n",
    "\n",
    "# All of these countries are either British Overseas Territories, Australian Territories, self-governing island territories or Special Administrative Regions\n",
    "    # None of the unmatched regions are UN Members\n",
    "\n",
    "# Check df with new variable english_official_language\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82540d8a-b656-45fb-aeb0-2fd5b4d7108a",
   "metadata": {},
   "source": [
    "#### New variable: Permanent member security council"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fb92fe4-5c22-43fa-8b48-62ec2240d716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      country_code   country_name  security_council_permanent  year\n",
      "36             FRA         France                           1  1979\n",
      "46             USA  United States                           1  2003\n",
      "48             USA  United States                           1  1964\n",
      "49             FRA         France                           1  2007\n",
      "74             CHN          China                           1  2019\n",
      "...            ...            ...                         ...   ...\n",
      "10793          FRA         France                           1  2013\n",
      "10824          USA  United States                           1  1952\n",
      "10845          USA  United States                           1  1965\n",
      "10885          FRA         France                           1  2012\n",
      "10924          RUS         Russia                           1  1951\n",
      "\n",
      "[388 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define permanent members of the UN Security Council and create dummy\n",
    "permanent_members = ['RUS', 'USA', 'FRA', 'GBR', 'CHN']\n",
    "\n",
    "df_raw['security_council_permanent'] = df_raw['country_code'].isin(permanent_members).astype(int)\n",
    "\n",
    "print(df_raw[df_raw['country_code'].isin(permanent_members)][\n",
    "    ['country_code', 'country_name', 'security_council_permanent', 'year']\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523547d9-9b10-473c-8c85-d1e2fc4fd3e1",
   "metadata": {},
   "source": [
    "#### New variables: Speaker, Position & Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7efb21ee-d51a-4f5a-a162-4d1ff6ceabfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "       Year  Session ISO Code                              Country  \\\n",
      "10699  1951        6      RUS  Union of Soviet Socialist Republics   \n",
      "10745  1951        6      RUS  Union of Soviet Socialist Republics   \n",
      "10574  1954        9      PHL                          Philippines   \n",
      "10608  1954        9      PHL                          Philippines   \n",
      "10454  1956       11      IRQ                                 Iraq   \n",
      "10517  1956       11      IRQ                                 Iraq   \n",
      "10516  1956       11      SYR                                Syria   \n",
      "10519  1956       11      SYR                                Syria   \n",
      "10398  1957       12      CSK                       Czechoslovakia   \n",
      "10436  1957       12      CSK                       Czechoslovakia   \n",
      "10352  1958       13      BGR                             Bulgaria   \n",
      "10374  1958       13      BGR                             Bulgaria   \n",
      "10320  1958       13      CSK                       Czechoslovakia   \n",
      "10376  1958       13      CSK                       Czechoslovakia   \n",
      "10333  1958       13      IRQ                                 Iraq   \n",
      "10379  1958       13      IRQ                                 Iraq   \n",
      "10309  1958       13      RUS  Union of Soviet Socialist Republics   \n",
      "10364  1958       13      RUS  Union of Soviet Socialist Republics   \n",
      "10232  1959       14      RUS  Union of Soviet Socialist Republics   \n",
      "10302  1959       14      RUS  Union of Soviet Socialist Republics   \n",
      "\n",
      "        Name of Person Speaking                                  Post  \\\n",
      "10699            Mr. VYSHINSKY                                    NaN   \n",
      "10745            Mr. VYSHINSKY                                    NaN   \n",
      "10574                Mr. ROMULO                                   NaN   \n",
      "10608               Mr. SERRANO                                   NaN   \n",
      "10454                Mr. JAMALI                                   NaN   \n",
      "10517                Mr. JAMALI                                   NaN   \n",
      "10516            Mr. ZEINEDDINE                                   NaN   \n",
      "10519           Mr. ZEINEDDINE                                    NaN   \n",
      "10398                Mr. DAVID                                    NaN   \n",
      "10436                 Mr. DAVID                                   NaN   \n",
      "10352               Mr. Lukanov                                   NaN   \n",
      "10374               Mr. Lukanov                                   NaN   \n",
      "10320                 Mr. David                                   NaN   \n",
      "10376                 Mr. David                                   NaN   \n",
      "10333                 Mr. Jawad                                   NaN   \n",
      "10379                Mr. Jomard                                   NaN   \n",
      "10309               Mr. Gromyko                                   NaN   \n",
      "10364              Mr. GROMYKO                                    NaN   \n",
      "10232  Mr. Nikita S. KHRUSHCHEV  Chairman of the Council of Ministers   \n",
      "10302             Mr. Kuznetsov                                   NaN   \n",
      "\n",
      "      Unnamed: 6  \n",
      "10699        NaN  \n",
      "10745        NaN  \n",
      "10574        NaN  \n",
      "10608        NaN  \n",
      "10454        NaN  \n",
      "10517        NaN  \n",
      "10516        NaN  \n",
      "10519        NaN  \n",
      "10398        NaN  \n",
      "10436        NaN  \n",
      "10352        NaN  \n",
      "10374        NaN  \n",
      "10320        NaN  \n",
      "10376        NaN  \n",
      "10333        NaN  \n",
      "10379        NaN  \n",
      "10309        NaN  \n",
      "10364        NaN  \n",
      "10232        NaN  \n",
      "10302        NaN  \n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Load suplementary data set which contains information on the speakers name (sometimes including gender) and their position\n",
    "df_speakers = pd.read_excel(os.path.join(data_c, \"data_original\", \"Speakers_by_session.xlsx\"))\n",
    "\n",
    "# Check uniqueness of keys in df_speakers\n",
    "print(df_speakers.duplicated(subset=['Year', 'ISO Code']).sum())\n",
    "\n",
    "# Check for duplicates in df_speakers\n",
    "dupes_speakers = df_speakers[df_speakers.duplicated(subset=['Year', 'ISO Code'], keep=False)]\n",
    "print(dupes_speakers.sort_values(['Year', 'ISO Code']).head(20))\n",
    "\n",
    "# For two observations the noted speakers differ, therefore an additional UN Resource was used to determine the real speaker\n",
    "# for 1958 Iraq Mr. Jomard see https://digitallibrary.un.org/record/380721\n",
    "# for 1954 Phillipines Mr. Romulo see https://digitallibrary.un.org/record/380429\n",
    "\n",
    "# Overwrite the wrongfully noted speaker for Iraq and the Philipines and afterwards drop each of the double observation\n",
    "df_speakers_cleaned = (\n",
    "    df_speakers[~(\n",
    "        ((df_speakers['ISO Code'] == \"IRQ\") & (df_speakers['Year'] == 1958) & (df_speakers['Name of Person Speaking'] == \"Mr. Jawad\")) |\n",
    "        ((df_speakers['ISO Code'] == \"PHL\") & (df_speakers['Year'] == 1954) & (df_speakers['Name of Person Speaking'] == \"Mr. SERRANO\"))\n",
    "    )]\n",
    "    .drop_duplicates(subset=['Year', 'ISO Code'], keep='first')\n",
    ")\n",
    "\n",
    "# Check if there are duplicates that are still unaccounted for\n",
    "print(df_speakers_cleaned.duplicated(subset=['Year', 'ISO Code']).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de783a81-efbf-49a6-96f6-ae6a93fa1573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              filename  year country_code    country_name\n",
      "419    GIN_34_1979.txt  1979          GIN          Guinea\n",
      "431    YMD_28_1973.txt  1973          YMD     South Yemen\n",
      "561    YMD_23_1968.txt  1968          YMD     South Yemen\n",
      "647    KEN_49_1994.txt  1994          KEN           Kenya\n",
      "670    AUS_49_1994.txt  1994          AUS       Australia\n",
      "...                ...   ...          ...             ...\n",
      "10118  CAN_21_1966.txt  1966          CAN          Canada\n",
      "10438  YMD_35_1980.txt  1980          YMD     South Yemen\n",
      "10646  DNK_24_1969.txt  1969          DNK         Denmark\n",
      "10712  CHN_18_1963.txt  1963          CHN           China\n",
      "10897   EU_68_2013.txt  2013           EU  European Union\n",
      "\n",
      "[84 rows x 4 columns]\n",
      "84 rows could not be matched\n",
      "Empty DataFrame\n",
      "Columns: [filename, speech, country_code, year, country_name, speech_length_words, english_official_language, security_council_permanent, Year, ISO Code, Name of Person Speaking, Post, _merge]\n",
      "Index: []\n",
      "    gender_dummy  count\n",
      "0       0 (male)   4521\n",
      "1     1 (female)    183\n",
      "2  NaN (unknown)   6248\n"
     ]
    }
   ],
   "source": [
    "# == Create variable speaker_name and position ==\n",
    "\n",
    "# Merge new infrormation to dataframe\n",
    "df_merged = df_raw.merge(\n",
    "    df_speakers_cleaned[['Year', 'ISO Code', 'Name of Person Speaking', 'Post']],\n",
    "    left_on=['year', 'country_code'],\n",
    "    right_on=['Year', 'ISO Code'],\n",
    "    how='left',\n",
    "    indicator=True)\n",
    "\n",
    "# Detect unmatched rows\n",
    "unmatched = df_merged[df_merged['_merge'] == 'left_only']\n",
    "unmatched_count = (df_merged['_merge'] == 'left_only').sum()\n",
    "\n",
    "print(unmatched[['filename', 'year', 'country_code', 'country_name']])\n",
    "print(f\"{unmatched_count} rows could not be matched\")\n",
    "\n",
    "# Check if any duplicates were created\n",
    "dupes_speakers = df_merged[df_merged.duplicated(subset=['year', 'country_code'], keep=False)]\n",
    "print(dupes_speakers.sort_values(['year', 'country_code']).head(20))\n",
    "\n",
    "# Clean up: \n",
    "#- Keep all rows, unmateched rows are being set to NA\n",
    "#- Drop redundant columns and rename some columns\n",
    "df_merged = df_merged.drop(columns=['Year', 'ISO Code', '_merge']).rename(columns={\n",
    "    'Name of Person Speaking': 'speaker_name',\n",
    "    'Post': 'position'})\n",
    "\n",
    "# == Create gender dummy ==\n",
    "\n",
    "df_merged['gender_dummy'] = df_merged['speaker_name'].apply(\n",
    "    lambda name: 0 if pd.notnull(name) and re.search(r'^(?:Mr|Sir)\\b', name, re.IGNORECASE)\n",
    "    else 1 if pd.notnull(name) and re.search(r'^(?:Mrs|Ms)\\b', name, re.IGNORECASE)\n",
    "    else None)\n",
    "\n",
    "# Count all values including NaN\n",
    "counts = df_merged['gender_dummy'].value_counts(dropna=False)\n",
    "\n",
    "# Build summary using .get() to handle missing keys\n",
    "gender_summary = pd.DataFrame({\n",
    "    'gender_dummy': ['0 (male)', '1 (female)', 'NaN (unknown)'],\n",
    "    'count': [\n",
    "        counts.get(0, 0),\n",
    "        counts.get(1, 0),\n",
    "        counts.get(np.nan, 0)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(gender_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a0e4d80-447a-4a8f-b305-864943eafae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['President of the Council of Ministers' 'President'\n",
      " 'Minister for Foreign Affairs' nan 'President '\n",
      " 'Commander in Chief, Head of State' 'Prime Minister'\n",
      " 'Deputy Prime Minister' 'UN Representative' 'Prime minister'\n",
      " 'Prime Minister and Minister for Reform' 'Head of Government'\n",
      " 'Minister for Foreign Affairs and Minister for Intelligence'\n",
      " 'State Councillor and Minister for Foreign Affairs' 'Vice-President'\n",
      " 'Minister of Foreign Affairs '\n",
      " 'Minister for Foreign Affairs, Justice and Culture'\n",
      " 'Minister for Foreign Affairs and Cooperation of Congolese Living Abroad'\n",
      " 'Minister for Foreign and Political Affairs and Justice'\n",
      " 'Permanent Representative' 'Constitutional President'\n",
      " 'President and Commander-in-Chief of the Defence Forces' 'Vice President'\n",
      " 'President of the Government'\n",
      " 'Minister of Foreign and European Affairs and Minister of Immigration and Asylum'\n",
      " 'Prime Minister and Minister for Foreign Affairs'\n",
      " \"Vice-President of the Prime Minister's Council\"\n",
      " 'Minister for Foreign Affairs and Cooperation'\n",
      " 'First Vice-President of the Supreme Military Council and Commis·sioner for External Affairs of Equatorial Guinea'\n",
      " 'Head of State'\n",
      " 'Minister for Foreign Affairs, African Integration and Chadians Abroad'\n",
      " 'Minister for Foregn Affairs' 'Deputy Prime Minister '\n",
      " 'Chair of the Delegation' 'President of the European Council' 'Prince'\n",
      " 'Second Minister for Foreign Affairs and Trade'\n",
      " 'Prime Minister, Minister for Finance, the Public Service, National Security, Legal Affairs and Grenadines Affairs'\n",
      " 'Secretary for Relations with States'\n",
      " 'Minister of State and Minister for Foreign Affairs, Cooperation and Regional Integration'\n",
      " 'Foreign Minister' 'Amir'\n",
      " 'Deputy Prime Minister, Minister for Foreign Affairs and International Cooperation'\n",
      " 'Prime Minister and Minister of Defence, Economic Growth and Job Creation'\n",
      " 'Minister for Development Cooperation' 'Prime Minister '\n",
      " 'Chair of the delegation' 'Delegation'\n",
      " 'Secretary of Relations with States'\n",
      " 'Deputy Prime Minister and Minister for Foreign Affairs and International Cooperation'\n",
      " 'Vice President and Misnter of External Relations' 'Foreign minister'\n",
      " 'Vice-President and Minister for Foreign Affairs'\n",
      " 'Prime Minister and Minister for General Affairs'\n",
      " 'Special Envoy of the Prime Minister and Minister for Public Service'\n",
      " 'President of the Presidensy'\n",
      " 'Deputy Prime Minister and Minister for Home Affairs'\n",
      " 'Minister for Foreign Affairs, Minister for Education and Minister for Cultural Affairs'\n",
      " 'Secretary of State'\n",
      " 'Prime Minister, Minister for Defence and Foreign Affairs'\n",
      " 'President, Head of Government and Minister for Foreign Affairs and Immigration'\n",
      " 'First Vice-President' 'President and Head of Government'\n",
      " 'Coordinator of the Junta of the Government of National Reconstruction'\n",
      " 'Minister for Foreign Affairs and International Cooperation'\n",
      " 'Prime Minister and Minister for Public Utilities'\n",
      " 'Prime Minister and Minister for Finance and Corporate Governance'\n",
      " 'PRIME MINISTER' 'Minister Responsible for Foreign Affairs'\n",
      " 'Head  of the goverment' 'Head of State '\n",
      " 'Prime Minister and Minister for iTaukei Affairs, Sugar Industry, and Foreign Affairs'\n",
      " 'Prime Minister and Minister for Finance and Public Service'\n",
      " 'Minister for Foreign Affairs and Immigration'\n",
      " 'Minister for Foreign Affairs, Communities and Defence'\n",
      " 'Premier of the State Council'\n",
      " 'Prime Minister and Minister for iTaukei Affairs and Sugar Industry'\n",
      " 'President of the Republic of Mali and Chairman of the Conference of Heads of State of the Permanent Inter-State Committee on Drought Control in the Sahel'\n",
      " 'Chairman of the military council' 'Minister of State'\n",
      " 'Chairman of the Presidency ' 'Chairman of the Presidency'\n",
      " 'Permanent Representative ' 'Chair of Delegation'\n",
      " 'Minister of Foreign Affairs'\n",
      " 'Union Minister for the Office of the State Counsellor'\n",
      " 'Minister for Foreign Affairs, East Africa, Regional and International Cooperation'\n",
      " 'Crown Prince'\n",
      " 'Prime Minister, Minister for National Security and the Civil Service, and Minister for Finance, Economic Affairs and Investment'\n",
      " 'First Prime Minister' 'President of the Republic of Pananma'\n",
      " 'Senior Minister and Minister for Foreign Affairs and International Cooperation'\n",
      " 'King'\n",
      " 'Minister for Foreign Affairs and Cooperation and of Congolese Living Abroad'\n",
      " 'President and comander in chief of the armed forces' 'Emir'\n",
      " 'Minister for Foreign Affairs and Senegalese Abroad'\n",
      " 'Federal Minister for European and International Affairs'\n",
      " 'Prime Minister and Minister for External Affairs '\n",
      " 'Senior Minister, Minister for Foreign Affairs and International Cooperation'\n",
      " 'Minister of Youth'\n",
      " 'Minister for Foreign Affairs, African Integration and International Cooperation'\n",
      " 'Chairman' 'Deputy Minister for Foreign Affairs'\n",
      " 'Prime Minister and Minister of Finance ' 'Acting Head of State '\n",
      " 'Vice President of the Republic'\n",
      " 'Prime Minister, Minister for Home Affairs, External Communications and National Development Unit, Minister for Finance and Economic Development'\n",
      " 'Prime Minister and Minister for Foreign Affairs and Trade'\n",
      " 'Prime Minister and Minister for Sustainable Development, National Security, People Empowerment and Constituency Empowerment'\n",
      " 'Federal Minister for Europe, Integration and Foreign Affairs'\n",
      " 'Prime Minister, Minister of State, Minister for Communications and Media and Minister for Religious Affairs'\n",
      " 'vice-President' 'First Vice-President and Minister for Foreign Affairs'\n",
      " 'Sovereign Prince'\n",
      " 'Deputy Prime Minister and Minister for Foreign Affairs and Expatriates'\n",
      " 'Federal Minister for Foreign Affairs'\n",
      " 'President of the Arab Republic of Egypt'\n",
      " 'First Vice-President and Prime Minister '\n",
      " 'Chairman of the Council of Ministers'\n",
      " 'Minister for Foreign and Political Affairs' 'Vice-President '\n",
      " 'Secretary for Foreign Affairs'\n",
      " 'Minister for Foreign Affairs and Trade with responsibility for Brexit'\n",
      " 'Minister for Foreign and CARICOM Affairs'\n",
      " 'President of the Council of the State '\n",
      " 'President of the United Republic of Tanzania' 'interim president'\n",
      " 'Deputy Prime Minister and Minister for Foreign Affairs, International Trade and Regional Integration'\n",
      " 'Head of Government ' 'King '\n",
      " 'Minister for Foreign Affairs and Foreign Trade'\n",
      " 'Vice-Chancellor and Federal Minister for Foreign Affairs'\n",
      " 'Prime Minister and Head of Government' 'Acting President'\n",
      " 'Prime Minister  '\n",
      " 'Prime Minister and Minister for Finance, Economic Growth, Job Creation, External Affairs and the Public Service'\n",
      " 'president' 'Prime Minister and Minister for Public Enterprises'\n",
      " 'President and Comander of the Armed Forces and Minister of Defence '\n",
      " \"Chairman of the Council of Ministers of the People's Republic of Albania\"\n",
      " \"First Secretary of the Central Committee of the Polish United Workers' Party\"\n",
      " 'Chief Executive Officer' 'Prime MInister'\n",
      " 'Minister for Foreign Affairs and Trade'\n",
      " 'Union Minister of the Office of the State Counsellor'\n",
      " 'Prime Minister in charge of Beliris and Federal Cultural Institutions'\n",
      " 'President of the Presidency Council of the Government of National Accord'\n",
      " 'Prime Minister, Minister of Defence, Economic Growth and Job Creation'\n",
      " 'Minister for Foreign Affairs, International Cooperation and Regional Integration'\n",
      " 'Minister for Foreign Affairs, International Cooperation, Regional Integration, Francophonie and Gabonese Abroad'\n",
      " 'Prime Minister, Minister of State, Minister of Communications and Media and Minister for Religious Affairs'\n",
      " 'Vice Minister of Foreign Affairs'\n",
      " 'Minister for Foreign Affairs and East African Cooperation'\n",
      " 'Minister for Foreign Affairs and External Trade'\n",
      " 'Minister for Foreign Affairs, Legal Affairs, Carriacou and Petite Martinique Affairs and Local Government'\n",
      " 'Minister for Foreign Affairs, International Cooperation and Communities'\n",
      " 'Minister for Foreign Affairs and Investment Promotion'\n",
      " 'Prime Minister and Minister for Home Affairs, External Communications and National Development Unit, Minister for Finance and Economic Development'\n",
      " 'First Executive President'\n",
      " 'Chairman of the United National Front of Kampuchea French'\n",
      " 'Prime Minister, First Lord of the Treasury and Minister for the Civil Service'\n",
      " 'Minister for External Affairs' 'Cairman of the Presidency'\n",
      " 'MInister for Foreign Affairs'\n",
      " 'Prime Minister and Minister for foreign affairs '\n",
      " 'Attourney General and Minister for Foreign Affairs and Foreign Trade'\n",
      " 'President Republic' 'President of the Supreme National Council'\n",
      " 'Chairman of the military council and coucil of ministers'\n",
      " 'Head of State and Commander-in-Chief of the Armed Forces'\n",
      " 'Coordinator of the Junta of the Govermnent of National Reconstruction '\n",
      " 'Prime Minister, Minister of Defence and Internatl Security, Minister of Information, Minister of Reform Institutions and Minister of External Commnication '\n",
      " 'President  ' 'Head of the Church ' 'Head of government'\n",
      " 'Prime Minister and Minister for Arts, Culture and Heritage, and National Security and Intelligence'\n",
      " 'Member of the Presidency'\n",
      " 'Cabinet Secretary for Foreign Affairs and International Trade'\n",
      " 'Chancellor'\n",
      " 'Minister for Foreign Affairs, Cooperation, African Integration and Nigeriens Living Abroad'\n",
      " 'Spanish ' 'Head of State and Head of Government'\n",
      " 'Prime Minister and Minister for Bougainville Affairs'\n",
      " 'Minister for Foreign Affairs, International Economic Cooperation and Telecommunications'\n",
      " \"President of  the people's Republic of Mozambique\"\n",
      " 'Chairman of the Assembly Presidium' 'prime minister'\n",
      " 'Minister for Foreign Affairs and Communities'\n",
      " 'President of the Republic of Venezuela'\n",
      " 'Prime Minister, Minister of State, Minister for Communication and Media, and Minister for Worship'\n",
      " 'King and President of the Council of Ministers'\n",
      " 'Prime Minister and Minister for Itaukei Affairs, Sugar Industry and Foreign Affairs'\n",
      " 'Attorney General and Minister for Foreign Affairs'\n",
      " 'Prime Minister and Minister for General and '\n",
      " 'Minister for Foreign Affairs, Cooperation and Congolese Nationals Abroad'\n",
      " ' Deputy Chairman of the Cabinet of Ministers and Minister for Foreign Affairs'\n",
      " 'Prime Minister and Minister of Reform'\n",
      " 'State Counsellor and Minister for Foreign Affairs'\n",
      " 'Prime Minister and Minister for Arts, Culture and Heritage and National Security and Intelligence'\n",
      " 'CHAIRMAN OF THE SUPREME COUNCIL' ' '\n",
      " 'Minister for International Affairs and Cooperation'\n",
      " 'Minister for Foreign Affairs and Labour' 'Head of state'\n",
      " 'Head of the Federal Military Government '\n",
      " 'Minister for External Relations and International Cooperation'\n",
      " 'President of the Eastern Republic of Uruguay'\n",
      " 'Minister for Foreign Affairs and CARICOM Affairs'\n",
      " 'Deputy Prime Minister and Minister for Foreign Affairs and Trade with responsibility for Brexit'\n",
      " 'King of the Hasemite Kingdom of Jordan'\n",
      " 'Vice-President and Minister for Women’s Affairs' 'Sheikh '\n",
      " 'Attorney General and Minister for Foreign Affairs and Foreign Trade'\n",
      " 'President of Government' 'Emperor'\n",
      " 'Minister for Foreign Affairs, Cooperation, African Integration and Nigeriens Abroad'\n",
      " 'Acting Minister for Foreign Affairs and International Cooperation'\n",
      " 'Chief Advisor of the Interim Government and Minister for Foreign Affairs'\n",
      " 'CROWN PRINCE' 'President    ' 'Prime Minister   '\n",
      " 'Minister for Foreign Affairs, Foreign Trade and Immigration'\n",
      " 'President and Minister for Defense' 'Chief Executive'\n",
      " 'Minister for Foreign Affairs and Human Mobility'\n",
      " 'President of the Government and Minister for Foreign Affairs and Commerce'\n",
      " 'Pope' 'Ministry of Foreign Affairs '\n",
      " 'rime Minister and Minister for Europe and Foreign Affairs'\n",
      " 'Prime Minister and Minister for General and Foreign Affairs '\n",
      " 'President of Democratic Kampuchea'\n",
      " 'Minister at the Prime Minister’s Office and Second Minister for Foreign Affairs and Trade'\n",
      " 'Minsiter for Foreign and CARICOM Affairs'\n",
      " 'PREMIER OF THE ADMINISTRATIVE COUNCIL '\n",
      " 'President of the Republic of Malawi, Minister for Defense and Commander-in-Chief of the Malawi Defense Force and the Malawi Police Service'\n",
      " 'Chairman of the Presidential Council '\n",
      " 'Prime Minister, Minister of Defence and Home Affairs, Minister of Rodrigues and National Development Unit'\n",
      " 'President ad interim' 'Minister for Foreign Affairs and Aviation'\n",
      " 'President of the Republic of Peru'\n",
      " 'Minister for Foreign Affairs and Caribbean Community Affairs'\n",
      " 'Prince and Head of State '\n",
      " 'Minister for Foreign Affairs and Senegalese Living Abroad'\n",
      " 'President and Head of State'\n",
      " 'Federal Minister for Europe, Integration, and Foreign Affairs'\n",
      " 'Deputy Minister Foreign Affairs'\n",
      " 'Prime Minister, Minister for Home Affairs, External Communications and National Development Unit, and Minister for Finance and Economic Development'\n",
      " 'President of the Councils of State and of Ministers' 'Not indicated'\n",
      " 'President   ' 'Head of the delegation'\n",
      " 'Deputy Prime Minister, Minister of the Interior and Acting Minister for Foreign Affairs'\n",
      " 'Ministry of External Relations '\n",
      " 'Prime Minister, Minister for Foreign Affairs, Minister for the Economy, Minister for Communications, and Minister for Regional Cooperation'\n",
      " 'Deputy Prime Minister and Minister for Foreign Affairs'\n",
      " 'Minister of External Relations'\n",
      " 'Deputy Chairman of the Cabinet of Ministers and Minister for Foreign Affairs'\n",
      " 'President of the Council of State and of the Government '\n",
      " 'President of the Federative Republic of Brazil'\n",
      " 'Chairman of the Transitional Military Council '\n",
      " 'Member of the Junta of the Government of National Reconstruction'\n",
      " 'Minister for Foreign Affairs and for Guineans Abroad'\n",
      " 'Prime Minister and Minister for the Autonomous Region of Bougainville'\n",
      " 'Chairman of the Council of Ministers of the Union of Soviet Socialist Republics'\n",
      " 'President of Faso and President of the Council of Ministers of Burkina Faso'\n",
      " 'Personal Representative of the Head of State of the Republic of the Philippines'\n",
      " 'Minister for Foreign Affairs and Immigration of the Commonwealth'\n",
      " 'Minister for Foreign Affairs, International Business and Diaspora Relations'\n",
      " 'Minister for Foreign Affairs and International Development Coperation'\n",
      " 'President of the National Council of Government'\n",
      " ' Coordinator of the Junta of the Government' 'First Vice-President '\n",
      " 'Deputy prime minister and minister of Foreign Affairs and specail envoy '\n",
      " 'Minister of Foreign Affairs and Cooperation'\n",
      " 'Minister for Foreign Affairs and Worship'\n",
      " 'Minister for International Relations and Cooperation']\n",
      "Unmatched position: Commander in Chief, Head of State\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'row' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 122\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnmatched position:\u001b[39m\u001b[38;5;124m\"\u001b[39m, pos) \n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOthers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 122\u001b[0m df_merged[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposition\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_merged[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposition\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(normalize_position)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4943\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4808\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4809\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4810\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4815\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4816\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4817\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4818\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4819\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4934\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4935\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4936\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4937\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4938\u001b[0m         func,\n\u001b[0;32m   4939\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4940\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4941\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4942\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4943\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1422\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1502\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1500\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1501\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1502\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1503\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1504\u001b[0m )\n\u001b[0;32m   1506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1507\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1508\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:925\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mpandas/_libs/lib.pyx:2999\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[42], line 81\u001b[0m, in \u001b[0;36mnormalize_position\u001b[1;34m(pos)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMonarch detected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(?i)^head of state\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, pos):\n\u001b[1;32m---> 81\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHead of State detected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(?i)^head of gover?ment\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, pos):\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHead of Government detected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'row' is not defined"
     ]
    }
   ],
   "source": [
    "# == Adjust position variable ==\n",
    "\n",
    "print(df_merged['position'].unique())\n",
    "# For speakers that have more than one position it appears that the higher position is always listed first, therefore the second position will be dropped in the following function\n",
    "\n",
    "# Since there are many expression for the position variable and it dos not seem to be unified, the position variable is adjusted to enable more consitency\n",
    "\n",
    "def normalize_position(pos):\n",
    "    pos = row[\"position\"]\n",
    "    country\n",
    "    if pd.isna(pos):\n",
    "        return pos\n",
    "\n",
    "    pos = pos.strip()\n",
    "\n",
    "    # Fix common typos and extra spaces\n",
    "    pos = re.sub(r'\\s+', ' ', pos) \n",
    "    pos_lower = pos.lower()\n",
    "\n",
    "     # Turn all ministers that deal with foreign affairs and international relations to \"Minister for Foreign Affairs\"\n",
    "    foreign_affairs_variants = [\n",
    "        'minister for foregn affairs',\n",
    "        'minister responsible for foreign affairs',\n",
    "        'minsiter for foreign and caricom affairs',\n",
    "        'minister for external affairs',\n",
    "        'minister of external relations',  # <-- added\n",
    "        'foreign minister',\n",
    "        'minister for international affairs and cooperation',\n",
    "        'minister for external relations',\n",
    "        'federal minister for european and international affairs',\n",
    "        'international cooperation',\n",
    "        'federal minister for foreign affairs',\n",
    "        'minister for foreign and caricom affairs',\n",
    "        'minister of foreign affairs and cooperation',\n",
    "        'minister for international relations and cooperation',\n",
    "        'ministry of external relations',\n",
    "        'acting minister for foreign affairs and international cooperation',\n",
    "        'ministry of foreign affairs',\n",
    "        'minister for foreign and political affairs',\n",
    "        'federal minister for europe, integration, and foreign affairs',\n",
    "        'federal minister for europe, integration and foreign affairs',\n",
    "        'minister of foreign and european affaris',\n",
    "        'minister of foreign affairs',\n",
    "        'minister for foreign',\n",
    "        'minister of foreign and european affairs and minister of immigration and asylum',\n",
    "        'minister for foreign affairs and senegalese living abroad',\n",
    "        'minister for foreign affairs with responsibility for brexit',\n",
    "        'minister for foreign affairs and investment promotion'\n",
    "       \n",
    "    ]\n",
    "    if any(variant in pos_lower for variant in foreign_affairs_variants):\n",
    "        return \"Minister for Foreign Affairs\"\n",
    "\n",
    "    # Fix \"rime minister\" typo\n",
    "    pos = re.sub(r'(?i)\\brime[- ]?minister\\b', 'Prime Minister', pos)\n",
    "\n",
    "    # Normalize different versions of Head of Government, President, Prime Minsiter and Vice-President-\n",
    "    exact_matches = {\n",
    "        r'(?i)^president of (the )?government$': 'Head of Government',\n",
    "        r'(?i)^acting president$': 'President',\n",
    "        r'(?i)^interim president$': 'President',\n",
    "        r'(?i)^constitutional president$': 'President',\n",
    "        r'(?i)^first executive president$': 'President',\n",
    "        r'(?i)^first prime[- ]?minister$': 'Prime Minister',\n",
    "        r'(?i)^head of the goverment$': 'Head of Government',  # <-- catch typo + spaces\n",
    "        r'(?i)^head\\s+of\\s+govern?ment$': 'Head of Government',\n",
    "        r'(?i)^first vice[- ]?president$': 'Vice-President'\n",
    "    }\n",
    "    for pattern, replacement in exact_matches.items():\n",
    "        if re.fullmatch(pattern, pos):\n",
    "            return replacement\n",
    "\n",
    "    # Normalize prefixes\n",
    "    pos = re.sub(r'(?i)^first vice[- ]?president\\b', 'Vice-President', pos)\n",
    "    pos = re.sub(r'(?i)\\bprime[- ]?minister\\b', 'Prime Minister', pos)\n",
    "    pos = re.sub(r'(?i)\\bpresident\\b', 'President', pos)\n",
    "    pos = re.sub(r'(?i)\\bvice[- ]?president\\b', 'Vice-President', pos)\n",
    "\n",
    "    # Print observations for Monarchs, Heads of State, and Heads of Government\n",
    "    if re.search(r'(?i)\\b(crown prince|prince|king|emir|amir|sheikh)\\b', pos):\n",
    "        print(f\"Monarch detected: {row['country_name']}, {row['year']}\")\n",
    "    elif re.search(r'(?i)^head of state\\b', pos):\n",
    "        print(f\"Head of State detected: {row['country_name']}, {row['year']}\")\n",
    "    elif re.search(r'(?i)^head of gover?ment\\b', pos):\n",
    "        print(f\"Head of Government detected: {row['country_name']}, {row['year']}\")\n",
    "\n",
    "\n",
    "    # Collapse primary roles if they appear at start\n",
    "    primary_roles = [\n",
    "        (r'(?i)^prime[- ]?minister\\b', 'Prime Minister'),\n",
    "        (r'(?i)^deputy prime[- ]?minister\\b', 'Deputy Prime Minister'),\n",
    "        (r'(?i)^president\\b', 'President'),\n",
    "        (r'(?i)^vice[- ]?president\\b', 'Vice-President'),\n",
    "        (r'(?i)^head of state\\b', 'Head of State'),\n",
    "        (r'(?i)^head of government\\b', 'Head of Government'),\n",
    "        (r'(?i)^(crown prince|prince|king|emir|amir)\\b', 'Monarch'),\n",
    "        (r'(?i)^(un representative|permanent representative|delegation|chair of (the )?delegation|chair of diplomatic representative)\\b', 'Diplomatic Representative')\n",
    "    ]\n",
    "    for pattern, replacement in primary_roles:\n",
    "        if re.match(pattern, pos):\n",
    "            return replacement\n",
    "\n",
    "    # Monarchs\n",
    "    #if re.search(r'(?i)\\b(crown prince|prince|king|emir|amir)\\b', pos):\n",
    "     #   return \"Monarch\"\n",
    "\n",
    "    # Head of State\n",
    "    #if re.search(r'(?i)head of state', pos):\n",
    "     #   return \"Head of State\"\n",
    "        \n",
    "    # Diplomatic Representatives\n",
    "    #if re.search(r'(?i)(un representative|permanent representative|delegation|chair of (the )?delegation|chair of diplomatic representative)', pos):\n",
    "     #   return \"Diplomatic Representative\"\n",
    "\n",
    "    # Not indicated\n",
    "    #if re.search(r'(?i)not indicated', pos):\n",
    "     #   return np.nan\n",
    "\n",
    "    # Everything that is leftover becomes Others\n",
    "    print(\"Unmatched position:\", pos) \n",
    "    return \"Others\"\n",
    "\n",
    "\n",
    "df_merged[\"position\"] = df_merged[\"position\"].apply(normalize_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "7a99b3df-550a-41a3-a751-2692c0f7ea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize position titles by merging deputy, second, and vice roles into their corresponding official positions for consistent categorization\n",
    "\n",
    "def merge_positions(pos):\n",
    "    if pd.isna(pos):\n",
    "        return pos \n",
    "    \n",
    "    # Merge Prime Minister roles\n",
    "    if pos in [\"Prime Minister\", \"Deputy Prime Minister\"]:\n",
    "        return \"(Deputy) Prime Minister\"\n",
    "    \n",
    "    # Merge President roles\n",
    "    if pos in [\"President\", \"Vice-President\"]:\n",
    "        return \"(Vice-) President\"\n",
    "    \n",
    "    # Move these roles to Others\n",
    "    if pos in [\"Head of State\", \"Head of Government\", \"Monarch\"]:\n",
    "        return \"Others\"\n",
    "        \n",
    "    if pos in [\"Minister for Foreign Affairs\", \"Deputy Minister for Foreign Affairs\",\n",
    "        \"Deputy Minister Foreign Affairs\",\n",
    "        \"Second Minister for Foreign Affairs\",\n",
    "        \"Second Minister for Foreign Affairs and Trade\",\n",
    "        \"Vice Minister for Foreign Affairs\"]:\n",
    "        return \"(Deputy) Minister for Foreign Affairs\"\n",
    "    \n",
    "    return pos\n",
    "\n",
    "df_merged[\"position\"] = df_merged[\"position\"].apply(merge_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "14ececb8-262d-4270-8d52-5742a864cf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN                                      4679\n",
      "(Deputy) Minister for Foreign Affairs    2387\n",
      "(Vice-) President                        2060\n",
      "(Deputy) Prime Minister                  1239\n",
      "Diplomatic Representative                 340\n",
      "Others                                    247\n",
      "Name: position, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "position_counts = df_merged['position'].value_counts(dropna=False)\n",
    "print(position_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41d97e2-840b-40eb-b032-197040a0246c",
   "metadata": {},
   "source": [
    "### New Variable: Country (Year)\n",
    "\n",
    "This variable is later needed to create clean description plots and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "d647e412-7bd4-4190-9c34-39874b4620a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_merged.copy()\n",
    "df_merged['speech_label'] = df_merged['country_name'] + \" (\" + df_merged['year'].astype(str) + \")\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc05d65d-be72-42ae-b753-0940de5df17c",
   "metadata": {},
   "source": [
    "### Save dataframe with all new variables as un_corpus_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "94a901c8-3fc4-4b33-a1ff-cede22bd196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(wd)\n",
    "\n",
    "merged_pickle_path = r\".\\data\\un_corpus_merged.pkl\"\n",
    "df_merged.to_pickle(merged_pickle_path)\n",
    " \n",
    "merged_output_path = r\".\\data\\un_corpus_merged.csv\"\n",
    "df_merged.to_csv(merged_output_path, index=False, sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea36f19a-789d-4c62-a470-3e73a6d848bf",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe0f4af-e939-4100-9618-5b522bdd72a1",
   "metadata": {},
   "source": [
    "## Cleaning & Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2661f431-a194-47d1-9a1c-89162bf85a91",
   "metadata": {},
   "source": [
    "#### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "cd37e08d-b4f2-459e-823c-3816a825ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Clean corpus by removing empty spaces, line breaks, hyphenation, stray characters, and escape quote ==\n",
    "\n",
    "def cleaning(content):\n",
    "    if pd.isna(content):\n",
    "        return \"\"\n",
    "    \n",
    "    content = content.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    content = ' '.join(content.split())\n",
    "\n",
    "    # Ensure spacing after punctuation\n",
    "    content = re.sub(r'(?<=[.,])(?=[^\\s])', r' ', content)\n",
    "\n",
    "    # Remove hyphenation at line breaks (e.g. \"inter- national\" → \"international\")\n",
    "    content = re.sub(r'-\\s', '', content)\n",
    "\n",
    "    # Replace hyphen between letters with a space to prevent merging words (e.g. \"russian-and\" → \"russian and\")\n",
    "    content = re.sub(r'(?<=\\w)-(?=\\w)', ' ', content)\n",
    "\n",
    "    content = content.replace(\"\\\\\", \"\")\n",
    "\n",
    "    return content\n",
    "\n",
    "df_merged['speech'] = df_merged['speech'].astype(str)  # Ensure column is string type\n",
    "df_clean = df_merged.copy()\n",
    "df_clean['speech'] = df_clean['speech'].apply(cleaning)\n",
    "\n",
    "# Drop rows with empty speeches after cleaning\n",
    "df_clean = df_clean[df_clean['speech'].str.strip().astype(bool)].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "7c6bd148-d5cf-4eb3-a66d-fc706e99c13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved clean speeches chunks in 'C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp'\n"
     ]
    }
   ],
   "source": [
    "# == Split cleaned data into chunks and save as separate files ==\n",
    "\n",
    "clean_data = df_clean[['filename', 'speech']].values.tolist()\n",
    "\n",
    "data_id1 = clean_data[:int(len(clean_data)/4)]\n",
    "data_id2 = clean_data[int(len(clean_data)/4): int(2*len(clean_data)/4)]\n",
    "data_id3 = clean_data[int(2*len(clean_data)/4): int(3*len(clean_data)/4)]\n",
    "data_id4 = clean_data[int(3*len(clean_data)/4):]\n",
    "\n",
    "os.chdir(data_temp)\n",
    "\n",
    "joblib.dump(data_id1, 'clean_speeches_indexed1.pkl')\n",
    "joblib.dump(data_id2, 'clean_speeches_indexed2.pkl')\n",
    "joblib.dump(data_id3, 'clean_speeches_indexed3.pkl')\n",
    "joblib.dump(data_id4, 'clean_speeches_indexed4.pkl')\n",
    "\n",
    "# Store list of cleaned data chunk paths to feed into preprocessing function later\n",
    "clean_files = [\n",
    "    os.path.join(data_temp, 'clean_speeches_indexed1.pkl'),\n",
    "    os.path.join(data_temp, 'clean_speeches_indexed2.pkl'),\n",
    "    os.path.join(data_temp, 'clean_speeches_indexed3.pkl'),\n",
    "    os.path.join(data_temp, 'clean_speeches_indexed4.pkl')\n",
    "]\n",
    "\n",
    "print(f\"Saved clean speeches chunks in '{data_temp}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6e5109-8cf7-41e3-8fb9-8e7dde29b4e0",
   "metadata": {},
   "source": [
    "### Text Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfca8dd9-c6d7-48c7-96b0-f5a449814336",
   "metadata": {},
   "source": [
    "#### Extend stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "0fd36a77-c5b3-4f7b-9c5e-04611ff909e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\n"
     ]
    }
   ],
   "source": [
    "#os.chdir(data_c)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "c1221ea1-aeeb-46ec-882d-73262ee9040f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full path to stopwords pickle\n",
    "#stopwords_path = os.path.join(data_c, \"stopwords.pkl\")\n",
    "\n",
    "# Load & sort stopwords\n",
    "#stopwords = joblib.load(stopwords_path)\n",
    "#stopwords = sorted(stopwords)\n",
    "\n",
    "#print(f\"Loaded {len(stopwords)} stopwords (sorted alphabetically)\")\n",
    "#print(stopwords[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "dae59238-3f9c-454d-baf1-a9f18b8685e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 315 stemmed stopwords to C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\spacy_stopwords_stemmed.pkl\n",
      "[\"'d\", \"'m\", \"'s\", 'a', 'about', 'abov', 'across', 'afirca', 'after', 'afterward', 'again', 'against', 'all', 'almost', 'alon', 'along', 'alreadi', 'also', 'although', 'alway', 'am', 'among', 'amongst', 'amount', 'an', 'and', 'ani', 'anoth', 'anyon', 'anyth']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Stem SpaCy stopwords and convert to set\n",
    "stemmed_spacy = set(stemmer.stem(w) for w in SPACY_STOPWORDS)\n",
    "\n",
    "exclude_words = {\"please\", \"empti\", \"somehow\", \"anyhow\", \"somewher\"}  # can add more\n",
    "stemmed_spacy -= exclude_words\n",
    "\n",
    "# Step 2: Your already-stemmed custom stopwords\n",
    "my_stemmed_stopwords = {\"year\", \"time\", \"member\", \"session\", \"work\", \"oper\",\n",
    "                        \"nation\", \"south\", \"east\", \"countri\", \"afirca\", \"deleg\",\n",
    "                        \"state\", \"peopl\", \"general\", \"organ\", \"assembl\",\n",
    "                        \"way\", \"role\", \"present\", \"foreign\", \"presid\"}\n",
    "\n",
    "# Step 3: Merge sets and sort to get a list\n",
    "\n",
    "STEMMED_STOPWORDS = sorted(stemmed_spacy.union(my_stemmed_stopwords))\n",
    "\n",
    "# Step 4: Save\n",
    "save_path = os.path.join(data_c, \"spacy_stopwords_stemmed.pkl\")\n",
    "joblib.dump(STEMMED_STOPWORDS, save_path)\n",
    "\n",
    "print(f\"Saved {len(STEMMED_STOPWORDS)} stemmed stopwords to {save_path}\")\n",
    "print(STEMMED_STOPWORDS[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "11d056b5-6729-45f8-a233-3e8e6de1f69d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get SpaCy stopwords\n",
    "#SPACY_STOPWORDS = list(nlp.Defaults.stop_words)\n",
    "\n",
    "# Stem each stopword and deduplicate\n",
    "#STEMMED_STOPWORDS = sorted(set(stemmer.stem(w) for w in SPACY_STOPWORDS))\n",
    "\n",
    "# Save as a pickle file inside your data_c folder\n",
    "#save_path = os.path.join(data_c, \"spacy_stopwords_stemmed.pkl\")\n",
    "#joblib.dump(STEMMED_STOPWORDS, save_path)\n",
    "\n",
    "#print(f\"Saved {len(STEMMED_STOPWORDS)} stemmed stopwords to {save_path}\")\n",
    "#print(STEMMED_STOPWORDS[:300])  # preview first 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "25ae7409-b068-4025-b94d-480acf39f7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dictionaries            ##### How did they come up with this dictionary? Why did they exclude words?\n",
    "\n",
    "affect_path = os.path.join(data_dict, 'dictionary_affect.pkl')\n",
    "cognition_path = os.path.join(data_dict, 'dictionary_cognition.pkl')\n",
    "\n",
    "affect = joblib.load(affect_path)\n",
    "cognition = joblib.load(cognition_path)\n",
    "\n",
    "# Check that none of the stopwords are part of the stemmed affect or cognition dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "38485712-22fa-4780-9503-a0db2367207c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affect dictionary contains 1 stopwords: ['pleas']\n",
      "Cognition dictionary contains 0 stopwords: []\n"
     ]
    }
   ],
   "source": [
    "# Beispiel: affect dictionary gegen gestemmte Stopwords prüfen\n",
    "affect_stopwords = [w for w in affect if w in STEMMED_STOPWORDS]\n",
    "cognition_stopwords = [w for w in cognition if w in STEMMED_STOPWORDS]\n",
    "\n",
    "print(f\"Affect dictionary contains {len(affect_stopwords)} stopwords: {affect_stopwords}\")\n",
    "print(f\"Cognition dictionary contains {len(cognition_stopwords)} stopwords: {cognition_stopwords}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "0aa75944-4b75-4aa4-9430-9aa6bd9ac934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Functions to remove punctioation, tokenize, lowercase, pure digit tokens, words shorter than 2 letters, POS-Tagging, stemm, stopword removal ==\n",
    "\n",
    "def pro1(lista):\n",
    "    return [[row[0], row[1].translate(translator)] for row in lista]\n",
    "\n",
    "def pro2(lista):\n",
    "    return [[row[0], gensim.utils.simple_preprocess(row[1])] for row in lista]\n",
    "\n",
    "def pro3(lista):\n",
    "        a = [[row[0], [w for w in row[1] if not w.isdigit()]] for row in lista]\n",
    "        return a\n",
    "    \n",
    "def pro4(lista):\n",
    "    return [[row[0], [w for w in row[1] if len(w) > 2]] for row in lista]\n",
    "\n",
    "\n",
    "def tags(lista):\n",
    "    t = [[row[0], tagger.tag(row[1])] for row in lista]  # tag each tokenlist\n",
    "    t = [[row[0], [i[0] for i in row[1] if i[1].startswith(('N', 'V', 'J'))]] for row in t]\n",
    "    return t\n",
    "    \n",
    "def pro5(lista):\n",
    "    return [\n",
    "        [row[0], [stemmer.stem(token) for token in row[1]]]\n",
    "        for row in lista\n",
    "    ]\n",
    "    \n",
    "def pro6(lista):\n",
    "    return [[row[0], [w for w in row[1] if w not in STEMMED_STOPWORDS]] for row in lista]\n",
    "      \n",
    "########################## Question for Max: They removed procedural words in the paper\n",
    "\n",
    "def dropnull(lista):\n",
    "    return [row for row in lista if len(' '.join(row[1])) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "524a274a-004c-4d87-9bb3-5f789a42b90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl] Before tagging: 14.19s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl] After tagging: 218.21s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed1.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl] Done. Total time: 287.26s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl] Before tagging: 13.72s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl] After tagging: 220.26s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed2.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl] Done. Total time: 289.97s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl] Before tagging: 13.51s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl] After tagging: 216.50s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed3.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl] Done. Total time: 284.56s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl] Before tagging: 13.21s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl] After tagging: 217.05s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed4.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl] Done. Total time: 285.38s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# == Create full pre-processing function and call it\n",
    "\n",
    "def preprocessing(data_name):\n",
    "    t0 = time.time()\n",
    "    print(f\"Starting preprocessing for {data_name}...\")\n",
    "\n",
    "    data = joblib.load(data_name)\n",
    "    data = pro1(data)\n",
    "    data = pro2(data)\n",
    "    data = pro3(data)\n",
    "    data = pro4(data)\n",
    "\n",
    "    print(f\"[{data_name}] Before tagging: {time.time() - t0:.2f}s\")\n",
    "    data = tags(data)\n",
    "    print(f\"[{data_name}] After tagging: {time.time() - t0:.2f}s\")\n",
    "\n",
    "    data = pro5(data)\n",
    "    data = pro6(data)\n",
    "    \n",
    "    data = dropnull(data)\n",
    "\n",
    "    filename_preprocessed = data_name.replace('clean_speeches_', 'preprocessed_speeches_').replace('.pkl', '.pkl')\n",
    "    out_preprocessed = os.path.join(data_preprocessed, os.path.basename(filename_preprocessed))\n",
    "    joblib.dump(data, out_preprocessed\n",
    "               )\n",
    "    print(f\"[{data_name}] Saved stemmed version: {out_preprocessed}\")\n",
    "\n",
    "    print(f\"[{data_name}] Done. Total time: {time.time() - t0:.2f}s\\n\")\n",
    "\n",
    "def main():\n",
    "    for fname in clean_files:\n",
    "        preprocessing(fname)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "d3409987-485c-47c9-9430-01fa2c0e650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the pre-processed data\n",
    "preprocessed_files = [\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed1.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed2.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed3.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed4.pkl')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "5782a5ba-2b38-4b49-9c3a-3e8e6310ae84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          filename                                             speech  \\\n",
      "0  HTI_70_2015.txt  Mr. President, I would like to express my warm...   \n",
      "1  PRY_58_2003.txt  ﻿Two hundred years after the first cry of free...   \n",
      "2  GMB_72_2017.txt  With warm greetings to all members of the Gene...   \n",
      "3  SLV_04_1949.txt  Mr. Castro stated that the election of General...   \n",
      "4  LBY_56_2001.txt  ﻿At the\\noutset, I would like to congratulate ...   \n",
      "\n",
      "  country_code  year country_name  speech_length_words  \\\n",
      "0          HTI  2015        Haiti                 1601   \n",
      "1          PRY  2003     Paraguay                 1385   \n",
      "2          GMB  2017       Gambia                 1696   \n",
      "3          SLV  1949  El Salvador                 2211   \n",
      "4          LBY  2001        Libya                 4110   \n",
      "\n",
      "   english_official_language  security_council_permanent  \\\n",
      "0                          0                           0   \n",
      "1                          0                           0   \n",
      "2                          1                           0   \n",
      "3                          0                           0   \n",
      "4                          0                           0   \n",
      "\n",
      "                   speaker_name                               position  \\\n",
      "0    Mr. Michel Joseph Martelly                      (Vice-) President   \n",
      "1         Nicanor Duarte Frutos                      (Vice-) President   \n",
      "2              Mr. Adama Barrow                      (Vice-) President   \n",
      "3                   Mr. Castro                                     NaN   \n",
      "4  Abdurrahman Mohamed Shalghem  (Deputy) Minister for Foreign Affairs   \n",
      "\n",
      "   gender_dummy        speech_label  \\\n",
      "0           0.0        Haiti (2015)   \n",
      "1           NaN     Paraguay (2003)   \n",
      "2           0.0       Gambia (2017)   \n",
      "3           0.0  El Salvador (1949)   \n",
      "4           NaN        Libya (2001)   \n",
      "\n",
      "                                 speech_preprocessed  \n",
      "0  [like, express, warmest, congratul, elect, pre...  \n",
      "1  [cri, freedom, latin, propel, new, wind, emanc...  \n",
      "2  [warm, greet, new, gambia, thank, almighti, go...  \n",
      "3  [castro, elect, romulo, head, philippin, point...  \n",
      "4  [outset, like, congratul, unanim, elect, sixth...  \n"
     ]
    }
   ],
   "source": [
    "# Load all preprocessed pickle files\n",
    "preprocessed_data = []\n",
    "for f in preprocessed_files:\n",
    "    preprocessed_data.extend(joblib.load(f))\n",
    "\n",
    "# Turn into DataFrame\n",
    "df_preprocessed = pd.DataFrame(preprocessed_data, columns=[\"filename\", \"speech_preprocessed\"])\n",
    "\n",
    "# Merge into df_merged\n",
    "df_merged = df_merged.merge(df_preprocessed, on=\"filename\", how=\"left\")\n",
    "\n",
    "print(df_merged.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "62beb4e4-e324-4398-bb0e-637eabe6af25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          filename  speech_length_preprocessed\n",
      "0  HTI_70_2015.txt                         692\n",
      "1  PRY_58_2003.txt                         595\n",
      "2  GMB_72_2017.txt                         769\n",
      "3  SLV_04_1949.txt                         722\n",
      "4  LBY_56_2001.txt                        1610\n",
      "Total unique tokens: 40001\n",
      "Average number of tokens per speech: 1177.18\n"
     ]
    }
   ],
   "source": [
    "# == New variable: Speech length of the preprocessed corpus ==\n",
    "\n",
    "# Count tokens in preprocessed speech\n",
    "df_merged[\"speech_length_preprocessed\"] = df_merged[\"speech_preprocessed\"].apply(\n",
    "    lambda x: len(x) if isinstance(x, list) else 0\n",
    ")\n",
    "\n",
    "# Quick check\n",
    "print(df_merged[[\"filename\", \"speech_length_preprocessed\"]].head())\n",
    "all_tokens = [token for speech in df_merged[\"speech_preprocessed\"].dropna() for token in speech]\n",
    "unique_tokens = set(all_tokens)\n",
    "print(\"Total unique tokens:\", len(unique_tokens))\n",
    "\n",
    "# Average length of preprocessed speeches\n",
    "average_length = df_merged[\"speech_length_preprocessed\"].mean()\n",
    "\n",
    "print(f\"Average number of tokens per speech: {average_length:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "746e374b-d840-4c4e-88be-ec29c1d08587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          filename                                             speech  \\\n",
      "0  HTI_70_2015.txt  Mr. President, I would like to express my warm...   \n",
      "1  PRY_58_2003.txt  ﻿Two hundred years after the first cry of free...   \n",
      "2  GMB_72_2017.txt  With warm greetings to all members of the Gene...   \n",
      "3  SLV_04_1949.txt  Mr. Castro stated that the election of General...   \n",
      "4  LBY_56_2001.txt  ﻿At the\\noutset, I would like to congratulate ...   \n",
      "\n",
      "  country_code  year country_name  speech_length_words  \\\n",
      "0          HTI  2015        Haiti                 1601   \n",
      "1          PRY  2003     Paraguay                 1385   \n",
      "2          GMB  2017       Gambia                 1696   \n",
      "3          SLV  1949  El Salvador                 2211   \n",
      "4          LBY  2001        Libya                 4110   \n",
      "\n",
      "   english_official_language  security_council_permanent  \\\n",
      "0                          0                           0   \n",
      "1                          0                           0   \n",
      "2                          1                           0   \n",
      "3                          0                           0   \n",
      "4                          0                           0   \n",
      "\n",
      "                   speaker_name                               position  \\\n",
      "0    Mr. Michel Joseph Martelly                      (Vice-) President   \n",
      "1         Nicanor Duarte Frutos                      (Vice-) President   \n",
      "2              Mr. Adama Barrow                      (Vice-) President   \n",
      "3                   Mr. Castro                                     NaN   \n",
      "4  Abdurrahman Mohamed Shalghem  (Deputy) Minister for Foreign Affairs   \n",
      "\n",
      "   gender_dummy        speech_label  \\\n",
      "0           0.0        Haiti (2015)   \n",
      "1           NaN     Paraguay (2003)   \n",
      "2           0.0       Gambia (2017)   \n",
      "3           0.0  El Salvador (1949)   \n",
      "4           NaN        Libya (2001)   \n",
      "\n",
      "                                 speech_preprocessed  \\\n",
      "0  [like, express, warmest, congratul, elect, pre...   \n",
      "1  [cri, freedom, latin, propel, new, wind, emanc...   \n",
      "2  [warm, greet, new, gambia, thank, almighti, go...   \n",
      "3  [castro, elect, romulo, head, philippin, point...   \n",
      "4  [outset, like, congratul, unanim, elect, sixth...   \n",
      "\n",
      "   speech_length_preprocessed  \n",
      "0                         692  \n",
      "1                         595  \n",
      "2                         769  \n",
      "3                         722  \n",
      "4                        1610  \n"
     ]
    }
   ],
   "source": [
    "print(df_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "134ddbf7-9b95-4697-9d9c-f9c55b52ec80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73db5f7-566d-4276-a631-c1f33fab494d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d006e-6a23-453b-9c91-0e60bd3f8301",
   "metadata": {},
   "source": [
    "## Word-Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf2443e-4c76-41b2-bc16-e72d91c318a8",
   "metadata": {},
   "source": [
    "### Count frequencies of all tokens and display the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "154d978d-cddd-431c-9d53-73c9faf8fdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:19<00:00,  4.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Stemmed] Top 300 most common words:\n",
      "unit: 187107\n",
      "intern: 161305\n",
      "develop: 147090\n",
      "peac: 134933\n",
      "world: 133091\n",
      "secur: 86024\n",
      "govern: 75551\n",
      "econom: 73475\n",
      "right: 67482\n",
      "new: 59536\n",
      "effort: 57570\n",
      "human: 57178\n",
      "problem: 56809\n",
      "support: 55692\n",
      "continu: 53696\n",
      "communiti: 49388\n",
      "region: 48963\n",
      "polit: 48469\n",
      "war: 41867\n",
      "need: 41509\n",
      "council: 41193\n",
      "import: 40881\n",
      "achiev: 39487\n",
      "power: 38401\n",
      "hope: 38377\n",
      "conflict: 37632\n",
      "situat: 36254\n",
      "principl: 36246\n",
      "global: 36179\n",
      "resolut: 35258\n",
      "africa: 34787\n",
      "republ: 34661\n",
      "forc: 34483\n",
      "great: 34066\n",
      "relat: 33847\n",
      "order: 33512\n",
      "concern: 33294\n",
      "action: 32668\n",
      "nuclear: 32246\n",
      "solut: 32199\n",
      "establish: 31713\n",
      "confer: 31362\n",
      "polici: 30730\n",
      "commit: 30720\n",
      "social: 30684\n",
      "respect: 30371\n",
      "effect: 30316\n",
      "independ: 29254\n",
      "chang: 28912\n",
      "interest: 28268\n",
      "charter: 28158\n",
      "today: 27749\n",
      "african: 27745\n",
      "agreement: 27700\n",
      "system: 27065\n",
      "progress: 27042\n",
      "weapon: 26744\n",
      "end: 26438\n",
      "contribut: 26390\n",
      "respons: 26218\n",
      "process: 26067\n",
      "territori: 26013\n",
      "issu: 25878\n",
      "negoti: 25658\n",
      "live: 25488\n",
      "cooper: 25361\n",
      "believ: 25098\n",
      "implement: 25093\n",
      "question: 24838\n",
      "remain: 24466\n",
      "repres: 24412\n",
      "futur: 24306\n",
      "meet: 23731\n",
      "challeng: 23616\n",
      "secretari: 23601\n",
      "arm: 23169\n",
      "law: 23135\n",
      "result: 23061\n",
      "posit: 23054\n",
      "wish: 22905\n",
      "like: 22714\n",
      "possibl: 22524\n",
      "ensur: 22450\n",
      "area: 22357\n",
      "mean: 22125\n",
      "disarma: 21892\n",
      "adopt: 21693\n",
      "major: 21638\n",
      "place: 21622\n",
      "increas: 21585\n",
      "face: 21570\n",
      "resourc: 21427\n",
      "view: 21370\n",
      "includ: 21305\n",
      "determin: 21259\n",
      "measur: 20918\n",
      "fact: 20897\n",
      "express: 20545\n",
      "strengthen: 20530\n",
      "assist: 20496\n",
      "promot: 20382\n",
      "democrat: 20034\n",
      "base: 19887\n",
      "sustain: 19747\n",
      "provid: 19700\n",
      "special: 19413\n",
      "freedom: 19404\n",
      "non: 19390\n",
      "caus: 19069\n",
      "trade: 19033\n",
      "decis: 18832\n",
      "propos: 18690\n",
      "initi: 18570\n",
      "militari: 18500\n",
      "parti: 18331\n",
      "opportun: 18180\n",
      "climat: 17991\n",
      "union: 17881\n",
      "elect: 17863\n",
      "welcom: 17852\n",
      "act: 17708\n",
      "threat: 17597\n",
      "exist: 17555\n",
      "level: 17527\n",
      "success: 17486\n",
      "activ: 17427\n",
      "goal: 17426\n",
      "reform: 17252\n",
      "come: 17224\n",
      "arab: 17211\n",
      "crisi: 17155\n",
      "economi: 17135\n",
      "creat: 17023\n",
      "middl: 16866\n",
      "terror: 16682\n",
      "bring: 16583\n",
      "stabil: 16576\n",
      "israel: 16528\n",
      "particip: 16241\n",
      "declar: 16194\n",
      "common: 16135\n",
      "justic: 15992\n",
      "necessari: 15863\n",
      "free: 15843\n",
      "group: 15813\n",
      "requir: 15807\n",
      "good: 15752\n",
      "settlement: 15740\n",
      "taken: 15730\n",
      "help: 15669\n",
      "small: 15631\n",
      "integr: 15598\n",
      "condit: 15341\n",
      "past: 15129\n",
      "life: 15070\n",
      "recent: 15026\n",
      "prevent: 14969\n",
      "object: 14952\n",
      "struggl: 14857\n",
      "aggress: 14819\n",
      "step: 14740\n",
      "treati: 14731\n",
      "build: 14643\n",
      "coloni: 14632\n",
      "institut: 14598\n",
      "term: 14406\n",
      "find: 14382\n",
      "programm: 14363\n",
      "protect: 14359\n",
      "lead: 14250\n",
      "reach: 14132\n",
      "palestinian: 14093\n",
      "particular: 14072\n",
      "soviet: 13992\n",
      "consid: 13899\n",
      "natur: 13840\n",
      "financi: 13807\n",
      "recogn: 13768\n",
      "basi: 13722\n",
      "univers: 13516\n",
      "agenda: 13505\n",
      "high: 13501\n",
      "aim: 13486\n",
      "follow: 13441\n",
      "set: 13439\n",
      "constitut: 13438\n",
      "poverti: 13372\n",
      "matter: 13345\n",
      "differ: 13194\n",
      "europ: 13173\n",
      "democraci: 13087\n",
      "plan: 13061\n",
      "histori: 12984\n",
      "address: 12971\n",
      "reason: 12900\n",
      "share: 12802\n",
      "purpos: 12612\n",
      "equal: 12588\n",
      "held: 12559\n",
      "suffer: 12543\n",
      "day: 12502\n",
      "number: 12475\n",
      "understand: 12456\n",
      "rule: 12453\n",
      "form: 12422\n",
      "point: 12418\n",
      "abl: 12409\n",
      "dialogu: 12385\n",
      "let: 12381\n",
      "european: 12324\n",
      "improv: 12318\n",
      "product: 12301\n",
      "danger: 12300\n",
      "popul: 12253\n",
      "given: 12244\n",
      "societi: 12138\n",
      "convent: 12107\n",
      "regim: 12026\n",
      "seek: 11959\n",
      "congratul: 11899\n",
      "affair: 11874\n",
      "central: 11861\n",
      "direct: 11800\n",
      "certain: 11748\n",
      "island: 11738\n",
      "accord: 11726\n",
      "accept: 11712\n",
      "maintain: 11575\n",
      "open: 11560\n",
      "long: 11555\n",
      "sovereignti: 11504\n",
      "task: 11467\n",
      "fundament: 11455\n",
      "play: 11403\n",
      "tension: 11394\n",
      "self: 11328\n",
      "framework: 11260\n",
      "practic: 11222\n",
      "multilater: 11069\n",
      "mankind: 11020\n",
      "america: 11007\n",
      "encourag: 10968\n",
      "resolv: 10913\n",
      "american: 10875\n",
      "solidar: 10872\n",
      "confid: 10864\n",
      "destruct: 10857\n",
      "contin: 10835\n",
      "clear: 10815\n",
      "greater: 10809\n",
      "field: 10796\n",
      "cours: 10686\n",
      "second: 10613\n",
      "allow: 10582\n",
      "decad: 10535\n",
      "spirit: 10459\n",
      "event: 10412\n",
      "environ: 10291\n",
      "attent: 10282\n",
      "stand: 10266\n",
      "growth: 10208\n",
      "emerg: 10202\n",
      "confront: 10169\n",
      "case: 10149\n",
      "committe: 10145\n",
      "experi: 10134\n",
      "cent: 10133\n",
      "fight: 10087\n",
      "collect: 10068\n",
      "approach: 10022\n",
      "carri: 10017\n",
      "cultur: 9988\n",
      "report: 9973\n",
      "discuss: 9969\n",
      "leader: 9952\n",
      "want: 9942\n",
      "centuri: 9921\n",
      "involv: 9910\n",
      "complet: 9887\n",
      "hand: 9826\n",
      "women: 9799\n",
      "agre: 9753\n",
      "comprehens: 9753\n",
      "desir: 9752\n",
      "realiti: 9727\n",
      "valu: 9719\n",
      "current: 9684\n",
      "essenti: 9675\n",
      "control: 9656\n",
      "civil: 9651\n",
      "know: 9625\n",
      "limit: 9592\n",
      "look: 9556\n",
      "author: 9490\n",
      "energi: 9489\n",
      "industri: 9463\n",
      "liber: 9416\n",
      "violat: 9393\n",
      "demand: 9373\n",
      "perman: 9365\n",
      "\n",
      "[Stemmed] Top 50 least common words:\n",
      "zaglio: 1\n",
      "xxhi: 1\n",
      "btt: 1\n",
      "unpob: 1\n",
      "turban: 1\n",
      "inanit: 1\n",
      "quinquennium: 1\n",
      "edutech: 1\n",
      "kamechlieh: 1\n",
      "tinderdevelop: 1\n",
      "aspectsbecaus: 1\n",
      "entel: 1\n",
      "petrolífero: 1\n",
      "sapit: 1\n",
      "macha: 1\n",
      "biocentr: 1\n",
      "harsaw: 1\n",
      "gastronom: 1\n",
      "tradesman: 1\n",
      "secureand: 1\n",
      "leitao: 1\n",
      "writeoff: 1\n",
      "lanto: 1\n",
      "sudandarfur: 1\n",
      "comunidad: 1\n",
      "país: 1\n",
      "portuguesa: 1\n",
      "cpld: 1\n",
      "neater: 1\n",
      "humanto: 1\n",
      "jsepj: 1\n",
      "tavan: 1\n",
      "tolgoi: 1\n",
      "coke: 1\n",
      "tumen: 1\n",
      "mala: 1\n",
      "tradebarri: 1\n",
      "geriatr: 1\n",
      "situationth: 1\n",
      "americaha: 1\n",
      "asiawhich: 1\n",
      "wodd: 1\n",
      "egyptianunit: 1\n",
      "almostt: 1\n",
      "sessionto: 1\n",
      "tumul: 1\n",
      "fid: 1\n",
      "aran: 1\n",
      "wok: 1\n",
      "worlddepend: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit\\\\data\\\\freq\\\\word_counts.pkl']"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#== Count token frequencies ==\n",
    "\n",
    "def count_frequencies(filenames):\n",
    "    total_freq = Counter()\n",
    "    for fname in tqdm(filenames):\n",
    "        data = joblib.load(fname)\n",
    "        tokens = chain.from_iterable(row[1] for row in data if isinstance(row[1], list))\n",
    "        total_freq.update(tokens)\n",
    "    return total_freq\n",
    "\n",
    "#def remove_rare_words(filenames, freqs, min_count=10):\n",
    "   # for fname in filenames:\n",
    "       # data = joblib.load(fname)\n",
    "       # filtered_data = []\n",
    "        #for doc_id, tokens in data:\n",
    "          #  filtered_tokens = [w for w in tokens if freqs.get(w, 0) >= min_count]\n",
    "          #  filtered_data.append([doc_id, filtered_tokens])\n",
    "       # joblib.dump(filtered_data, fname)  # overwrite or save as new file\n",
    "       # print(f\"Processed {fname}: removed words with freq < {min_count}\")\n",
    "\n",
    "# === Count for preprocessed (stemmed) speeches ===\n",
    "word_counts = count_frequencies(preprocessed_files)\n",
    "\n",
    "#remove_rare_words(preprocessed_files, word_counts, min_count=10)\n",
    "\n",
    "print(\"\\n[Stemmed] Top 300 most common words:\")\n",
    "for word, count in word_counts.most_common(300):\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\n[Stemmed] Top 50 least common words:\")\n",
    "for word, count in word_counts.most_common()[-50:]:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Save stemmed word counts\n",
    "save_path = os.path.join(data_freq, 'word_counts.pkl')\n",
    "joblib.dump(word_counts, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "0ef220ae-b830-42a8-81e6-06a6f0b723a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 40001\n"
     ]
    }
   ],
   "source": [
    "num_unique_words = len(word_counts)\n",
    "print(f\"Number of unique words: {num_unique_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "932ce36d-d21c-4f1c-aff7-899c321dcfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_c)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39acefa1-c504-478e-98e5-9d465963dc82",
   "metadata": {},
   "source": [
    "### Count the frequency of the dictionary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "9e28f481-7e20-486c-a6b3-8729d6aeaad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of affect dictionary:\n",
      "['forbid', 'unattract', 'cruelti', 'crappi', 'apathi', 'scari', 'unimpress', 'sin', 'dumbest', 'eas', 'agit', 'sob', 'shocker', 'tragedi', 'fabul', 'strongest', 'giver', 'sigh', 'aw', 'witch', 'hurtl', 'fucktard', 'cruel', 'glamor', 'funni', 'smarter', 'brillianc', 'irrate', 'alright', 'honest', 'profit', 'fearless', 'grievous', 'relax', 'isolationist', 'hah', 'shyness', 'poorest', 'cruelest', 'troublemak', 'disagre', 'agon', 'terror', 'fight', 'pleas', 'poor', 'crazi', 'hostil', 'stupid', 'damnat', 'vain', 'jade', 'heartless', 'nag', 'gloomi', 'damn', 'dishearten', 'pleaser', 'credit', 'warmth', 'greatest', 'whine', 'shame', 'angriest', 'envious', 'grin', 'blameless', 'sweeter', 'laidback', 'stupidest', 'unprotect', 'whiner', 'unlov', 'shake', 'boredom', 'fairer', 'weaker', 'wellb', 'bold', 'sucki', 'unsuccess', 'mourner', 'liken', 'defens', 'invigor', 'tedious', 'paranoid', 'cynic', 'dignifi', 'paranoia', 'sweetest', 'contented', 'humili', 'crush', 'terrif', 'alarm', 'asham', 'impati', 'fake', 'flirti', 'laziest', 'libertarian', 'empti', 'panicki', 'rape', 'contemptu', 'incent', 'reward', 'brutal', 'ignoramus', 'bereav', 'wimp', 'laugh', 'nicer', 'warm', 'resent', 'threat', 'hell', 'tortur', 'mad', 'worship', 'uncomfort', 'depress', 'unsaf', 'faultless', 'numb', 'humorless', 'murder', 'prettiest', 'jealous', 'tragic', 'discomfort', 'loveli', 'ungrat', 'divin', 'lamest', 'weaken', 'horribl', 'fuck', 'agoni', 'grievanc', 'ach', 'isolation', 'masoch', 'uglier', 'amaz', 'devilish', 'bitter', 'devil', 'smartest', 'glori', 'trite', 'vicious', 'unsavori', 'grossest', 'sinist', 'cheeri', 'dismay', 'easier', 'flatter', 'virtuous', 'egot', 'generos', 'cherish', 'heartwarm', 'advers', 'tough', 'adorn', 'overwhelm', 'devast', 'egotist', 'sociabl', 'lazier', 'affection', 'sarcasm', 'humor', 'funniest', 'startl', 'innoc', 'vomit', 'shyli', 'kiss', 'burdensom', 'melancholi', 'weakest', 'worthless', 'gentlest', 'proud', 'alarmist', 'pervert', 'demeanor', 'loneli', 'reassur', 'weird', 'passionless', 'loyalti', 'frustrat', 'weak', 'immor', 'hater', 'libertin', 'flirt', 'terribl', 'threaten', 'fatalist', 'melancholia', 'avers', 'prick', 'sucker', 'meanest', 'virtuos', 'benefici', 'neurot', 'disappoint', 'restless', 'romant', 'bastard', 'angri', 'flatteri', 'stun', 'bolder', 'amor', 'forbidden', 'hug', 'demean', 'disgust', 'truer', 'benefic', 'teas', 'stinki', 'condemnatori', 'turmoil', 'poison', 'cunt', 'gorgeous', 'harm', 'lousi', 'glorifi', 'piti', 'wimpish', 'impression', 'freakish', 'happiest', 'apathet', 'pitiabl', 'unpleas', 'suffer', 'miseri', 'toler', 'mess', 'casual', 'anxieti', 'stale', 'fieri', 'grudg', 'arrog', 'outrag', 'thrill', 'faithless', 'popular', 'vaniti', 'palat', 'fantasi', 'decay', 'domin', 'mood', 'proudest', 'scream', 'stress', 'calm', 'fatigu', 'dumber', 'benign', 'rude', 'steal', 'tension', 'enthusiast', 'intimid', 'sweetheart', 'prettier', 'perv', 'pathet', 'flawless', 'contradictori', 'distress', 'moron', 'gratifi', 'frighten', 'anguish', 'snob', 'defenseless', 'fantast', 'sad', 'thief', 'worsen', 'tear', 'handsom', 'attract', 'messi', 'smother', 'scarier', 'afraid', 'insult', 'disreput', 'wept', 'victim', 'traumat', 'prejudic', 'amus', 'decent', 'joyous', 'masochist', 'abandon', 'foe', 'pleasantri', 'shitless', 'strengthen', 'antagonist', 'liar', 'romantic', 'violenc', 'sentimentalist', 'gratif', 'decept', 'witchcraft', 'liveli', 'impressionist', 'guilt', 'griev', 'rotten', 'disgrac', 'insincer', 'pushi', 'enjoy', 'freaki', 'ador', 'grate', 'homesick', 'braver', 'battlefield', 'niceti', 'hooray', 'incompet', 'thiev', 'timid', 'adventur', 'needi', 'joyless', 'grimac', 'gloom', 'dump', 'horrid', 'magnific', 'unlucki', 'killer', 'uncontrol', 'funnier', 'ruin', 'weepi', 'play', 'encourag', 'fantas', 'irrit', 'sincer', 'rage', 'despis', 'unhappi', 'hate', 'bravest', 'harmless', 'hero', 'vigor', 'pervers', 'dumb', 'degrad', 'shameless', 'villaini', 'envi', 'aggress', 'fucker', 'courag', 'craziest', 'delici', 'scare', 'finer', 'bittersweet', 'trivial', 'wealthi', 'heroic', 'ecstat', 'irrat', 'prejudici', 'smug', 'stammer', 'feroci', 'phobic', 'villain', 'anger', 'fond', 'obsession', 'awesom', 'wors', 'compassion', 'weakl', 'calmer', 'inhibit', 'bother', 'rancid', 'hatr', 'beaten', 'opportunist', 'offens', 'haunt', 'appal', 'painless', 'pessimist', 'molest', 'lol', 'wrongdo', 'frantic', 'heroin', 'rejoic', 'woe', 'miser', 'meaner', 'saddest', 'mooch', 'attack', 'enthusiasm', 'joke', 'meritocraci', 'angrier', 'emot', 'weirdest', 'silliest', 'kisser', 'hurrah', 'crueler', 'nervous', 'dishonor', 'gracious', 'sicker', 'hopeless', 'delect', 'compliment', 'petrifi', 'cuter', 'reek', 'loveless', 'daze', 'weirder', 'triumphant', 'impolit', 'offend', 'richer', 'violent', 'crazier', 'comedown', 'heroism', 'stubborn', 'maniac', 'strain', 'broke', 'jerk', 'unkind', 'asshol', 'reveng', 'jealousi', 'damnabl', 'unimport', 'careless', 'gentler', 'bitch', 'shi', 'terrorist', 'rigid', 'harsh', 'passion', 'annoy', 'compass', 'geek', 'weirdo', 'agre', 'whore', 'splendor', 'suck', 'fun', 'chuckl', 'snobberi', 'mock', 'doom', 'reluct', 'mourn', 'boldest', 'helpless', 'trembl', 'pestil', 'curs', 'desper', 'harass', 'complain', 'joker', 'obsess', 'uneasi', 'nasti', 'greedi', 'belov', 'grim', 'upset', 'greed', 'honesti', 'phoni', 'feudal', 'loveliest', 'trauma', 'dwell', 'ecstasi', 'blame', 'legit', 'calmest', 'sickest', 'scariest', 'sarcast', 'cheat', 'fatal', 'stronger', 'energet', 'feroc', 'healthi', 'goddam', 'disast', 'flirtati', 'defeatist', 'glorious', 'terrifi', 'gossip', 'forbad', 'sicken', 'poorer', 'strangest', 'cute', 'struggl', 'woebegon', 'worri', 'gratitud', 'cheer', 'distraught', 'ok', 'vulner', 'blessed', 'fairest', 'lame', 'optim', 'grouch', 'fright', 'pessim', 'sadder', 'worst', 'despair', 'bash', 'intellectu', 'promis', 'prais', 'heartfelt', 'horror', 'crap', 'repress', 'prouder', 'fought', 'discourag', 'heal', 'lazi', 'precious', 'melanchol', 'isol', 'aggressor', 'giggl', 'idiot', 'twitchi', 'glorif', 'kid', 'selfish', 'smile', 'grief', 'hellish', 'shit', 'fume', 'bitchi', 'fool', 'trustworthi', 'optimist', 'dumpi', 'unwelcom', 'neat', 'nicest', 'furi', 'unfriend', 'ridicul', 'arguabl', 'shamefac', 'pervi', 'upbeat', 'wow', 'easygo', 'blameworthi', 'charmer', 'stink', 'phobia', 'adversari', 'sillier', 'distrust', 'dread', 'happier', 'vital', 'dissatisfact', 'insecur', 'assault', 'ineffect', 'panic', 'powerless', 'nightmar', 'wealth', 'enrag', 'splendid', 'foolish', 'ugliest', 'contradict', 'worrier', 'freak']\n",
      "Number of words in affect dictionary: 629\n",
      "\n",
      "Contents of cognition dictionary:\n",
      "['analyt', 'guess', 'suspect', 'coz', 'mysteri', 'unawar', 'proverbi', 'dissimilar', 'consequenti', 'stimul', 'occasion', 'rethink', 'purest', 'entireti', 'abnorm', 'referenti', 'differenti', 'diagnos', 'ambigu', 'concentr', 'undon', 'undecid', 'disclosur', 'fundamentalist', 'blur', 'reaction', 'variabl', 'recogniz', 'meant', 'reconstruct', 'supposit', 'acknowledg', 'choosi', 'accuraci', 'mislead', 'perceiv', 'hypothesi', 'unknow', 'cuz', 'reveal', 'potenti', 'puriti', 'persuad', 'indirect', 'generat', 'presumpt', 'clue', 'justif', 'evidenti', 'realize', 'deduct', 'causal', 'inevit', 'thinker', 'evidentiari', 'factor', 'undeni', 'unclear', 'pick', 'induc', 'accur', 'absolut', 'everytim', 'reconsider', 'diagnost', 'solut', 'diagnosi', 'discoveri', 'ration', 'unambigu', 'explicit', 'decis', 'insight', 'confess', 'fuzzi', 'led', 'lesson', 'theoriz', 'analyz', 'learner', 'split', 'enlighten', 'cos', 'respons', 'dubious', 'react', 'guarante', 'someday', 'blatant', 'undo', 'perceptu', 'infal', 'afterthought', 'inequ', 'feasibl', 'knowabl', 'approxim', 'logic', 'meaning', 'unknown', 'categor', 'trigger', 'incomplet', 'curious', 'attribut', 'pretend', 'reactionari', 'prove', 'meaningless', 'reactiv', 'proof', 'bet', 'purpos', 'suppos', 'discern', 'imagin', 'kinda', 'comprehens', 'sens', 'percept', 'everyday', 'rational', 'unquestion', 'random', 'implicit', 'clarif', 'theori', 'dunno', 'analysi', 'obedi', 'memor', 'genuin', 'evalu', 'explor', 'linkag', 'grasp', 'curios', 'methink', 'somewher', 'deduc', 'unwant', 'induct', 'unrel', 'outstand', 'discov', 'unresolv', 'exploratori', 'choos', 'perspect', 'inferenti', 'disclos', 'factual', 'arbitrari', 'conting', 'vagu', 'persuas', 'manipul', 'impli', 'somehow', 'sorta', 'borderlin', 'reconcil', 'anyhow', 'conscious', 'probabilist', 'precis', 'rearrang', 'referenc', 'obscur']\n",
      "Number of words in cognition dictionary: 169\n",
      "Unique affect words in text: 541\n",
      "Unique cognition words in text: 154\n",
      "Affect words with count < 10: 118\n",
      "Cognition words with count < 10: 7\n",
      "Top 100 words by weighted frequency:\n",
      "muchhop: 0.9999224415944489\n",
      "giaka: 0.9999224415944489\n",
      "gauci: 0.9999224415944489\n",
      "mst: 0.9999224415944489\n",
      "abdelbaset: 0.9999224415944489\n",
      "mohm: 0.9999224415944489\n",
      "fhimah: 0.9999224415944489\n",
      "kˆchler: 0.9999224415944489\n",
      "existenceof: 0.9999224415944489\n",
      "andlongterm: 0.9999224415944489\n",
      "foodexport: 0.9999224415944489\n",
      "commonheritag: 0.9999224415944489\n",
      "governmentin: 0.9999224415944489\n",
      "legwaila: 0.9999224415944489\n",
      "senamo: 0.9999224415944489\n",
      "лаг: 0.9999224415944489\n",
      "genderbalanc: 0.9999224415944489\n",
      "hoff: 0.9999224415944489\n",
      "florilegium: 0.9999224415944489\n",
      "mesoeconom: 0.9999224415944489\n",
      "topolog: 0.9999224415944489\n",
      "romelus: 0.9999224415944489\n",
      "legliz: 0.9999224415944489\n",
      "lafanmi: 0.9999224415944489\n",
      "selavi: 0.9999224415944489\n",
      "anaxagora: 0.9999224415944489\n",
      "ouousqu: 0.9999224415944489\n",
      "heidegg: 0.9999224415944489\n",
      "tonton: 0.9999224415944489\n",
      "macout: 0.9999224415944489\n",
      "lelei: 0.9999224415944489\n",
      "soria: 0.9999224415944489\n",
      "hu: 0.9999224415944489\n",
      "logwood: 0.9999224415944489\n",
      "mahogani: 0.9999224415944489\n",
      "logger: 0.9999224415944489\n",
      "carimba: 0.9999224415944489\n",
      "sarstum: 0.9999224415944489\n",
      "kekchi: 0.9999224415944489\n",
      "mopan: 0.9999224415944489\n",
      "alio: 0.9999224415944489\n",
      "acta: 0.9999224415944489\n",
      "idioci: 0.9999224415944489\n",
      "syllogist: 0.9999224415944489\n",
      "sessionâ: 0.9999224415944489\n",
      "societyâ: 0.9999224415944489\n",
      "limitedincom: 0.9999224415944489\n",
      "karolo: 0.9999224415944489\n",
      "offert: 0.9999224415944489\n",
      "fckade: 0.9999224415944489\n",
      "fishmeal: 0.9999224415944489\n",
      "bgdsism: 0.9999224415944489\n",
      "subworld: 0.9999224415944489\n",
      "nonr: 0.9999224415944489\n",
      "isabella: 0.9999224415944489\n",
      "absolutori: 0.9999224415944489\n",
      "sumptuous: 0.9999224415944489\n",
      "victoriesno: 0.9999224415944489\n",
      "swagger: 0.9999224415944489\n",
      "preachment: 0.9999224415944489\n",
      "unseal: 0.9999224415944489\n",
      "khadaffi: 0.9999224415944489\n",
      "jemus: 0.9999224415944489\n",
      "junkung: 0.9999224415944489\n",
      "babili: 0.9999224415944489\n",
      "lambast: 0.9999224415944489\n",
      "sisu: 0.9999224415944489\n",
      "perfec: 0.9999224415944489\n",
      "picion: 0.9999224415944489\n",
      "sociat: 0.9999224415944489\n",
      "gurat: 0.9999224415944489\n",
      "esort: 0.9999224415944489\n",
      "growthreduc: 0.9999224415944489\n",
      "megastorm: 0.9999224415944489\n",
      "gmo: 0.9999224415944489\n",
      "unwound: 0.9999224415944489\n",
      "lamennai: 0.9999224415944489\n",
      "costilla: 0.9999224415944489\n",
      "pavon: 0.9999224415944489\n",
      "zapotec: 0.9999224415944489\n",
      "magon: 0.9999224415944489\n",
      "emiliano: 0.9999224415944489\n",
      "ssri: 0.9999224415944489\n",
      "proimperialist: 0.9999224415944489\n",
      "melieu: 0.9999224415944489\n",
      "manhassett: 0.9999224415944489\n",
      "shara: 0.9999224415944489\n",
      "badamasi: 0.9999224415944489\n",
      "naira: 0.9999224415944489\n",
      "thebulwark: 0.9999224415944489\n",
      "thesoviet: 0.9999224415944489\n",
      "calt: 0.9999224415944489\n",
      "inflationarycum: 0.9999224415944489\n",
      "shirazi: 0.9999224415944489\n",
      "welldispos: 0.9999224415944489\n",
      "crespo: 0.9999224415944489\n",
      "eychell: 0.9999224415944489\n",
      "scoresheet: 0.9999224415944489\n",
      "lug: 0.9999224415944489\n",
      "misportray: 0.9999224415944489\n"
     ]
    }
   ],
   "source": [
    "# == Count dictionary words\n",
    "\n",
    "print(\"Contents of affect dictionary:\")\n",
    "print(affect)\n",
    "print(\"Number of words in affect dictionary:\", len(affect))\n",
    "\n",
    "print(\"\\nContents of cognition dictionary:\")\n",
    "print(cognition)\n",
    "print(\"Number of words in cognition dictionary:\", len(cognition))\n",
    "\n",
    "a_list = [[i, word_counts[i]] for i in affect if i in word_counts]\n",
    "c_list = [[i, word_counts[i]] for i in cognition if i in word_counts]\n",
    "\n",
    "a_list = sorted(a_list, key=lambda x: x[1], reverse=True)\n",
    "c_list = sorted(c_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "a = [[i[0], f\"({i[1]}),\"] for i in a_list]\n",
    "c = [[i[0], f\"({i[1]}),\"] for i in c_list]\n",
    "\n",
    "a1 = ' '.join(str(r) for v in a for r in v)\n",
    "c1 = ' '.join(str(r) for v in c for r in v)\n",
    "\n",
    "affect_out_path = os.path.join(data_freq, \"affect_words.txt\")\n",
    "cog_out_path = os.path.join(data_freq, \"cog_words.txt\")\n",
    "\n",
    "with open(affect_out_path, \"w\") as output:\n",
    "    output.write(a1)\n",
    "\n",
    "with open(cog_out_path, \"w\") as output:\n",
    "    output.write(c1)\n",
    "\n",
    "# number of affect/cognitive words that appear in word_counts\n",
    "num_affect_words = len(a_list)\n",
    "num_cog_words = len(c_list)\n",
    "\n",
    "# out of those which appear less than 10 times\n",
    "num_affect_lt10 = sum(1 for _, count in a_list if count < 10)\n",
    "num_cog_lt10 = sum(1 for _, count in c_list if count < 10)\n",
    "\n",
    "print(f\"Unique affect words in text: {num_affect_words}\")\n",
    "print(f\"Unique cognition words in text: {num_cog_words}\")\n",
    "print(f\"Affect words with count < 10: {num_affect_lt10}\")\n",
    "print(f\"Cognition words with count < 10: {num_cog_lt10}\")\n",
    "\n",
    "\n",
    "# == Calculate weighted frequencies for all words\n",
    "\n",
    "l = sum(word_counts.values())\n",
    "\n",
    "a = 0.001 # Method to downweight with a smoothing parameter: For frequent words (large v/1), weight approaches 0; for rare words (small v/1) closer to 1\n",
    "word_counts_weighted = {k: a / (a + (v / l)) for k, v in word_counts.items()}\n",
    "\n",
    "joblib.dump(word_counts_weighted, os.path.join(data_freq, 'word_counts_weighted.pkl'))\n",
    "\n",
    "################################################################################ ISSUE ##################\n",
    "# To print top 100 by weighted values, sort the dictionary by value descending:\n",
    "top_100_weighted = sorted(word_counts_weighted.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "\n",
    "print(\"Top 100 words by weighted frequency:\")\n",
    "for word, weight in top_100_weighted:\n",
    "    print(f\"{word}: {weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a7ec60-2d73-4c68-948c-592970c4ad93",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df626ded-5e49-4a34-a3d1-315ba370226f",
   "metadata": {},
   "source": [
    "## Final Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "0d0856a2-44f1-4304-b221-09a9ab72ad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_freq)\n",
    "\n",
    "word_counts = joblib.load('word_counts.pkl')  # load stemmed counts\n",
    "# For each speech only keep tokens that appear at least 10x\n",
    "\n",
    "def select(lista):\n",
    "    for i in range(len(lista)):\n",
    "        x = lista[i][0]\n",
    "        y = lista[i][1]\n",
    "        y = [w for w in y if word_counts.get(w, 0) >= 10]\n",
    "        lista[i] = [x, y]\n",
    "    return lista\n",
    "\n",
    "for data_path in preprocessed_files:\n",
    "    data = joblib.load(data_path)\n",
    "    data = select(data)\n",
    "    cleaned_path = data_path.replace('.pkl', '_final.pkl')\n",
    "    joblib.dump(data, cleaned_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "605ff6af-7346-4c8e-9729-f7fcbda0f927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          filename  speech_length_final\n",
      "0  HTI_70_2015.txt                  689\n",
      "1  PRY_58_2003.txt                  593\n",
      "2  GMB_72_2017.txt                  768\n",
      "3  SLV_04_1949.txt                  722\n",
      "4  LBY_56_2001.txt                 1593\n",
      "Total unique tokens across all final speeches: 12500\n",
      "Average tokens per final speech: 1171.5978816654492\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_preprocessed)\n",
    "\n",
    "final_files = [\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed1_final.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed2_final.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed3_final.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed4_final.pkl')\n",
    "]\n",
    "\n",
    "final_data = []\n",
    "for fname in final_files:\n",
    "    final_data.extend(joblib.load(fname))\n",
    "\n",
    "# Merge with df_merged\n",
    "df_final = pd.DataFrame(final_data, columns=[\"filename\", \"speech_final\"])\n",
    "df_merged = df_merged.merge(df_final, on=\"filename\", how=\"left\")\n",
    "\n",
    "# Create speech_length_final column\n",
    "df_merged[\"speech_length_final\"] = df_merged[\"speech_final\"].apply(\n",
    "    lambda x: len(x) if isinstance(x, list) else 0\n",
    ")\n",
    "\n",
    "# Quick check\n",
    "print(df_merged[[\"filename\", \"speech_length_final\"]].head())\n",
    "\n",
    "all_tokens_final = [token for speech in df_merged[\"speech_final\"].dropna() for token in speech]\n",
    "unique_tokens_final = set(all_tokens_final)\n",
    "print(\"Total unique tokens across all final speeches:\", len(unique_tokens_final))\n",
    "\n",
    "print(\"Average tokens per final speech:\", df_merged[\"speech_length_final\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a0407e-dde7-43ba-816b-34130facd211",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
