{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b41cb22-e190-48d5-9926-7ac6fac8310d",
   "metadata": {},
   "source": [
    "# Emotion and Reason in Political Language: Examining the UN General speeches\n",
    "## Script 1: Preprocessing & Token Frequencies\n",
    "## Author: Sarah Franzen\n",
    "\n",
    "### Instructions BEFORE running this script:\n",
    "- Ensure all required packages are installed. If not, set `InstallPackages = TRUE` (see code cells below).  \n",
    "- Set your working directory appropriately.  \n",
    "- The script will automatically create the required folder structure.  \n",
    "- Later, you will be asked to download the data from:  \n",
    "  https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/0TJX8Y  \n",
    "  and store it **unzipped** inside the created folder *data_original*\n",
    "\n",
    "\n",
    "### Description: \n",
    "- Extract documents from their original txt documents and store them as one csv\n",
    "- Data Cleaning and Pre-Processing\n",
    "- Count word frequencies and weight themh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930ebd56-def9-4a16-91eb-78dbb7d037ae",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5588afc-105a-426e-9348-742a830a2732",
   "metadata": {},
   "source": [
    "## Setup, Installation and Verification of required Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ebb8987a-a0da-453e-97e2-aafa209d0e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "InstallPackages = False # Set this to True to install the following packages \n",
    "\n",
    "if InstallPackages:\n",
    "    import sys\n",
    "\n",
    "    packages = [\n",
    "        \"pandas\",\n",
    "        \"nltk\",\n",
    "        \"spacy\",\n",
    "        \"numpy\",\n",
    "        \"gensim\",\n",
    "        \"pycountry\",\n",
    "        \"wordcloud\",\n",
    "        \"matplotlib\",\n",
    "        \"tqdm\"\n",
    "    ]\n",
    "\n",
    "    for package in packages:\n",
    "        !{sys.executable} -m pip install {package}\n",
    "\n",
    "\n",
    "DownloadAdditions = False # Set this to True to download these additional resources\n",
    "if DownloadAdditions:\n",
    "    nltk.download(\"punkt\")\n",
    "    nltk.download(\"averaged_perceptron_tagger\")\n",
    "    spacy.cli.download('en_core_web_lg')         # Download spaCy English model (large)\n",
    "\n",
    "#########################\n",
    "# Check if all packages are included\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "32d7d499-0563-46e0-9730-914c9f106d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Import standard and third-party libraries for data processing, NLP, and visualization ==\n",
    "\n",
    "import gensim\n",
    "import joblib\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import pycountry\n",
    "import random\n",
    "import re\n",
    "import spacy\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from multiprocessing import Pool, freeze_support\n",
    "from nltk.stem import SnowballStemmer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOPWORDS\n",
    "from string import punctuation\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# === Initialize NLP Tools ===\n",
    "\n",
    "translator = str.maketrans('', '', punctuation)\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "tagger = nltk.perceptron.PerceptronTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d495156-bbb2-403c-86f2-83b47d8cb03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Folder structure created:\n",
      "- C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\n",
      "  - data_original\n",
      "  - dictionaries\n",
      "  - freq\n",
      "  - preprocessed\n",
      "  - temp\n",
      "  - tokenized\n"
     ]
    }
   ],
   "source": [
    "# === Set Working Directory and create folder structure ===\n",
    "\n",
    "# Prompt user to enter working directory path\n",
    "#wd = input(\"Please enter your working directory path (e.g., C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit): \").strip()\n",
    "\n",
    "# Change to the entered working directory\n",
    "#try:\n",
    "   # os.chdir(wd)\n",
    "    #print(f\"Working directory set to: {os.getcwd()}\")\n",
    "#except FileNotFoundError:\n",
    "   # print(\"ERROR: The directory you entered does not exist. Please restart and enter a valid path.\")\n",
    "    #exit(1)\n",
    "\n",
    "# Set your working directory (adjust this as needed)\n",
    "wd = r\"C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\"\n",
    "os.chdir(wd)\n",
    "\n",
    "# Set base path to current working directory\n",
    "base_path = Path.cwd()\n",
    "data_path = base_path / \"data\"\n",
    "\n",
    "# List of subfolders to create inside 'data'\n",
    "subfolders = [\"data_original\", \"dictionaries\", \"freq\", \"preprocessed\", \"temp\", \"tokenized\"]\n",
    "\n",
    "# Create 'data' folder if it doesn't exist\n",
    "data_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Create subfolders\n",
    "for name in subfolders:\n",
    "    (data_path / name).mkdir(exist_ok=True)\n",
    "\n",
    "print(\"\\nFolder structure created:\")\n",
    "print(f\"- {data_path}\")\n",
    "for name in subfolders:\n",
    "    print(f\"  - {name}\")\n",
    "\n",
    "# Prompt user to place raw data files\n",
    "#print(f\"\\nPlease place your raw data files (unzipped) into the folder:\\n  {data_path / 'data_original'}\")\n",
    "#input(\"Press Enter after you have placed the files to continue...\")\n",
    "\n",
    "#print(\"Continuing with the script...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56fa9a21-650f-4d8e-8a21-3d9629655c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define Folder Paths ===\n",
    "\n",
    "# If an error occurs, make sure that you actually have these folders in your working directory\n",
    "data_c = os.path.join(wd, 'data')\n",
    "data_temp = os.path.join(data_c, 'temp')\n",
    "data_freq = os.path.join(data_c, 'freq')\n",
    "data_dict = os.path.join(data_c, 'dictionaries')\n",
    "data_preprocessed = os.path.join(data_c, 'preprocessed')\n",
    "data_tokenized = os.path.join(data_c, 'tokenized')\n",
    "fig = os.path.join(wd, 'fig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f26260-bb61-4315-bb69-725862b171ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "296fc7d6-a2ac-406e-a322-6c9f2702ee1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\n"
     ]
    }
   ],
   "source": [
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bd410e-bc90-4e0b-aa08-a9bff13fab47",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff34b6-cbee-4f8c-8076-5f385f596b58",
   "metadata": {},
   "source": [
    "## Load and Prepare Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18067b9-6337-4fb3-a570-603083a7deae",
   "metadata": {},
   "source": [
    "### This chunk can be skipped at the moment\n",
    "### Think of proper header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "93d247a0-5ff2-4062-add9-f3267ae1fe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total speeches found: 10761\n",
      "\n",
      " Saved raw data with 761 speeches to '.\\data\\un_corpus_raw.csv'\n"
     ]
    }
   ],
   "source": [
    "# == Load Sample from UN General Debate Corpus ==\n",
    "\n",
    "# Set Folder path containing the original TXT files    \n",
    "base_folder = r\".\\data\\data_original\\UN General Debate Corpus\\UNGDC_1946-2023\\TXT\"\n",
    "\n",
    "# Collect txt-files\n",
    "all_txt_files = []\n",
    "for root, dirs, files in os.walk(base_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.txt') and not file.startswith('._'):\n",
    "            all_txt_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Total speeches found: {len(all_txt_files)}\")\n",
    "\n",
    "# Randomly pick 800 files from the full collection   ###################################################### REMOVE AT LATER POINT\n",
    "sampled_files = random.sample(all_txt_files,761)\n",
    "\n",
    "# Read the selected files into a list\n",
    "raw_data = []\n",
    "for filepath in sampled_files:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        raw_data.append({'filename': os.path.basename(filepath), 'speech': content})\n",
    "\n",
    "df_raw = pd.DataFrame(raw_data)\n",
    "\n",
    "# == Store as csv and pkl ==\n",
    "\n",
    "df_raw = df_raw[df_raw['filename'] != '.DS_Store-to-UTF-8.txt'].copy()\n",
    "\n",
    "raw_pickle_path = r\".\\data\\un_corpus_raw.pkl\"\n",
    "df_raw.to_pickle(raw_pickle_path)\n",
    "\n",
    "raw_output_path = r\".\\data\\un_corpus_raw.csv\"\n",
    "df_raw.to_csv(raw_output_path, index=False, sep=';', encoding='utf-8')\n",
    "\n",
    "print(f\"\\n Saved raw data with {len(df_raw)} speeches to '{raw_output_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6405ff20-92fa-4c97-becb-ac5fff70f84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>THA_67_2012.txt</td>\n",
       "      <td>﻿On behalf of the\\ndelegation of the Kingdom o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EGY_02_1947.txt</td>\n",
       "      <td>In the name of my delegation, permit me to ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IRQ_62_2007.txt</td>\n",
       "      <td>I am  honoured to address the General Assembly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IRN_10_1955.txt</td>\n",
       "      <td>104. May I be permitted to convey to Mr. Maza ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VNM_71_2016.txt</td>\n",
       "      <td>I congratulate Mr. Peter Thomson on his electi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech\n",
       "0  THA_67_2012.txt  ﻿On behalf of the\\ndelegation of the Kingdom o...\n",
       "1  EGY_02_1947.txt  In the name of my delegation, permit me to ass...\n",
       "2  IRQ_62_2007.txt  I am  honoured to address the General Assembly...\n",
       "3  IRN_10_1955.txt  104. May I be permitted to convey to Mr. Maza ...\n",
       "4  VNM_71_2016.txt  I congratulate Mr. Peter Thomson on his electi..."
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# == Load data & drop empty speeches ==\n",
    "\n",
    "df_raw = pd.read_pickle(r\".\\data\\un_corpus_raw.pkl\")\n",
    "\n",
    "# Drop empty speeches\n",
    "df_raw['speech'] = df_raw['speech'].astype(str)\n",
    "df_raw = df_raw[df_raw['speech'].str.strip() != ''].copy()\n",
    "\n",
    "df_raw.head()         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7fb6b142-5356-4173-9aba-b34d08400bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [filename, speech]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "dupe_labels = df_raw[df_raw['filename'].duplicated(keep=False)]\n",
    "print(dupe_labels[['filename', 'speech']].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7980e945-9d56-4df3-99e5-acf491617568",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7d3162-ebbd-449b-a2be-1148fceda725",
   "metadata": {},
   "source": [
    "## Create new variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25710e79-5068-4d39-8023-f4f97513f165",
   "metadata": {},
   "source": [
    "#### New Variables: Year, Country Code and Country Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "162da07c-1112-43e8-a434-2bef0ed32de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min year: 1946\n",
      "Max year: 2023\n",
      "Missing codes: []\n"
     ]
    }
   ],
   "source": [
    "# == Create variable: country code & year\n",
    "\n",
    "# Create contry_code and year variable\n",
    "df_raw['country_code'] = df_raw['filename'].str.extract(r'^([A-Z]{2,3})')\n",
    "df_raw['year'] = df_raw['filename'].str.extract(r'_(\\d{4})\\.txt$').astype(int)\n",
    "\n",
    "print(\"Min year:\", df_raw['year'].min())\n",
    "print(\"Max year:\", df_raw['year'].max())\n",
    "# Speeches range from 1946 to 2023\n",
    "\n",
    "# == Create variable: country_name by matching ISO country code \n",
    "code_to_name = {country.alpha_3: country.name for country in pycountry.countries}\n",
    "\n",
    "# Add custom short names and legacy codes\n",
    "custom_names = {\n",
    "    \"BOL\": \"Bolivia\",\n",
    "    \"COD\": \"Democratic Republic of Congo\",\n",
    "    \"IRN\": \"Iran\",\n",
    "    \"LAO\": \"Laos\",\n",
    "    \"MDA\": \"Moldova\",\n",
    "    \"PRK\": \"North Korea\",\n",
    "    \"PSE\": \"Palestine\",\n",
    "    \"RUS\": \"Russia\",\n",
    "    \"SYR\": \"Syria\",\n",
    "    \"TZA\": \"Tanzania\",\n",
    "    \"VAT\": \"Vatican City State\",\n",
    "    \"VEN\": \"Venezuela\",\n",
    "    \"VNM\": \"Vietnam\",\n",
    "    \"YMD\": \"South Yemen\",\n",
    "    \"YUG\": \"Yugoslavia\",\n",
    "    \"DDR\": \"East Germany\",\n",
    "    \"EU\": \"European Union\",\n",
    "    \"CSK\": \"Czechoslovakia\",\n",
    "    \"FSM\": \"Micronesia\",\n",
    "    \"KOR\": \"South Korea\"\n",
    "}\n",
    "\n",
    "code_to_name.update(custom_names)\n",
    "df_raw['country_name'] = df_raw['country_code'].map(code_to_name)\n",
    "\n",
    "# Check missing mappings\n",
    "missing = df_raw.loc[df_raw['country_name'].isna(), 'country_code'].unique()\n",
    "print(\"Missing codes:\", missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "383d2904-f015-4c9d-9aeb-51a94f03f950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    country_code                      country_name\n",
      "0            AFG                       Afghanistan\n",
      "1            AGO                            Angola\n",
      "2            ALB                           Albania\n",
      "3            AND                           Andorra\n",
      "4            ARE              United Arab Emirates\n",
      "5            ARG                         Argentina\n",
      "6            ARM                           Armenia\n",
      "7            ATG               Antigua and Barbuda\n",
      "8            AUS                         Australia\n",
      "9            AUT                           Austria\n",
      "10           BDI                           Burundi\n",
      "11           BEL                           Belgium\n",
      "12           BEN                             Benin\n",
      "13           BFA                      Burkina Faso\n",
      "14           BGD                        Bangladesh\n",
      "15           BGR                          Bulgaria\n",
      "16           BHR                           Bahrain\n",
      "17           BHS                           Bahamas\n",
      "18           BIH            Bosnia and Herzegovina\n",
      "19           BLR                           Belarus\n",
      "20           BLZ                            Belize\n",
      "21           BOL                           Bolivia\n",
      "22           BRA                            Brazil\n",
      "23           BRB                          Barbados\n",
      "24           BTN                            Bhutan\n",
      "25           BWA                          Botswana\n",
      "26           CAF          Central African Republic\n",
      "27           CAN                            Canada\n",
      "28           CHE                       Switzerland\n",
      "29           CHL                             Chile\n",
      "30           CHN                             China\n",
      "31           CIV                     Côte d'Ivoire\n",
      "32           CMR                          Cameroon\n",
      "33           COD      Democratic Republic of Congo\n",
      "34           COG                             Congo\n",
      "35           COL                          Colombia\n",
      "36           COM                           Comoros\n",
      "37           CPV                        Cabo Verde\n",
      "38           CRI                        Costa Rica\n",
      "39           CSK                    Czechoslovakia\n",
      "40           CUB                              Cuba\n",
      "41           CYP                            Cyprus\n",
      "42           CZE                           Czechia\n",
      "43           DEU                           Germany\n",
      "44           DJI                          Djibouti\n",
      "45           DMA                          Dominica\n",
      "46           DNK                           Denmark\n",
      "47           DOM                Dominican Republic\n",
      "48           DZA                           Algeria\n",
      "49           ECU                           Ecuador\n",
      "50           EGY                             Egypt\n",
      "51           ERI                           Eritrea\n",
      "52           ESP                             Spain\n",
      "53           EST                           Estonia\n",
      "54           ETH                          Ethiopia\n",
      "55            EU                    European Union\n",
      "56           FIN                           Finland\n",
      "57           FJI                              Fiji\n",
      "58           FRA                            France\n",
      "59           FSM                        Micronesia\n",
      "60           GAB                             Gabon\n",
      "61           GBR                    United Kingdom\n",
      "62           GEO                           Georgia\n",
      "63           GHA                             Ghana\n",
      "64           GIN                            Guinea\n",
      "65           GMB                            Gambia\n",
      "66           GNB                     Guinea-Bissau\n",
      "67           GNQ                 Equatorial Guinea\n",
      "68           GRC                            Greece\n",
      "69           GRD                           Grenada\n",
      "70           GTM                         Guatemala\n",
      "71           GUY                            Guyana\n",
      "72           HND                          Honduras\n",
      "73           HRV                           Croatia\n",
      "74           HTI                             Haiti\n",
      "75           HUN                           Hungary\n",
      "76           IDN                         Indonesia\n",
      "77           IND                             India\n",
      "78           IRL                           Ireland\n",
      "79           IRN                              Iran\n",
      "80           IRQ                              Iraq\n",
      "81           ISR                            Israel\n",
      "82           ITA                             Italy\n",
      "83           JAM                           Jamaica\n",
      "84           JOR                            Jordan\n",
      "85           JPN                             Japan\n",
      "86           KAZ                        Kazakhstan\n",
      "87           KEN                             Kenya\n",
      "88           KGZ                        Kyrgyzstan\n",
      "89           KHM                          Cambodia\n",
      "90           KIR                          Kiribati\n",
      "91           KNA             Saint Kitts and Nevis\n",
      "92           KOR                       South Korea\n",
      "93           KWT                            Kuwait\n",
      "94           LAO                              Laos\n",
      "95           LBN                           Lebanon\n",
      "96           LBR                           Liberia\n",
      "97           LBY                             Libya\n",
      "98           LCA                       Saint Lucia\n",
      "99           LIE                     Liechtenstein\n",
      "100          LKA                         Sri Lanka\n",
      "101          LSO                           Lesotho\n",
      "102          LTU                         Lithuania\n",
      "103          LUX                        Luxembourg\n",
      "104          LVA                            Latvia\n",
      "105          MAR                           Morocco\n",
      "106          MCO                            Monaco\n",
      "107          MDA                           Moldova\n",
      "108          MDG                        Madagascar\n",
      "109          MDV                          Maldives\n",
      "110          MEX                            Mexico\n",
      "111          MHL                  Marshall Islands\n",
      "112          MKD                   North Macedonia\n",
      "113          MLI                              Mali\n",
      "114          MLT                             Malta\n",
      "115          MMR                           Myanmar\n",
      "116          MNE                        Montenegro\n",
      "117          MNG                          Mongolia\n",
      "118          MOZ                        Mozambique\n",
      "119          MRT                        Mauritania\n",
      "120          MUS                         Mauritius\n",
      "121          MWI                            Malawi\n",
      "122          MYS                          Malaysia\n",
      "123          NAM                           Namibia\n",
      "124          NER                             Niger\n",
      "125          NGA                           Nigeria\n",
      "126          NIC                         Nicaragua\n",
      "127          NLD                       Netherlands\n",
      "128          NOR                            Norway\n",
      "129          NPL                             Nepal\n",
      "130          NRU                             Nauru\n",
      "131          NZL                       New Zealand\n",
      "132          OMN                              Oman\n",
      "133          PAK                          Pakistan\n",
      "134          PAN                            Panama\n",
      "135          PER                              Peru\n",
      "136          PHL                       Philippines\n",
      "137          PLW                             Palau\n",
      "138          PNG                  Papua New Guinea\n",
      "139          POL                            Poland\n",
      "140          PRK                       North Korea\n",
      "141          PRT                          Portugal\n",
      "142          PRY                          Paraguay\n",
      "143          PSE                         Palestine\n",
      "144          QAT                             Qatar\n",
      "145          ROU                           Romania\n",
      "146          RUS                            Russia\n",
      "147          RWA                            Rwanda\n",
      "148          SAU                      Saudi Arabia\n",
      "149          SDN                             Sudan\n",
      "150          SEN                           Senegal\n",
      "151          SGP                         Singapore\n",
      "152          SLB                   Solomon Islands\n",
      "153          SLE                      Sierra Leone\n",
      "154          SLV                       El Salvador\n",
      "155          SMR                        San Marino\n",
      "156          SOM                           Somalia\n",
      "157          SRB                            Serbia\n",
      "158          STP             Sao Tome and Principe\n",
      "159          SUR                          Suriname\n",
      "160          SVK                          Slovakia\n",
      "161          SVN                          Slovenia\n",
      "162          SWE                            Sweden\n",
      "163          SWZ                          Eswatini\n",
      "164          SYC                        Seychelles\n",
      "165          SYR                             Syria\n",
      "166          TCD                              Chad\n",
      "167          TGO                              Togo\n",
      "168          THA                          Thailand\n",
      "169          TKM                      Turkmenistan\n",
      "170          TLS                       Timor-Leste\n",
      "171          TON                             Tonga\n",
      "172          TTO               Trinidad and Tobago\n",
      "173          TUN                           Tunisia\n",
      "174          TUR                           Türkiye\n",
      "175          TUV                            Tuvalu\n",
      "176          TZA                          Tanzania\n",
      "177          UGA                            Uganda\n",
      "178          UKR                           Ukraine\n",
      "179          URY                           Uruguay\n",
      "180          USA                     United States\n",
      "181          UZB                        Uzbekistan\n",
      "182          VAT                Vatican City State\n",
      "183          VCT  Saint Vincent and the Grenadines\n",
      "184          VEN                         Venezuela\n",
      "185          VNM                           Vietnam\n",
      "186          VUT                           Vanuatu\n",
      "187          WSM                             Samoa\n",
      "188          YEM                             Yemen\n",
      "189          YMD                       South Yemen\n",
      "190          YUG                        Yugoslavia\n",
      "191          ZAF                      South Africa\n",
      "192          ZMB                            Zambia\n"
     ]
    }
   ],
   "source": [
    "# == Check country names and structure\n",
    "\n",
    "df_raw.head() \n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(df_raw[['country_code', 'country_name']].drop_duplicates().sort_values('country_code').reset_index(drop=True))\n",
    "# Reset to default afterward\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5928538b-8519-40e2-bdee-ac72024f9f48",
   "metadata": {},
   "source": [
    "#### New Variable: Length of speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "911e2229-337f-4479-984c-3a6ab9dff5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique tokens in the corpus: 59551\n",
      "Total number of tokens in the corpus: 2222278\n",
      "Average speech length (words): 2920.21\n",
      "20 shortest speeches:\n",
      "            filename           country_name  year  speech_length_words\n",
      "128  GNB_76_2021.txt          Guinea-Bissau  2021                  480\n",
      "706  JOR_61_2006.txt                 Jordan  2006                  571\n",
      "492  IDN_76_2021.txt              Indonesia  2021                  611\n",
      "692  RWA_63_2008.txt                 Rwanda  2008                  624\n",
      "425  SVN_69_2014.txt               Slovenia  2014                  693\n",
      "219  LTU_71_2016.txt              Lithuania  2016                  739\n",
      "334   EU_74_2019.txt         European Union  2019                  752\n",
      "463  STP_36_1981.txt  Sao Tome and Principe  1981                  796\n",
      "80   URY_76_2021.txt                Uruguay  2021                  854\n",
      "70   HTI_10_1955.txt                  Haiti  1955                  882\n",
      "307  PRT_76_2021.txt               Portugal  2021                  887\n",
      "301  GIN_75_2020.txt                 Guinea  2020                  903\n",
      "453  YUG_58_2003.txt             Yugoslavia  2003                  909\n",
      "499  CMR_71_2016.txt               Cameroon  2016                  917\n",
      "586  SYC_56_2001.txt             Seychelles  2001                  924\n",
      "148  YEM_28_1973.txt                  Yemen  1973                  948\n",
      "578  LVA_68_2013.txt                 Latvia  2013                  960\n",
      "140  ITA_73_2018.txt                  Italy  2018                  971\n",
      "574  LKA_71_2016.txt              Sri Lanka  2016                  976\n",
      "523  USA_75_2020.txt          United States  2020                  981\n",
      "\n",
      "20 longest speeches:\n",
      "            filename                  country_name  year  speech_length_words\n",
      "347  RUS_02_1947.txt                        Russia  1947                11519\n",
      "670  BFA_29_1974.txt                  Burkina Faso  1974                10246\n",
      "154  NGA_17_1962.txt                       Nigeria  1962                 9518\n",
      "616  ALB_23_1968.txt                       Albania  1968                 9399\n",
      "445  COD_16_1961.txt  Democratic Republic of Congo  1961                 9292\n",
      "35   EGY_15_1960.txt                         Egypt  1960                 8425\n",
      "404  LUX_35_1980.txt                    Luxembourg  1980                 8043\n",
      "723  UKR_15_1960.txt                       Ukraine  1960                 7935\n",
      "759  YMD_23_1968.txt                   South Yemen  1968                 7852\n",
      "55   COD_38_1983.txt  Democratic Republic of Congo  1983                 7810\n",
      "203  RUS_26_1971.txt                        Russia  1971                 7582\n",
      "209  ARG_37_1982.txt                     Argentina  1982                 7250\n",
      "158  AUS_14_1959.txt                     Australia  1959                 6981\n",
      "593  ZMB_29_1974.txt                        Zambia  1974                 6823\n",
      "722  ETH_31_1976.txt                      Ethiopia  1976                 6570\n",
      "676  CHN_27_1972.txt                         China  1972                 6567\n",
      "18   CUB_38_1983.txt                          Cuba  1983                 6537\n",
      "579  HUN_28_1973.txt                       Hungary  1973                 6415\n",
      "382  ARG_20_1965.txt                     Argentina  1965                 6386\n",
      "224  GHA_18_1963.txt                         Ghana  1963                 6352\n"
     ]
    }
   ],
   "source": [
    "all_tokens = set()\n",
    "for speech in df_raw['speech']:\n",
    "    all_tokens.update(str(speech).split())\n",
    "print(\"Total number of unique tokens in the corpus:\", len(all_tokens))\n",
    "\n",
    "total_tokens = df_raw['speech'].apply(lambda x: len(str(x).split())).sum()\n",
    "print(\"Total number of tokens in the corpus:\", total_tokens)\n",
    "\n",
    "# Add a new column: speech length in words\n",
    "df_raw['speech_length_words'] = df_raw['speech'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Calculate average length\n",
    "avg_length = df_raw['speech_length_words'].mean()\n",
    "print(\"Average speech length (words):\", round(avg_length, 2))\n",
    "\n",
    "# 20 shortest & longest speeches\n",
    "print(\"20 shortest speeches:\")\n",
    "print(df_raw.nsmallest(20, 'speech_length_words')[['filename', 'country_name', 'year', 'speech_length_words']])\n",
    "\n",
    "print(\"\\n20 longest speeches:\")\n",
    "print(df_raw.nlargest(20, 'speech_length_words')[['filename', 'country_name', 'year', 'speech_length_words']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424f152-b786-45bf-bf86-e169a900f628",
   "metadata": {},
   "source": [
    "#### New variable: English as Official Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d1d2c2a6-23d6-4044-84cc-8785167e6b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries not matched in df_raw['country_name']:\n",
      "Anguilla\n",
      "Bermuda\n",
      "British Virgin Islands\n",
      "Cayman Islands\n",
      "Christmas Island\n",
      "Cook Islands\n",
      "Hong Kong\n",
      "Jersey\n",
      "Niue\n",
      "Norfolk Island\n",
      "Northern Mariana Islands\n",
      "Pitcairn Islands\n",
      "Sint Maarten\n",
      "South Sudan\n",
      "Turks and Caicos Islands\n",
      "Zimbabwe\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "      <th>country_code</th>\n",
       "      <th>year</th>\n",
       "      <th>country_name</th>\n",
       "      <th>speech_length_words</th>\n",
       "      <th>english_official_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>THA_67_2012.txt</td>\n",
       "      <td>﻿On behalf of the\\ndelegation of the Kingdom o...</td>\n",
       "      <td>THA</td>\n",
       "      <td>2012</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>1448</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EGY_02_1947.txt</td>\n",
       "      <td>In the name of my delegation, permit me to ass...</td>\n",
       "      <td>EGY</td>\n",
       "      <td>1947</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>2631</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IRQ_62_2007.txt</td>\n",
       "      <td>I am  honoured to address the General Assembly...</td>\n",
       "      <td>IRQ</td>\n",
       "      <td>2007</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>2590</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IRN_10_1955.txt</td>\n",
       "      <td>104. May I be permitted to convey to Mr. Maza ...</td>\n",
       "      <td>IRN</td>\n",
       "      <td>1955</td>\n",
       "      <td>Iran</td>\n",
       "      <td>2315</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VNM_71_2016.txt</td>\n",
       "      <td>I congratulate Mr. Peter Thomson on his electi...</td>\n",
       "      <td>VNM</td>\n",
       "      <td>2016</td>\n",
       "      <td>Vietnam</td>\n",
       "      <td>1412</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech  \\\n",
       "0  THA_67_2012.txt  ﻿On behalf of the\\ndelegation of the Kingdom o...   \n",
       "1  EGY_02_1947.txt  In the name of my delegation, permit me to ass...   \n",
       "2  IRQ_62_2007.txt  I am  honoured to address the General Assembly...   \n",
       "3  IRN_10_1955.txt  104. May I be permitted to convey to Mr. Maza ...   \n",
       "4  VNM_71_2016.txt  I congratulate Mr. Peter Thomson on his electi...   \n",
       "\n",
       "  country_code  year country_name  speech_length_words  \\\n",
       "0          THA  2012     Thailand                 1448   \n",
       "1          EGY  1947        Egypt                 2631   \n",
       "2          IRQ  2007         Iraq                 2590   \n",
       "3          IRN  1955         Iran                 2315   \n",
       "4          VNM  2016      Vietnam                 1412   \n",
       "\n",
       "   english_official_language  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source for english as official language : https://gradschool.utk.edu/future-students/office-of-graduate-admissions/applying-to-graduate-school/admission-requirements/testing-requirements/countries-with-english-as-official-language/\n",
    "# They are quoting: https://www.cia.gov/the-world-factbook/field/languages/\n",
    "\n",
    "english_countries = [\n",
    "    \"Anguilla\", \"Antigua and Barbuda\", \"Bahamas\", \"Barbados\", \"Belize\", \"Belgium\",\n",
    "    \"Bermuda\", \"Botswana\", \"British Virgin Islands\", \"Burundi\", \"Cameroon\", \"Canada\",\n",
    "    \"Cayman Islands\", \"Christmas Island\", \"Cook Islands\", \"Dominica\", \"Fiji\", \"Gambia\",\n",
    "    \"Ghana\", \"Grenada\", \"Guyana\", \"Hong Kong\", \"India\", \"Ireland\", \"Jersey\", \"Kenya\",\n",
    "    \"Liberia\", \"Malawi\", \"Malta\", \"Marshall Islands\", \"Micronesia\",\n",
    "    \"Namibia\", \"New Zealand\", \"Nigeria\", \"Niue\", \"Norfolk Island\", \"Northern Mariana Islands\",\n",
    "    \"Pakistan\", \"Palau\", \"Papua New Guinea\", \"Philippines\", \"Pitcairn Islands\", \"Rwanda\",\n",
    "    \"Saint Kitts and Nevis\", \"Saint Lucia\", \"Samoa\", \"Seychelles\", \"Sierra Leone\", \"Singapore\",\n",
    "    \"Sint Maarten\", \"Solomon Islands\", \"Somalia\", \"South Africa\", \"South Sudan\", \"Sudan\",\n",
    "    \"Eswatini\", \"Tanzania\", \"Tonga\", \"Trinidad and Tobago\", \"Turks and Caicos Islands\",\n",
    "    \"Tuvalu\", \"Uganda\", \"Zambia\", \"Zimbabwe\"\n",
    "]\n",
    "\n",
    "# Create dummy column\n",
    "df_raw['english_official_language'] = df_raw['country_name'].apply(\n",
    "    lambda x: 1 if x in english_countries else 0\n",
    ")\n",
    "\n",
    "# Detect unmatched countries \n",
    "matched = set(df_raw['country_name'])\n",
    "unmatched = [country for country in english_countries if country not in matched]\n",
    "\n",
    "print(\"Countries not matched in df_raw['country_name']:\")\n",
    "for country in unmatched:\n",
    "    print(country)\n",
    "\n",
    "# All of these countries are either British Overseas Territories, Australian Territories, self-governing island territories or Special Administrative Regions\n",
    "    # None of the unmatched regions are UN Members\n",
    "\n",
    "# Check df with new variable english_official_language\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82540d8a-b656-45fb-aeb0-2fd5b4d7108a",
   "metadata": {},
   "source": [
    "#### New variable: Permanent member security council"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4fb92fe4-5c22-43fa-8b48-62ec2240d716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    country_code    country_name  security_council_permanent  year\n",
      "26           USA   United States                           1  1990\n",
      "43           FRA          France                           1  1983\n",
      "77           CHN           China                           1  1991\n",
      "97           USA   United States                           1  2019\n",
      "110          CHN           China                           1  1946\n",
      "113          FRA          France                           1  1991\n",
      "116          RUS          Russia                           1  1993\n",
      "203          RUS          Russia                           1  1971\n",
      "221          GBR  United Kingdom                           1  2007\n",
      "227          USA   United States                           1  2002\n",
      "295          FRA          France                           1  2002\n",
      "314          CHN           China                           1  2000\n",
      "347          RUS          Russia                           1  1947\n",
      "371          RUS          Russia                           1  2003\n",
      "375          USA   United States                           1  1979\n",
      "411          RUS          Russia                           1  1950\n",
      "428          GBR  United Kingdom                           1  2006\n",
      "507          GBR  United Kingdom                           1  2010\n",
      "523          USA   United States                           1  2020\n",
      "633          RUS          Russia                           1  1986\n",
      "642          FRA          France                           1  1966\n",
      "676          CHN           China                           1  1972\n",
      "690          FRA          France                           1  1968\n",
      "701          USA   United States                           1  1965\n",
      "711          USA   United States                           1  1952\n",
      "731          GBR  United Kingdom                           1  1966\n",
      "749          GBR  United Kingdom                           1  2008\n",
      "751          GBR  United Kingdom                           1  1974\n"
     ]
    }
   ],
   "source": [
    "# Define permanent members of the UN Security Council and create dummy\n",
    "permanent_members = ['RUS', 'USA', 'FRA', 'GBR', 'CHN']\n",
    "\n",
    "df_raw['security_council_permanent'] = df_raw['country_code'].isin(permanent_members).astype(int)\n",
    "\n",
    "print(df_raw[df_raw['country_code'].isin(permanent_members)][\n",
    "    ['country_code', 'country_name', 'security_council_permanent', 'year']\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e3990c63-e221-457f-ab3f-b408b147cd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            filename                                             speech  \\\n",
      "0    THA_67_2012.txt  ﻿On behalf of the\\ndelegation of the Kingdom o...   \n",
      "1    EGY_02_1947.txt  In the name of my delegation, permit me to ass...   \n",
      "2    IRQ_62_2007.txt  I am  honoured to address the General Assembly...   \n",
      "3    IRN_10_1955.txt  104. May I be permitted to convey to Mr. Maza ...   \n",
      "4    VNM_71_2016.txt  I congratulate Mr. Peter Thomson on his electi...   \n",
      "..               ...                                                ...   \n",
      "756  SVK_49_1994.txt  I should like to congratulate\\nMr. Essy of Côt...   \n",
      "757  URY_34_1979.txt  ﻿I should like to begin by congratulating Mr. ...   \n",
      "758  SVN_74_2019.txt  It is a distinct honour for me to address the ...   \n",
      "759  YMD_23_1968.txt  94. Sir, I greet the President, Mr. Arenales, ...   \n",
      "760   EU_72_2017.txt  The European Union (EU) stands for freedom and...   \n",
      "\n",
      "    country_code  year    country_name  speech_length_words  \\\n",
      "0            THA  2012        Thailand                 1448   \n",
      "1            EGY  1947           Egypt                 2631   \n",
      "2            IRQ  2007            Iraq                 2590   \n",
      "3            IRN  1955            Iran                 2315   \n",
      "4            VNM  2016         Vietnam                 1412   \n",
      "..           ...   ...             ...                  ...   \n",
      "756          SVK  1994        Slovakia                 2957   \n",
      "757          URY  1979         Uruguay                 5972   \n",
      "758          SVN  2019        Slovenia                 1540   \n",
      "759          YMD  1968     South Yemen                 7852   \n",
      "760           EU  2017  European Union                 1150   \n",
      "\n",
      "     english_official_language  security_council_permanent  \n",
      "0                            0                           0  \n",
      "1                            0                           0  \n",
      "2                            0                           0  \n",
      "3                            0                           0  \n",
      "4                            0                           0  \n",
      "..                         ...                         ...  \n",
      "756                          0                           0  \n",
      "757                          0                           0  \n",
      "758                          0                           0  \n",
      "759                          0                           0  \n",
      "760                          0                           0  \n",
      "\n",
      "[761 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523547d9-9b10-473c-8c85-d1e2fc4fd3e1",
   "metadata": {},
   "source": [
    "#### New variables: Speaker, Position & Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7efb21ee-d51a-4f5a-a162-4d1ff6ceabfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "       Year  Session ISO Code                              Country  \\\n",
      "10506  1951        6      RUS  Union of Soviet Socialist Republics   \n",
      "10552  1951        6      RUS  Union of Soviet Socialist Republics   \n",
      "10381  1954        9      PHL                          Philippines   \n",
      "10415  1954        9      PHL                          Philippines   \n",
      "10261  1956       11      IRQ                                 Iraq   \n",
      "10324  1956       11      IRQ                                 Iraq   \n",
      "10323  1956       11      SYR                                Syria   \n",
      "10326  1956       11      SYR                                Syria   \n",
      "10205  1957       12      CSK                       Czechoslovakia   \n",
      "10243  1957       12      CSK                       Czechoslovakia   \n",
      "10159  1958       13      BGR                             Bulgaria   \n",
      "10181  1958       13      BGR                             Bulgaria   \n",
      "10127  1958       13      CSK                       Czechoslovakia   \n",
      "10183  1958       13      CSK                       Czechoslovakia   \n",
      "10140  1958       13      IRQ                                 Iraq   \n",
      "10186  1958       13      IRQ                                 Iraq   \n",
      "10116  1958       13      RUS  Union of Soviet Socialist Republics   \n",
      "10171  1958       13      RUS  Union of Soviet Socialist Republics   \n",
      "10039  1959       14      RUS  Union of Soviet Socialist Republics   \n",
      "10109  1959       14      RUS  Union of Soviet Socialist Republics   \n",
      "\n",
      "        Name of Person Speaking                                  Post  \\\n",
      "10506            Mr. VYSHINSKY                                    NaN   \n",
      "10552            Mr. VYSHINSKY                                    NaN   \n",
      "10381                Mr. ROMULO                                   NaN   \n",
      "10415               Mr. SERRANO                                   NaN   \n",
      "10261                Mr. JAMALI                                   NaN   \n",
      "10324                Mr. JAMALI                                   NaN   \n",
      "10323            Mr. ZEINEDDINE                                   NaN   \n",
      "10326           Mr. ZEINEDDINE                                    NaN   \n",
      "10205                Mr. DAVID                                    NaN   \n",
      "10243                 Mr. DAVID                                   NaN   \n",
      "10159               Mr. Lukanov                                   NaN   \n",
      "10181               Mr. Lukanov                                   NaN   \n",
      "10127                 Mr. David                                   NaN   \n",
      "10183                 Mr. David                                   NaN   \n",
      "10140                 Mr. Jawad                                   NaN   \n",
      "10186                Mr. Jomard                                   NaN   \n",
      "10116               Mr. Gromyko                                   NaN   \n",
      "10171              Mr. GROMYKO                                    NaN   \n",
      "10039  Mr. Nikita S. KHRUSHCHEV  Chairman of the Council of Ministers   \n",
      "10109             Mr. Kuznetsov                                   NaN   \n",
      "\n",
      "      Unnamed: 6  \n",
      "10506        NaN  \n",
      "10552        NaN  \n",
      "10381        NaN  \n",
      "10415        NaN  \n",
      "10261        NaN  \n",
      "10324        NaN  \n",
      "10323        NaN  \n",
      "10326        NaN  \n",
      "10205        NaN  \n",
      "10243        NaN  \n",
      "10159        NaN  \n",
      "10181        NaN  \n",
      "10127        NaN  \n",
      "10183        NaN  \n",
      "10140        NaN  \n",
      "10186        NaN  \n",
      "10116        NaN  \n",
      "10171        NaN  \n",
      "10039        NaN  \n",
      "10109        NaN  \n",
      "0\n"
     ]
    }
   ],
   "source": [
    "df_speakers = pd.read_excel(os.path.join(data_c, \"data_original\", \"UN General Debate Corpus\", \"Speakers_by_session.xlsx\"))\n",
    "\n",
    "# Check uniqueness of keys in df_speakers\n",
    "print(df_speakers.duplicated(subset=['Year', 'ISO Code']).sum())\n",
    "\n",
    "# Check for duplicates in df_speakers\n",
    "dupes_speakers = df_speakers[df_speakers.duplicated(subset=['Year', 'ISO Code'], keep=False)]\n",
    "print(dupes_speakers.sort_values(['Year', 'ISO Code']).head(20))\n",
    "\n",
    "# for 1958 Iraq Mr. Jomard see https://digitallibrary.un.org/record/380721\n",
    "# for 1954 Phillipines Mr. Romulo see https://digitallibrary.un.org/record/380429\n",
    "\n",
    "df_speakers_cleaned = (\n",
    "    df_speakers[~(\n",
    "        ((df_speakers['ISO Code'] == \"IRQ\") & (df_speakers['Year'] == 1958) & (df_speakers['Name of Person Speaking'] == \"Mr. Jawad\")) |\n",
    "        ((df_speakers['ISO Code'] == \"PHL\") & (df_speakers['Year'] == 1954) & (df_speakers['Name of Person Speaking'] == \"Mr. SERRANO\"))\n",
    "    )]\n",
    "    .drop_duplicates(subset=['Year', 'ISO Code'], keep='first')\n",
    ")\n",
    "print(df_speakers_cleaned.duplicated(subset=['Year', 'ISO Code']).sum())  # should be 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "de783a81-efbf-49a6-96f6-ae6a93fa1573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            filename  year country_code    country_name\n",
      "123  YMD_26_1971.txt  1971          YMD     South Yemen\n",
      "566  YMD_33_1978.txt  1978          YMD     South Yemen\n",
      "607  YMD_32_1977.txt  1977          YMD     South Yemen\n",
      "618   EU_68_2013.txt  2013           EU  European Union\n",
      "653  YMD_38_1983.txt  1983          YMD     South Yemen\n",
      "703  CYP_18_1963.txt  1963          CYP          Cyprus\n",
      "759  YMD_23_1968.txt  1968          YMD     South Yemen\n",
      "7 rows could not be matched\n",
      "    gender_dummy  count\n",
      "0       0 (male)    328\n",
      "1     1 (female)     12\n",
      "2  NaN (unknown)    421\n"
     ]
    }
   ],
   "source": [
    "# Supplmentary xlsx-file from the UN Dataset provides information on the speaker and their position\n",
    "\n",
    "####### CHECK IF THIS WOULD WORK IN A REPLICATION #####################################\n",
    "\n",
    "# == Create variable speaker_name and position ==\n",
    "\n",
    "# Merge new infrormation to dataframe\n",
    "df_merged = df_raw.merge(\n",
    "    df_speakers_cleaned[['Year', 'ISO Code', 'Name of Person Speaking', 'Post']],\n",
    "    left_on=['year', 'country_code'],\n",
    "    right_on=['Year', 'ISO Code'],\n",
    "    how='left',\n",
    "    indicator=True)\n",
    "\n",
    "# Detect unmatched rows\n",
    "unmatched = df_merged[df_merged['_merge'] == 'left_only']\n",
    "unmatched_count = (df_merged['_merge'] == 'left_only').sum()\n",
    "\n",
    "print(unmatched[['filename', 'year', 'country_code', 'country_name']])\n",
    "print(f\"{unmatched_count} rows could not be matched\")\n",
    "\n",
    "# Clean up \n",
    "df_merged = df_merged.drop(columns=['Year', 'ISO Code', '_merge']).rename(columns={\n",
    "    'Name of Person Speaking': 'speaker_name',\n",
    "    'Post': 'position'\n",
    "})\n",
    "\n",
    "# == Create gender dummy ==\n",
    "df_merged['gender_dummy'] = df_merged['speaker_name'].apply(\n",
    "    lambda name: 0 if pd.notnull(name) and re.search(r'^(?:Mr|Sir)\\b', name, re.IGNORECASE)\n",
    "    else 1 if pd.notnull(name) and re.search(r'^(?:Mrs|Ms)\\b', name, re.IGNORECASE)\n",
    "    else None\n",
    ")\n",
    "\n",
    "# Count all values including NaN\n",
    "counts = df_merged['gender_dummy'].value_counts(dropna=False)\n",
    "\n",
    "# Build summary using .get() to handle missing keys\n",
    "gender_summary = pd.DataFrame({\n",
    "    'gender_dummy': ['0 (male)', '1 (female)', 'NaN (unknown)'],\n",
    "    'count': [\n",
    "        counts.get(0, 0),\n",
    "        counts.get(1, 0),\n",
    "        counts.get(np.nan, 0)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(gender_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3cfdc9-d972-454b-9719-3285b1e5d3e2",
   "metadata": {},
   "source": [
    "Looking at the structure, highest position always seems to be mentioned first --> drop everything else if speaker has more than one position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "462ad8f6-1fd1-4c56-9d3f-3f8e90b5c454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [filename, speech, country_code, year, country_name, speech_length_words, english_official_language, security_council_permanent, speaker_name, position, gender_dummy]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "dupes_speakers = df_merged[df_merged.duplicated(subset=['year', 'country_code'], keep=False)]\n",
    "print(dupes_speakers.sort_values(['year', 'country_code']).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6a1ce3d9-a03d-46f6-90aa-923660bc4474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmatched position: Cairman of the Presidency\n",
      "Unmatched position: Emperor\n",
      "Unmatched position: Chairman\n",
      "Unmatched position: Secretary of State\n",
      "Unmatched position: Coordinator of the Junta of the Government\n"
     ]
    }
   ],
   "source": [
    "# == Adjust position variable\n",
    "def normalize_position(pos):\n",
    "    if pd.isna(pos):\n",
    "        return pos\n",
    "\n",
    "    pos = pos.strip()\n",
    "\n",
    "    # --- Fix common typos and extra spaces ---\n",
    "    pos = re.sub(r'\\s+', ' ', pos)  # collapse multiple spaces\n",
    "    pos_lower = pos.lower()\n",
    "\n",
    "     # Turn all ministers that deal with foreign affairs and international relations to \"Minister for Foreign Affairs\n",
    "    foreign_affairs_variants = [\n",
    "        'minister for foregn affairs',\n",
    "        'minister responsible for foreign affairs',\n",
    "        'minsiter for foreign and caricom affairs',\n",
    "        'minister for external affairs',\n",
    "        'minister of external relations',  # <-- added\n",
    "        'foreign minister',\n",
    "        'minister for international affairs and cooperation',\n",
    "        'minister for external relations',\n",
    "        'federal minister for european and international affairs',\n",
    "        'international cooperation',\n",
    "        'federal minister for foreign affairs',\n",
    "        'minister for foreign and caricom affairs',\n",
    "        'minister of foreign affairs and cooperation',\n",
    "        'minister for international relations and cooperation',\n",
    "        'ministry of external relations',\n",
    "        'acting minister for foreign affairs and international cooperation',\n",
    "        'ministry of foreign affairs',\n",
    "        'minister for foreign and political affairs',\n",
    "        'federal minister for europe, integration, and foreign affairs',\n",
    "        'federal minister for europe, integration and foreign affairs',\n",
    "        'minister of foreign and european affaris',\n",
    "        'minister of foreign affairs',\n",
    "        'minister for foreign',\n",
    "        'minister of foreign and european affairs and minister of immigration and asylum',\n",
    "        'minister for foreign affairs and senegalese living abroad',\n",
    "        'minister for foreign affairs with responsibility for brexit',\n",
    "        'minister for foreign affairs and investment promotion'\n",
    "       \n",
    "    ]\n",
    "    if any(variant in pos_lower for variant in foreign_affairs_variants):\n",
    "        return \"Minister for Foreign Affairs\"\n",
    "\n",
    "    # --- Fix \"rime minister\" typo ---\n",
    "    pos = re.sub(r'(?i)\\brime[- ]?minister\\b', 'Prime Minister', pos)\n",
    "\n",
    "    # Normalize different versions of Head of Government, President, Prime Minsiter and Vice-President-\n",
    "    exact_matches = {\n",
    "        r'(?i)^president of (the )?government$': 'Head of Government',\n",
    "        r'(?i)^acting president$': 'President',\n",
    "        r'(?i)^interim president$': 'President',\n",
    "        r'(?i)^constitutional president$': 'President',\n",
    "        r'(?i)^first executive president$': 'President',\n",
    "        r'(?i)^first prime[- ]?minister$': 'Prime Minister',\n",
    "        r'(?i)^head of the goverment$': 'Head of Government',  # <-- catch typo + spaces\n",
    "        r'(?i)^head\\s+of\\s+govern?ment$': 'Head of Government',\n",
    "        r'(?i)^first vice[- ]?president$': 'Vice-President'\n",
    "    }\n",
    "    for pattern, replacement in exact_matches.items():\n",
    "        if re.fullmatch(pattern, pos):\n",
    "            return replacement\n",
    "\n",
    "    # --- Normalize prefixes ---\n",
    "    pos = re.sub(r'(?i)^first vice[- ]?president\\b', 'Vice-President', pos)\n",
    "    pos = re.sub(r'(?i)\\bprime[- ]?minister\\b', 'Prime Minister', pos)\n",
    "    pos = re.sub(r'(?i)\\bpresident\\b', 'President', pos)\n",
    "    pos = re.sub(r'(?i)\\bvice[- ]?president\\b', 'Vice-President', pos)\n",
    "\n",
    "    # --- Collapse primary roles if they appear at start ---\n",
    "    primary_roles = [\n",
    "        (r'(?i)^prime[- ]?minister\\b', 'Prime Minister'),\n",
    "        (r'(?i)^deputy prime[- ]?minister\\b', 'Deputy Prime Minister'),\n",
    "        (r'(?i)^president\\b', 'President'),\n",
    "        (r'(?i)^vice[- ]?president\\b', 'Vice-President'),\n",
    "        (r'(?i)^head of state\\b', 'Head of State'),\n",
    "        (r'(?i)^(crown prince|prince|king|emir|amir)\\b', 'Monarch'),\n",
    "        (r'(?i)^(un representative|permanent representative|delegation|chair of (the )?delegation|chair of diplomatic representative)\\b', 'Diplomatic Representative')\n",
    "    ]\n",
    "    for pattern, replacement in primary_roles:\n",
    "        if re.match(pattern, pos):\n",
    "            return replacement\n",
    "\n",
    "    # --- Monarchs ---\n",
    "    if re.search(r'(?i)\\b(crown prince|prince|king|emir|amir)\\b', pos):\n",
    "        return \"Monarch\"\n",
    "\n",
    "    # --- Head of State ---\n",
    "    if re.search(r'(?i)head of state', pos):\n",
    "        return \"Head of State\"\n",
    "        \n",
    "    # --- Diplomatic Representatives ---\n",
    "    if re.search(r'(?i)(un representative|permanent representative|delegation|chair of (the )?delegation|chair of diplomatic representative)', pos):\n",
    "        return \"Diplomatic Representative\"\n",
    "\n",
    "    # --- Everything else ---\n",
    "    print(\"Unmatched position:\", pos)  # print before assigning Others\n",
    "    return \"Others\"\n",
    "\n",
    "# Apply\n",
    "df_merged[\"position\"] = df_merged[\"position\"].apply(normalize_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7a99b3df-550a-41a3-a751-2692c0f7ea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_positions(pos):\n",
    "    if pd.isna(pos):\n",
    "        return pos  # keep NaN\n",
    "    \n",
    "    if pos in [\"Prime Minister\", \"Deputy Prime Minister\"]:\n",
    "        return \"(Deputy) Prime Minister\"\n",
    "    \n",
    "    if pos in [\"President\", \"Vice-President\"]:\n",
    "        return \"(Vice-) President\"\n",
    "        \n",
    "    if pos in [\"Minister for Foreign Affairs\", \"Deputy Minister for Foreign Affairs\",\n",
    "        \"Deputy Minister Foreign Affairs\",\n",
    "        \"Second Minister for Foreign Affairs\",\n",
    "        \"Second Minister for Foreign Affairs and Trade\",\n",
    "        \"Vice Minister for Foreign Affairs\"]:\n",
    "        return \"(Deputy) Minister for Foreign Affairs\"\n",
    "    \n",
    "    return pos\n",
    "\n",
    "df_merged[\"position\"] = df_merged[\"position\"].apply(merge_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "14ececb8-262d-4270-8d52-5742a864cf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN                                      320\n",
      "(Deputy) Minister for Foreign Affairs    155\n",
      "(Vice-) President                        149\n",
      "(Deputy) Prime Minister                  103\n",
      "Diplomatic Representative                 20\n",
      "Others                                     5\n",
      "Head of State                              4\n",
      "Head of Government                         3\n",
      "Monarch                                    2\n",
      "Name: position, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Pandas so einstellen, dass es alles ausgibt\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "# Alle Positionen mit Häufigkeit\n",
    "position_counts = df_merged['position'].value_counts(dropna=False)\n",
    "\n",
    "print(position_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "df4b9986-4599-46cc-8d2b-c49667f86e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      total_rows  missing  not_missing\n",
      "year                                  \n",
      "1946           3        3            0\n",
      "1947           3        3            0\n",
      "1948           4        4            0\n",
      "1949           2        2            0\n",
      "1950           3        3            0\n",
      "1951           3        3            0\n",
      "1952           3        3            0\n",
      "1953           2        2            0\n",
      "1954           2        2            0\n",
      "1955           4        4            0\n",
      "1956           4        4            0\n",
      "1957           3        3            0\n",
      "1958           2        2            0\n",
      "1959           7        7            0\n",
      "1960           6        4            2\n",
      "1961           4        4            0\n",
      "1962           4        4            0\n",
      "1963           7        7            0\n",
      "1964           9        9            0\n",
      "1965           7        7            0\n",
      "1966           6        6            0\n",
      "1967           7        7            0\n",
      "1968           8        8            0\n",
      "1969           8        7            1\n",
      "1970           3        3            0\n",
      "1971           9        9            0\n",
      "1972           7        7            0\n",
      "1973           8        8            0\n",
      "1974           9        8            1\n",
      "1975           8        7            1\n",
      "1976          13       11            2\n",
      "1977           9        6            3\n",
      "1978           6        6            0\n",
      "1979           7        6            1\n",
      "1980          10        9            1\n",
      "1981          14       13            1\n",
      "1982           9        8            1\n",
      "1983          10        7            3\n",
      "1984          12       12            0\n",
      "1985          16       14            2\n",
      "1986          10        9            1\n",
      "1987          12       11            1\n",
      "1988          15       12            3\n",
      "1989           8        7            1\n",
      "1990          11        9            2\n",
      "1991           8        8            0\n",
      "1992          11       10            1\n",
      "1993          14       11            3\n",
      "1994          15        0           15\n",
      "1995           9        0            9\n",
      "1996           7        0            7\n",
      "1997          16        0           16\n",
      "1998          12        0           12\n",
      "1999          10        0           10\n",
      "2000          16        0           16\n",
      "2001          14        0           14\n",
      "2002          16        0           16\n",
      "2003          13        0           13\n",
      "2004          16        0           16\n",
      "2005           5        0            5\n",
      "2006          17        0           17\n",
      "2007          14        0           14\n",
      "2008          19        0           19\n",
      "2009          12        0           12\n",
      "2010          11        0           11\n",
      "2011          10        0           10\n",
      "2012          15        0           15\n",
      "2013          12        1           11\n",
      "2014          16        0           16\n",
      "2015           9        0            9\n",
      "2016          21        0           21\n",
      "2017          16        0           16\n",
      "2018          11        0           11\n",
      "2019          16        0           16\n",
      "2020          14        0           14\n",
      "2021          19        0           19\n",
      "2022          15        0           15\n",
      "2023          15        0           15\n"
     ]
    }
   ],
   "source": [
    "# Started to document positions properly from 1986 on, before yearly sample size per year mostly less than 20 samples\n",
    "\n",
    "yearly_counts = df_merged.groupby('year')['position'].agg(\n",
    "    total_rows='size',\n",
    "    missing=lambda x: x.isna().sum()\n",
    ")\n",
    "\n",
    "# Add not_missing column\n",
    "yearly_counts['not_missing'] = yearly_counts['total_rows'] - yearly_counts['missing']\n",
    "\n",
    "\n",
    "# Print the entire table\n",
    "pd.set_option('display.max_rows', None)  # show all rows\n",
    "print(yearly_counts)\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41d97e2-840b-40eb-b032-197040a0246c",
   "metadata": {},
   "source": [
    "#### New Variable: Country (Year)\n",
    "\n",
    "This variable is later needed to create clean description plots and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d647e412-7bd4-4190-9c34-39874b4620a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_merged.copy()\n",
    "df_merged['speech_label'] = df_merged['country_name'] + \" (\" + df_merged['year'].astype(str) + \")\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc05d65d-be72-42ae-b753-0940de5df17c",
   "metadata": {},
   "source": [
    "#### Save dataframe with all new variables as un_corpus_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "94a901c8-3fc4-4b33-a1ff-cede22bd196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(wd)\n",
    "\n",
    "# Save df_merged as a pickle file for quick future loading\n",
    "merged_pickle_path = r\".\\data\\un_corpus_merged.pkl\"\n",
    "df_merged.to_pickle(merged_pickle_path)\n",
    "\n",
    "# Export df as CSV \n",
    "merged_output_path = r\".\\data\\un_corpus_merged.csv\"\n",
    "df_merged.to_csv(merged_output_path, index=False, sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea36f19a-789d-4c62-a470-3e73a6d848bf",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe0f4af-e939-4100-9618-5b522bdd72a1",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2661f431-a194-47d1-9a1c-89162bf85a91",
   "metadata": {},
   "source": [
    "#### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cd37e08d-b4f2-459e-823c-3816a825ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Clean text by removing empty spaces, line breaks, hyphenation, stray characters, and escape quote ==\n",
    "\n",
    "def cleaning(content):\n",
    "    if pd.isna(content):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove line breaks and carriage returns\n",
    "    content = content.replace('\\n', ' ').replace('\\r', ' ')\n",
    "\n",
    "    # Collapse multiple spaces\n",
    "    content = ' '.join(content.split())\n",
    "\n",
    "    # Ensure spacing after punctuation\n",
    "    content = re.sub(r'(?<=[.,])(?=[^\\s])', r' ', content)\n",
    "\n",
    "    # Remove hyphenation at line breaks (e.g. \"inter- national\" → \"international\")\n",
    "    content = re.sub(r'-\\s', '', content)\n",
    "\n",
    "     # Replace hyphen between letters with a space to prevent merging words (e.g. \"russian-and\" → \"russian and\")\n",
    "    content = re.sub(r'(?<=\\w)-(?=\\w)', ' ', content)\n",
    "\n",
    "    # Remove stray backslashes\n",
    "    content = content.replace(\"\\\\\", \"\")\n",
    "\n",
    "    return content\n",
    "\n",
    "# Apply cleaning to each speech\n",
    "df_merged['speech'] = df_merged['speech'].astype(str)  # Ensure column is string type\n",
    "df_clean = df_merged.copy()\n",
    "df_clean['speech'] = df_clean['speech'].apply(cleaning)\n",
    "\n",
    "# Drop rows with empty speeches after cleaning\n",
    "df_clean = df_clean[df_clean['speech'].str.strip().astype(bool)].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7c6bd148-d5cf-4eb3-a66d-fc706e99c13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved clean speeches chunks in 'C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp'\n"
     ]
    }
   ],
   "source": [
    "# == Split cleaned data into chunks and save as separate files ==\n",
    "\n",
    "# Convert cleaned DataFrame to list of lists\n",
    "clean_data = df_clean[['filename', 'speech']].values.tolist()\n",
    "\n",
    "# Split cleaned data into 4 equal chunks\n",
    "data_id1 = clean_data[:int(len(clean_data)/4)]\n",
    "data_id2 = clean_data[int(len(clean_data)/4): int(2*len(clean_data)/4)]\n",
    "data_id3 = clean_data[int(2*len(clean_data)/4): int(3*len(clean_data)/4)]\n",
    "data_id4 = clean_data[int(3*len(clean_data)/4):]\n",
    "\n",
    "# Change directory to the temp folder\n",
    "os.chdir(data_temp)  # make sure `data_temp` exists and is defined\n",
    "\n",
    "# Save each chunk with joblib\n",
    "joblib.dump(data_id1, 'clean_speeches_indexed1.pkl')\n",
    "joblib.dump(data_id2, 'clean_speeches_indexed2.pkl')\n",
    "joblib.dump(data_id3, 'clean_speeches_indexed3.pkl')\n",
    "joblib.dump(data_id4, 'clean_speeches_indexed4.pkl')\n",
    "\n",
    "# Store list of cleaned data chunk paths to feed into preprocessing function later\n",
    "clean_files = [\n",
    "    os.path.join(data_temp, 'clean_speeches_indexed1.pkl'),\n",
    "    os.path.join(data_temp, 'clean_speeches_indexed2.pkl'),\n",
    "    os.path.join(data_temp, 'clean_speeches_indexed3.pkl'),\n",
    "    os.path.join(data_temp, 'clean_speeches_indexed4.pkl')\n",
    "]\n",
    "\n",
    "print(f\"Saved clean speeches chunks in '{data_temp}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6e5109-8cf7-41e3-8fb9-8e7dde29b4e0",
   "metadata": {},
   "source": [
    "### Advanced Text Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfca8dd9-c6d7-48c7-96b0-f5a449814336",
   "metadata": {},
   "source": [
    "#### Extend stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c1221ea1-aeeb-46ec-882d-73262ee9040f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18997 stopwords (sorted alphabetically)\n",
      "['88', 'a', 'a.', 'aandahl', 'aaron', 'aaronsburg', 'aarp', 'abac', 'abbevill', 'abbitt', 'abbot', 'abbotsford', 'abbott', 'abbottstown', 'abbyvill', 'abdnor', 'abe', 'abel', 'abercrombi', 'aberdeen', 'abern', 'abernathi', 'abernethi', 'abi', 'abiel', 'abijah', 'abilen', 'abingdon', 'abington', 'abiquiu', 'abmp', 'abner', 'abourezk', 'about', 'abov', 'abraham', 'abram', 'absalom', 'absaraka', 'absaroke', 'absecon', 'abzug', 'acadia', 'acampo', 'accid', 'accokeek', 'accomac', 'accomack', 'accord', 'accovill', 'ace', 'acevedo-vilã¡', 'acheson', 'achill', 'acker', 'ackerman', 'ackermanvill', 'acklen', 'ackley', 'ackworth', 'acm', 'acosta', 'acra', 'acton', 'acushnet', 'acworth', 'ada', 'adah', 'adair', 'adairsvill', 'adairvill', 'adak', 'adam', 'adamsburg', 'adamson', 'adamstown', 'adamsvill', 'addabbo', 'addam', 'addi', 'addievill', 'addington', 'addison', 'addonizio', 'addyston', 'adel', 'adelanto', 'adelbert', 'adelphi', 'adelphia', 'adena', 'aderholt', 'adgat', 'adger', 'adin', 'adirondack', 'adjunta', 'adkin', 'adlai', 'adler', 'admir', 'adna', 'adolph', 'adolphus', 'adona', 'adonijah', 'adoniram', 'adrain', 'adrian', 'adriano', 'advanc', 'advent', 'aedanus', 'after', 'afton', 'ag', 'again', 'against', 'agar', 'agat', 'agawam', 'agenc', 'agenda', 'agnew', 'agra', 'aguada', 'aguadilla', 'aguanga', 'aguila', 'aguilar', 'aguirr', 'ahiman', 'ahl', 'ahmeek', 'ahoski', 'ahsahka', 'ahwahne', 'aibonito', 'aiea', 'aiken', 'ailey', 'aim', 'aimwel', 'ain', 'ainey', 'ainsli', 'ainsworth', 'airvill', 'aitken', 'aitkin', 'ajo', 'akaka', 'akaska', 'akeley', 'aker', 'akiachak', 'akiak', 'akin', 'akron', 'akutan', 'al', 'alabama', 'alabast', 'alachua', 'aladdin', 'alakanuk', 'alam', 'alameda', 'alamo', 'alamogordo', 'alamosa', 'alan', 'alanre', 'alanson', 'alapaha', 'alaska', 'alba', 'albani', 'albaugh', 'albemarl', 'alben', 'alber', 'albert', 'alberta', 'alberton', 'albertson', 'albertvill', 'albia', 'albin', 'albio', 'albion', 'alborn', 'albosta', 'albright', 'albrightsvill', 'albuquerqu', 'alburgh', 'alburnett', 'alburti', 'alcald', 'alce', 'alcest', 'alcoa', 'alcolu', 'alcona', 'alcorn', 'alcov', 'alcova', 'alcã©', 'alda', 'alden', 'alder', 'alderpoint', 'alderson', 'aldi', 'aldrich', 'alec', 'aledo', 'aleknagik', 'alem', 'aleppo', 'aleshir', 'alex', 'alexand', 'alexandria', 'alexi', 'alfalfa', 'alfons', 'alford', 'alfr', 'alger', 'algernon', 'algodon', 'algoma', 'algona', 'algonac', 'algonquin', 'alhambra', 'alic', 'alicevill', 'alicia', 'alief', 'alin', 'aliquippa', 'alix', 'alkol', 'all', 'allakaket', 'allamake', 'allamuchi', 'allan', 'allard', 'allardt', 'alle', 'alledonia', 'alleen', 'allegan', 'allegani', 'alleghani', 'allegheni', 'alleman', 'allen', 'allendal', 'allenhurst', 'allenport', 'allenspark', 'allensvill', 'allenton', 'allentown', 'allenwood', 'allerton', 'alley', 'alleyton', 'allgood', 'allianc', 'allig', 'allison', 'allon', 'allott', 'allouez', 'alloway', 'alloy', 'allport', 'allr', 'allston', 'allyn', 'allyson', 'alma', 'almelund', 'almena', 'almer', 'almira', 'almo', 'almon', 'almond', 'almont', 'almyra', 'alna', 'alney', 'alonzo', 'alpaugh', 'alpena', 'alpha', 'alpharetta', 'alpheus', 'alphons', 'alphonso', 'alphonzo', 'alpin', 'alplaus', 'alsea', 'alsen', 'alsey', 'alsip', 'alstead', 'alston', 'alta', 'altadena', 'altair', 'altamahaw', 'altamont', 'altavill', 'altavista', 'altenburg', 'altha', 'altheim', 'altmar', 'altmir', 'alto', 'alton', 'altona', 'altonah', 'altoona', 'altura', 'altus', 'alva', 'alvada', 'alvador', 'alvah', 'alvan', 'alvarado', 'alvaton', 'alverda', 'alverton', 'alvin', 'alviso', 'alvo', 'alvord', 'alvordton', 'alzada', 'am', 'ama', 'amado', 'amador', 'amagansett', 'amagon', 'amalia', 'amana', 'amanda', 'amarillo', 'amasa', 'amash', 'amawalk', 'amaziah', 'amazonia', 'amber', 'amberg', 'amberson', 'ambia', 'ambler', 'amboy', 'ambridg', 'ambro', 'ambros', 'ame', 'amelia', 'amenia', 'ameri', 'americus', 'amerman', 'amesburi', 'amesvill', 'amf', 'amherst', 'amherstdal', 'ami', 'amidon', 'amigo', 'amissvill', 'amistad', 'amit', 'amiti', 'amityvill', 'amli', 'amlin', 'amma', 'ammerman', 'ammon', 'amo', 'amodei', 'amon', 'amoret', 'amori', 'amorita', 'amsterdam', 'amston', 'an', 'anabel', 'anacoco', 'anaconda', 'anacort', 'anadarko', 'anaheim', 'anahola', 'anahuac', 'analomink', 'anamoos', 'anamosa', 'anasco', 'anaton', 'anawalt', 'ancher', 'anchor', 'anchorag', 'anchorvill', 'ancona', 'ancram', 'ancramdal', 'and', 'andal', 'andalusia', 'ander', 'andersen', 'anderson', 'andersonvill', 'andes', 'andi', 'andov', 'andr', 'andrea', 'andresen', 'andrew', 'andrieus', 'androscoggin', 'andrus', 'aneta', 'aneth', 'anfuso', 'angel', 'angela', 'angelica', 'angelina', 'angelo', 'angi', 'angier', 'angleton', 'angola', 'angoon', 'angora', 'anguilla', 'angus', 'angwin', 'anh', 'ani', 'aniak', 'anima', 'anita', 'aniwa', 'ankeni', 'anmoor', 'ann', 'anna', 'annabella', 'annada', 'annandal', 'annapoli', 'annawan', 'annemani', 'anniston', 'annona', 'annunzio', 'annvill', 'anoka', 'ansberri', 'ansel', 'anselm', 'anselmo', 'ansley', 'anson', 'ansonia', 'ansonvill', 'ansorg', 'anst', 'antelop', 'anthon', 'anthoni', 'antigo', 'antimoni', 'antioch', 'antler']\n"
     ]
    }
   ],
   "source": [
    "# Full path to stopwords pickle\n",
    "stopwords_path = os.path.join(data_c, \"stopwords.pkl\")\n",
    "\n",
    "# Load stopwords\n",
    "stopwords = joblib.load(stopwords_path)\n",
    "# Sort alphabetically\n",
    "stopwords = sorted(stopwords)\n",
    "\n",
    "print(f\"Loaded {len(stopwords)} stopwords (sorted alphabetically)\")\n",
    "print(stopwords[:500])  # show first 50 for a quick check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "11d056b5-6729-45f8-a233-3e8e6de1f69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SpaCy stopwords\n",
    "SPACY_STOPWORDS = list(nlp.Defaults.stop_words)\n",
    "# Save stopwords\n",
    "#joblib.dump(SPACY_STOPWORDS, stopwords_path)\n",
    "\n",
    "#print(f\"Saved {len(SPACY_STOPWORDS)} stopwords to {stopwords_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0aa75944-4b75-4aa4-9430-9aa6bd9ac934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Define function to Tokenize, eliminate digits, remove stopwords, lemmatize, POS-Tagging ==\n",
    "\n",
    "def pro1(lista):\n",
    "    # Remove punctuation\n",
    "    return [[row[0], row[1].translate(translator)] for row in lista]\n",
    "\n",
    "def pro2(lista):\n",
    "    # Tokenize and lowercase with gensim\n",
    "    return [[row[0], gensim.utils.simple_preprocess(row[1])] for row in lista]\n",
    "\n",
    "def pro3(lista):\n",
    "    # Remove tokens that are only digits\n",
    "        a = [[row[0], [w for w in row[1] if not w.isdigit()]] for row in lista]\n",
    "        return a\n",
    "    \n",
    "def pro4(lista):\n",
    "    # Drop short words\n",
    "    return [[row[0], [w for w in row[1] if len(w) > 2]] for row in lista]\n",
    "\n",
    "\n",
    "def tags(lista):\n",
    "    t = [[row[0], tagger.tag(row[1])] for row in lista]  # tag each tokenlist\n",
    "    t = [[row[0], [i[0] for i in row[1] if i[1].startswith(('N', 'V', 'J'))]] for row in t]\n",
    "    return t\n",
    "\n",
    "\n",
    "#def tags_spacy(lista):\n",
    "   # texts = [' '.join(row[1]) for row in lista]\n",
    "   # docs = list(nlp.pipe(texts, batch_size=20, n_process=1))\n",
    "   # result = []\n",
    "   # for i, doc in enumerate(docs):\n",
    "     #   filtered_tokens = [token.text for token in doc if token.tag_.startswith(('N', 'V', 'J'))]\n",
    "      #  result.append([lista[i][0], filtered_tokens])\n",
    "   # return result\n",
    "\n",
    "\n",
    "def pro5(lista):\n",
    "    return [\n",
    "        [row[0], [stemmer.stem(token) for token in row[1]]]\n",
    "        for row in lista\n",
    "    ]\n",
    "    \n",
    "def pro6(lista):\n",
    "    # Remove stopwords using stopword list # No source how this list was created?\n",
    "    return [[row[0], [w for w in row[1] if w not in stopwords]] for row in lista]\n",
    "      \n",
    "########################## Question for Max: They removed procedural words in the paper\n",
    "\n",
    "def dropnull(lista):\n",
    "    # Drop empty speeches\n",
    "    return [row for row in lista if len(' '.join(row[1])) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "524a274a-004c-4d87-9bb3-5f789a42b90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl] Before tagging: 1.64s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl] After tagging: 28.52s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed1.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl] Done. Total time: 188.58s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl] Before tagging: 1.50s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl] After tagging: 27.26s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed2.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl] Done. Total time: 181.81s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl] Before tagging: 1.46s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl] After tagging: 27.50s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed3.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl] Done. Total time: 181.24s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl] Before tagging: 1.60s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl] After tagging: 30.16s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed4.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl] Done. Total time: 200.48s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# == Create full pre-processing function and call it\n",
    "\n",
    "def preprocessing(data_name):\n",
    "    t0 = time.time()\n",
    "    print(f\"Starting preprocessing for {data_name}...\")\n",
    "\n",
    "    data = joblib.load(data_name)\n",
    "    data = pro1(data)\n",
    "    data = pro2(data)\n",
    "    data = pro3(data)\n",
    "    data = pro4(data)\n",
    "\n",
    "    print(f\"[{data_name}] Before tagging: {time.time() - t0:.2f}s\")\n",
    "    data = tags(data)\n",
    "    print(f\"[{data_name}] After tagging: {time.time() - t0:.2f}s\")\n",
    "\n",
    "    data = pro5(data)\n",
    "    # Apply stemming\n",
    "    data = pro6(data)\n",
    "    \n",
    "    data = dropnull(data)\n",
    "\n",
    "    # out_name = data_name.replace('cleanspeeches_', 'preprocessed_speeches_').replace('.pkl', '_temp.pkl')\n",
    "\n",
    "    # Store preprocessed corupus (before stemming) for wordcloud\n",
    "    #filename_wordcloud = data_name.replace('clean_speeches_', 'wordcloud_speeches_').replace('.pkl', '.pkl')\n",
    "    #out_name_wordcloud = os.path.join(data_preprocessed, os.path.basename(filename_wordcloud))\n",
    "    #joblib.dump(data, out_name_wordcloud)\n",
    "\n",
    "    filename_preprocessed = data_name.replace('clean_speeches_', 'preprocessed_speeches_').replace('.pkl', '.pkl')\n",
    "    out_preprocessed = os.path.join(data_preprocessed, os.path.basename(filename_preprocessed))\n",
    "    joblib.dump(data, out_preprocessed\n",
    "               )\n",
    "    print(f\"[{data_name}] Saved stemmed version: {out_preprocessed}\")\n",
    "\n",
    "    print(f\"[{data_name}] Done. Total time: {time.time() - t0:.2f}s\\n\")\n",
    "\n",
    "def main():\n",
    "    for fname in clean_files:\n",
    "        preprocessing(fname)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d3409987-485c-47c9-9430-01fa2c0e650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the pre-processed data\n",
    "preprocessed_files = [\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed1.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed2.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed3.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed4.pkl')\n",
    "]\n",
    "\n",
    "wordcloud_files = [\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed1.pkl'),\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed2.pkl'),\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed3.pkl'),\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed4.pkl')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "134ddbf7-9b95-4697-9d9c-f9c55b52ec80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e242768e-06bf-4914-a7ef-6a08ce19e844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens across all files: 12767\n"
     ]
    }
   ],
   "source": [
    "# Initialize a set to store all unique tokens\n",
    "all_unique_tokens = set()\n",
    "\n",
    "for dataname in preprocessed_files:\n",
    "    data = joblib.load(dataname)  # load list of [speech_id, tokenlist]\n",
    "    for _id, tokenlist in data:\n",
    "        all_unique_tokens.update(tokenlist)  # add tokens to the set\n",
    "\n",
    "print(f\"Total unique tokens across all files: {len(all_unique_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73db5f7-566d-4276-a631-c1f33fab494d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d006e-6a23-453b-9c91-0e60bd3f8301",
   "metadata": {},
   "source": [
    "## Word-Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf2443e-4c76-41b2-bc16-e72d91c318a8",
   "metadata": {},
   "source": [
    "### Count frequencies of all tokens and display the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "154d978d-cddd-431c-9d53-73c9faf8fdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:02<00:00,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Stemmed] Top 50 most common words:\n",
      "nation: 16625\n",
      "countri: 12472\n",
      "intern: 11073\n",
      "develop: 9680\n",
      "world: 9342\n",
      "peopl: 8994\n",
      "secur: 5873\n",
      "general: 5420\n",
      "govern: 5145\n",
      "econom: 4935\n",
      "year: 4769\n",
      "right: 4709\n",
      "assembl: 4252\n",
      "problem: 4106\n",
      "support: 3929\n",
      "human: 3874\n",
      "continu: 3756\n",
      "polit: 3501\n",
      "region: 3425\n",
      "communiti: 3423\n",
      "time: 3322\n",
      "need: 2939\n",
      "import: 2884\n",
      "make: 2675\n",
      "achiev: 2665\n",
      "situat: 2614\n",
      "conflict: 2566\n",
      "resolut: 2558\n",
      "principl: 2556\n",
      "global: 2474\n",
      "presid: 2470\n",
      "relat: 2419\n",
      "take: 2414\n",
      "great: 2411\n",
      "africa: 2368\n",
      "solut: 2364\n",
      "oper: 2335\n",
      "nuclear: 2333\n",
      "concern: 2306\n",
      "order: 2304\n",
      "action: 2293\n",
      "made: 2267\n",
      "confer: 2217\n",
      "establish: 2207\n",
      "commit: 2140\n",
      "polici: 2115\n",
      "part: 2102\n",
      "respect: 2088\n",
      "chang: 2088\n",
      "interest: 2086\n",
      "\n",
      "[Stemmed] Top 50 least common words:\n",
      "keita: 1\n",
      "roadmap: 1\n",
      "politick: 1\n",
      "popularis: 1\n",
      "haemorrhag: 1\n",
      "elysé: 1\n",
      "depositor: 1\n",
      "shortsel: 1\n",
      "disclosur: 1\n",
      "drawbridg: 1\n",
      "rebuf: 1\n",
      "refram: 1\n",
      "macro: 1\n",
      "midrand: 1\n",
      "kwasniewski: 1\n",
      "triad: 1\n",
      "allur: 1\n",
      "indigest: 1\n",
      "lunchtim: 1\n",
      "peror: 1\n",
      "wafer: 1\n",
      "oaij: 1\n",
      "prima: 1\n",
      "nwaliau: 1\n",
      "yerevan: 1\n",
      "bioenergi: 1\n",
      "djibril: 1\n",
      "yipènè: 1\n",
      "tuareg: 1\n",
      "eroğlu: 1\n",
      "integrationist: 1\n",
      "bratislava: 1\n",
      "heel: 1\n",
      "overs: 1\n",
      "agglomer: 1\n",
      "imput: 1\n",
      "delinqu: 1\n",
      "etiolog: 1\n",
      "rarifi: 1\n",
      "catchi: 1\n",
      "underutil: 1\n",
      "reus: 1\n",
      "algocraci: 1\n",
      "ljubljana: 1\n",
      "radfan: 1\n",
      "ihala: 1\n",
      "crystail: 1\n",
      "rebuff: 1\n",
      "univoc: 1\n",
      "egoist: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit\\\\data\\\\freq\\\\word_counts.pkl']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#== Count token frequencies ==\n",
    "\n",
    "def count_frequencies(filenames):\n",
    "    total_freq = Counter()\n",
    "    for fname in tqdm(filenames):\n",
    "        data = joblib.load(fname)\n",
    "        tokens = chain.from_iterable(row[1] for row in data if isinstance(row[1], list))\n",
    "        total_freq.update(tokens)\n",
    "    return total_freq\n",
    "\n",
    "#def remove_rare_words(filenames, freqs, min_count=10):\n",
    "   # for fname in filenames:\n",
    "       # data = joblib.load(fname)\n",
    "       # filtered_data = []\n",
    "        #for doc_id, tokens in data:\n",
    "          #  filtered_tokens = [w for w in tokens if freqs.get(w, 0) >= min_count]\n",
    "          #  filtered_data.append([doc_id, filtered_tokens])\n",
    "       # joblib.dump(filtered_data, fname)  # overwrite or save as new file\n",
    "       # print(f\"Processed {fname}: removed words with freq < {min_count}\")\n",
    "\n",
    "# === Count for preprocessed (stemmed) speeches ===\n",
    "word_counts = count_frequencies(preprocessed_files)\n",
    "\n",
    "#remove_rare_words(preprocessed_files, word_counts, min_count=10)\n",
    "\n",
    "print(\"\\n[Stemmed] Top 50 most common words:\")\n",
    "for word, count in word_counts.most_common(50):\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\n[Stemmed] Top 50 least common words:\")\n",
    "for word, count in word_counts.most_common()[-50:]:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Save stemmed word counts\n",
    "save_path = os.path.join(data_freq, 'word_counts.pkl')\n",
    "joblib.dump(word_counts, save_path)\n",
    "\n",
    "# === Count for wordcloud (unstemmed) speeches ===\n",
    "#word_counts_wordcloud = count_frequencies(wordcloud_files)\n",
    "\n",
    "#print(\"\\n[Wordcloud] Top 50 most common words:\")\n",
    "#for word, count in word_counts_wordcloud.most_common(50):\n",
    "    #print(f\"{word}: {count}\")\n",
    "\n",
    "#print(\"\\n[Wordcloud] Top 50 least common words:\")\n",
    "#for word, count in word_counts_wordcloud.most_common()[-50:]:\n",
    "    #print(f\"{word}: {count}\")\n",
    "\n",
    "# Save unstemmed word counts\n",
    "#save_path_wordcloud = os.path.join(data_freq, 'word_counts_wordcloud.pkl')\n",
    "#joblib.dump(word_counts_wordcloud, save_path_wordcloud)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "411b6ae0-8019-4722-9cee-a1507bfe5218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words with frequency >= 10: 4293\n"
     ]
    }
   ],
   "source": [
    "# Filter words with frequency >= 10\n",
    "min_count = 10\n",
    "frequent_words = {word: count for word, count in word_counts.items() if count >= min_count}\n",
    "\n",
    "# Number of unique words appearing at least 10 times\n",
    "num_unique_frequent_words = len(frequent_words)\n",
    "print(f\"Number of unique words with frequency >= {min_count}: {num_unique_frequent_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "932ce36d-d21c-4f1c-aff7-899c321dcfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_c)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39acefa1-c504-478e-98e5-9d465963dc82",
   "metadata": {},
   "source": [
    "### Count the frequency of the dictionary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "9e28f481-7e20-486c-a6b3-8729d6aeaad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of affect dictionary:\n",
      "['forbid', 'unattract', 'cruelti', 'crappi', 'apathi', 'scari', 'unimpress', 'sin', 'dumbest', 'eas', 'agit', 'sob', 'shocker', 'tragedi', 'fabul', 'strongest', 'giver', 'sigh', 'aw', 'witch', 'hurtl', 'fucktard', 'cruel', 'glamor', 'funni', 'smarter', 'brillianc', 'irrate', 'alright', 'honest', 'profit', 'fearless', 'grievous', 'relax', 'isolationist', 'hah', 'shyness', 'poorest', 'cruelest', 'troublemak', 'disagre', 'agon', 'terror', 'fight', 'pleas', 'poor', 'crazi', 'hostil', 'stupid', 'damnat', 'vain', 'jade', 'heartless', 'nag', 'gloomi', 'damn', 'dishearten', 'pleaser', 'credit', 'warmth', 'greatest', 'whine', 'shame', 'angriest', 'envious', 'grin', 'blameless', 'sweeter', 'laidback', 'stupidest', 'unprotect', 'whiner', 'unlov', 'shake', 'boredom', 'fairer', 'weaker', 'wellb', 'bold', 'sucki', 'unsuccess', 'mourner', 'liken', 'defens', 'invigor', 'tedious', 'paranoid', 'cynic', 'dignifi', 'paranoia', 'sweetest', 'contented', 'humili', 'crush', 'terrif', 'alarm', 'asham', 'impati', 'fake', 'flirti', 'laziest', 'libertarian', 'empti', 'panicki', 'rape', 'contemptu', 'incent', 'reward', 'brutal', 'ignoramus', 'bereav', 'wimp', 'laugh', 'nicer', 'warm', 'resent', 'threat', 'hell', 'tortur', 'mad', 'worship', 'uncomfort', 'depress', 'unsaf', 'faultless', 'numb', 'humorless', 'murder', 'prettiest', 'jealous', 'tragic', 'discomfort', 'loveli', 'ungrat', 'divin', 'lamest', 'weaken', 'horribl', 'fuck', 'agoni', 'grievanc', 'ach', 'isolation', 'masoch', 'uglier', 'amaz', 'devilish', 'bitter', 'devil', 'smartest', 'glori', 'trite', 'vicious', 'unsavori', 'grossest', 'sinist', 'cheeri', 'dismay', 'easier', 'flatter', 'virtuous', 'egot', 'generos', 'cherish', 'heartwarm', 'advers', 'tough', 'adorn', 'overwhelm', 'devast', 'egotist', 'sociabl', 'lazier', 'affection', 'sarcasm', 'humor', 'funniest', 'startl', 'innoc', 'vomit', 'shyli', 'kiss', 'burdensom', 'melancholi', 'weakest', 'worthless', 'gentlest', 'proud', 'alarmist', 'pervert', 'demeanor', 'loneli', 'reassur', 'weird', 'passionless', 'loyalti', 'frustrat', 'weak', 'immor', 'hater', 'libertin', 'flirt', 'terribl', 'threaten', 'fatalist', 'melancholia', 'avers', 'prick', 'sucker', 'meanest', 'virtuos', 'benefici', 'neurot', 'disappoint', 'restless', 'romant', 'bastard', 'angri', 'flatteri', 'stun', 'bolder', 'amor', 'forbidden', 'hug', 'demean', 'disgust', 'truer', 'benefic', 'teas', 'stinki', 'condemnatori', 'turmoil', 'poison', 'cunt', 'gorgeous', 'harm', 'lousi', 'glorifi', 'piti', 'wimpish', 'impression', 'freakish', 'happiest', 'apathet', 'pitiabl', 'unpleas', 'suffer', 'miseri', 'toler', 'mess', 'casual', 'anxieti', 'stale', 'fieri', 'grudg', 'arrog', 'outrag', 'thrill', 'faithless', 'popular', 'vaniti', 'palat', 'fantasi', 'decay', 'domin', 'mood', 'proudest', 'scream', 'stress', 'calm', 'fatigu', 'dumber', 'benign', 'rude', 'steal', 'tension', 'enthusiast', 'intimid', 'sweetheart', 'prettier', 'perv', 'pathet', 'flawless', 'contradictori', 'distress', 'moron', 'gratifi', 'frighten', 'anguish', 'snob', 'defenseless', 'fantast', 'sad', 'thief', 'worsen', 'tear', 'handsom', 'attract', 'messi', 'smother', 'scarier', 'afraid', 'insult', 'disreput', 'wept', 'victim', 'traumat', 'prejudic', 'amus', 'decent', 'joyous', 'masochist', 'abandon', 'foe', 'pleasantri', 'shitless', 'strengthen', 'antagonist', 'liar', 'romantic', 'violenc', 'sentimentalist', 'gratif', 'decept', 'witchcraft', 'liveli', 'impressionist', 'guilt', 'griev', 'rotten', 'disgrac', 'insincer', 'pushi', 'enjoy', 'freaki', 'ador', 'grate', 'homesick', 'braver', 'battlefield', 'niceti', 'hooray', 'incompet', 'thiev', 'timid', 'adventur', 'needi', 'joyless', 'grimac', 'gloom', 'dump', 'horrid', 'magnific', 'unlucki', 'killer', 'uncontrol', 'funnier', 'ruin', 'weepi', 'play', 'encourag', 'fantas', 'irrit', 'sincer', 'rage', 'despis', 'unhappi', 'hate', 'bravest', 'harmless', 'hero', 'vigor', 'pervers', 'dumb', 'degrad', 'shameless', 'villaini', 'envi', 'aggress', 'fucker', 'courag', 'craziest', 'delici', 'scare', 'finer', 'bittersweet', 'trivial', 'wealthi', 'heroic', 'ecstat', 'irrat', 'prejudici', 'smug', 'stammer', 'feroci', 'phobic', 'villain', 'anger', 'fond', 'obsession', 'awesom', 'wors', 'compassion', 'weakl', 'calmer', 'inhibit', 'bother', 'rancid', 'hatr', 'beaten', 'opportunist', 'offens', 'haunt', 'appal', 'painless', 'pessimist', 'molest', 'lol', 'wrongdo', 'frantic', 'heroin', 'rejoic', 'woe', 'miser', 'meaner', 'saddest', 'mooch', 'attack', 'enthusiasm', 'joke', 'meritocraci', 'angrier', 'emot', 'weirdest', 'silliest', 'kisser', 'hurrah', 'crueler', 'nervous', 'dishonor', 'gracious', 'sicker', 'hopeless', 'delect', 'compliment', 'petrifi', 'cuter', 'reek', 'loveless', 'daze', 'weirder', 'triumphant', 'impolit', 'offend', 'richer', 'violent', 'crazier', 'comedown', 'heroism', 'stubborn', 'maniac', 'strain', 'broke', 'jerk', 'unkind', 'asshol', 'reveng', 'jealousi', 'damnabl', 'unimport', 'careless', 'gentler', 'bitch', 'shi', 'terrorist', 'rigid', 'harsh', 'passion', 'annoy', 'compass', 'geek', 'weirdo', 'agre', 'whore', 'splendor', 'suck', 'fun', 'chuckl', 'snobberi', 'mock', 'doom', 'reluct', 'mourn', 'boldest', 'helpless', 'trembl', 'pestil', 'curs', 'desper', 'harass', 'complain', 'joker', 'obsess', 'uneasi', 'nasti', 'greedi', 'belov', 'grim', 'upset', 'greed', 'honesti', 'phoni', 'feudal', 'loveliest', 'trauma', 'dwell', 'ecstasi', 'blame', 'legit', 'calmest', 'sickest', 'scariest', 'sarcast', 'cheat', 'fatal', 'stronger', 'energet', 'feroc', 'healthi', 'goddam', 'disast', 'flirtati', 'defeatist', 'glorious', 'terrifi', 'gossip', 'forbad', 'sicken', 'poorer', 'strangest', 'cute', 'struggl', 'woebegon', 'worri', 'gratitud', 'cheer', 'distraught', 'ok', 'vulner', 'blessed', 'fairest', 'lame', 'optim', 'grouch', 'fright', 'pessim', 'sadder', 'worst', 'despair', 'bash', 'intellectu', 'promis', 'prais', 'heartfelt', 'horror', 'crap', 'repress', 'prouder', 'fought', 'discourag', 'heal', 'lazi', 'precious', 'melanchol', 'isol', 'aggressor', 'giggl', 'idiot', 'twitchi', 'glorif', 'kid', 'selfish', 'smile', 'grief', 'hellish', 'shit', 'fume', 'bitchi', 'fool', 'trustworthi', 'optimist', 'dumpi', 'unwelcom', 'neat', 'nicest', 'furi', 'unfriend', 'ridicul', 'arguabl', 'shamefac', 'pervi', 'upbeat', 'wow', 'easygo', 'blameworthi', 'charmer', 'stink', 'phobia', 'adversari', 'sillier', 'distrust', 'dread', 'happier', 'vital', 'dissatisfact', 'insecur', 'assault', 'ineffect', 'panic', 'powerless', 'nightmar', 'wealth', 'enrag', 'splendid', 'foolish', 'ugliest', 'contradict', 'worrier', 'freak']\n",
      "Number of words in affect dictionary: 629\n",
      "\n",
      "Contents of cognition dictionary:\n",
      "['analyt', 'guess', 'suspect', 'coz', 'mysteri', 'unawar', 'proverbi', 'dissimilar', 'consequenti', 'stimul', 'occasion', 'rethink', 'purest', 'entireti', 'abnorm', 'referenti', 'differenti', 'diagnos', 'ambigu', 'concentr', 'undon', 'undecid', 'disclosur', 'fundamentalist', 'blur', 'reaction', 'variabl', 'recogniz', 'meant', 'reconstruct', 'supposit', 'acknowledg', 'choosi', 'accuraci', 'mislead', 'perceiv', 'hypothesi', 'unknow', 'cuz', 'reveal', 'potenti', 'puriti', 'persuad', 'indirect', 'generat', 'presumpt', 'clue', 'justif', 'evidenti', 'realize', 'deduct', 'causal', 'inevit', 'thinker', 'evidentiari', 'factor', 'undeni', 'unclear', 'pick', 'induc', 'accur', 'absolut', 'everytim', 'reconsider', 'diagnost', 'solut', 'diagnosi', 'discoveri', 'ration', 'unambigu', 'explicit', 'decis', 'insight', 'confess', 'fuzzi', 'led', 'lesson', 'theoriz', 'analyz', 'learner', 'split', 'enlighten', 'cos', 'respons', 'dubious', 'react', 'guarante', 'someday', 'blatant', 'undo', 'perceptu', 'infal', 'afterthought', 'inequ', 'feasibl', 'knowabl', 'approxim', 'logic', 'meaning', 'unknown', 'categor', 'trigger', 'incomplet', 'curious', 'attribut', 'pretend', 'reactionari', 'prove', 'meaningless', 'reactiv', 'proof', 'bet', 'purpos', 'suppos', 'discern', 'imagin', 'kinda', 'comprehens', 'sens', 'percept', 'everyday', 'rational', 'unquestion', 'random', 'implicit', 'clarif', 'theori', 'dunno', 'analysi', 'obedi', 'memor', 'genuin', 'evalu', 'explor', 'linkag', 'grasp', 'curios', 'methink', 'somewher', 'deduc', 'unwant', 'induct', 'unrel', 'outstand', 'discov', 'unresolv', 'exploratori', 'choos', 'perspect', 'inferenti', 'disclos', 'factual', 'arbitrari', 'conting', 'vagu', 'persuas', 'manipul', 'impli', 'somehow', 'sorta', 'borderlin', 'reconcil', 'anyhow', 'conscious', 'probabilist', 'precis', 'rearrang', 'referenc', 'obscur']\n",
      "Number of words in cognition dictionary: 169\n",
      "Top 100 words by weighted frequency:\n",
      "unbecom: 0.998784014163803\n",
      "chasten: 0.998784014163803\n",
      "multipartit: 0.998784014163803\n",
      "lawgovern: 0.998784014163803\n",
      "anfaal: 0.998784014163803\n",
      "halabja: 0.998784014163803\n",
      "askari: 0.998784014163803\n",
      "samara: 0.998784014163803\n",
      "anbar: 0.998784014163803\n",
      "diyala: 0.998784014163803\n",
      "airmen: 0.998784014163803\n",
      "friendlier: 0.998784014163803\n",
      "newgener: 0.998784014163803\n",
      "teleconfer: 0.998784014163803\n",
      "soundest: 0.998784014163803\n",
      "itinerari: 0.998784014163803\n",
      "ammonium: 0.998784014163803\n",
      "nitrateload: 0.998784014163803\n",
      "ghajar: 0.998784014163803\n",
      "kfarshouba: 0.998784014163803\n",
      "verv: 0.998784014163803\n",
      "skylin: 0.998784014163803\n",
      "unorthodox: 0.998784014163803\n",
      "intrast: 0.998784014163803\n",
      "sanitair: 0.998784014163803\n",
      "trepid: 0.998784014163803\n",
      "centuriesold: 0.998784014163803\n",
      "nepali: 0.998784014163803\n",
      "caveat: 0.998784014163803\n",
      "requit: 0.998784014163803\n",
      "shrewd: 0.998784014163803\n",
      "metaphys: 0.998784014163803\n",
      "edifi: 0.998784014163803\n",
      "compendium: 0.998784014163803\n",
      "width: 0.998784014163803\n",
      "backstop: 0.998784014163803\n",
      "copra: 0.998784014163803\n",
      "sugari: 0.998784014163803\n",
      "weto: 0.998784014163803\n",
      "mour: 0.998784014163803\n",
      "ahrc: 0.998784014163803\n",
      "crossstrait: 0.998784014163803\n",
      "sharper: 0.998784014163803\n",
      "inapplic: 0.998784014163803\n",
      "sae: 0.998784014163803\n",
      "jeopardis: 0.998784014163803\n",
      "salah: 0.998784014163803\n",
      "flour: 0.998784014163803\n",
      "incontin: 0.998784014163803\n",
      "repercussionsof: 0.998784014163803\n",
      "nonliv: 0.998784014163803\n",
      "overflight: 0.998784014163803\n",
      "iiadd: 0.998784014163803\n",
      "diploma: 0.998784014163803\n",
      "niceti: 0.998784014163803\n",
      "chessmen: 0.998784014163803\n",
      "guguletu: 0.998784014163803\n",
      "scarecrow: 0.998784014163803\n",
      "hydraul: 0.998784014163803\n",
      "appendic: 0.998784014163803\n",
      "nonresort: 0.998784014163803\n",
      "citat: 0.998784014163803\n",
      "neocoloni: 0.998784014163803\n",
      "mom: 0.998784014163803\n",
      "kabila: 0.998784014163803\n",
      "unionl: 0.998784014163803\n",
      "trebl: 0.998784014163803\n",
      "autism: 0.998784014163803\n",
      "rimbaud: 0.998784014163803\n",
      "compil: 0.998784014163803\n",
      "wellconduct: 0.998784014163803\n",
      "kiir: 0.998784014163803\n",
      "rohingyan: 0.998784014163803\n",
      "hindsight: 0.998784014163803\n",
      "teeter: 0.998784014163803\n",
      "nzo: 0.998784014163803\n",
      "caprivi: 0.998784014163803\n",
      "transkalahari: 0.998784014163803\n",
      "centrepiec: 0.998784014163803\n",
      "juggl: 0.998784014163803\n",
      "salvadoran: 0.998784014163803\n",
      "stoic: 0.998784014163803\n",
      "groupmad: 0.998784014163803\n",
      "somozist: 0.998784014163803\n",
      "castilla: 0.998784014163803\n",
      "palmerola: 0.998784014163803\n",
      "comayagua: 0.998784014163803\n",
      "bludgeon: 0.998784014163803\n",
      "dirtier: 0.998784014163803\n",
      "entrail: 0.998784014163803\n",
      "laughabl: 0.998784014163803\n",
      "blacker: 0.998784014163803\n",
      "uncur: 0.998784014163803\n",
      "contributori: 0.998784014163803\n",
      "winkl: 0.998784014163803\n",
      "peradventur: 0.998784014163803\n",
      "unexcit: 0.998784014163803\n",
      "purs: 0.998784014163803\n",
      "actingin: 0.998784014163803\n",
      "uncoven: 0.998784014163803\n"
     ]
    }
   ],
   "source": [
    "# == Count dictionary words\n",
    "\n",
    "# Load dictionaries            ##### How did they come up with this dictionary? Why did they exclude words?\n",
    "\n",
    "affect_path = os.path.join(data_dict, 'dictionary_affect.pkl')\n",
    "cognition_path = os.path.join(data_dict, 'dictionary_cognition.pkl')\n",
    "\n",
    "affect = joblib.load(affect_path)\n",
    "cognition = joblib.load(cognition_path)\n",
    "\n",
    "print(\"Contents of affect dictionary:\")\n",
    "print(affect)\n",
    "print(\"Number of words in affect dictionary:\", len(affect))\n",
    "\n",
    "print(\"\\nContents of cognition dictionary:\")\n",
    "print(cognition)\n",
    "print(\"Number of words in cognition dictionary:\", len(cognition))\n",
    "\n",
    "#with open(affect_path, 'rb') as f:\n",
    "  #  affect_dict = pickle.load(f)\n",
    "#print(\"Contents of affect dictionary:\")\n",
    "#print(affect_dict)\n",
    "#print(\"Number of words in affect dictionary:\", len(affect_dict))\n",
    "\n",
    "#with open(cognition_path, 'rb') as f:\n",
    "  #  cognition_dict = pickle.load(f)\n",
    "#print(\"Contents of cognition dictionary:\")\n",
    "#print(cognition_dict)\n",
    "#print(\"Number of words in cognition dictionary:\", len(cognition_dict))\n",
    "\n",
    "a = [[i, word_counts[i]] for i in affect if i in word_counts]\n",
    "c = [[i, word_counts[i]] for i in cognition if i in word_counts]\n",
    "\n",
    "a = sorted(a, key=lambda x: x[1], reverse=True)\n",
    "c = sorted(c, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "a = [[i[0], f\"({i[1]}),\"] for i in a]\n",
    "c = [[i[0], f\"({i[1]}),\"] for i in c]\n",
    "\n",
    "a1 = ' '.join(str(r) for v in a for r in v)\n",
    "c1 = ' '.join(str(r) for v in c for r in v)\n",
    "\n",
    "affect_out_path = os.path.join(data_freq, \"affect_words.txt\")\n",
    "cog_out_path = os.path.join(data_freq, \"cog_words.txt\")\n",
    "\n",
    "os.makedirs(data_freq, exist_ok=True)  # ensure directory exists\n",
    "\n",
    "with open(affect_out_path, \"w\") as output:\n",
    "    output.write(a1)\n",
    "\n",
    "with open(cog_out_path, \"w\") as output:\n",
    "    output.write(c1)\n",
    "\n",
    "\n",
    "# == Calculate weighted frequencies for all words\n",
    "\n",
    "# - downweights very common words by giving more importance to rare ones\n",
    "#word_counts = joblib.load(os.path.join(data_freq, 'word_counts.pkl'))\n",
    "\n",
    "l = sum(word_counts.values())\n",
    "\n",
    "a = 0.001\n",
    "word_counts_weighted = {k: a / (a + (v / l)) for k, v in word_counts.items()}\n",
    "\n",
    "joblib.dump(word_counts_weighted, os.path.join(data_freq, 'word_counts_weighted.pkl'))\n",
    "\n",
    "################################################################################ ISSUE##################\n",
    "# To print top 100 by weighted values, sort the dictionary by value descending:\n",
    "top_100_weighted = sorted(word_counts_weighted.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "\n",
    "print(\"Top 100 words by weighted frequency:\")\n",
    "for word, weight in top_100_weighted:\n",
    "    print(f\"{word}: {weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a7ec60-2d73-4c68-948c-592970c4ad93",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df626ded-5e49-4a34-a3d1-315ba370226f",
   "metadata": {},
   "source": [
    "## Final Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0d0856a2-44f1-4304-b221-09a9ab72ad17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens across all files: 12767\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_c)\n",
    "os.chdir(data_freq)\n",
    "\n",
    "count = joblib.load('word_counts.pkl')  # load stemmed counts\n",
    "\n",
    "# For each speech only keep tokens that appear at least 10x\n",
    "\n",
    "def select(lista):\n",
    "    for i in range(len(lista)):\n",
    "        x = lista[i][0]\n",
    "        y = lista[i][1]\n",
    "        y = [w for w in y if count.get(w, 0) >= 10]\n",
    "        lista[i] = [x, y]\n",
    "    return lista\n",
    "\n",
    "for data_path in preprocessed_files:\n",
    "    data = joblib.load(data_path)\n",
    "    data = select(data)\n",
    "    cleaned_path = data_path.replace('.pkl', '_final.pkl')\n",
    "    joblib.dump(data, cleaned_path)\n",
    "\n",
    "# Initialize a set to store all unique tokens\n",
    "all_unique_tokens = set()\n",
    "\n",
    "for dataname in preprocessed_files:\n",
    "    data = joblib.load(dataname)  # load list of [speech_id, tokenlist]\n",
    "    for _id, tokenlist in data:\n",
    "        all_unique_tokens.update(tokenlist)  # add tokens to the set\n",
    "\n",
    "print(f\"Total unique tokens across all files: {len(all_unique_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "86873a31-3f1e-4d31-aa16-0f2813161647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens across all files: 4293\n",
      "Total number of speeches processed: 761\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "os.chdir(data_preprocessed)\n",
    "\n",
    "final_files = [\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed1_final.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed2_final.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed3_final.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed4_final.pkl')\n",
    "]\n",
    "\n",
    "# Initialize a set to store all unique tokens\n",
    "all_unique_tokens = set()\n",
    "total_entries = 0\n",
    "\n",
    "for dataname in final_files:\n",
    "    data = joblib.load(dataname)\n",
    "    total_entries += len(data)\n",
    "    for _, tokenlist in data:\n",
    "        all_unique_tokens.update(tokenlist)\n",
    "\n",
    "print(f\"Total unique tokens across all files: {len(all_unique_tokens)}\")\n",
    "print(f\"Total number of speeches processed: {total_entries}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d523d2cf-5c05-4406-a78e-a31f719e538d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
