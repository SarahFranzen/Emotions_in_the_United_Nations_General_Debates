{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b41cb22-e190-48d5-9926-7ac6fac8310d",
   "metadata": {},
   "source": [
    "# Emotion and Reason in Political Language: Examining the UN General speeches\n",
    "## Script 1: Preprocessing & Token Frequencies\n",
    "## Author: Sarah Franzen\n",
    "\n",
    "### Instructions BEFORE running this script:\n",
    "- Ensure all required packages are installed. If not, set `InstallPackages = TRUE` (see code cells below).  \n",
    "- Set your working directory appropriately.  \n",
    "- The script will automatically create the required folder structure.  \n",
    "- Later, you will be asked to download the data from:  \n",
    "  https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/0TJX8Y  \n",
    "  and store it **unzipped** inside the created folder *data_original*\n",
    "\n",
    "\n",
    "### Description: \n",
    "- Extract documents from their original txt documents and store them as one csv\n",
    "- Data Cleaning and Pre-Processing\n",
    "- Count word frequencies and weight themh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930ebd56-def9-4a16-91eb-78dbb7d037ae",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5588afc-105a-426e-9348-742a830a2732",
   "metadata": {},
   "source": [
    "## Setup, Installation and Verification of required Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "ebb8987a-a0da-453e-97e2-aafa209d0e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "InstallPackages = False # Set this to True to install the following packages \n",
    "\n",
    "if InstallPackages:\n",
    "    import sys\n",
    "\n",
    "    packages = [\n",
    "        \"pandas\",\n",
    "        \"nltk\",\n",
    "        \"spacy\",\n",
    "        \"numpy\",\n",
    "        \"gensim\",\n",
    "        \"pycountry\",\n",
    "        \"wordcloud\",\n",
    "        \"matplotlib\",\n",
    "        \"tqdm\"\n",
    "    ]\n",
    "\n",
    "    for package in packages:\n",
    "        !{sys.executable} -m pip install {package}\n",
    "\n",
    "\n",
    "DownloadAdditions = False # Set this to True to download these additional resources\n",
    "if DownloadAdditions:\n",
    "    nltk.download(\"punkt\")\n",
    "    nltk.download(\"averaged_perceptron_tagger\")\n",
    "    spacy.cli.download('en_core_web_lg')         # Download spaCy English model (large)\n",
    "\n",
    "#########################\n",
    "# Check if all packages are included\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "32d7d499-0563-46e0-9730-914c9f106d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Import standard and third-party libraries for data processing, NLP, and visualization ==\n",
    "\n",
    "import gensim\n",
    "import joblib\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import pycountry\n",
    "import random\n",
    "import re\n",
    "import spacy\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from multiprocessing import Pool, freeze_support\n",
    "from nltk.stem import SnowballStemmer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOPWORDS\n",
    "from string import punctuation\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# === Initialize NLP Tools ===\n",
    "\n",
    "translator = str.maketrans('', '', punctuation)\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "4d495156-bbb2-403c-86f2-83b47d8cb03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Folder structure created:\n",
      "- C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\n",
      "  - data_original\n",
      "  - dictionaries\n",
      "  - freq\n",
      "  - preprocessed\n",
      "  - temp\n",
      "  - tokenized\n"
     ]
    }
   ],
   "source": [
    "# === Set Working Directory and create folder structure ===\n",
    "\n",
    "# Prompt user to enter working directory path\n",
    "#wd = input(\"Please enter your working directory path (e.g., C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit): \").strip()\n",
    "\n",
    "# Change to the entered working directory\n",
    "#try:\n",
    "   # os.chdir(wd)\n",
    "    #print(f\"Working directory set to: {os.getcwd()}\")\n",
    "#except FileNotFoundError:\n",
    "   # print(\"ERROR: The directory you entered does not exist. Please restart and enter a valid path.\")\n",
    "    #exit(1)\n",
    "\n",
    "# Set your working directory (adjust this as needed)\n",
    "wd = r\"C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\"\n",
    "os.chdir(wd)\n",
    "\n",
    "# Set base path to current working directory\n",
    "base_path = Path.cwd()\n",
    "data_path = base_path / \"data\"\n",
    "\n",
    "# List of subfolders to create inside 'data'\n",
    "subfolders = [\"data_original\", \"dictionaries\", \"freq\", \"preprocessed\", \"temp\", \"tokenized\"]\n",
    "\n",
    "# Create 'data' folder if it doesn't exist\n",
    "data_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Create subfolders\n",
    "for name in subfolders:\n",
    "    (data_path / name).mkdir(exist_ok=True)\n",
    "\n",
    "print(\"\\nFolder structure created:\")\n",
    "print(f\"- {data_path}\")\n",
    "for name in subfolders:\n",
    "    print(f\"  - {name}\")\n",
    "\n",
    "# Prompt user to place raw data files\n",
    "#print(f\"\\nPlease place your raw data files (unzipped) into the folder:\\n  {data_path / 'data_original'}\")\n",
    "#input(\"Press Enter after you have placed the files to continue...\")\n",
    "\n",
    "#print(\"Continuing with the script...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "56fa9a21-650f-4d8e-8a21-3d9629655c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define Folder Paths ===\n",
    "\n",
    "# If an error occurs, make sure that you actually have these folders in your working directory\n",
    "data_c = os.path.join(wd, 'data')\n",
    "data_temp = os.path.join(data_c, 'temp')\n",
    "data_freq = os.path.join(data_c, 'freq')\n",
    "data_dict = os.path.join(data_c, 'dictionaries')\n",
    "data_preprocessed = os.path.join(data_c, 'preprocessed')\n",
    "data_tokenized = os.path.join(data_c, 'tokenized')\n",
    "fig = os.path.join(wd, 'fig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "296fc7d6-a2ac-406e-a322-6c9f2702ee1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\n"
     ]
    }
   ],
   "source": [
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bd410e-bc90-4e0b-aa08-a9bff13fab47",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff34b6-cbee-4f8c-8076-5f385f596b58",
   "metadata": {},
   "source": [
    "## Load and Prepare Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18067b9-6337-4fb3-a570-603083a7deae",
   "metadata": {},
   "source": [
    "### This chunk can be skipped at the moment\n",
    "### Think of proper header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "id": "93d247a0-5ff2-4062-add9-f3267ae1fe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total speeches found: 10761\n",
      "\n",
      " Saved raw data with 800 speeches to '.\\data\\un_corpus_raw.csv'\n"
     ]
    }
   ],
   "source": [
    "# == Load Sample from UN General Debate Corpus ==\n",
    "\n",
    "# Set Folder path containing the original TXT files    \n",
    "base_folder = r\".\\data\\data_original\\UN General Debate Corpus\\UNGDC_1946-2023\\TXT\"\n",
    "\n",
    "# Collect txt-files\n",
    "all_txt_files = []\n",
    "for root, dirs, files in os.walk(base_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.txt') and not file.startswith('._'):\n",
    "            all_txt_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Total speeches found: {len(all_txt_files)}\")\n",
    "\n",
    "# Randomly pick 800 files from the full collection   ###################################################### REMOVE AT LATER POINT\n",
    "sampled_files = random.sample(all_txt_files,800)\n",
    "\n",
    "# Read the selected files into a list\n",
    "raw_data = []\n",
    "for filepath in sampled_files:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        raw_data.append({'filename': os.path.basename(filepath), 'speech': content})\n",
    "\n",
    "df_raw = pd.DataFrame(raw_data)\n",
    "\n",
    "# == Store as csv & pkl ==\n",
    "\n",
    "df_raw = df_raw[df_raw['filename'] != '.DS_Store-to-UTF-8.txt'].copy()\n",
    "\n",
    "raw_pickle_path = r\".\\data\\un_corpus_raw.pkl\"\n",
    "df_raw.to_pickle(raw_pickle_path)\n",
    "\n",
    "raw_output_path = r\".\\data\\un_corpus_raw.csv\"\n",
    "df_raw.to_csv(raw_output_path, index=False, sep=';', encoding='utf-8')\n",
    "\n",
    "print(f\"\\n Saved raw data with {len(df_raw)} speeches to '{raw_output_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "id": "6405ff20-92fa-4c97-becb-ac5fff70f84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NZL_66_2011.txt</td>\n",
       "      <td>As we say in Maori, \\nto all peoples and to al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>THA_22_1967.txt</td>\n",
       "      <td>31. The past few months have been for the Unit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GTM_26_1971.txt</td>\n",
       "      <td>Mr. President, I offer you my most sincere con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LBY_55_2000.txt</td>\n",
       "      <td>It gives me great pleasure, Sir, to extend to\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CMR_56_2001.txt</td>\n",
       "      <td>﻿I should like at the outset to express the\\np...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech\n",
       "0  NZL_66_2011.txt  As we say in Maori, \\nto all peoples and to al...\n",
       "1  THA_22_1967.txt  31. The past few months have been for the Unit...\n",
       "2  GTM_26_1971.txt  Mr. President, I offer you my most sincere con...\n",
       "3  LBY_55_2000.txt  It gives me great pleasure, Sir, to extend to\\...\n",
       "4  CMR_56_2001.txt  ﻿I should like at the outset to express the\\np..."
      ]
     },
     "execution_count": 652,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# == Load data & drop empty speeches ==\n",
    "\n",
    "df_raw = pd.read_pickle(r\".\\data\\un_corpus_raw.pkl\")\n",
    "\n",
    "# Drop empty speeches\n",
    "df_raw['speech'] = df_raw['speech'].astype(str)\n",
    "df_raw = df_raw[df_raw['speech'].str.strip() != ''].copy()\n",
    "\n",
    "df_raw.head()         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7980e945-9d56-4df3-99e5-acf491617568",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7d3162-ebbd-449b-a2be-1148fceda725",
   "metadata": {},
   "source": [
    "## Create new variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25710e79-5068-4d39-8023-f4f97513f165",
   "metadata": {},
   "source": [
    "#### New Variables: Year, Country Code and Country Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "id": "162da07c-1112-43e8-a434-2bef0ed32de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min year: 1946\n",
      "Max year: 2023\n",
      "Missing codes: []\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "      <th>country_code</th>\n",
       "      <th>year</th>\n",
       "      <th>country_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NZL_66_2011.txt</td>\n",
       "      <td>As we say in Maori, \\nto all peoples and to al...</td>\n",
       "      <td>NZL</td>\n",
       "      <td>2011</td>\n",
       "      <td>New Zealand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>THA_22_1967.txt</td>\n",
       "      <td>31. The past few months have been for the Unit...</td>\n",
       "      <td>THA</td>\n",
       "      <td>1967</td>\n",
       "      <td>Thailand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GTM_26_1971.txt</td>\n",
       "      <td>Mr. President, I offer you my most sincere con...</td>\n",
       "      <td>GTM</td>\n",
       "      <td>1971</td>\n",
       "      <td>Guatemala</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LBY_55_2000.txt</td>\n",
       "      <td>It gives me great pleasure, Sir, to extend to\\...</td>\n",
       "      <td>LBY</td>\n",
       "      <td>2000</td>\n",
       "      <td>Libya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CMR_56_2001.txt</td>\n",
       "      <td>﻿I should like at the outset to express the\\np...</td>\n",
       "      <td>CMR</td>\n",
       "      <td>2001</td>\n",
       "      <td>Cameroon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech  \\\n",
       "0  NZL_66_2011.txt  As we say in Maori, \\nto all peoples and to al...   \n",
       "1  THA_22_1967.txt  31. The past few months have been for the Unit...   \n",
       "2  GTM_26_1971.txt  Mr. President, I offer you my most sincere con...   \n",
       "3  LBY_55_2000.txt  It gives me great pleasure, Sir, to extend to\\...   \n",
       "4  CMR_56_2001.txt  ﻿I should like at the outset to express the\\np...   \n",
       "\n",
       "  country_code  year country_name  \n",
       "0          NZL  2011  New Zealand  \n",
       "1          THA  1967     Thailand  \n",
       "2          GTM  1971    Guatemala  \n",
       "3          LBY  2000        Libya  \n",
       "4          CMR  2001     Cameroon  "
      ]
     },
     "execution_count": 656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# == Create variable: country code & year\n",
    "\n",
    "# Create contry_code and year variable\n",
    "df_raw['country_code'] = df_raw['filename'].str.extract(r'^([A-Z]{2,3})')\n",
    "df_raw['year'] = df_raw['filename'].str.extract(r'_(\\d{4})\\.txt$').astype(int)\n",
    "\n",
    "print(\"Min year:\", df_raw['year'].min())\n",
    "print(\"Max year:\", df_raw['year'].max())\n",
    "# Speeches range from 1946 to 2023\n",
    "\n",
    "# == Create variable: country_name by matching ISO country code \n",
    "code_to_name = {country.alpha_3: country.name for country in pycountry.countries}\n",
    "\n",
    "# Add custom short names and legacy codes\n",
    "custom_names = {\n",
    "    \"BOL\": \"Bolivia\",\n",
    "    \"COD\": \"The Democratic Republic of the Congo\",\n",
    "    \"IRN\": \"Iran\",\n",
    "    \"LAO\": \"Laos\",\n",
    "    \"MDA\": \"Moldova\",\n",
    "    \"PRK\": \"North Korea\",\n",
    "    \"PSE\": \"Palestine\",\n",
    "    \"RUS\": \"Russia\",\n",
    "    \"SYR\": \"Syria\",\n",
    "    \"TZA\": \"Tanzania\",\n",
    "    \"VAT\": \"Vatican City State\",\n",
    "    \"VEN\": \"Venezuela\",\n",
    "    \"VNM\": \"Vietnam\",\n",
    "    \"YMD\": \"South Yemen\",\n",
    "    \"YUG\": \"Yugoslavia\",\n",
    "    \"DDR\": \"East Germany\",\n",
    "    \"EU\": \"European Union\",\n",
    "    \"CSK\": \"Czechoslovakia\",\n",
    "    \"FSM\": \"Micronesia\",\n",
    "    \"KOR\": \"South Korea\"\n",
    "}\n",
    "\n",
    "code_to_name.update(custom_names)\n",
    "df_raw['country_name'] = df_raw['country_code'].map(code_to_name)\n",
    "\n",
    "# Check missing mappings\n",
    "missing = df_raw.loc[df_raw['country_name'].isna(), 'country_code'].unique()\n",
    "print(\"Missing codes:\", missing)\n",
    "\n",
    "# == Check structure ==\n",
    "\n",
    "df_raw.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "id": "383d2904-f015-4c9d-9aeb-51a94f03f950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    country_code                          country_name\n",
      "0            AFG                           Afghanistan\n",
      "1            AGO                                Angola\n",
      "2            ALB                               Albania\n",
      "3            AND                               Andorra\n",
      "4            ARE                  United Arab Emirates\n",
      "5            ARG                             Argentina\n",
      "6            ARM                               Armenia\n",
      "7            ATG                   Antigua and Barbuda\n",
      "8            AUS                             Australia\n",
      "9            AUT                               Austria\n",
      "10           AZE                            Azerbaijan\n",
      "11           BDI                               Burundi\n",
      "12           BEL                               Belgium\n",
      "13           BEN                                 Benin\n",
      "14           BFA                          Burkina Faso\n",
      "15           BGD                            Bangladesh\n",
      "16           BGR                              Bulgaria\n",
      "17           BHR                               Bahrain\n",
      "18           BHS                               Bahamas\n",
      "19           BIH                Bosnia and Herzegovina\n",
      "20           BLR                               Belarus\n",
      "21           BLZ                                Belize\n",
      "22           BOL                               Bolivia\n",
      "23           BRA                                Brazil\n",
      "24           BRB                              Barbados\n",
      "25           BRN                     Brunei Darussalam\n",
      "26           BTN                                Bhutan\n",
      "27           BWA                              Botswana\n",
      "28           CAF              Central African Republic\n",
      "29           CAN                                Canada\n",
      "30           CHE                           Switzerland\n",
      "31           CHL                                 Chile\n",
      "32           CHN                                 China\n",
      "33           CIV                         Côte d'Ivoire\n",
      "34           CMR                              Cameroon\n",
      "35           COD  The Democratic Republic of the Congo\n",
      "36           COG                                 Congo\n",
      "37           COL                              Colombia\n",
      "38           COM                               Comoros\n",
      "39           CPV                            Cabo Verde\n",
      "40           CRI                            Costa Rica\n",
      "41           CSK                        Czechoslovakia\n",
      "42           CUB                                  Cuba\n",
      "43           CYP                                Cyprus\n",
      "44           CZE                               Czechia\n",
      "45           DDR                          East Germany\n",
      "46           DEU                               Germany\n",
      "47           DJI                              Djibouti\n",
      "48           DMA                              Dominica\n",
      "49           DNK                               Denmark\n",
      "50           DOM                    Dominican Republic\n",
      "51           DZA                               Algeria\n",
      "52           ECU                               Ecuador\n",
      "53           EGY                                 Egypt\n",
      "54           ERI                               Eritrea\n",
      "55           ESP                                 Spain\n",
      "56           EST                               Estonia\n",
      "57           ETH                              Ethiopia\n",
      "58            EU                        European Union\n",
      "59           FIN                               Finland\n",
      "60           FJI                                  Fiji\n",
      "61           FRA                                France\n",
      "62           GAB                                 Gabon\n",
      "63           GBR                        United Kingdom\n",
      "64           GEO                               Georgia\n",
      "65           GHA                                 Ghana\n",
      "66           GIN                                Guinea\n",
      "67           GMB                                Gambia\n",
      "68           GNB                         Guinea-Bissau\n",
      "69           GNQ                     Equatorial Guinea\n",
      "70           GRC                                Greece\n",
      "71           GRD                               Grenada\n",
      "72           GTM                             Guatemala\n",
      "73           GUY                                Guyana\n",
      "74           HND                              Honduras\n",
      "75           HRV                               Croatia\n",
      "76           HTI                                 Haiti\n",
      "77           HUN                               Hungary\n",
      "78           IDN                             Indonesia\n",
      "79           IND                                 India\n",
      "80           IRL                               Ireland\n",
      "81           IRN                                  Iran\n",
      "82           IRQ                                  Iraq\n",
      "83           ISL                               Iceland\n",
      "84           ISR                                Israel\n",
      "85           ITA                                 Italy\n",
      "86           JAM                               Jamaica\n",
      "87           JOR                                Jordan\n",
      "88           JPN                                 Japan\n",
      "89           KAZ                            Kazakhstan\n",
      "90           KEN                                 Kenya\n",
      "91           KGZ                            Kyrgyzstan\n",
      "92           KHM                              Cambodia\n",
      "93           KIR                              Kiribati\n",
      "94           KNA                 Saint Kitts and Nevis\n",
      "95           KOR                           South Korea\n",
      "96           KWT                                Kuwait\n",
      "97           LAO                                  Laos\n",
      "98           LBN                               Lebanon\n",
      "99           LBR                               Liberia\n",
      "100          LBY                                 Libya\n",
      "101          LCA                           Saint Lucia\n",
      "102          LKA                             Sri Lanka\n",
      "103          LSO                               Lesotho\n",
      "104          LTU                             Lithuania\n",
      "105          LUX                            Luxembourg\n",
      "106          LVA                                Latvia\n",
      "107          MAR                               Morocco\n",
      "108          MCO                                Monaco\n",
      "109          MDA                               Moldova\n",
      "110          MDG                            Madagascar\n",
      "111          MDV                              Maldives\n",
      "112          MEX                                Mexico\n",
      "113          MHL                      Marshall Islands\n",
      "114          MKD                       North Macedonia\n",
      "115          MLI                                  Mali\n",
      "116          MLT                                 Malta\n",
      "117          MMR                               Myanmar\n",
      "118          MNE                            Montenegro\n",
      "119          MNG                              Mongolia\n",
      "120          MOZ                            Mozambique\n",
      "121          MRT                            Mauritania\n",
      "122          MUS                             Mauritius\n",
      "123          MWI                                Malawi\n",
      "124          MYS                              Malaysia\n",
      "125          NAM                               Namibia\n",
      "126          NER                                 Niger\n",
      "127          NGA                               Nigeria\n",
      "128          NIC                             Nicaragua\n",
      "129          NLD                           Netherlands\n",
      "130          NOR                                Norway\n",
      "131          NPL                                 Nepal\n",
      "132          NRU                                 Nauru\n",
      "133          NZL                           New Zealand\n",
      "134          OMN                                  Oman\n",
      "135          PAK                              Pakistan\n",
      "136          PAN                                Panama\n",
      "137          PER                                  Peru\n",
      "138          PHL                           Philippines\n",
      "139          PNG                      Papua New Guinea\n",
      "140          POL                                Poland\n",
      "141          PRK                           North Korea\n",
      "142          PRT                              Portugal\n",
      "143          PRY                              Paraguay\n",
      "144          PSE                             Palestine\n",
      "145          QAT                                 Qatar\n",
      "146          ROU                               Romania\n",
      "147          RUS                                Russia\n",
      "148          RWA                                Rwanda\n",
      "149          SAU                          Saudi Arabia\n",
      "150          SDN                                 Sudan\n",
      "151          SEN                               Senegal\n",
      "152          SGP                             Singapore\n",
      "153          SLB                       Solomon Islands\n",
      "154          SLE                          Sierra Leone\n",
      "155          SLV                           El Salvador\n",
      "156          SRB                                Serbia\n",
      "157          SSD                           South Sudan\n",
      "158          STP                 Sao Tome and Principe\n",
      "159          SUR                              Suriname\n",
      "160          SVK                              Slovakia\n",
      "161          SVN                              Slovenia\n",
      "162          SWE                                Sweden\n",
      "163          SWZ                              Eswatini\n",
      "164          SYC                            Seychelles\n",
      "165          SYR                                 Syria\n",
      "166          TCD                                  Chad\n",
      "167          TGO                                  Togo\n",
      "168          THA                              Thailand\n",
      "169          TJK                            Tajikistan\n",
      "170          TKM                          Turkmenistan\n",
      "171          TON                                 Tonga\n",
      "172          TTO                   Trinidad and Tobago\n",
      "173          TUN                               Tunisia\n",
      "174          TUR                               Türkiye\n",
      "175          TZA                              Tanzania\n",
      "176          UGA                                Uganda\n",
      "177          UKR                               Ukraine\n",
      "178          URY                               Uruguay\n",
      "179          USA                         United States\n",
      "180          UZB                            Uzbekistan\n",
      "181          VAT                    Vatican City State\n",
      "182          VEN                             Venezuela\n",
      "183          VNM                               Vietnam\n",
      "184          VUT                               Vanuatu\n",
      "185          WSM                                 Samoa\n",
      "186          YEM                                 Yemen\n",
      "187          YUG                            Yugoslavia\n",
      "188          ZAF                          South Africa\n",
      "189          ZMB                                Zambia\n",
      "190          ZWE                              Zimbabwe\n"
     ]
    }
   ],
   "source": [
    "# == Check country names \n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(df_raw[['country_code', 'country_name']].drop_duplicates().sort_values('country_code').reset_index(drop=True))\n",
    "# Reset to default afterward\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5928538b-8519-40e2-bdee-ac72024f9f48",
   "metadata": {},
   "source": [
    "#### New Variable: Length of speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "id": "911e2229-337f-4479-984c-3a6ab9dff5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average speech length (words): 2954.49\n",
      "20 shortest speeches:\n",
      "            filename                          country_name  year  \\\n",
      "721  GNB_76_2021.txt                         Guinea-Bissau  2021   \n",
      "722  RWA_70_2015.txt                                Rwanda  2015   \n",
      "611  JOR_65_2010.txt                                Jordan  2010   \n",
      "118  JOR_61_2006.txt                                Jordan  2006   \n",
      "343  IDN_76_2021.txt                             Indonesia  2021   \n",
      "476  LTU_71_2016.txt                             Lithuania  2016   \n",
      "430  BGD_42_1987.txt                            Bangladesh  1987   \n",
      "505  IRN_03_1948.txt                                  Iran  1948   \n",
      "542  LBR_04_1949.txt                               Liberia  1949   \n",
      "246  LUX_01_1946.txt                            Luxembourg  1946   \n",
      "270  KHM_59_2004.txt                              Cambodia  2004   \n",
      "643  QAT_61_2006.txt                                 Qatar  2006   \n",
      "522  HUN_34_1979.txt                               Hungary  1979   \n",
      "439  UGA_71_2016.txt                                Uganda  2016   \n",
      "119  SVN_68_2013.txt                              Slovenia  2013   \n",
      "437  JOR_66_2011.txt                                Jordan  2011   \n",
      "210  COD_59_2004.txt  The Democratic Republic of the Congo  2004   \n",
      "115  MDA_78_2023.txt                               Moldova  2023   \n",
      "417  ZWE_71_2016.txt                              Zimbabwe  2016   \n",
      "752  BFA_64_2009.txt                          Burkina Faso  2009   \n",
      "\n",
      "     speech_length_words  \n",
      "721                  480  \n",
      "722                  539  \n",
      "611                  563  \n",
      "118                  571  \n",
      "343                  611  \n",
      "476                  739  \n",
      "430                  783  \n",
      "505                  813  \n",
      "542                  885  \n",
      "246                  888  \n",
      "270                  927  \n",
      "643                  948  \n",
      "522                  949  \n",
      "439                  979  \n",
      "119                 1016  \n",
      "437                 1021  \n",
      "210                 1022  \n",
      "115                 1042  \n",
      "417                 1055  \n",
      "752                 1085  \n",
      "\n",
      "20 longest speeches:\n",
      "            filename  country_name  year  speech_length_words\n",
      "302  IND_16_1961.txt         India  1961                13951\n",
      "259  GIN_15_1960.txt        Guinea  1960                13916\n",
      "787  SAU_15_1960.txt  Saudi Arabia  1960                10779\n",
      "536  NGA_18_1963.txt       Nigeria  1963                 9804\n",
      "641  ZAF_17_1962.txt  South Africa  1962                 9354\n",
      "5    BRA_18_1963.txt        Brazil  1963                 8388\n",
      "233  GHA_15_1960.txt         Ghana  1960                 7995\n",
      "741  FRA_07_1952.txt        France  1952                 7921\n",
      "548  LBN_12_1957.txt       Lebanon  1957                 7699\n",
      "296  GMB_33_1978.txt        Gambia  1978                 7605\n",
      "681  DEU_38_1983.txt       Germany  1983                 7566\n",
      "366  ARG_37_1982.txt     Argentina  1982                 7250\n",
      "779  POL_09_1954.txt        Poland  1954                 7219\n",
      "228  FRA_75_2020.txt        France  2020                 7082\n",
      "188  ALB_18_1963.txt       Albania  1963                 7060\n",
      "745  PHL_23_1968.txt   Philippines  1968                 7055\n",
      "451  ALB_24_1969.txt       Albania  1969                 6753\n",
      "271  BFA_39_1984.txt  Burkina Faso  1984                 6546\n",
      "359  VEN_29_1974.txt     Venezuela  1974                 6405\n",
      "782  GIN_26_1971.txt        Guinea  1971                 6360\n"
     ]
    }
   ],
   "source": [
    "# Add a new column: speech length in words\n",
    "df_raw['speech_length_words'] = df_raw['speech'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Calculate average length\n",
    "avg_length = df_raw['speech_length_words'].mean()\n",
    "print(\"Average speech length (words):\", round(avg_length, 2))\n",
    "\n",
    "# 20 shortest & longest speeches\n",
    "print(\"20 shortest speeches:\")\n",
    "print(df_raw.nsmallest(20, 'speech_length_words')[['filename', 'country_name', 'year', 'speech_length_words']])\n",
    "\n",
    "print(\"\\n20 longest speeches:\")\n",
    "print(df_raw.nlargest(20, 'speech_length_words')[['filename', 'country_name', 'year', 'speech_length_words']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424f152-b786-45bf-bf86-e169a900f628",
   "metadata": {},
   "source": [
    "#### New variable: English as Official Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "id": "d1d2c2a6-23d6-4044-84cc-8785167e6b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries not matched in df_raw['country_name']:\n",
      "Anguilla\n",
      "Bermuda\n",
      "British Virgin Islands\n",
      "Cayman Islands\n",
      "Christmas Island\n",
      "Cook Islands\n",
      "Hong Kong\n",
      "Jersey\n",
      "Micronesia\n",
      "Niue\n",
      "Norfolk Island\n",
      "Northern Mariana Islands\n",
      "Palau\n",
      "Pitcairn Islands\n",
      "Sint Maarten\n",
      "Somalia\n",
      "Turks and Caicos Islands\n",
      "Tuvalu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "      <th>country_code</th>\n",
       "      <th>year</th>\n",
       "      <th>country_name</th>\n",
       "      <th>speech_length_words</th>\n",
       "      <th>english_official_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NZL_66_2011.txt</td>\n",
       "      <td>As we say in Maori, \\nto all peoples and to al...</td>\n",
       "      <td>NZL</td>\n",
       "      <td>2011</td>\n",
       "      <td>New Zealand</td>\n",
       "      <td>2432</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>THA_22_1967.txt</td>\n",
       "      <td>31. The past few months have been for the Unit...</td>\n",
       "      <td>THA</td>\n",
       "      <td>1967</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>3989</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GTM_26_1971.txt</td>\n",
       "      <td>Mr. President, I offer you my most sincere con...</td>\n",
       "      <td>GTM</td>\n",
       "      <td>1971</td>\n",
       "      <td>Guatemala</td>\n",
       "      <td>2464</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LBY_55_2000.txt</td>\n",
       "      <td>It gives me great pleasure, Sir, to extend to\\...</td>\n",
       "      <td>LBY</td>\n",
       "      <td>2000</td>\n",
       "      <td>Libya</td>\n",
       "      <td>3525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CMR_56_2001.txt</td>\n",
       "      <td>﻿I should like at the outset to express the\\np...</td>\n",
       "      <td>CMR</td>\n",
       "      <td>2001</td>\n",
       "      <td>Cameroon</td>\n",
       "      <td>1784</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech  \\\n",
       "0  NZL_66_2011.txt  As we say in Maori, \\nto all peoples and to al...   \n",
       "1  THA_22_1967.txt  31. The past few months have been for the Unit...   \n",
       "2  GTM_26_1971.txt  Mr. President, I offer you my most sincere con...   \n",
       "3  LBY_55_2000.txt  It gives me great pleasure, Sir, to extend to\\...   \n",
       "4  CMR_56_2001.txt  ﻿I should like at the outset to express the\\np...   \n",
       "\n",
       "  country_code  year country_name  speech_length_words  \\\n",
       "0          NZL  2011  New Zealand                 2432   \n",
       "1          THA  1967     Thailand                 3989   \n",
       "2          GTM  1971    Guatemala                 2464   \n",
       "3          LBY  2000        Libya                 3525   \n",
       "4          CMR  2001     Cameroon                 1784   \n",
       "\n",
       "   english_official_language  \n",
       "0                          1  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          1  "
      ]
     },
     "execution_count": 661,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source for english as official language : https://gradschool.utk.edu/future-students/office-of-graduate-admissions/applying-to-graduate-school/admission-requirements/testing-requirements/countries-with-english-as-official-language/\n",
    "# They are quoting: https://www.cia.gov/the-world-factbook/field/languages/\n",
    "\n",
    "english_countries = [\n",
    "    \"Anguilla\", \"Antigua and Barbuda\", \"Bahamas\", \"Barbados\", \"Belize\", \"Belgium\",\n",
    "    \"Bermuda\", \"Botswana\", \"British Virgin Islands\", \"Burundi\", \"Cameroon\", \"Canada\",\n",
    "    \"Cayman Islands\", \"Christmas Island\", \"Cook Islands\", \"Dominica\", \"Fiji\", \"Gambia\",\n",
    "    \"Ghana\", \"Grenada\", \"Guyana\", \"Hong Kong\", \"India\", \"Ireland\", \"Jersey\", \"Kenya\",\n",
    "    \"Liberia\", \"Malawi\", \"Malta\", \"Marshall Islands\", \"Micronesia\",\n",
    "    \"Namibia\", \"New Zealand\", \"Nigeria\", \"Niue\", \"Norfolk Island\", \"Northern Mariana Islands\",\n",
    "    \"Pakistan\", \"Palau\", \"Papua New Guinea\", \"Philippines\", \"Pitcairn Islands\", \"Rwanda\",\n",
    "    \"Saint Kitts and Nevis\", \"Saint Lucia\", \"Samoa\", \"Seychelles\", \"Sierra Leone\", \"Singapore\",\n",
    "    \"Sint Maarten\", \"Solomon Islands\", \"Somalia\", \"South Africa\", \"South Sudan\", \"Sudan\",\n",
    "    \"Eswatini\", \"Tanzania\", \"Tonga\", \"Trinidad and Tobago\", \"Turks and Caicos Islands\",\n",
    "    \"Tuvalu\", \"Uganda\", \"Zambia\", \"Zimbabwe\"\n",
    "]\n",
    "\n",
    "# Create dummy column\n",
    "df_raw['english_official_language'] = df_raw['country_name'].apply(\n",
    "    lambda x: 1 if x in english_countries else 0\n",
    ")\n",
    "\n",
    "# Detect unmatched countries \n",
    "matched = set(df_raw['country_name'])\n",
    "unmatched = [country for country in english_countries if country not in matched]\n",
    "\n",
    "print(\"Countries not matched in df_raw['country_name']:\")\n",
    "for country in unmatched:\n",
    "    print(country)\n",
    "\n",
    "# All of these countries are either British Overseas Territories, Australian Territories, self-governing island territories or Special Administrative Regions\n",
    "    # None of the unmatched regions are UN Members\n",
    "\n",
    "# Check df with new variable english_official_language\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82540d8a-b656-45fb-aeb0-2fd5b4d7108a",
   "metadata": {},
   "source": [
    "#### New variable: Permanent member security council"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "id": "4fb92fe4-5c22-43fa-8b48-62ec2240d716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    country_code    country_name  security_council_permanent  year\n",
      "10           FRA          France                           1  1948\n",
      "18           USA   United States                           1  2012\n",
      "49           USA   United States                           1  1992\n",
      "60           FRA          France                           1  2012\n",
      "66           USA   United States                           1  1962\n",
      "74           USA   United States                           1  1972\n",
      "80           FRA          France                           1  2001\n",
      "174          USA   United States                           1  1957\n",
      "228          FRA          France                           1  2020\n",
      "251          FRA          France                           1  2019\n",
      "268          CHN           China                           1  1993\n",
      "278          GBR  United Kingdom                           1  1952\n",
      "347          FRA          France                           1  2007\n",
      "404          FRA          France                           1  1959\n",
      "435          GBR  United Kingdom                           1  1996\n",
      "453          USA   United States                           1  2009\n",
      "484          RUS          Russia                           1  2023\n",
      "506          GBR  United Kingdom                           1  1981\n",
      "557          GBR  United Kingdom                           1  1990\n",
      "596          CHN           China                           1  1985\n",
      "638          FRA          France                           1  2002\n",
      "678          FRA          France                           1  1976\n",
      "709          CHN           China                           1  1960\n",
      "720          USA   United States                           1  1949\n",
      "728          RUS          Russia                           1  2008\n",
      "741          FRA          France                           1  1952\n"
     ]
    }
   ],
   "source": [
    "# Define permanent members of the UN Security Council and create dummy\n",
    "permanent_members = ['RUS', 'USA', 'FRA', 'GBR', 'CHN']\n",
    "\n",
    "df_raw['security_council_permanent'] = df_raw['country_code'].isin(permanent_members).astype(int)\n",
    "\n",
    "print(df_raw[df_raw['country_code'].isin(permanent_members)][\n",
    "    ['country_code', 'country_name', 'security_council_permanent', 'year']\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523547d9-9b10-473c-8c85-d1e2fc4fd3e1",
   "metadata": {},
   "source": [
    "#### New variables: Speaker, Position & Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "de783a81-efbf-49a6-96f6-ae6a93fa1573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            filename  year country_code  country_name\n",
      "2    GTM_26_1971.txt  1971          GTM     Guatemala\n",
      "89   SLE_18_1963.txt  1963          SLE  Sierra Leone\n",
      "197  KEN_49_1994.txt  1994          KEN         Kenya\n",
      "213  CUB_49_1994.txt  1994          CUB          Cuba\n",
      "325  KWT_49_1994.txt  1994          KWT        Kuwait\n",
      "492  PRY_16_1961.txt  1961          PRY      Paraguay\n",
      "518  MOZ_49_1994.txt  1994          MOZ    Mozambique\n",
      "7 rows could not be matched\n",
      "    gender_dummy  count\n",
      "0       0 (male)    336\n",
      "1     1 (female)     14\n",
      "2  NaN (unknown)    451\n"
     ]
    }
   ],
   "source": [
    "# Supplmentary xlsx-file from the UN Dataset provides information on the speaker and their position\n",
    "\n",
    "# == Create variable speaker_name and position ==\n",
    "df_speakers = pd.read_excel(os.path.join(data_c, \"data_original\", \"UN General Debate Corpus\", \"Speakers_by_session.xlsx\"))\n",
    "\n",
    "df_speakers.head()\n",
    "\n",
    "# Merge new infrormation to dataframe\n",
    "df_merged = df_raw.merge(\n",
    "    df_speakers[['Year', 'ISO Code', 'Name of Person Speaking', 'Post']],\n",
    "    left_on=['year', 'country_code'],\n",
    "    right_on=['Year', 'ISO Code'],\n",
    "    how='left',\n",
    "    indicator=True)\n",
    "\n",
    "# Detect unmatched rows\n",
    "unmatched = df_merged[df_merged['_merge'] == 'left_only']\n",
    "unmatched_count = (df_merged['_merge'] == 'left_only').sum()\n",
    "\n",
    "print(unmatched[['filename', 'year', 'country_code', 'country_name']])\n",
    "print(f\"{unmatched_count} rows could not be matched\")\n",
    "\n",
    "# Clean up \n",
    "df_merged = df_merged.drop(columns=['Year', 'ISO Code', '_merge']).rename(columns={\n",
    "    'Name of Person Speaking': 'speaker_name',\n",
    "    'Post': 'position'\n",
    "})\n",
    "\n",
    "# == Create gender dummy ==\n",
    "df_merged['gender_dummy'] = df_merged['speaker_name'].apply(\n",
    "    lambda name: 0 if pd.notnull(name) and re.search(r'^(?:Mr|Sir)\\b', name, re.IGNORECASE)\n",
    "    else 1 if pd.notnull(name) and re.search(r'^(?:Mrs|Ms)\\b', name, re.IGNORECASE)\n",
    "    else None\n",
    ")\n",
    "\n",
    "# Count all values including NaN\n",
    "counts = df_merged['gender_dummy'].value_counts(dropna=False)\n",
    "\n",
    "# Build summary using .get() to handle missing keys\n",
    "gender_summary = pd.DataFrame({\n",
    "    'gender_dummy': ['0 (male)', '1 (female)', 'NaN (unknown)'],\n",
    "    'count': [\n",
    "        counts.get(0, 0),\n",
    "        counts.get(1, 0),\n",
    "        counts.get(np.nan, 0)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(gender_summary)\n",
    "\n",
    "# == Adjust position variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3cfdc9-d972-454b-9719-3285b1e5d3e2",
   "metadata": {},
   "source": [
    "Looking at the structure, highest position always seems to be mentioned first --> drop everything else if speaker has more than one position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "id": "969f9568-9b6c-454f-ab3f-c9fcdf50323a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Presidents ===\n",
      "Series([], Name: position, dtype: int64)\n",
      "\n",
      "=== Vice-Presidents ===\n",
      "(Vice-) President    1990\n",
      "Name: position, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# All President (but not Vice)\n",
    "president_positions = position_counts[\n",
    "    position_counts.index.str.contains(r'\\bpresident\\b', case=False, na=False)\n",
    "    & ~position_counts.index.str.contains(\"vice\", case=False, na=False)\n",
    "]\n",
    "\n",
    "# All Vice-President / Vice President\n",
    "vice_president_positions = position_counts[\n",
    "    position_counts.index.str.contains(\"vice\", case=False, na=False)\n",
    "    & position_counts.index.str.contains(\"president\", case=False, na=False)\n",
    "]\n",
    "\n",
    "print(\"=== Presidents ===\")\n",
    "print(president_positions)\n",
    "\n",
    "print(\"\\n=== Vice-Presidents ===\")\n",
    "print(vice_president_positions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "id": "6a1ce3d9-a03d-46f6-90aa-923660bc4474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmatched position: PREMIER OF THE ADMINISTRATIVE COUNCIL\n",
      "Unmatched position: Chairman of the Presidency\n",
      "Unmatched position: First Secretary of the Central Committee of the Polish United Workers' Party\n",
      "Unmatched position: Head of the Church\n",
      "Unmatched position: Union Minister for the Office of the State Counsellor\n"
     ]
    }
   ],
   "source": [
    "def normalize_position(pos):\n",
    "    if pd.isna(pos):\n",
    "        return pos\n",
    "\n",
    "    pos = pos.strip()\n",
    "\n",
    "    # --- Fix common typos and extra spaces ---\n",
    "    pos = re.sub(r'\\s+', ' ', pos)  # collapse multiple spaces\n",
    "    pos_lower = pos.lower()\n",
    "\n",
    "     # Turn all ministers that deal with foreign affairs and international relations to \"Minister for Foreign Affairs\n",
    "    foreign_affairs_variants = [\n",
    "        'minister for foregn affairs',\n",
    "        'minister responsible for foreign affairs',\n",
    "        'minsiter for foreign and caricom affairs',\n",
    "        'minister for external affairs',\n",
    "        'minister of external relations',  # <-- added\n",
    "        'foreign minister',\n",
    "        'minister for international affairs and cooperation',\n",
    "        'minister for external relations',\n",
    "        'federal minister for european and international affairs',\n",
    "        'international cooperation',\n",
    "        'federal minister for foreign affairs',\n",
    "        'minister for foreign and caricom affairs',\n",
    "        'minister of foreign affairs and cooperation',\n",
    "        'minister for international relations and cooperation',\n",
    "        'ministry of external relations',\n",
    "        'acting minister for foreign affairs and international cooperation',\n",
    "        'ministry of foreign affairs',\n",
    "        'minister for foreign and political affairs',\n",
    "        'federal minister for europe, integration, and foreign affairs',\n",
    "        'federal minister for europe, integration and foreign affairs',\n",
    "        'minister of foreign and european affaris',\n",
    "        'minister of foreign affairs',\n",
    "        'minister for foreign',\n",
    "        'minister of foreign and european affairs and minister of immigration and asylum',\n",
    "        'minister for foreign affairs and senegalese living abroad',\n",
    "        'minister for foreign affairs with responsibility for brexit',\n",
    "        'minister for foreign affairs and investment promotion'\n",
    "       \n",
    "    ]\n",
    "    if any(variant in pos_lower for variant in foreign_affairs_variants):\n",
    "        return \"Minister for Foreign Affairs\"\n",
    "\n",
    "    # --- Fix \"rime minister\" typo ---\n",
    "    pos = re.sub(r'(?i)\\brime[- ]?minister\\b', 'Prime Minister', pos)\n",
    "\n",
    "    # Normalize different versions of Head of Government, President, Prime Minsiter and Vice-President-\n",
    "    exact_matches = {\n",
    "        r'(?i)^president of (the )?government$': 'Head of Government',\n",
    "        r'(?i)^acting president$': 'President',\n",
    "        r'(?i)^interim president$': 'President',\n",
    "        r'(?i)^constitutional president$': 'President',\n",
    "        r'(?i)^first executive president$': 'President',\n",
    "        r'(?i)^first prime[- ]?minister$': 'Prime Minister',\n",
    "        r'(?i)^head of the goverment$': 'Head of Government',  # <-- catch typo + spaces\n",
    "        r'(?i)^head\\s+of\\s+govern?ment$': 'Head of Government',\n",
    "        r'(?i)^first vice[- ]?president$': 'Vice-President'\n",
    "    }\n",
    "    for pattern, replacement in exact_matches.items():\n",
    "        if re.fullmatch(pattern, pos):\n",
    "            return replacement\n",
    "\n",
    "    # --- Normalize prefixes ---\n",
    "    pos = re.sub(r'(?i)^first vice[- ]?president\\b', 'Vice-President', pos)\n",
    "    pos = re.sub(r'(?i)\\bprime[- ]?minister\\b', 'Prime Minister', pos)\n",
    "    pos = re.sub(r'(?i)\\bpresident\\b', 'President', pos)\n",
    "    pos = re.sub(r'(?i)\\bvice[- ]?president\\b', 'Vice-President', pos)\n",
    "\n",
    "    # --- Collapse primary roles if they appear at start ---\n",
    "    primary_roles = [\n",
    "        (r'(?i)^prime[- ]?minister\\b', 'Prime Minister'),\n",
    "        (r'(?i)^deputy prime[- ]?minister\\b', 'Deputy Prime Minister'),\n",
    "        (r'(?i)^president\\b', 'President'),\n",
    "        (r'(?i)^vice[- ]?president\\b', 'Vice-President'),\n",
    "        (r'(?i)^head of state\\b', 'Head of State'),\n",
    "        (r'(?i)^(crown prince|prince|king|emir|amir)\\b', 'Monarch'),\n",
    "        (r'(?i)^(un representative|permanent representative|delegation|chair of (the )?delegation|chair of diplomatic representative)\\b', 'Diplomatic Representative')\n",
    "    ]\n",
    "    for pattern, replacement in primary_roles:\n",
    "        if re.match(pattern, pos):\n",
    "            return replacement\n",
    "\n",
    "    # --- Monarchs ---\n",
    "    if re.search(r'(?i)\\b(crown prince|prince|king|emir|amir)\\b', pos):\n",
    "        return \"Monarch\"\n",
    "\n",
    "    # --- Head of State ---\n",
    "    if re.search(r'(?i)head of state', pos):\n",
    "        return \"Head of State\"\n",
    "        \n",
    "    # --- Diplomatic Representatives ---\n",
    "    if re.search(r'(?i)(un representative|permanent representative|delegation|chair of (the )?delegation|chair of diplomatic representative)', pos):\n",
    "        return \"Diplomatic Representative\"\n",
    "\n",
    "    # --- Everything else ---\n",
    "    print(\"Unmatched position:\", pos)  # print before assigning Others\n",
    "    return \"Others\"\n",
    "\n",
    "# Apply\n",
    "df_merged[\"position\"] = df_merged[\"position\"].apply(normalize_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "7a99b3df-550a-41a3-a751-2692c0f7ea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_positions(pos):\n",
    "    if pd.isna(pos):\n",
    "        return pos  # keep NaN\n",
    "    \n",
    "    if pos in [\"Prime Minister\", \"Deputy Prime Minister\"]:\n",
    "        return \"(Deputy) Prime Minister\"\n",
    "    \n",
    "    if pos in [\"President\", \"Vice-President\"]:\n",
    "        return \"(Vice-) President\"\n",
    "        \n",
    "    if pos in [\"Minister for Foreign Affairs\", \"Deputy Minister for Foreign Affairs\",\n",
    "        \"Deputy Minister Foreign Affairs\",\n",
    "        \"Second Minister for Foreign Affairs\",\n",
    "        \"Second Minister for Foreign Affairs and Trade\",\n",
    "        \"Vice Minister for Foreign Affairs\"]:\n",
    "        return \"(Deputy) Minister for Foreign Affairs\"\n",
    "    \n",
    "    return pos\n",
    "\n",
    "df_merged[\"position\"] = df_merged[\"position\"].apply(merge_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "id": "14ececb8-262d-4270-8d52-5742a864cf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN                                      340\n",
      "(Deputy) Minister for Foreign Affairs    167\n",
      "(Vice-) President                        152\n",
      "(Deputy) Prime Minister                   97\n",
      "Diplomatic Representative                 22\n",
      "Head of State                              8\n",
      "Monarch                                    6\n",
      "Others                                     5\n",
      "Head of Government                         4\n",
      "Name: position, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Pandas so einstellen, dass es alles ausgibt\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "# Alle Positionen mit Häufigkeit\n",
    "position_counts = df_merged['position'].value_counts(dropna=False)\n",
    "\n",
    "print(position_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "id": "df4b9986-4599-46cc-8d2b-c49667f86e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      total_rows  missing  not_missing\n",
      "year                                  \n",
      "1946           1        1            0\n",
      "1947           1        1            0\n",
      "1948           4        4            0\n",
      "1949           3        3            0\n",
      "1950           3        3            0\n",
      "1951           1        1            0\n",
      "1952           6        6            0\n",
      "1953           2        2            0\n",
      "1954           3        3            0\n",
      "1955           3        3            0\n",
      "1956           1        1            0\n",
      "1957           7        7            0\n",
      "1958           4        4            0\n",
      "1959           3        3            0\n",
      "1960           9        6            3\n",
      "1961           6        6            0\n",
      "1962          10       10            0\n",
      "1963           8        8            0\n",
      "1964           3        3            0\n",
      "1965           7        7            0\n",
      "1966           5        5            0\n",
      "1967           8        8            0\n",
      "1968           8        8            0\n",
      "1969          15       15            0\n",
      "1970           3        3            0\n",
      "1971           8        8            0\n",
      "1972           7        7            0\n",
      "1973          11       11            0\n",
      "1974           7        6            1\n",
      "1975          11       11            0\n",
      "1976          13       12            1\n",
      "1977          12       11            1\n",
      "1978          16       14            2\n",
      "1979           9        8            1\n",
      "1980           7        7            0\n",
      "1981          15       14            1\n",
      "1982          12       11            1\n",
      "1983          16       12            4\n",
      "1984           9        8            1\n",
      "1985          12       10            2\n",
      "1986          16       13            3\n",
      "1987           9        7            2\n",
      "1988          13       11            2\n",
      "1989           8        6            2\n",
      "1990           7        4            3\n",
      "1991          12        8            4\n",
      "1992           8        7            1\n",
      "1993          13        9            4\n",
      "1994          18        4           14\n",
      "1995           9        0            9\n",
      "1996          20        0           20\n",
      "1997          15        0           15\n",
      "1998           9        0            9\n",
      "1999           9        0            9\n",
      "2000          18        0           18\n",
      "2001          16        0           16\n",
      "2002          10        0           10\n",
      "2003          16        0           16\n",
      "2004          14        0           14\n",
      "2005          15        0           15\n",
      "2006          14        0           14\n",
      "2007          13        0           13\n",
      "2008          17        0           17\n",
      "2009          14        0           14\n",
      "2010          14        0           14\n",
      "2011          20        0           20\n",
      "2012          16        0           16\n",
      "2013          10        0           10\n",
      "2014           8        0            8\n",
      "2015          16        0           16\n",
      "2016          17        0           17\n",
      "2017          14        0           14\n",
      "2018          10        0           10\n",
      "2019          24        0           24\n",
      "2020          12        0           12\n",
      "2021          15        0           15\n",
      "2022           9        0            9\n",
      "2023          14        0           14\n"
     ]
    }
   ],
   "source": [
    "# Started to document positions properly from 1986 on, before yearly sample size per year mostly less than 20 samples\n",
    "# \n",
    "yearly_counts = df_merged.groupby('year')['position'].agg(\n",
    "    total_rows='size',\n",
    "    missing=lambda x: x.isna().sum()\n",
    ")\n",
    "\n",
    "# Add not_missing column\n",
    "yearly_counts['not_missing'] = yearly_counts['total_rows'] - yearly_counts['missing']\n",
    "\n",
    "\n",
    "# Print the entire table\n",
    "pd.set_option('display.max_rows', None)  # show all rows\n",
    "print(yearly_counts)\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41d97e2-840b-40eb-b032-197040a0246c",
   "metadata": {},
   "source": [
    "#### New Variable: Country (Year)\n",
    "\n",
    "This variable is later needed to create clean description plots and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "d647e412-7bd4-4190-9c34-39874b4620a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_merged.copy()\n",
    "df_merged['speech_label'] = df_merged['country_name'] + \" (\" + df_merged['year'].astype(str) + \")\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc05d65d-be72-42ae-b753-0940de5df17c",
   "metadata": {},
   "source": [
    "#### Save dataframe with all new variables as ?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "id": "94a901c8-3fc4-4b33-a1ff-cede22bd196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(wd)\n",
    "\n",
    "# Save df_merged as a pickle file for quick future loading\n",
    "merged_pickle_path = r\".\\data\\un_corpus_merged.pkl\"\n",
    "df_merged.to_pickle(merged_pickle_path)\n",
    "\n",
    "# Export df as CSV \n",
    "merged_output_path = r\".\\data\\un_corpus_merged.csv\"\n",
    "df_merged.to_csv(merged_output_path, index=False, sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea36f19a-789d-4c62-a470-3e73a6d848bf",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe0f4af-e939-4100-9618-5b522bdd72a1",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2661f431-a194-47d1-9a1c-89162bf85a91",
   "metadata": {},
   "source": [
    "#### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "id": "cd37e08d-b4f2-459e-823c-3816a825ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Clean text by removing empty spaces, line breaks, hyphenation, stray characters, and escape quote ==\n",
    "\n",
    "# Define cleaning function\n",
    "def clean_text(content):\n",
    "    if pd.isna(content):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove line breaks and carriage returns\n",
    "    content = content.replace('\\n', ' ').replace('\\r', ' ')\n",
    "\n",
    "    # Collapse multiple spaces\n",
    "    content = ' '.join(content.split())\n",
    "\n",
    "    # Ensure spacing after punctuation\n",
    "    content = re.sub(r'(?<=[.,])(?=[^\\s])', r' ', content)\n",
    "\n",
    "    # Remove hyphenation at line breaks (e.g. \"inter- national\" → \"international\")\n",
    "    content = re.sub(r'-\\s', '', content)\n",
    "\n",
    "     # Replace hyphen between letters with a space to prevent merging words (e.g. \"russian-and\" → \"russian and\")\n",
    "    content = re.sub(r'(?<=\\w)-(?=\\w)', ' ', content)\n",
    "\n",
    "    # Remove stray backslashes\n",
    "    content = content.replace(\"\\\\\", \"\")\n",
    "\n",
    "    return content\n",
    "\n",
    "# Apply cleaning to each speech\n",
    "df_merged['speech'] = df_merged['speech'].astype(str)  # Ensure column is string type\n",
    "df_clean = df_merged.copy()\n",
    "df_clean['speech'] = df_clean['speech'].apply(clean_text)\n",
    "\n",
    "# Drop rows with empty speeches after cleaning\n",
    "df_clean = df_clean[df_clean['speech'].str.strip().astype(bool)].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "7c6bd148-d5cf-4eb3-a66d-fc706e99c13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved clean speeches chunks in 'C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp'\n"
     ]
    }
   ],
   "source": [
    "# == Split cleaned data into chunks and save as separate files ==\n",
    "\n",
    "# Convert cleaned DataFrame to list of lists\n",
    "clean_data = df_clean[['filename', 'speech']].values.tolist()\n",
    "\n",
    "# Split cleaned data into 4 equal chunks\n",
    "data_id1 = clean_data[:int(len(clean_data)/4)]\n",
    "data_id2 = clean_data[int(len(clean_data)/4): int(2*len(clean_data)/4)]\n",
    "data_id3 = clean_data[int(2*len(clean_data)/4): int(3*len(clean_data)/4)]\n",
    "data_id4 = clean_data[int(3*len(clean_data)/4):]\n",
    "\n",
    "# Change directory to the temp folder\n",
    "os.chdir(data_temp)  # make sure `data_temp` exists and is defined\n",
    "\n",
    "# Save each chunk with joblib\n",
    "joblib.dump(data_id1, 'clean_speeches_indexed1.pkl')\n",
    "joblib.dump(data_id2, 'clean_speeches_indexed2.pkl')\n",
    "joblib.dump(data_id3, 'clean_speeches_indexed3.pkl')\n",
    "joblib.dump(data_id4, 'clean_speeches_indexed4.pkl')\n",
    "\n",
    "# Store list of cleaned data chunk paths to feed into preprocessing function later\n",
    "clean_files = [\n",
    "    os.path.join(data_temp, 'clean_speeches_indexed1.pkl'),\n",
    "    os.path.join(data_temp, 'clean_speeches_indexed2.pkl'),\n",
    "    os.path.join(data_temp, 'clean_speeches_indexed3.pkl'),\n",
    "    os.path.join(data_temp, 'clean_speeches_indexed4.pkl')\n",
    "]\n",
    "\n",
    "print(f\"✅ Saved clean speeches chunks in '{data_temp}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6e5109-8cf7-41e3-8fb9-8e7dde29b4e0",
   "metadata": {},
   "source": [
    "### Advanced Text Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfca8dd9-c6d7-48c7-96b0-f5a449814336",
   "metadata": {},
   "source": [
    "#### Extend stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "id": "11d056b5-6729-45f8-a233-3e8e6de1f69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 326 stopwords to C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\stopwords.pkl\n"
     ]
    }
   ],
   "source": [
    "# Get SpaCy stopwords\n",
    "SPACY_STOPWORDS = list(nlp.Defaults.stop_words)\n",
    "\n",
    "# Path to save\n",
    "stopwords_path = os.path.join(data_c, \"stopwords.pkl\")\n",
    "\n",
    "# Save stopwords\n",
    "joblib.dump(SPACY_STOPWORDS, stopwords_path)\n",
    "\n",
    "print(f\"Saved {len(SPACY_STOPWORDS)} stopwords to {stopwords_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "id": "0aa75944-4b75-4aa4-9430-9aa6bd9ac934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Define function to Tokenize, eliminate digits, remove stopwords, lemmatize, POS-Tagging ==\n",
    "\n",
    "def pro1(lista):\n",
    "    # Remove punctuation\n",
    "    return [[row[0], row[1].translate(translator)] for row in lista]\n",
    "\n",
    "def pro2(lista):\n",
    "    # Tokenize and lowercase with gensim\n",
    "    return [[row[0], gensim.utils.simple_preprocess(row[1])] for row in lista]\n",
    "\n",
    "def pro3(lista):\n",
    "    # Remove tokens that are only digits\n",
    "        a = [[row[0], [w for w in row[1] if not w.isdigit()]] for row in lista]\n",
    "        return a\n",
    "    \n",
    "def pro4(lista):\n",
    "    # Drop short words\n",
    "    return [[row[0], [w for w in row[1] if len(w) > 2]] for row in lista]\n",
    "\n",
    "def tags_spacy(lista):\n",
    "    texts = [' '.join(row[1]) for row in lista]\n",
    "    docs = list(nlp.pipe(texts, batch_size=20, n_process=1))\n",
    "    result = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        filtered_tokens = [token.text for token in doc if token.tag_.startswith(('N', 'V', 'J'))]\n",
    "        result.append([lista[i][0], filtered_tokens])\n",
    "    return result\n",
    "\n",
    "\n",
    "def pro5(lista):\n",
    "    # Remove stopwords using SpaCy stopword list\n",
    "    return [[row[0], [w for w in row[1] if w not in SPACY_STOPWORDS]] for row in lista]\n",
    "\n",
    "def pro6(lista):\n",
    "      return [\n",
    "        [row[0], [stemmer.stem(token) for token in row[1]]]\n",
    "        for row in lista\n",
    "    ]\n",
    "########################## Question for Max: They removed procedural words in the paper\n",
    "\n",
    "def dropnull(lista):\n",
    "    # Drop empty speeches\n",
    "    return [row for row in lista if len(' '.join(row[1])) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "524a274a-004c-4d87-9bb3-5f789a42b90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl] Before tagging: 1.30s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl] After tagging: 33.40s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed1.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed1.pkl] Done. Total time: 39.75s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl] Before tagging: 1.09s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl] After tagging: 33.41s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed2.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed2.pkl] Done. Total time: 40.60s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl] Before tagging: 2.06s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl] After tagging: 33.24s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed3.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed3.pkl] Done. Total time: 40.46s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl] Before tagging: 1.37s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl] After tagging: 38.81s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed4.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\clean_speeches_indexed4.pkl] Done. Total time: 46.58s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# == Create full pre-processing function and call it\n",
    "\n",
    "def preprocessing(data_name):\n",
    "    t0 = time.time()\n",
    "    print(f\"Starting preprocessing for {data_name}...\")\n",
    "\n",
    "    data = joblib.load(data_name)\n",
    "    data = pro1(data)\n",
    "    data = pro2(data)\n",
    "    data = pro3(data)\n",
    "    data = pro4(data)\n",
    "\n",
    "    print(f\"[{data_name}] Before tagging: {time.time() - t0:.2f}s\")\n",
    "    data = tags_spacy(data)\n",
    "    print(f\"[{data_name}] After tagging: {time.time() - t0:.2f}s\")\n",
    "\n",
    "    data = pro5(data)\n",
    "   # data = pro6(data)\n",
    "    data = dropnull(data)\n",
    "\n",
    "    # out_name = data_name.replace('cleanspeeches_', 'preprocessed_speeches_').replace('.pkl', '_temp.pkl')\n",
    "\n",
    "    # Store preprocessed corupus (before stemming) for wordcloud\n",
    "    filename_wordcloud = data_name.replace('clean_speeches_', 'wordcloud_speeches_').replace('.pkl', '.pkl')\n",
    "    out_name_wordcloud = os.path.join(data_preprocessed, os.path.basename(filename_wordcloud))\n",
    "    joblib.dump(data, out_name_wordcloud)\n",
    "\n",
    "    # Apply stemming\n",
    "    data_stemmed = pro6(data)\n",
    "\n",
    "    filename_preprocessed = data_name.replace('clean_speeches_', 'preprocessed_speeches_').replace('.pkl', '.pkl')\n",
    "    out_preprocessed = os.path.join(data_preprocessed, os.path.basename(filename_preprocessed))\n",
    "    joblib.dump(data_stemmed, out_preprocessed\n",
    "               )\n",
    "    print(f\"[{data_name}] Saved stemmed version: {out_preprocessed}\")\n",
    "\n",
    "    print(f\"[{data_name}] Done. Total time: {time.time() - t0:.2f}s\\n\")\n",
    "\n",
    "def main():\n",
    "    for fname in clean_files:\n",
    "        preprocessing(fname)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "id": "d3409987-485c-47c9-9430-01fa2c0e650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the pre-processed data\n",
    "preprocessed_files = [\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed1.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed2.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed3.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed4.pkl')\n",
    "]\n",
    "\n",
    "wordcloud_files = [\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed1.pkl'),\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed2.pkl'),\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed3.pkl'),\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed4.pkl')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "id": "134ddbf7-9b95-4697-9d9c-f9c55b52ec80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73db5f7-566d-4276-a631-c1f33fab494d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d006e-6a23-453b-9c91-0e60bd3f8301",
   "metadata": {},
   "source": [
    "## Word-Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf2443e-4c76-41b2-bc16-e72d91c318a8",
   "metadata": {},
   "source": [
    "### Count frequencies of all tokens and display the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "id": "154d978d-cddd-431c-9d53-73c9faf8fdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:02<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed1.pkl: removed words with freq < 10\n",
      "Processed C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed2.pkl: removed words with freq < 10\n",
      "Processed C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed3.pkl: removed words with freq < 10\n",
      "Processed C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed4.pkl: removed words with freq < 10\n",
      "\n",
      "[Stemmed] Top 50 most common words:\n",
      "nation: 17288\n",
      "unit: 13694\n",
      "countri: 12852\n",
      "intern: 12265\n",
      "develop: 10939\n",
      "peac: 10230\n",
      "world: 9568\n",
      "state: 9437\n",
      "peopl: 9262\n",
      "secur: 6493\n",
      "general: 5735\n",
      "govern: 5683\n",
      "econom: 5559\n",
      "organ: 5272\n",
      "right: 5037\n",
      "year: 4922\n",
      "assembl: 4694\n",
      "new: 4610\n",
      "effort: 4349\n",
      "support: 4178\n",
      "human: 4152\n",
      "problem: 4118\n",
      "communiti: 3926\n",
      "continu: 3892\n",
      "polit: 3683\n",
      "region: 3560\n",
      "time: 3440\n",
      "africa: 3305\n",
      "session: 3171\n",
      "member: 3159\n",
      "import: 3131\n",
      "work: 3059\n",
      "achiev: 3056\n",
      "council: 3000\n",
      "war: 2992\n",
      "need: 2987\n",
      "conflict: 2824\n",
      "power: 2799\n",
      "principl: 2798\n",
      "hope: 2746\n",
      "resolut: 2721\n",
      "situat: 2708\n",
      "republ: 2695\n",
      "global: 2675\n",
      "presid: 2672\n",
      "oper: 2633\n",
      "south: 2619\n",
      "relat: 2562\n",
      "order: 2538\n",
      "forc: 2520\n",
      "\n",
      "[Stemmed] Top 50 least common words:\n",
      "diesel: 1\n",
      "workforc: 1\n",
      "everescal: 1\n",
      "penguin: 1\n",
      "urn: 1\n",
      "unascertain: 1\n",
      "lira: 1\n",
      "sensori: 1\n",
      "archaeolog: 1\n",
      "behoov: 1\n",
      "throb: 1\n",
      "itha: 1\n",
      "sanctifi: 1\n",
      "residuari: 1\n",
      "rodo: 1\n",
      "purer: 1\n",
      "ismat: 1\n",
      "crimeóth: 1\n",
      "andrea: 1\n",
      "goali: 1\n",
      "decor: 1\n",
      "andconvent: 1\n",
      "fbr: 1\n",
      "formanc: 1\n",
      "missionónam: 1\n",
      "remainóa: 1\n",
      "curabl: 1\n",
      "brace: 1\n",
      "lowcarbon: 1\n",
      "hoperais: 1\n",
      "mitch: 1\n",
      "healthiest: 1\n",
      "amartya: 1\n",
      "fischer: 1\n",
      "vita: 1\n",
      "tlatclolco: 1\n",
      "aseanj: 1\n",
      "maldistribut: 1\n",
      "doldrum: 1\n",
      "hobbsian: 1\n",
      "overran: 1\n",
      "ham: 1\n",
      "macrocosm: 1\n",
      "trebl: 1\n",
      "egot: 1\n",
      "kilogram: 1\n",
      "euratom: 1\n",
      "irratio: 1\n",
      "upshot: 1\n",
      "rumor: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:02<00:00,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Wordcloud] Top 50 most common words:\n",
      "nations: 13738\n",
      "united: 13484\n",
      "international: 11717\n",
      "world: 9392\n",
      "countries: 8572\n",
      "peace: 8445\n",
      "states: 6514\n",
      "development: 6427\n",
      "security: 6004\n",
      "people: 5848\n",
      "general: 5579\n",
      "economic: 5534\n",
      "new: 4610\n",
      "assembly: 4591\n",
      "government: 4375\n",
      "country: 4154\n",
      "organization: 3922\n",
      "human: 3655\n",
      "community: 3620\n",
      "efforts: 3555\n",
      "political: 3512\n",
      "rights: 3440\n",
      "peoples: 3412\n",
      "africa: 3179\n",
      "support: 3131\n",
      "session: 3042\n",
      "council: 2933\n",
      "time: 2792\n",
      "south: 2619\n",
      "war: 2618\n",
      "state: 2615\n",
      "republic: 2595\n",
      "national: 2518\n",
      "years: 2495\n",
      "nuclear: 2489\n",
      "problems: 2476\n",
      "great: 2466\n",
      "order: 2447\n",
      "year: 2418\n",
      "situation: 2377\n",
      "global: 2374\n",
      "developing: 2333\n",
      "work: 2328\n",
      "social: 2182\n",
      "conference: 2106\n",
      "african: 2094\n",
      "president: 2057\n",
      "charter: 2048\n",
      "important: 1986\n",
      "today: 1976\n",
      "\n",
      "[Wordcloud] Top 50 least common words:\n",
      "homicides: 1\n",
      "template: 1\n",
      "hoperaising: 1\n",
      "designer: 1\n",
      "mushroom: 1\n",
      "workplaces: 1\n",
      "delineate: 1\n",
      "mitch: 1\n",
      "adolescent: 1\n",
      "elder: 1\n",
      "retirement: 1\n",
      "healthiest: 1\n",
      "amartya: 1\n",
      "fischer: 1\n",
      "contaminate: 1\n",
      "dramatized: 1\n",
      "vita: 1\n",
      "prejudge: 1\n",
      "tlatclolco: 1\n",
      "aseanj: 1\n",
      "maldistribution: 1\n",
      "doldrums: 1\n",
      "gorbachevs: 1\n",
      "boring: 1\n",
      "elusiveness: 1\n",
      "hobbsian: 1\n",
      "takeover: 1\n",
      "aspirants: 1\n",
      "overran: 1\n",
      "analogous: 1\n",
      "realignments: 1\n",
      "fleshed: 1\n",
      "ham: 1\n",
      "devouring: 1\n",
      "macrocosm: 1\n",
      "string: 1\n",
      "excepted: 1\n",
      "graph: 1\n",
      "depicts: 1\n",
      "trebled: 1\n",
      "egotism: 1\n",
      "exhaust: 1\n",
      "quicken: 1\n",
      "kilograms: 1\n",
      "euratom: 1\n",
      "smolder: 1\n",
      "irratio: 1\n",
      "hammer: 1\n",
      "upshot: 1\n",
      "rumor: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit\\\\data\\\\freq\\\\word_counts_wordcloud.pkl']"
      ]
     },
     "execution_count": 717,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#== Count token frequencies ==\n",
    "\n",
    "def count_frequencies(filenames):\n",
    "    total_freq = Counter()\n",
    "    for fname in tqdm(filenames):\n",
    "        data = joblib.load(fname)\n",
    "        tokens = chain.from_iterable(row[1] for row in data if isinstance(row[1], list))\n",
    "        total_freq.update(tokens)\n",
    "    return total_freq\n",
    "\n",
    "def remove_rare_words(filenames, freqs, min_count=10):\n",
    "    for fname in filenames:\n",
    "        data = joblib.load(fname)\n",
    "        filtered_data = []\n",
    "        for doc_id, tokens in data:\n",
    "            filtered_tokens = [w for w in tokens if freqs.get(w, 0) >= min_count]\n",
    "            filtered_data.append([doc_id, filtered_tokens])\n",
    "        joblib.dump(filtered_data, fname)  # overwrite or save as new file\n",
    "        print(f\"Processed {fname}: removed words with freq < {min_count}\")\n",
    "\n",
    "# === Count for preprocessed (stemmed) speeches ===\n",
    "word_counts_stemmed = count_frequencies(preprocessed_files)\n",
    "\n",
    "remove_rare_words(preprocessed_files, word_counts_stemmed, min_count=10)\n",
    "\n",
    "print(\"\\n[Stemmed] Top 50 most common words:\")\n",
    "for word, count in word_counts_stemmed.most_common(50):\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\n[Stemmed] Top 50 least common words:\")\n",
    "for word, count in word_counts_stemmed.most_common()[-50:]:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Save stemmed word counts\n",
    "save_path_stemmed = os.path.join(data_freq, 'word_counts_stemmed.pkl')\n",
    "joblib.dump(word_counts_stemmed, save_path_stemmed)\n",
    "\n",
    "# === Count for wordcloud (unstemmed) speeches ===\n",
    "word_counts_wordcloud = count_frequencies(wordcloud_files)\n",
    "\n",
    "print(\"\\n[Wordcloud] Top 50 most common words:\")\n",
    "for word, count in word_counts_wordcloud.most_common(50):\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\n[Wordcloud] Top 50 least common words:\")\n",
    "for word, count in word_counts_wordcloud.most_common()[-50:]:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Save unstemmed word counts\n",
    "save_path_wordcloud = os.path.join(data_freq, 'word_counts_wordcloud.pkl')\n",
    "joblib.dump(word_counts_wordcloud, save_path_wordcloud)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "id": "932ce36d-d21c-4f1c-aff7-899c321dcfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_c)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39acefa1-c504-478e-98e5-9d465963dc82",
   "metadata": {},
   "source": [
    "### Count the frequency of the dictionary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "id": "9e28f481-7e20-486c-a6b3-8729d6aeaad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of affect dictionary:\n",
      "['forbid', 'unattract', 'cruelti', 'crappi', 'apathi', 'scari', 'unimpress', 'sin', 'dumbest', 'eas', 'agit', 'sob', 'shocker', 'tragedi', 'fabul', 'strongest', 'giver', 'sigh', 'aw', 'witch', 'hurtl', 'fucktard', 'cruel', 'glamor', 'funni', 'smarter', 'brillianc', 'irrate', 'alright', 'honest', 'profit', 'fearless', 'grievous', 'relax', 'isolationist', 'hah', 'shyness', 'poorest', 'cruelest', 'troublemak', 'disagre', 'agon', 'terror', 'fight', 'pleas', 'poor', 'crazi', 'hostil', 'stupid', 'damnat', 'vain', 'jade', 'heartless', 'nag', 'gloomi', 'damn', 'dishearten', 'pleaser', 'credit', 'warmth', 'greatest', 'whine', 'shame', 'angriest', 'envious', 'grin', 'blameless', 'sweeter', 'laidback', 'stupidest', 'unprotect', 'whiner', 'unlov', 'shake', 'boredom', 'fairer', 'weaker', 'wellb', 'bold', 'sucki', 'unsuccess', 'mourner', 'liken', 'defens', 'invigor', 'tedious', 'paranoid', 'cynic', 'dignifi', 'paranoia', 'sweetest', 'contented', 'humili', 'crush', 'terrif', 'alarm', 'asham', 'impati', 'fake', 'flirti', 'laziest', 'libertarian', 'empti', 'panicki', 'rape', 'contemptu', 'incent', 'reward', 'brutal', 'ignoramus', 'bereav', 'wimp', 'laugh', 'nicer', 'warm', 'resent', 'threat', 'hell', 'tortur', 'mad', 'worship', 'uncomfort', 'depress', 'unsaf', 'faultless', 'numb', 'humorless', 'murder', 'prettiest', 'jealous', 'tragic', 'discomfort', 'loveli', 'ungrat', 'divin', 'lamest', 'weaken', 'horribl', 'fuck', 'agoni', 'grievanc', 'ach', 'isolation', 'masoch', 'uglier', 'amaz', 'devilish', 'bitter', 'devil', 'smartest', 'glori', 'trite', 'vicious', 'unsavori', 'grossest', 'sinist', 'cheeri', 'dismay', 'easier', 'flatter', 'virtuous', 'egot', 'generos', 'cherish', 'heartwarm', 'advers', 'tough', 'adorn', 'overwhelm', 'devast', 'egotist', 'sociabl', 'lazier', 'affection', 'sarcasm', 'humor', 'funniest', 'startl', 'innoc', 'vomit', 'shyli', 'kiss', 'burdensom', 'melancholi', 'weakest', 'worthless', 'gentlest', 'proud', 'alarmist', 'pervert', 'demeanor', 'loneli', 'reassur', 'weird', 'passionless', 'loyalti', 'frustrat', 'weak', 'immor', 'hater', 'libertin', 'flirt', 'terribl', 'threaten', 'fatalist', 'melancholia', 'avers', 'prick', 'sucker', 'meanest', 'virtuos', 'benefici', 'neurot', 'disappoint', 'restless', 'romant', 'bastard', 'angri', 'flatteri', 'stun', 'bolder', 'amor', 'forbidden', 'hug', 'demean', 'disgust', 'truer', 'benefic', 'teas', 'stinki', 'condemnatori', 'turmoil', 'poison', 'cunt', 'gorgeous', 'harm', 'lousi', 'glorifi', 'piti', 'wimpish', 'impression', 'freakish', 'happiest', 'apathet', 'pitiabl', 'unpleas', 'suffer', 'miseri', 'toler', 'mess', 'casual', 'anxieti', 'stale', 'fieri', 'grudg', 'arrog', 'outrag', 'thrill', 'faithless', 'popular', 'vaniti', 'palat', 'fantasi', 'decay', 'domin', 'mood', 'proudest', 'scream', 'stress', 'calm', 'fatigu', 'dumber', 'benign', 'rude', 'steal', 'tension', 'enthusiast', 'intimid', 'sweetheart', 'prettier', 'perv', 'pathet', 'flawless', 'contradictori', 'distress', 'moron', 'gratifi', 'frighten', 'anguish', 'snob', 'defenseless', 'fantast', 'sad', 'thief', 'worsen', 'tear', 'handsom', 'attract', 'messi', 'smother', 'scarier', 'afraid', 'insult', 'disreput', 'wept', 'victim', 'traumat', 'prejudic', 'amus', 'decent', 'joyous', 'masochist', 'abandon', 'foe', 'pleasantri', 'shitless', 'strengthen', 'antagonist', 'liar', 'romantic', 'violenc', 'sentimentalist', 'gratif', 'decept', 'witchcraft', 'liveli', 'impressionist', 'guilt', 'griev', 'rotten', 'disgrac', 'insincer', 'pushi', 'enjoy', 'freaki', 'ador', 'grate', 'homesick', 'braver', 'battlefield', 'niceti', 'hooray', 'incompet', 'thiev', 'timid', 'adventur', 'needi', 'joyless', 'grimac', 'gloom', 'dump', 'horrid', 'magnific', 'unlucki', 'killer', 'uncontrol', 'funnier', 'ruin', 'weepi', 'play', 'encourag', 'fantas', 'irrit', 'sincer', 'rage', 'despis', 'unhappi', 'hate', 'bravest', 'harmless', 'hero', 'vigor', 'pervers', 'dumb', 'degrad', 'shameless', 'villaini', 'envi', 'aggress', 'fucker', 'courag', 'craziest', 'delici', 'scare', 'finer', 'bittersweet', 'trivial', 'wealthi', 'heroic', 'ecstat', 'irrat', 'prejudici', 'smug', 'stammer', 'feroci', 'phobic', 'villain', 'anger', 'fond', 'obsession', 'awesom', 'wors', 'compassion', 'weakl', 'calmer', 'inhibit', 'bother', 'rancid', 'hatr', 'beaten', 'opportunist', 'offens', 'haunt', 'appal', 'painless', 'pessimist', 'molest', 'lol', 'wrongdo', 'frantic', 'heroin', 'rejoic', 'woe', 'miser', 'meaner', 'saddest', 'mooch', 'attack', 'enthusiasm', 'joke', 'meritocraci', 'angrier', 'emot', 'weirdest', 'silliest', 'kisser', 'hurrah', 'crueler', 'nervous', 'dishonor', 'gracious', 'sicker', 'hopeless', 'delect', 'compliment', 'petrifi', 'cuter', 'reek', 'loveless', 'daze', 'weirder', 'triumphant', 'impolit', 'offend', 'richer', 'violent', 'crazier', 'comedown', 'heroism', 'stubborn', 'maniac', 'strain', 'broke', 'jerk', 'unkind', 'asshol', 'reveng', 'jealousi', 'damnabl', 'unimport', 'careless', 'gentler', 'bitch', 'shi', 'terrorist', 'rigid', 'harsh', 'passion', 'annoy', 'compass', 'geek', 'weirdo', 'agre', 'whore', 'splendor', 'suck', 'fun', 'chuckl', 'snobberi', 'mock', 'doom', 'reluct', 'mourn', 'boldest', 'helpless', 'trembl', 'pestil', 'curs', 'desper', 'harass', 'complain', 'joker', 'obsess', 'uneasi', 'nasti', 'greedi', 'belov', 'grim', 'upset', 'greed', 'honesti', 'phoni', 'feudal', 'loveliest', 'trauma', 'dwell', 'ecstasi', 'blame', 'legit', 'calmest', 'sickest', 'scariest', 'sarcast', 'cheat', 'fatal', 'stronger', 'energet', 'feroc', 'healthi', 'goddam', 'disast', 'flirtati', 'defeatist', 'glorious', 'terrifi', 'gossip', 'forbad', 'sicken', 'poorer', 'strangest', 'cute', 'struggl', 'woebegon', 'worri', 'gratitud', 'cheer', 'distraught', 'ok', 'vulner', 'blessed', 'fairest', 'lame', 'optim', 'grouch', 'fright', 'pessim', 'sadder', 'worst', 'despair', 'bash', 'intellectu', 'promis', 'prais', 'heartfelt', 'horror', 'crap', 'repress', 'prouder', 'fought', 'discourag', 'heal', 'lazi', 'precious', 'melanchol', 'isol', 'aggressor', 'giggl', 'idiot', 'twitchi', 'glorif', 'kid', 'selfish', 'smile', 'grief', 'hellish', 'shit', 'fume', 'bitchi', 'fool', 'trustworthi', 'optimist', 'dumpi', 'unwelcom', 'neat', 'nicest', 'furi', 'unfriend', 'ridicul', 'arguabl', 'shamefac', 'pervi', 'upbeat', 'wow', 'easygo', 'blameworthi', 'charmer', 'stink', 'phobia', 'adversari', 'sillier', 'distrust', 'dread', 'happier', 'vital', 'dissatisfact', 'insecur', 'assault', 'ineffect', 'panic', 'powerless', 'nightmar', 'wealth', 'enrag', 'splendid', 'foolish', 'ugliest', 'contradict', 'worrier', 'freak']\n",
      "Number of words in affect dictionary: 629\n",
      "Contents of cognition dictionary:\n",
      "['analyt', 'guess', 'suspect', 'coz', 'mysteri', 'unawar', 'proverbi', 'dissimilar', 'consequenti', 'stimul', 'occasion', 'rethink', 'purest', 'entireti', 'abnorm', 'referenti', 'differenti', 'diagnos', 'ambigu', 'concentr', 'undon', 'undecid', 'disclosur', 'fundamentalist', 'blur', 'reaction', 'variabl', 'recogniz', 'meant', 'reconstruct', 'supposit', 'acknowledg', 'choosi', 'accuraci', 'mislead', 'perceiv', 'hypothesi', 'unknow', 'cuz', 'reveal', 'potenti', 'puriti', 'persuad', 'indirect', 'generat', 'presumpt', 'clue', 'justif', 'evidenti', 'realize', 'deduct', 'causal', 'inevit', 'thinker', 'evidentiari', 'factor', 'undeni', 'unclear', 'pick', 'induc', 'accur', 'absolut', 'everytim', 'reconsider', 'diagnost', 'solut', 'diagnosi', 'discoveri', 'ration', 'unambigu', 'explicit', 'decis', 'insight', 'confess', 'fuzzi', 'led', 'lesson', 'theoriz', 'analyz', 'learner', 'split', 'enlighten', 'cos', 'respons', 'dubious', 'react', 'guarante', 'someday', 'blatant', 'undo', 'perceptu', 'infal', 'afterthought', 'inequ', 'feasibl', 'knowabl', 'approxim', 'logic', 'meaning', 'unknown', 'categor', 'trigger', 'incomplet', 'curious', 'attribut', 'pretend', 'reactionari', 'prove', 'meaningless', 'reactiv', 'proof', 'bet', 'purpos', 'suppos', 'discern', 'imagin', 'kinda', 'comprehens', 'sens', 'percept', 'everyday', 'rational', 'unquestion', 'random', 'implicit', 'clarif', 'theori', 'dunno', 'analysi', 'obedi', 'memor', 'genuin', 'evalu', 'explor', 'linkag', 'grasp', 'curios', 'methink', 'somewher', 'deduc', 'unwant', 'induct', 'unrel', 'outstand', 'discov', 'unresolv', 'exploratori', 'choos', 'perspect', 'inferenti', 'disclos', 'factual', 'arbitrari', 'conting', 'vagu', 'persuas', 'manipul', 'impli', 'somehow', 'sorta', 'borderlin', 'reconcil', 'anyhow', 'conscious', 'probabilist', 'precis', 'rearrang', 'referenc', 'obscur']\n",
      "Number of words in cognition dictionary: 169\n"
     ]
    }
   ],
   "source": [
    "# == Count dictionary words\n",
    "\n",
    "# Load dictionaries            ##### How did they come up with this dictionary? Why did they exclude words?\n",
    "\n",
    "affect_path = os.path.join(data_dict, 'dictionary_affect.pkl')\n",
    "cognition_path = os.path.join(data_dict, 'dictionary_cognition.pkl')\n",
    "\n",
    "with open(affect_path, 'rb') as f:\n",
    "    affect_dict = pickle.load(f)\n",
    "print(\"Contents of affect dictionary:\")\n",
    "print(affect_dict)\n",
    "print(\"Number of words in affect dictionary:\", len(affect_dict))\n",
    "\n",
    "with open(cognition_path, 'rb') as f:\n",
    "    cognition_dict = pickle.load(f)\n",
    "print(\"Contents of cognition dictionary:\")\n",
    "print(cognition_dict)\n",
    "print(\"Number of words in cognition dictionary:\", len(cognition_dict))\n",
    "\n",
    "affect = joblib.load(affect_path)\n",
    "cognition = joblib.load(cognition_path)\n",
    "\n",
    "a = [[i, word_counts_stemmed[i]] for i in affect if i in word_counts_stemmed]\n",
    "c = [[i, word_counts_stemmed[i]] for i in cognition if i in word_counts_stemmed]\n",
    "\n",
    "a = sorted(a, key=lambda x: x[1], reverse=True)\n",
    "c = sorted(c, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "a = [[i[0], f\"({i[1]}),\"] for i in a]\n",
    "c = [[i[0], f\"({i[1]}),\"] for i in c]\n",
    "\n",
    "a1 = ' '.join(str(r) for v in a for r in v)\n",
    "c1 = ' '.join(str(r) for v in c for r in v)\n",
    "\n",
    "affect_out_path = os.path.join(data_freq, \"affect_words.txt\")\n",
    "cog_out_path = os.path.join(data_freq, \"cog_words.txt\")\n",
    "\n",
    "os.makedirs(data_freq, exist_ok=True)  # ensure directory exists\n",
    "\n",
    "with open(affect_out_path, \"w\") as output:\n",
    "    output.write(a1)\n",
    "\n",
    "with open(cog_out_path, \"w\") as output:\n",
    "    output.write(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "id": "0dffe87f-d712-4d71-9305-fb288cf06402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 words by weighted frequency:\n",
      "aftershock: 0.9990720747388123\n",
      "dip: 0.9990720747388123\n",
      "autark: 0.9990720747388123\n",
      "desktop: 0.9990720747388123\n",
      "esoter: 0.9990720747388123\n",
      "lifejacket: 0.9990720747388123\n",
      "kiwa: 0.9990720747388123\n",
      "oceanscap: 0.9990720747388123\n",
      "spellbound: 0.9990720747388123\n",
      "adolf: 0.9990720747388123\n",
      "varieg: 0.9990720747388123\n",
      "pent: 0.9990720747388123\n",
      "outlandish: 0.9990720747388123\n",
      "das: 0.9990720747388123\n",
      "kapit: 0.9990720747388123\n",
      "molest: 0.9990720747388123\n",
      "apost: 0.9990720747388123\n",
      "cochairman: 0.9990720747388123\n",
      "aidsuppli: 0.9990720747388123\n",
      "focuss: 0.9990720747388123\n",
      "foragr: 0.9990720747388123\n",
      "arana: 0.9990720747388123\n",
      "osorio: 0.9990720747388123\n",
      "propinqu: 0.9990720747388123\n",
      "nondemocrat: 0.9990720747388123\n",
      "lomè: 0.9990720747388123\n",
      "persecutor: 0.9990720747388123\n",
      "smuggler: 0.9990720747388123\n",
      "vacuiti: 0.9990720747388123\n",
      "strangest: 0.9990720747388123\n",
      "acquitt: 0.9990720747388123\n",
      "airbus: 0.9990720747388123\n",
      "mastermind: 0.9990720747388123\n",
      "dous: 0.9990720747388123\n",
      "manicheist: 0.9990720747388123\n",
      "livingthrough: 0.9990720747388123\n",
      "theoretician: 0.9990720747388123\n",
      "triuniti: 0.9990720747388123\n",
      "manichean: 0.9990720747388123\n",
      "goulart: 0.9990720747388123\n",
      "eugen: 0.9990720747388123\n",
      "sputter: 0.9990720747388123\n",
      "inchoat: 0.9990720747388123\n",
      "cogit: 0.9990720747388123\n",
      "duress: 0.9990720747388123\n",
      "seventeen: 0.9990720747388123\n",
      "abstin: 0.9990720747388123\n",
      "loi: 0.9990720747388123\n",
      "fondamental: 0.9990720747388123\n",
      "comecon: 0.9990720747388123\n",
      "reconvert: 0.9990720747388123\n",
      "corrigendum: 0.9990720747388123\n",
      "sumner: 0.9990720747388123\n",
      "crimean: 0.9990720747388123\n",
      "tatar: 0.9990720747388123\n",
      "unfeas: 0.9990720747388123\n",
      "cyberthreat: 0.9990720747388123\n",
      "crosssector: 0.9990720747388123\n",
      "disunion: 0.9990720747388123\n",
      "sixteen: 0.9990720747388123\n",
      "transcript: 0.9990720747388123\n",
      "bogeyman: 0.9990720747388123\n",
      "haemorrhag: 0.9990720747388123\n",
      "elysé: 0.9990720747388123\n",
      "cram: 0.9990720747388123\n",
      "scotch: 0.9990720747388123\n",
      "deer: 0.9990720747388123\n",
      "juxtapos: 0.9990720747388123\n",
      "umaru: 0.9990720747388123\n",
      "yar: 0.9990720747388123\n",
      "adua: 0.9990720747388123\n",
      "weed: 0.9990720747388123\n",
      "overlay: 0.9990720747388123\n",
      "synerg: 0.9990720747388123\n",
      "teodoro: 0.9990720747388123\n",
      "assosa: 0.9990720747388123\n",
      "cowork: 0.9990720747388123\n",
      "coptic: 0.9990720747388123\n",
      "sufi: 0.9990720747388123\n",
      "alawit: 0.9990720747388123\n",
      "naypyidaw: 0.9990720747388123\n",
      "favela: 0.9990720747388123\n",
      "mumbai: 0.9990720747388123\n",
      "heartbeat: 0.9990720747388123\n",
      "gagauzia: 0.9990720747388123\n",
      "jockey: 0.9990720747388123\n",
      "sergey: 0.9990720747388123\n",
      "shoygu: 0.9990720747388123\n",
      "popescu: 0.9990720747388123\n",
      "myopic: 0.9990720747388123\n",
      "eitheror: 0.9990720747388123\n",
      "alto: 0.9990720747388123\n",
      "adig: 0.9990720747388123\n",
      "bee: 0.9990720747388123\n",
      "indi: 0.9990720747388123\n",
      "undelay: 0.9990720747388123\n",
      "wet: 0.9990720747388123\n",
      "neuter: 0.9990720747388123\n",
      "thereor: 0.9990720747388123\n",
      "realti: 0.9990720747388123\n"
     ]
    }
   ],
   "source": [
    "# == Calculate weighted frequencies for all words\n",
    "\n",
    "# - downweights very common words by giving more importance to rare ones\n",
    "word_counts_stemmed = joblib.load(os.path.join(data_freq, 'word_counts_stemmed.pkl'))\n",
    "\n",
    "l = sum(word_counts_stemmed.values())\n",
    "\n",
    "a = 0.001\n",
    "word_counts_weighted = {k: a / (a + (v / l)) for k, v in word_counts_stemmed.items()}\n",
    "#for key in word_counts.keys():\n",
    " #   word_counts[key] = a / (a + (word_counts[key] / l))\n",
    "\n",
    "joblib.dump(word_counts_weighted, os.path.join(data_freq, 'word_counts_weighted.pkl'))\n",
    "\n",
    "################################################################################ ISSUE##################\n",
    "# To print top 100 by weighted values, sort the dictionary by value descending:\n",
    "top_100_weighted = sorted(word_counts_weighted.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "\n",
    "print(\"Top 100 words by weighted frequency:\")\n",
    "for word, weight in top_100_weighted:\n",
    "    print(f\"{word}: {weight}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a7ec60-2d73-4c68-948c-592970c4ad93",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df626ded-5e49-4a34-a3d1-315ba370226f",
   "metadata": {},
   "source": [
    "## Final Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "id": "0d0856a2-44f1-4304-b221-09a9ab72ad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_c)\n",
    "os.chdir(data_freq)\n",
    "\n",
    "count = joblib.load('word_counts_stemmed.pkl')  # load stemmed counts\n",
    "\n",
    "# For each speech only keep tokens that appear at least 10x\n",
    "\n",
    "def select(lista):\n",
    "    for i in range(len(lista)):\n",
    "        x = lista[i][0]\n",
    "        y = lista[i][1]\n",
    "        y = [w for w in y if count.get(w, 0) >= 10]\n",
    "        lista[i] = [x, y]\n",
    "    return lista\n",
    "\n",
    "for data_path in preprocessed_files:\n",
    "    data = joblib.load(data_path)\n",
    "    data = select(data)\n",
    "    cleaned_path = data_path.replace('.pkl', '_final.pkl')\n",
    "    joblib.dump(data, cleaned_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86873a31-3f1e-4d31-aa16-0f2813161647",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
