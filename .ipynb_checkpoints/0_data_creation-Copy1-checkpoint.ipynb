{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b41cb22-e190-48d5-9926-7ac6fac8310d",
   "metadata": {},
   "source": [
    "# Emotion and Reason in Political Language: Examining the UN General speeches\n",
    "## Script 1: Preprocessing & Token Frequencies\n",
    "## Author: Sarah Franzen\n",
    "\n",
    "### Instructions BEFORE running this script:\n",
    "- Ensure all required packages are installed. If not, set `InstallPackages = TRUE` (see code cells below).  \n",
    "- Set your working directory appropriately.  \n",
    "- The script will automatically create the required folder structure.  \n",
    "- Later, you will be asked to download the data from:  \n",
    "  https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/0TJX8Y  \n",
    "  and store it **unzipped** inside the created folder *data_original*\n",
    "\n",
    "\n",
    "### Description: \n",
    "- Extract documents from their original txt documents and store them as one csv\n",
    "- Data Cleaning and Pre-Processing\n",
    "- Count word frequencies and weight themhemnal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5588afc-105a-426e-9348-742a830a2732",
   "metadata": {},
   "source": [
    "## Setup, Installation and Verification of required Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "ebb8987a-a0da-453e-97e2-aafa209d0e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "InstallPackages = False # Set this to True to install the following packages \n",
    "\n",
    "if InstallPackages:\n",
    "    import sys\n",
    "\n",
    "    packages = [\n",
    "        \"pandas\",\n",
    "        \"nltk\",\n",
    "        \"spacy\",\n",
    "        \"numpy\",\n",
    "        \"gensim\",\n",
    "        \"pycountry\",\n",
    "        \"wordcloud\",\n",
    "        \"matplotlib\",\n",
    "        \"tqdm\"\n",
    "    ]\n",
    "\n",
    "    for package in packages:\n",
    "        !{sys.executable} -m pip install {package}\n",
    "\n",
    "\n",
    "DownloadAdditions = False # Set this to True to download these additional resources\n",
    "if DownloadAdditions:\n",
    "    nltk.download(\"punkt\")\n",
    "    nltk.download(\"averaged_perceptron_tagger\")\n",
    "    spacy.cli.download('en_core_web_lg')         # Download spaCy English model (large)\n",
    "\n",
    "#########################\n",
    "# Check if all packages are included\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "32d7d499-0563-46e0-9730-914c9f106d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Import standard and third-party libraries for data processing, NLP, and visualization ==\n",
    "\n",
    "import gensim\n",
    "import joblib\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import pycountry\n",
    "import random\n",
    "import re\n",
    "import spacy\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "#from matplotlib.colors import ListedColormap\n",
    "from multiprocessing import Pool, freeze_support\n",
    "# from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOPWORDS\n",
    "from string import punctuation\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# === Initialize NLP Tools ===\n",
    "\n",
    "# Translator to remove punctuation\n",
    "translator = str.maketrans('', '', punctuation)\n",
    "\n",
    "# POS tagger (not used by SpaCy, but optionally available via NLTK)\n",
    "tagger = nltk.perceptron.PerceptronTagger()\n",
    "\n",
    "# Load SpaCy English model with unnecessary components disabled\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "4d495156-bbb2-403c-86f2-83b47d8cb03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Folder structure created:\n",
      "- C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\n",
      "  - data_original\n",
      "  - dictionaries\n",
      "  - freq\n",
      "  - preprocessed\n",
      "  - temp\n",
      "  - tokenized\n"
     ]
    }
   ],
   "source": [
    "# === Set Working Directory and create folder structure ===\n",
    "\n",
    "# Prompt user to enter working directory path\n",
    "#wd = input(\"Please enter your working directory path (e.g., C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit): \").strip()\n",
    "\n",
    "# Change to the entered working directory\n",
    "#try:\n",
    "   # os.chdir(wd)\n",
    "    #print(f\"Working directory set to: {os.getcwd()}\")\n",
    "#except FileNotFoundError:\n",
    "   # print(\"ERROR: The directory you entered does not exist. Please restart and enter a valid path.\")\n",
    "    #exit(1)\n",
    "\n",
    "# Set your working directory (adjust this as needed)\n",
    "wd = r\"C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\"\n",
    "os.chdir(wd)\n",
    "\n",
    "# Set base path to current working directory\n",
    "base_path = Path.cwd()\n",
    "data_path = base_path / \"data\"\n",
    "\n",
    "# List of subfolders to create inside 'data'\n",
    "subfolders = [\"data_original\", \"dictionaries\", \"freq\", \"preprocessed\", \"temp\", \"tokenized\"]\n",
    "\n",
    "# Create 'data' folder if it doesn't exist\n",
    "data_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Create subfolders\n",
    "for name in subfolders:\n",
    "    (data_path / name).mkdir(exist_ok=True)\n",
    "\n",
    "print(\"\\nFolder structure created:\")\n",
    "print(f\"- {data_path}\")\n",
    "for name in subfolders:\n",
    "    print(f\"  - {name}\")\n",
    "\n",
    "# Prompt user to place raw data files\n",
    "#print(f\"\\nPlease place your raw data files (unzipped) into the folder:\\n  {data_path / 'data_original'}\")\n",
    "#input(\"Press Enter after you have placed the files to continue...\")\n",
    "\n",
    "#print(\"Continuing with the script...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "56fa9a21-650f-4d8e-8a21-3d9629655c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define Folder Paths ===\n",
    "\n",
    "# Make sure that you have these folders in your working directory\n",
    "data_c = os.path.join(wd, 'data')\n",
    "data_temp = os.path.join(data_c, 'temp')\n",
    "data_freq = os.path.join(data_c, 'freq')\n",
    "data_dict = os.path.join(data_c, 'dictionaries')\n",
    "data_preprocessed = os.path.join(data_c, 'preprocessed')\n",
    "data_tokenized = os.path.join(data_c, 'tokenized')\n",
    "fig = os.path.join(wd, 'fig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "296fc7d6-a2ac-406e-a322-6c9f2702ee1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\n"
     ]
    }
   ],
   "source": [
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff34b6-cbee-4f8c-8076-5f385f596b58",
   "metadata": {},
   "source": [
    "## Load and Prepare Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18067b9-6337-4fb3-a570-603083a7deae",
   "metadata": {},
   "source": [
    "### This chunk can be skipped at the moment\n",
    "### Think of proper header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "93d247a0-5ff2-4062-add9-f3267ae1fe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¾ Total speeches found: 10761\n",
      "\n",
      "âœ… Saved raw data with 800 speeches to '.\\data\\un_corpus_raw.csv'\n"
     ]
    }
   ],
   "source": [
    "# == Load and Save Sample from UN General Debate Corpus ==\n",
    "\n",
    "# Set Folder path containing the original TXT files    \n",
    "base_folder = r\".\\data\\data_original\\UN General Debate Corpus\\UNGDC_1946-2023\\TXT\"\n",
    "\n",
    "#  Gather all relevant txt-files\n",
    "all_txt_files = []\n",
    "for root, dirs, files in os.walk(base_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.txt') and not file.startswith('._'):\n",
    "            all_txt_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"ðŸ§¾ Total speeches found: {len(all_txt_files)}\")\n",
    "\n",
    "# Randomly pick 800 files from the full collection   ###################################################### REMOVE AT LATER POINT\n",
    "sampled_files = random.sample(all_txt_files,800)\n",
    "\n",
    "# Read the selected files into a list\n",
    "raw_data = []\n",
    "for filepath in sampled_files:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        raw_data.append({'filename': os.path.basename(filepath), 'speech': content})\n",
    "\n",
    "# Create DataFrame from the collected speeches\n",
    "df_raw = pd.DataFrame(raw_data)\n",
    "\n",
    "df_raw = df_raw[df_raw['filename'] != '.DS_Store-to-UTF-8.txt'].copy()\n",
    "\n",
    "# Save df_raw as a pickle file for quick future loading\n",
    "raw_pickle_path = r\".\\data\\un_corpus_raw.pkl\"\n",
    "df_raw.to_pickle(raw_pickle_path)\n",
    "\n",
    "# Export df as CSV \n",
    "raw_output_path = r\".\\data\\un_corpus_raw.csv\"\n",
    "df_raw.to_csv(raw_output_path, index=False, sep=';', encoding='utf-8')\n",
    "\n",
    "print(f\"\\nâœ… Saved raw data with {len(df_raw)} speeches to '{raw_output_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "6405ff20-92fa-4c97-becb-ac5fff70f84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TTO_73_2018.txt</td>\n",
       "      <td>I am honoured to address this eminent gatherin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BRB_45_1990.txt</td>\n",
       "      <td>ï»¿Mr. President, I am pleased to join those who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VCT_44_1989.txt</td>\n",
       "      <td>ï»¿The delegation of Saint Vincent and the Grena...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CRI_22_1967.txt</td>\n",
       "      <td>98. In an important speech made at the Univers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YEM_46_1991.txt</td>\n",
       "      <td>ï»¿It gives me pleasure to congratulate the Pres...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech\n",
       "0  TTO_73_2018.txt  I am honoured to address this eminent gatherin...\n",
       "1  BRB_45_1990.txt  ï»¿Mr. President, I am pleased to join those who...\n",
       "2  VCT_44_1989.txt  ï»¿The delegation of Saint Vincent and the Grena...\n",
       "3  CRI_22_1967.txt  98. In an important speech made at the Univers...\n",
       "4  YEM_46_1991.txt  ï»¿It gives me pleasure to congratulate the Pres..."
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# == Check if everything worked & drop empty speeches ==\n",
    "\n",
    "# Load df_raw\n",
    "df_raw = pd.read_pickle(r\".\\data\\un_corpus_raw.pkl\")\n",
    "\n",
    "# Drop empty speeches\n",
    "df_raw['speech'] = df_raw['speech'].astype(str)\n",
    "df_raw = df_raw[df_raw['speech'].str.strip() != ''].copy()\n",
    "\n",
    "# View df to check structure\n",
    "df_raw.head()         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7d3162-ebbd-449b-a2be-1148fceda725",
   "metadata": {},
   "source": [
    "## Create new variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25710e79-5068-4d39-8023-f4f97513f165",
   "metadata": {},
   "source": [
    "### Year, country_code and country_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "162da07c-1112-43e8-a434-2bef0ed32de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min year: 1946\n",
      "Max year: 2023\n",
      "Missing codes: []\n",
      "df_raw saved to C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\un_corpus_raw.pkl\n"
     ]
    }
   ],
   "source": [
    "# Extract country code (first 3 letters) and year (last 4 digits before .txt)\n",
    "df_raw['country_code'] = df_raw['filename'].str.extract(r'^([A-Z]{2,3})')\n",
    "df_raw['year'] = df_raw['filename'].str.extract(r'_(\\d{4})\\.txt$').astype(int)\n",
    "\n",
    "print(\"Min year:\", df_raw['year'].min())\n",
    "print(\"Max year:\", df_raw['year'].max())\n",
    "\n",
    "# Match country codes to country names\n",
    "code_to_name = {country.alpha_3: country.name for country in pycountry.countries}\n",
    "\n",
    "# Add custom short names and legacy codes\n",
    "custom_names = {\n",
    "    \"BOL\": \"Bolivia\",\n",
    "    \"COD\": \"The Democratic Republic of the Congo\",\n",
    "    \"IRN\": \"Iran\",\n",
    "    \"LAO\": \"Laos\",\n",
    "    \"MDA\": \"Moldova\",\n",
    "    \"PRK\": \"North Korea\",\n",
    "    \"PSE\": \"Palestine\",\n",
    "    \"RUS\": \"Russia\",\n",
    "    \"SYR\": \"Syria\",\n",
    "    \"TZA\": \"Tanzania\",\n",
    "    \"VAT\": \"Vatican City State\",\n",
    "    \"VEN\": \"Venezuela\",\n",
    "    \"VNM\": \"Vietnam\",\n",
    "    \"YMD\": \"South Yemen\",\n",
    "    \"YUG\": \"Yugoslavia\",\n",
    "    \"DDR\": \"East Germany\",\n",
    "    # \"SUN\": \"Soviet Union\",\n",
    "    \"EU\": \"European Union\",\n",
    "    \"CSK\": \"Czechoslovakia\",\n",
    "    \"FSM\": \"Micronesia\",\n",
    "    \"KOR\": \"South Korea\"\n",
    "    \n",
    "}\n",
    "\n",
    "# Update the main mapping with custom names\n",
    "code_to_name.update(custom_names)\n",
    "\n",
    "# Map with updated dictionary\n",
    "df_raw['country_name'] = df_raw['country_code'].map(code_to_name)\n",
    "\n",
    "# Check missing mappings\n",
    "missing = df_raw.loc[df_raw['country_name'].isna(), 'country_code'].unique()\n",
    "print(\"Missing codes:\", missing)\n",
    "\n",
    "# Check structure of the df\n",
    "df_raw.head() \n",
    "\n",
    "save_path = os.path.join(data_c, 'un_corpus_raw.pkl')\n",
    "df_raw.to_pickle(save_path)\n",
    "print(f\"df_raw saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "383d2904-f015-4c9d-9aeb-51a94f03f950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    country_code                          country_name\n",
      "0            AFG                           Afghanistan\n",
      "1            AGO                                Angola\n",
      "2            ALB                               Albania\n",
      "3            AND                               Andorra\n",
      "4            ARE                  United Arab Emirates\n",
      "5            ARG                             Argentina\n",
      "6            ATG                   Antigua and Barbuda\n",
      "7            AUS                             Australia\n",
      "8            AUT                               Austria\n",
      "9            AZE                            Azerbaijan\n",
      "10           BDI                               Burundi\n",
      "11           BEL                               Belgium\n",
      "12           BEN                                 Benin\n",
      "13           BFA                          Burkina Faso\n",
      "14           BGD                            Bangladesh\n",
      "15           BGR                              Bulgaria\n",
      "16           BHR                               Bahrain\n",
      "17           BHS                               Bahamas\n",
      "18           BIH                Bosnia and Herzegovina\n",
      "19           BLR                               Belarus\n",
      "20           BLZ                                Belize\n",
      "21           BOL                               Bolivia\n",
      "22           BRA                                Brazil\n",
      "23           BRB                              Barbados\n",
      "24           BRN                     Brunei Darussalam\n",
      "25           BTN                                Bhutan\n",
      "26           BWA                              Botswana\n",
      "27           CAF              Central African Republic\n",
      "28           CAN                                Canada\n",
      "29           CHE                           Switzerland\n",
      "30           CHL                                 Chile\n",
      "31           CHN                                 China\n",
      "32           CIV                         CÃ´te d'Ivoire\n",
      "33           CMR                              Cameroon\n",
      "34           COD  The Democratic Republic of the Congo\n",
      "35           COG                                 Congo\n",
      "36           COL                              Colombia\n",
      "37           COM                               Comoros\n",
      "38           CPV                            Cabo Verde\n",
      "39           CRI                            Costa Rica\n",
      "40           CSK                        Czechoslovakia\n",
      "41           CUB                                  Cuba\n",
      "42           CYP                                Cyprus\n",
      "43           CZE                               Czechia\n",
      "44           DEU                               Germany\n",
      "45           DJI                              Djibouti\n",
      "46           DMA                              Dominica\n",
      "47           DNK                               Denmark\n",
      "48           DOM                    Dominican Republic\n",
      "49           DZA                               Algeria\n",
      "50           ECU                               Ecuador\n",
      "51           EGY                                 Egypt\n",
      "52           ERI                               Eritrea\n",
      "53           ESP                                 Spain\n",
      "54           EST                               Estonia\n",
      "55           ETH                              Ethiopia\n",
      "56            EU                        European Union\n",
      "57           FIN                               Finland\n",
      "58           FJI                                  Fiji\n",
      "59           FRA                                France\n",
      "60           FSM                            Micronesia\n",
      "61           GAB                                 Gabon\n",
      "62           GBR                        United Kingdom\n",
      "63           GEO                               Georgia\n",
      "64           GHA                                 Ghana\n",
      "65           GIN                                Guinea\n",
      "66           GMB                                Gambia\n",
      "67           GNB                         Guinea-Bissau\n",
      "68           GNQ                     Equatorial Guinea\n",
      "69           GRC                                Greece\n",
      "70           GRD                               Grenada\n",
      "71           GTM                             Guatemala\n",
      "72           GUY                                Guyana\n",
      "73           HND                              Honduras\n",
      "74           HRV                               Croatia\n",
      "75           HTI                                 Haiti\n",
      "76           HUN                               Hungary\n",
      "77           IDN                             Indonesia\n",
      "78           IND                                 India\n",
      "79           IRL                               Ireland\n",
      "80           IRN                                  Iran\n",
      "81           IRQ                                  Iraq\n",
      "82           ISL                               Iceland\n",
      "83           ISR                                Israel\n",
      "84           ITA                                 Italy\n",
      "85           JAM                               Jamaica\n",
      "86           JOR                                Jordan\n",
      "87           JPN                                 Japan\n",
      "88           KAZ                            Kazakhstan\n",
      "89           KEN                                 Kenya\n",
      "90           KGZ                            Kyrgyzstan\n",
      "91           KHM                              Cambodia\n",
      "92           KIR                              Kiribati\n",
      "93           KNA                 Saint Kitts and Nevis\n",
      "94           KOR                           South Korea\n",
      "95           KWT                                Kuwait\n",
      "96           LAO                                  Laos\n",
      "97           LBN                               Lebanon\n",
      "98           LBR                               Liberia\n",
      "99           LBY                                 Libya\n",
      "100          LCA                           Saint Lucia\n",
      "101          LIE                         Liechtenstein\n",
      "102          LKA                             Sri Lanka\n",
      "103          LSO                               Lesotho\n",
      "104          LTU                             Lithuania\n",
      "105          LUX                            Luxembourg\n",
      "106          MAR                               Morocco\n",
      "107          MCO                                Monaco\n",
      "108          MDA                               Moldova\n",
      "109          MDG                            Madagascar\n",
      "110          MDV                              Maldives\n",
      "111          MEX                                Mexico\n",
      "112          MHL                      Marshall Islands\n",
      "113          MKD                       North Macedonia\n",
      "114          MLI                                  Mali\n",
      "115          MLT                                 Malta\n",
      "116          MMR                               Myanmar\n",
      "117          MNE                            Montenegro\n",
      "118          MNG                              Mongolia\n",
      "119          MOZ                            Mozambique\n",
      "120          MRT                            Mauritania\n",
      "121          MUS                             Mauritius\n",
      "122          MWI                                Malawi\n",
      "123          MYS                              Malaysia\n",
      "124          NAM                               Namibia\n",
      "125          NER                                 Niger\n",
      "126          NGA                               Nigeria\n",
      "127          NIC                             Nicaragua\n",
      "128          NLD                           Netherlands\n",
      "129          NOR                                Norway\n",
      "130          NPL                                 Nepal\n",
      "131          NRU                                 Nauru\n",
      "132          NZL                           New Zealand\n",
      "133          OMN                                  Oman\n",
      "134          PAK                              Pakistan\n",
      "135          PAN                                Panama\n",
      "136          PER                                  Peru\n",
      "137          PHL                           Philippines\n",
      "138          PLW                                 Palau\n",
      "139          PNG                      Papua New Guinea\n",
      "140          POL                                Poland\n",
      "141          PRK                           North Korea\n",
      "142          PRT                              Portugal\n",
      "143          PRY                              Paraguay\n",
      "144          PSE                             Palestine\n",
      "145          QAT                                 Qatar\n",
      "146          ROU                               Romania\n",
      "147          RUS                                Russia\n",
      "148          RWA                                Rwanda\n",
      "149          SAU                          Saudi Arabia\n",
      "150          SDN                                 Sudan\n",
      "151          SEN                               Senegal\n",
      "152          SGP                             Singapore\n",
      "153          SLB                       Solomon Islands\n",
      "154          SLE                          Sierra Leone\n",
      "155          SLV                           El Salvador\n",
      "156          SOM                               Somalia\n",
      "157          SUR                              Suriname\n",
      "158          SVK                              Slovakia\n",
      "159          SWE                                Sweden\n",
      "160          SWZ                              Eswatini\n",
      "161          SYC                            Seychelles\n",
      "162          SYR                                 Syria\n",
      "163          TCD                                  Chad\n",
      "164          TGO                                  Togo\n",
      "165          THA                              Thailand\n",
      "166          TJK                            Tajikistan\n",
      "167          TKM                          Turkmenistan\n",
      "168          TLS                           Timor-Leste\n",
      "169          TON                                 Tonga\n",
      "170          TTO                   Trinidad and Tobago\n",
      "171          TUN                               Tunisia\n",
      "172          TUR                               TÃ¼rkiye\n",
      "173          TUV                                Tuvalu\n",
      "174          TZA                              Tanzania\n",
      "175          UGA                                Uganda\n",
      "176          UKR                               Ukraine\n",
      "177          URY                               Uruguay\n",
      "178          USA                         United States\n",
      "179          UZB                            Uzbekistan\n",
      "180          VAT                    Vatican City State\n",
      "181          VCT      Saint Vincent and the Grenadines\n",
      "182          VEN                             Venezuela\n",
      "183          VNM                               Vietnam\n",
      "184          VUT                               Vanuatu\n",
      "185          WSM                                 Samoa\n",
      "186          YEM                                 Yemen\n",
      "187          YMD                           South Yemen\n",
      "188          YUG                            Yugoslavia\n",
      "189          ZAF                          South Africa\n",
      "190          ZMB                                Zambia\n",
      "191          ZWE                              Zimbabwe\n"
     ]
    }
   ],
   "source": [
    "# == Check the country names \n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(df_raw[['country_code', 'country_name']].drop_duplicates().sort_values('country_code').reset_index(drop=True))\n",
    "# Reset to default afterward\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5928538b-8519-40e2-bdee-ac72024f9f48",
   "metadata": {},
   "source": [
    "### Length of raw speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "9841ec29-b451-419b-957e-7803fbe9401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column: speech length in words\n",
    "df_raw['speech_length_words'] = df_raw['speech'].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "911e2229-337f-4479-984c-3a6ab9dff5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average speech length (words): 3027.42\n",
      "20 shortest speeches:\n",
      "            filename            country_name  year  speech_length_words\n",
      "777  LTU_73_2018.txt               Lithuania  2018                  508\n",
      "294  RWA_74_2019.txt                  Rwanda  2019                  611\n",
      "353  CZE_72_2017.txt                 Czechia  2017                  636\n",
      "290  UZB_61_2006.txt              Uzbekistan  2006                  881\n",
      "523  YUG_58_2003.txt              Yugoslavia  2003                  909\n",
      "667  IDN_21_1966.txt               Indonesia  1966                  941\n",
      "538  UZB_69_2014.txt              Uzbekistan  2014                  966\n",
      "133  BTN_57_2002.txt                  Bhutan  2002                  968\n",
      "757  LKA_71_2016.txt               Sri Lanka  2016                  976\n",
      "364  OMN_73_2018.txt                    Oman  2018                 1025\n",
      "98   BIH_65_2010.txt  Bosnia and Herzegovina  2010                 1028\n",
      "525   EU_70_2015.txt          European Union  2015                 1029\n",
      "725  DOM_61_2006.txt      Dominican Republic  2006                 1029\n",
      "315  UZB_62_2007.txt              Uzbekistan  2007                 1045\n",
      "177  ZWE_71_2016.txt                Zimbabwe  2016                 1055\n",
      "788  BTN_42_1987.txt                  Bhutan  1987                 1059\n",
      "433  ERI_73_2018.txt                 Eritrea  2018                 1069\n",
      "589   EU_71_2016.txt          European Union  2016                 1071\n",
      "360  LAO_61_2006.txt                    Laos  2006                 1072\n",
      "778  MOZ_71_2016.txt              Mozambique  2016                 1075\n",
      "\n",
      "20 longest speeches:\n",
      "            filename                          country_name  year  \\\n",
      "636  BLR_15_1960.txt                               Belarus  1960   \n",
      "242  UGA_30_1975.txt                                Uganda  1975   \n",
      "495  COD_28_1973.txt  The Democratic Republic of the Congo  1973   \n",
      "168  YUG_15_1960.txt                            Yugoslavia  1960   \n",
      "143  RUS_32_1977.txt                                Russia  1977   \n",
      "311  RUS_23_1968.txt                                Russia  1968   \n",
      "758  CHL_28_1973.txt                                 Chile  1973   \n",
      "438  MYS_19_1964.txt                              Malaysia  1964   \n",
      "347  RUS_26_1971.txt                                Russia  1971   \n",
      "630  ISR_24_1969.txt                                Israel  1969   \n",
      "529  RUS_20_1965.txt                                Russia  1965   \n",
      "308  UKR_19_1964.txt                               Ukraine  1964   \n",
      "72   IRQ_15_1960.txt                                  Iraq  1960   \n",
      "212  ALB_27_1972.txt                               Albania  1972   \n",
      "736  RUS_33_1978.txt                                Russia  1978   \n",
      "376  FJI_27_1972.txt                                  Fiji  1972   \n",
      "304  RUS_29_1974.txt                                Russia  1974   \n",
      "95   CYP_19_1964.txt                                Cyprus  1964   \n",
      "173  MUS_40_1985.txt                             Mauritius  1985   \n",
      "5    GRC_38_1983.txt                                Greece  1983   \n",
      "\n",
      "     speech_length_words  \n",
      "636                 9616  \n",
      "242                 9300  \n",
      "495                 9219  \n",
      "168                 8884  \n",
      "143                 8297  \n",
      "311                 7788  \n",
      "758                 7758  \n",
      "438                 7703  \n",
      "347                 7582  \n",
      "630                 7514  \n",
      "529                 7450  \n",
      "308                 7431  \n",
      "72                  7221  \n",
      "212                 6923  \n",
      "736                 6910  \n",
      "376                 6823  \n",
      "304                 6735  \n",
      "95                  6672  \n",
      "173                 6530  \n",
      "5                   6495  \n"
     ]
    }
   ],
   "source": [
    "# Calculate average length\n",
    "avg_length = df_raw['speech_length_words'].mean()\n",
    "\n",
    "# Print it\n",
    "print(\"Average speech length (words):\", round(avg_length, 2))\n",
    "\n",
    "# 20 shortest speeches\n",
    "print(\"20 shortest speeches:\")\n",
    "print(df_raw.nsmallest(20, 'speech_length_words')[['filename', 'country_name', 'year', 'speech_length_words']])\n",
    "\n",
    "# 20 longest speeches\n",
    "print(\"\\n20 longest speeches:\")\n",
    "print(df_raw.nlargest(20, 'speech_length_words')[['filename', 'country_name', 'year', 'speech_length_words']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424f152-b786-45bf-bf86-e169a900f628",
   "metadata": {},
   "source": [
    "### New variable: English as Official Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "d1d2c2a6-23d6-4044-84cc-8785167e6b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries not matched in df_raw['country_name']:\n",
      "Anguilla\n",
      "Bermuda\n",
      "British Virgin Islands\n",
      "Cayman Islands\n",
      "Christmas Island\n",
      "Cook Islands\n",
      "Hong Kong\n",
      "Jersey\n",
      "Niue\n",
      "Norfolk Island\n",
      "Northern Mariana Islands\n",
      "Pitcairn Islands\n",
      "Sint Maarten\n",
      "South Sudan\n",
      "Turks and Caicos Islands\n"
     ]
    }
   ],
   "source": [
    "# Source for english as official language : https://gradschool.utk.edu/future-students/office-of-graduate-admissions/applying-to-graduate-school/admission-requirements/testing-requirements/countries-with-english-as-official-language/\n",
    "# They are quoting: https://www.cia.gov/the-world-factbook/field/languages/\n",
    "\n",
    "english_countries = [\n",
    "    \"Anguilla\", \"Antigua and Barbuda\", \"Bahamas\", \"Barbados\", \"Belize\", \"Belgium\",\n",
    "    \"Bermuda\", \"Botswana\", \"British Virgin Islands\", \"Burundi\", \"Cameroon\", \"Canada\",\n",
    "    \"Cayman Islands\", \"Christmas Island\", \"Cook Islands\", \"Dominica\", \"Fiji\", \"Gambia\",\n",
    "    \"Ghana\", \"Grenada\", \"Guyana\", \"Hong Kong\", \"India\", \"Ireland\", \"Jersey\", \"Kenya\",\n",
    "    \"Liberia\", \"Malawi\", \"Malta\", \"Marshall Islands\", \"Micronesia\",\n",
    "    \"Namibia\", \"New Zealand\", \"Nigeria\", \"Niue\", \"Norfolk Island\", \"Northern Mariana Islands\",\n",
    "    \"Pakistan\", \"Palau\", \"Papua New Guinea\", \"Philippines\", \"Pitcairn Islands\", \"Rwanda\",\n",
    "    \"Saint Kitts and Nevis\", \"Saint Lucia\", \"Samoa\", \"Seychelles\", \"Sierra Leone\", \"Singapore\",\n",
    "    \"Sint Maarten\", \"Solomon Islands\", \"Somalia\", \"South Africa\", \"South Sudan\", \"Sudan\",\n",
    "    \"Eswatini\", \"Tanzania\", \"Tonga\", \"Trinidad and Tobago\", \"Turks and Caicos Islands\",\n",
    "    \"Tuvalu\", \"Uganda\", \"Zambia\", \"Zimbabwe\"\n",
    "]\n",
    "\n",
    "# 2. Create dummy column\n",
    "df_raw['english_official_language'] = df_raw['country_name'].apply(\n",
    "    lambda x: 1 if x in english_countries else 0\n",
    ")\n",
    "\n",
    "# 3. Find countries in the list that did not match any entry in df_raw\n",
    "matched = set(df_raw['country_name'])\n",
    "unmatched = [country for country in english_countries if country not in matched]\n",
    "\n",
    "# 4. Print unmatched country names\n",
    "print(\"Countries not matched in df_raw['country_name']:\")\n",
    "for country in unmatched:\n",
    "    print(country)\n",
    "\n",
    "# All of these countries are either British Overseas Territories, Australian Territories, self-governing island territories or Special Administrative Regions\n",
    "    # None of the unmatched regions are UN Members\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "f094c95f-834e-4454-9442-0d28d4a66090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "      <th>country_code</th>\n",
       "      <th>year</th>\n",
       "      <th>country_name</th>\n",
       "      <th>speech_length_words</th>\n",
       "      <th>english_official_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TTO_73_2018.txt</td>\n",
       "      <td>I am honoured to address this eminent gatherin...</td>\n",
       "      <td>TTO</td>\n",
       "      <td>2018</td>\n",
       "      <td>Trinidad and Tobago</td>\n",
       "      <td>2698</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BRB_45_1990.txt</td>\n",
       "      <td>ï»¿Mr. President, I am pleased to join those who...</td>\n",
       "      <td>BRB</td>\n",
       "      <td>1990</td>\n",
       "      <td>Barbados</td>\n",
       "      <td>2263</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VCT_44_1989.txt</td>\n",
       "      <td>ï»¿The delegation of Saint Vincent and the Grena...</td>\n",
       "      <td>VCT</td>\n",
       "      <td>1989</td>\n",
       "      <td>Saint Vincent and the Grenadines</td>\n",
       "      <td>2212</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CRI_22_1967.txt</td>\n",
       "      <td>98. In an important speech made at the Univers...</td>\n",
       "      <td>CRI</td>\n",
       "      <td>1967</td>\n",
       "      <td>Costa Rica</td>\n",
       "      <td>3124</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YEM_46_1991.txt</td>\n",
       "      <td>ï»¿It gives me pleasure to congratulate the Pres...</td>\n",
       "      <td>YEM</td>\n",
       "      <td>1991</td>\n",
       "      <td>Yemen</td>\n",
       "      <td>2369</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech  \\\n",
       "0  TTO_73_2018.txt  I am honoured to address this eminent gatherin...   \n",
       "1  BRB_45_1990.txt  ï»¿Mr. President, I am pleased to join those who...   \n",
       "2  VCT_44_1989.txt  ï»¿The delegation of Saint Vincent and the Grena...   \n",
       "3  CRI_22_1967.txt  98. In an important speech made at the Univers...   \n",
       "4  YEM_46_1991.txt  ï»¿It gives me pleasure to congratulate the Pres...   \n",
       "\n",
       "  country_code  year                      country_name  speech_length_words  \\\n",
       "0          TTO  2018               Trinidad and Tobago                 2698   \n",
       "1          BRB  1990                          Barbados                 2263   \n",
       "2          VCT  1989  Saint Vincent and the Grenadines                 2212   \n",
       "3          CRI  1967                        Costa Rica                 3124   \n",
       "4          YEM  1991                             Yemen                 2369   \n",
       "\n",
       "   english_official_language  \n",
       "0                          1  \n",
       "1                          1  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          0  "
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82540d8a-b656-45fb-aeb0-2fd5b4d7108a",
   "metadata": {},
   "source": [
    "### New variable: permanent member security council"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "4fb92fe4-5c22-43fa-8b48-62ec2240d716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define permanent members of the UN Security Council\n",
    "permanent_members = ['RUS', 'USA', 'FRA', 'GBR', 'CHN']\n",
    "\n",
    "# Create dummy variable\n",
    "df_raw['security_council_permanent'] = df_raw['country_code'].isin(permanent_members).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "6b7ed94c-7369-4b52-a84e-21bbdccd51ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    country_code    country_name  security_council_permanent  year\n",
      "44           RUS          Russia                           1  1995\n",
      "65           USA   United States                           1  1957\n",
      "111          USA   United States                           1  1965\n",
      "143          RUS          Russia                           1  1977\n",
      "148          FRA          France                           1  2000\n",
      "183          USA   United States                           1  2006\n",
      "197          USA   United States                           1  1947\n",
      "209          GBR  United Kingdom                           1  1986\n",
      "271          USA   United States                           1  2015\n",
      "302          GBR  United Kingdom                           1  2021\n",
      "304          RUS          Russia                           1  1974\n",
      "311          RUS          Russia                           1  1968\n",
      "347          RUS          Russia                           1  1971\n",
      "387          GBR  United Kingdom                           1  2010\n",
      "388          GBR  United Kingdom                           1  1952\n",
      "412          USA   United States                           1  1991\n",
      "437          CHN           China                           1  2011\n",
      "463          FRA          France                           1  1992\n",
      "471          GBR  United Kingdom                           1  2023\n",
      "480          RUS          Russia                           1  2017\n",
      "490          GBR  United Kingdom                           1  1965\n",
      "514          FRA          France                           1  1987\n",
      "529          RUS          Russia                           1  1965\n",
      "549          GBR  United Kingdom                           1  1961\n",
      "594          USA   United States                           1  1960\n",
      "614          FRA          France                           1  1999\n",
      "631          CHN           China                           1  1985\n",
      "633          CHN           China                           1  2004\n",
      "685          USA   United States                           1  1978\n",
      "708          GBR  United Kingdom                           1  1973\n",
      "713          GBR  United Kingdom                           1  1972\n",
      "718          FRA          France                           1  1951\n",
      "736          RUS          Russia                           1  1978\n",
      "766          FRA          France                           1  1955\n",
      "783          RUS          Russia                           1  1950\n"
     ]
    }
   ],
   "source": [
    "print(df_raw[df_raw['country_code'].isin(permanent_members)][\n",
    "    ['country_code', 'country_name', 'security_council_permanent', 'year']\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523547d9-9b10-473c-8c85-d1e2fc4fd3e1",
   "metadata": {},
   "source": [
    "### New variables: speaker & position & gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "213e961b-816c-43f1-8ad6-a17bae6cca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_c)\n",
    "df_speakers = pd.read_excel(r\"data_original\\UN General Debate Corpus\\Speakers_by_session.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "cd158957-391b-4b86-9160-7c606336bacc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Session</th>\n",
       "      <th>ISO Code</th>\n",
       "      <th>Country</th>\n",
       "      <th>Name of Person Speaking</th>\n",
       "      <th>Post</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>78</td>\n",
       "      <td>BRA</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Luiz Inacio Lula da Silva</td>\n",
       "      <td>President</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023</td>\n",
       "      <td>78</td>\n",
       "      <td>USA</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>Joseph R. Biden</td>\n",
       "      <td>President</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023</td>\n",
       "      <td>78</td>\n",
       "      <td>COL</td>\n",
       "      <td>Colombia</td>\n",
       "      <td>Gustavo Petro Urrego</td>\n",
       "      <td>President</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023</td>\n",
       "      <td>78</td>\n",
       "      <td>JOR</td>\n",
       "      <td>Jordan</td>\n",
       "      <td>Abdullah II ibn Al Hussein</td>\n",
       "      <td>King</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023</td>\n",
       "      <td>78</td>\n",
       "      <td>POL</td>\n",
       "      <td>Poland</td>\n",
       "      <td>Andrzej Duda</td>\n",
       "      <td>President</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Session ISO Code                   Country  \\\n",
       "0  2023       78      BRA                   Brazil    \n",
       "1  2023       78      USA  United States of America   \n",
       "2  2023       78      COL                  Colombia   \n",
       "3  2023       78      JOR                    Jordan   \n",
       "4  2023       78      POL                    Poland   \n",
       "\n",
       "      Name of Person Speaking       Post Unnamed: 6  \n",
       "0   Luiz Inacio Lula da Silva  President        NaN  \n",
       "1             Joseph R. Biden  President        NaN  \n",
       "2        Gustavo Petro Urrego  President        NaN  \n",
       "3  Abdullah II ibn Al Hussein       King        NaN  \n",
       "4                Andrzej Duda  President        NaN  "
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_speakers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "18be2d52-b8d1-4cb1-a74d-491a523f26c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "      <th>country_code</th>\n",
       "      <th>year</th>\n",
       "      <th>country_name</th>\n",
       "      <th>speech_length_words</th>\n",
       "      <th>english_official_language</th>\n",
       "      <th>security_council_permanent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TTO_73_2018.txt</td>\n",
       "      <td>I am honoured to address this eminent gatherin...</td>\n",
       "      <td>TTO</td>\n",
       "      <td>2018</td>\n",
       "      <td>Trinidad and Tobago</td>\n",
       "      <td>2698</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BRB_45_1990.txt</td>\n",
       "      <td>ï»¿Mr. President, I am pleased to join those who...</td>\n",
       "      <td>BRB</td>\n",
       "      <td>1990</td>\n",
       "      <td>Barbados</td>\n",
       "      <td>2263</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VCT_44_1989.txt</td>\n",
       "      <td>ï»¿The delegation of Saint Vincent and the Grena...</td>\n",
       "      <td>VCT</td>\n",
       "      <td>1989</td>\n",
       "      <td>Saint Vincent and the Grenadines</td>\n",
       "      <td>2212</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CRI_22_1967.txt</td>\n",
       "      <td>98. In an important speech made at the Univers...</td>\n",
       "      <td>CRI</td>\n",
       "      <td>1967</td>\n",
       "      <td>Costa Rica</td>\n",
       "      <td>3124</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YEM_46_1991.txt</td>\n",
       "      <td>ï»¿It gives me pleasure to congratulate the Pres...</td>\n",
       "      <td>YEM</td>\n",
       "      <td>1991</td>\n",
       "      <td>Yemen</td>\n",
       "      <td>2369</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech  \\\n",
       "0  TTO_73_2018.txt  I am honoured to address this eminent gatherin...   \n",
       "1  BRB_45_1990.txt  ï»¿Mr. President, I am pleased to join those who...   \n",
       "2  VCT_44_1989.txt  ï»¿The delegation of Saint Vincent and the Grena...   \n",
       "3  CRI_22_1967.txt  98. In an important speech made at the Univers...   \n",
       "4  YEM_46_1991.txt  ï»¿It gives me pleasure to congratulate the Pres...   \n",
       "\n",
       "  country_code  year                      country_name  speech_length_words  \\\n",
       "0          TTO  2018               Trinidad and Tobago                 2698   \n",
       "1          BRB  1990                          Barbados                 2263   \n",
       "2          VCT  1989  Saint Vincent and the Grenadines                 2212   \n",
       "3          CRI  1967                        Costa Rica                 3124   \n",
       "4          YEM  1991                             Yemen                 2369   \n",
       "\n",
       "   english_official_language  security_council_permanent  \n",
       "0                          1                           0  \n",
       "1                          1                           0  \n",
       "2                          0                           0  \n",
       "3                          0                           0  \n",
       "4                          0                           0  "
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "170a6cc9-fa93-454c-9f81-f489a84afe1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [filename, speech, country_code, year, country_name, speech_length_words, english_official_language, security_council_permanent]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(df_raw[(df_raw['country_code'] == 'MEX') & (df_raw['year'] == 1982)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "672de39e-93ba-4808-82fd-f92b122f0843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "      <th>country_code</th>\n",
       "      <th>year</th>\n",
       "      <th>country_name</th>\n",
       "      <th>speech_length_words</th>\n",
       "      <th>english_official_language</th>\n",
       "      <th>security_council_permanent</th>\n",
       "      <th>Year</th>\n",
       "      <th>ISO Code</th>\n",
       "      <th>Name of Person Speaking</th>\n",
       "      <th>Post</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TTO_73_2018.txt</td>\n",
       "      <td>I am honoured to address this eminent gatherin...</td>\n",
       "      <td>TTO</td>\n",
       "      <td>2018</td>\n",
       "      <td>Trinidad and Tobago</td>\n",
       "      <td>2698</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>TTO</td>\n",
       "      <td>Mr. Denis Moses</td>\n",
       "      <td>Minsiter for Foreign and CARICOM Affairs</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BRB_45_1990.txt</td>\n",
       "      <td>ï»¿Mr. President, I am pleased to join those who...</td>\n",
       "      <td>BRB</td>\n",
       "      <td>1990</td>\n",
       "      <td>Barbados</td>\n",
       "      <td>2263</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>BRB</td>\n",
       "      <td>KING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VCT_44_1989.txt</td>\n",
       "      <td>ï»¿The delegation of Saint Vincent and the Grena...</td>\n",
       "      <td>VCT</td>\n",
       "      <td>1989</td>\n",
       "      <td>Saint Vincent and the Grenadines</td>\n",
       "      <td>2212</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>VCT</td>\n",
       "      <td>NANTON</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CRI_22_1967.txt</td>\n",
       "      <td>98. In an important speech made at the Univers...</td>\n",
       "      <td>CRI</td>\n",
       "      <td>1967</td>\n",
       "      <td>Costa Rica</td>\n",
       "      <td>3124</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1967.0</td>\n",
       "      <td>CRI</td>\n",
       "      <td>Mr. LARA BASTAMANTE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YEM_46_1991.txt</td>\n",
       "      <td>ï»¿It gives me pleasure to congratulate the Pres...</td>\n",
       "      <td>YEM</td>\n",
       "      <td>1991</td>\n",
       "      <td>Yemen</td>\n",
       "      <td>2369</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>YEM</td>\n",
       "      <td>AL-ERYANY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech  \\\n",
       "0  TTO_73_2018.txt  I am honoured to address this eminent gatherin...   \n",
       "1  BRB_45_1990.txt  ï»¿Mr. President, I am pleased to join those who...   \n",
       "2  VCT_44_1989.txt  ï»¿The delegation of Saint Vincent and the Grena...   \n",
       "3  CRI_22_1967.txt  98. In an important speech made at the Univers...   \n",
       "4  YEM_46_1991.txt  ï»¿It gives me pleasure to congratulate the Pres...   \n",
       "\n",
       "  country_code  year                      country_name  speech_length_words  \\\n",
       "0          TTO  2018               Trinidad and Tobago                 2698   \n",
       "1          BRB  1990                          Barbados                 2263   \n",
       "2          VCT  1989  Saint Vincent and the Grenadines                 2212   \n",
       "3          CRI  1967                        Costa Rica                 3124   \n",
       "4          YEM  1991                             Yemen                 2369   \n",
       "\n",
       "   english_official_language  security_council_permanent    Year ISO Code  \\\n",
       "0                          1                           0  2018.0      TTO   \n",
       "1                          1                           0  1990.0      BRB   \n",
       "2                          0                           0  1989.0      VCT   \n",
       "3                          0                           0  1967.0      CRI   \n",
       "4                          0                           0  1991.0      YEM   \n",
       "\n",
       "  Name of Person Speaking                                      Post _merge  \n",
       "0         Mr. Denis Moses  Minsiter for Foreign and CARICOM Affairs   both  \n",
       "1                    KING                                       NaN   both  \n",
       "2                 NANTON                                        NaN   both  \n",
       "3    Mr. LARA BASTAMANTE                                        NaN   both  \n",
       "4              AL-ERYANY                                        NaN   both  "
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged = df_raw.merge(\n",
    "    df_speakers[['Year', 'ISO Code', 'Name of Person Speaking', 'Post']],\n",
    "    left_on=['year', 'country_code'],\n",
    "    right_on=['Year', 'ISO Code'],\n",
    "    how='left',\n",
    "    indicator=True)\n",
    "\n",
    "df_merged.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "604ceb1f-a9fc-49ec-aca8-fdc28ad7e73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            filename  year country_code  country_name\n",
      "59   MLI_18_1963.txt  1963          MLI          Mali\n",
      "210  KWT_49_1994.txt  1994          KWT        Kuwait\n",
      "229  MOZ_49_1994.txt  1994          MOZ    Mozambique\n",
      "287  DNK_24_1969.txt  1969          DNK       Denmark\n",
      "334  GIN_36_1981.txt  1981          GIN        Guinea\n",
      "583  MEX_18_1963.txt  1963          MEX        Mexico\n",
      "585  BFA_18_1963.txt  1963          BFA  Burkina Faso\n",
      "696  GTM_26_1971.txt  1971          GTM     Guatemala\n",
      "709  YMD_40_1985.txt  1985          YMD   South Yemen\n",
      "743  YMD_31_1976.txt  1976          YMD   South Yemen\n"
     ]
    }
   ],
   "source": [
    "# Merge with indicator and set unmatched rows to NA\n",
    "df_merged = df_raw.merge(\n",
    "    df_speakers[['Year', 'ISO Code', 'Name of Person Speaking', 'Post']],\n",
    "    left_on=['year', 'country_code'],\n",
    "    right_on=['Year', 'ISO Code'],\n",
    "    how='left',\n",
    "    indicator=True)\n",
    "\n",
    "# Get rows with no match in df_speakers\n",
    "unmatched = df_merged[df_merged['_merge'] == 'left_only']\n",
    "\n",
    "# Print unmatched rows with selected columns (panda sets them to NA by default)\n",
    "print(unmatched[['filename', 'year', 'country_code', 'country_name']])\n",
    "\n",
    "\n",
    "# Drop the '_merge' column from merged df\n",
    "df_merged = df_merged.drop(columns=['Year', 'ISO Code', '_merge'])\n",
    "\n",
    "# Rename columns\n",
    "df_merged = df_merged.rename(columns={\n",
    "    'Name of Person Speaking': 'speaker_name',\n",
    "    'Post': 'position'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "c247f434-ab90-47ce-8c29-f40c65753755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gender_dummy \n",
    "df_merged['gender_dummy'] = df_merged['speaker_name'].apply(\n",
    "    lambda name: 0 if pd.notnull(name) and re.search(r'^(?:Mr|Sir)\\b', name, re.IGNORECASE)\n",
    "    else 1 if pd.notnull(name) and re.search(r'^(?:Mrs|Ms)\\b', name, re.IGNORECASE)\n",
    "    else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "452d80fd-3f71-41d5-9373-cc30ccbe5992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    gender_dummy  count\n",
      "0       0 (male)    337\n",
      "1     1 (female)     13\n",
      "2  NaN (unknown)    450\n"
     ]
    }
   ],
   "source": [
    "# Count all values including NaN\n",
    "counts = df_merged['gender_dummy'].value_counts(dropna=False)\n",
    "\n",
    "# Build summary using .get() to handle missing keys\n",
    "gender_summary = pd.DataFrame({\n",
    "    'gender_dummy': ['0 (male)', '1 (female)', 'NaN (unknown)'],\n",
    "    'count': [\n",
    "        counts.get(0, 0),\n",
    "        counts.get(1, 0),\n",
    "        counts.get(np.nan, 0)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(gender_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "2f9ba91d-cf5c-43e7-9fae-e35c868fbdf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speech</th>\n",
       "      <th>country_code</th>\n",
       "      <th>year</th>\n",
       "      <th>country_name</th>\n",
       "      <th>speech_length_words</th>\n",
       "      <th>english_official_language</th>\n",
       "      <th>security_council_permanent</th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>position</th>\n",
       "      <th>gender_dummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TTO_73_2018.txt</td>\n",
       "      <td>I am honoured to address this eminent gatherin...</td>\n",
       "      <td>TTO</td>\n",
       "      <td>2018</td>\n",
       "      <td>Trinidad and Tobago</td>\n",
       "      <td>2698</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Mr. Denis Moses</td>\n",
       "      <td>Minsiter for Foreign and CARICOM Affairs</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BRB_45_1990.txt</td>\n",
       "      <td>ï»¿Mr. President, I am pleased to join those who...</td>\n",
       "      <td>BRB</td>\n",
       "      <td>1990</td>\n",
       "      <td>Barbados</td>\n",
       "      <td>2263</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>KING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VCT_44_1989.txt</td>\n",
       "      <td>ï»¿The delegation of Saint Vincent and the Grena...</td>\n",
       "      <td>VCT</td>\n",
       "      <td>1989</td>\n",
       "      <td>Saint Vincent and the Grenadines</td>\n",
       "      <td>2212</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NANTON</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CRI_22_1967.txt</td>\n",
       "      <td>98. In an important speech made at the Univers...</td>\n",
       "      <td>CRI</td>\n",
       "      <td>1967</td>\n",
       "      <td>Costa Rica</td>\n",
       "      <td>3124</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mr. LARA BASTAMANTE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YEM_46_1991.txt</td>\n",
       "      <td>ï»¿It gives me pleasure to congratulate the Pres...</td>\n",
       "      <td>YEM</td>\n",
       "      <td>1991</td>\n",
       "      <td>Yemen</td>\n",
       "      <td>2369</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>AL-ERYANY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename                                             speech  \\\n",
       "0  TTO_73_2018.txt  I am honoured to address this eminent gatherin...   \n",
       "1  BRB_45_1990.txt  ï»¿Mr. President, I am pleased to join those who...   \n",
       "2  VCT_44_1989.txt  ï»¿The delegation of Saint Vincent and the Grena...   \n",
       "3  CRI_22_1967.txt  98. In an important speech made at the Univers...   \n",
       "4  YEM_46_1991.txt  ï»¿It gives me pleasure to congratulate the Pres...   \n",
       "\n",
       "  country_code  year                      country_name  speech_length_words  \\\n",
       "0          TTO  2018               Trinidad and Tobago                 2698   \n",
       "1          BRB  1990                          Barbados                 2263   \n",
       "2          VCT  1989  Saint Vincent and the Grenadines                 2212   \n",
       "3          CRI  1967                        Costa Rica                 3124   \n",
       "4          YEM  1991                             Yemen                 2369   \n",
       "\n",
       "   english_official_language  security_council_permanent  \\\n",
       "0                          1                           0   \n",
       "1                          1                           0   \n",
       "2                          0                           0   \n",
       "3                          0                           0   \n",
       "4                          0                           0   \n",
       "\n",
       "           speaker_name                                  position  \\\n",
       "0       Mr. Denis Moses  Minsiter for Foreign and CARICOM Affairs   \n",
       "1                  KING                                       NaN   \n",
       "2               NANTON                                        NaN   \n",
       "3  Mr. LARA BASTAMANTE                                        NaN   \n",
       "4            AL-ERYANY                                        NaN   \n",
       "\n",
       "   gender_dummy  \n",
       "0           0.0  \n",
       "1           NaN  \n",
       "2           NaN  \n",
       "3           0.0  \n",
       "4           NaN  "
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "94a901c8-3fc4-4b33-a1ff-cede22bd196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store df_merged\n",
    "\n",
    "os.chdir(wd)\n",
    "\n",
    "# Save df_merged as a pickle file for quick future loading\n",
    "merged_pickle_path = r\".\\data\\un_corpus_merged.pkl\"\n",
    "df_merged.to_pickle(merged_pickle_path)\n",
    "\n",
    "# Export df as CSV \n",
    "merged_output_path = r\".\\data\\un_corpus_merged.csv\"\n",
    "df_merged.to_csv(merged_output_path, index=False, sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe0f4af-e939-4100-9618-5b522bdd72a1",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2661f431-a194-47d1-9a1c-89162bf85a91",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "cd37e08d-b4f2-459e-823c-3816a825ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Clean text by removing empty spaces, line breaks, hyphenation, stray characters, and escape quote ==\n",
    "\n",
    "# Define cleaning function\n",
    "def clean_text(content):\n",
    "    if pd.isna(content):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove line breaks and carriage returns\n",
    "    content = content.replace('\\n', ' ').replace('\\r', ' ')\n",
    "\n",
    "    # Collapse multiple spaces\n",
    "    content = ' '.join(content.split())\n",
    "\n",
    "    # Fix punctuation spacing (e.g. \"word,another\" â†’ \"word, another\")\n",
    "    content = re.sub(r'(?<=[.,])(?=[^\\s])', r' ', content)\n",
    "\n",
    "    # Remove hyphenation at line breaks (e.g. \"inter- national\" â†’ \"international\")\n",
    "    content = re.sub(r'-\\s', '', content)\n",
    "\n",
    "    ############NEW\n",
    "\n",
    "     # Replace hyphen between letters with a space to prevent merging words (e.g. \"russian-and\" â†’ \"russian and\")\n",
    "    content = re.sub(r'(?<=\\w)-(?=\\w)', ' ', content)\n",
    "    #################NEW\n",
    "\n",
    "    # Remove stray backslashes\n",
    "    content = content.replace(\"\\\\\", \"\")\n",
    "\n",
    "    # Escape double quotes for CSV safety\n",
    "    content = content.replace('\"', '\"\"')\n",
    "\n",
    "    return content\n",
    "\n",
    "# Apply cleaning to each speech\n",
    "df_raw['speech'] = df_raw['speech'].astype(str)  # Ensure column is string type\n",
    "df_clean = df_raw.copy()\n",
    "df_clean['speech'] = df_clean['speech'].apply(clean_text)\n",
    "\n",
    "# Drop rows with empty speeches after cleaning\n",
    "df_clean = df_clean[df_clean['speech'].str.strip().astype(bool)].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "7c6bd148-d5cf-4eb3-a66d-fc706e99c13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved clean speeches chunks in 'C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp'\n"
     ]
    }
   ],
   "source": [
    "# == Split cleaned data into chunks and save as separate files ==\n",
    "\n",
    "# Convert cleaned DataFrame to list of lists\n",
    "clean_data = df_clean[['filename', 'speech']].values.tolist()\n",
    "\n",
    "# Split cleaned data into 4 equal chunks\n",
    "data_id1 = clean_data[:int(len(clean_data)/4)]\n",
    "data_id2 = clean_data[int(len(clean_data)/4): int(2*len(clean_data)/4)]\n",
    "data_id3 = clean_data[int(2*len(clean_data)/4): int(3*len(clean_data)/4)]\n",
    "data_id4 = clean_data[int(3*len(clean_data)/4):]\n",
    "\n",
    "# Change directory to the temp folder\n",
    "os.chdir(data_temp)  # make sure `data_temp` exists and is defined\n",
    "\n",
    "# Save each chunk with joblib\n",
    "joblib.dump(data_id1, 'cleanspeeches_indexed1.pkl')\n",
    "joblib.dump(data_id2, 'cleanspeeches_indexed2.pkl')\n",
    "joblib.dump(data_id3, 'cleanspeeches_indexed3.pkl')\n",
    "joblib.dump(data_id4, 'cleanspeeches_indexed4.pkl')\n",
    "\n",
    "# Store list of cleaned data chunk paths to feed into preprocessing function later\n",
    "data_files = [\n",
    "    os.path.join(data_temp, 'cleanspeeches_indexed1.pkl'),\n",
    "    os.path.join(data_temp, 'cleanspeeches_indexed2.pkl'),\n",
    "    os.path.join(data_temp, 'cleanspeeches_indexed3.pkl'),\n",
    "    os.path.join(data_temp, 'cleanspeeches_indexed4.pkl')\n",
    "]\n",
    "\n",
    "print(f\"âœ… Saved clean speeches chunks in '{data_temp}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6e5109-8cf7-41e3-8fb9-8e7dde29b4e0",
   "metadata": {},
   "source": [
    "### Advanced Text Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "0aa75944-4b75-4aa4-9430-9aa6bd9ac934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Define function to Tokenize, eliminate digits, remove stopwords, lemmatize, POS-Tagging ==\n",
    "\n",
    "def pro1(lista):\n",
    "    # Remove punctuation\n",
    "    return [[row[0], row[1].translate(translator)] for row in lista]\n",
    "\n",
    "def pro2(lista):\n",
    "    # Tokenize and lowercase with gensim\n",
    "    return [[row[0], gensim.utils.simple_preprocess(row[1])] for row in lista]\n",
    "\n",
    "def pro3(lista):\n",
    "    # Remove digits\n",
    "        return [[row[0], [w for w in row[1] if not any(char.isdigit() for char in w)]] for row in lista]\n",
    "\n",
    "def pro4(lista):\n",
    "    # Drop short words\n",
    "    return [[row[0], [w for w in row[1] if len(w) > 2]] for row in lista]\n",
    "\n",
    "def tags_spacy(lista):\n",
    "    texts = [' '.join(row[1]) for row in lista]\n",
    "    docs = list(nlp.pipe(texts, batch_size=20, n_process=1))\n",
    "    result = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        filtered_tokens = [token.text for token in doc if token.tag_.startswith(('N', 'V', 'J'))]\n",
    "        result.append([lista[i][0], filtered_tokens])\n",
    "    return result\n",
    "\n",
    "\n",
    "def pro5(lista):\n",
    "    # Remove stopwords using SpaCy stopword list\n",
    "    return [[row[0], [w for w in row[1] if w not in SPACY_STOPWORDS]] for row in lista]\n",
    "\n",
    "def pro6(lista):\n",
    "      return [\n",
    "        [row[0], [stemmer.stem(token) for token in row[1]]]\n",
    "        for row in lista\n",
    "    ]\n",
    "   # texts = [' '.join(row[1]) for row in lista]\n",
    "   # docs = list(nlp.pipe(texts, batch_size=20, n_process=1))\n",
    "   # result = []\n",
    "   # for i, doc in enumerate(docs):\n",
    "    # lemmatized = [token.lemma_ for token in doc]\n",
    "     #    result.append([lista[i][0], lemmatized])\n",
    "  #  return result\n",
    "\n",
    "########################## Question for Max: They removed procedural words in the paper\n",
    "\n",
    "def dropnull(lista):\n",
    "    # Drop empty speeches\n",
    "    return [row for row in lista if len(' '.join(row[1])) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "524a274a-004c-4d87-9bb3-5f789a42b90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed1.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed1.pkl] Before tagging: 2.93s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed1.pkl] After tagging: 65.64s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed1.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed1.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed1.pkl] Done. Total time: 76.19s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed2.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed2.pkl] Before tagging: 2.50s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed2.pkl] After tagging: 56.28s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed2.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed2.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed2.pkl] Done. Total time: 66.20s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed3.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed3.pkl] Before tagging: 2.76s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed3.pkl] After tagging: 56.12s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed3.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed3.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed3.pkl] Done. Total time: 67.53s\n",
      "\n",
      "Starting preprocessing for C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed4.pkl...\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed4.pkl] Before tagging: 2.78s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed4.pkl] After tagging: 63.51s\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed4.pkl] Saved stemmed version: C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed4.pkl\n",
      "[C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\\cleanspeeches_indexed4.pkl] Done. Total time: 73.79s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# == Create full pre-processing function and call it\n",
    "\n",
    "def preprocessing(data_name):\n",
    "    t0 = time.time()\n",
    "    print(f\"Starting preprocessing for {data_name}...\")\n",
    "\n",
    "    data = joblib.load(data_name)\n",
    "    data = pro1(data)\n",
    "    data = pro2(data)\n",
    "    data = pro3(data)\n",
    "    data = pro4(data)\n",
    "\n",
    "    print(f\"[{data_name}] Before tagging: {time.time() - t0:.2f}s\")\n",
    "    data = tags_spacy(data)\n",
    "    print(f\"[{data_name}] After tagging: {time.time() - t0:.2f}s\")\n",
    "\n",
    "    data = pro5(data)\n",
    "   # data = pro6(data)\n",
    "    data = dropnull(data)\n",
    "\n",
    "    # out_name = data_name.replace('cleanspeeches_', 'preprocessed_speeches_').replace('.pkl', '_temp.pkl')\n",
    "\n",
    "    # Store preprocessed corupus (before stemming) for wordcloud\n",
    "    filename_wordcloud = data_name.replace('cleanspeeches_', 'wordcloud_speeches_').replace('.pkl', '.pkl')\n",
    "    out_name_wordcloud = os.path.join(data_preprocessed, os.path.basename(filename_wordcloud))\n",
    "    joblib.dump(data, out_name_wordcloud)\n",
    "\n",
    "    # Apply stemming\n",
    "    data_stemmed = pro6(data)\n",
    "\n",
    "    filename_preprocessed = data_name.replace('cleanspeeches_', 'preprocessed_speeches_').replace('.pkl', '.pkl')\n",
    "    out_preprocessed = os.path.join(data_preprocessed, os.path.basename(filename_preprocessed))\n",
    "    joblib.dump(data_stemmed, out_preprocessed\n",
    "               )\n",
    "    print(f\"[{data_name}] Saved stemmed version: {out_preprocessed}\")\n",
    "\n",
    "    print(f\"[{data_name}] Done. Total time: {time.time() - t0:.2f}s\\n\")\n",
    "\n",
    "def main():\n",
    "    for fname in data_files:\n",
    "        preprocessing(fname)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "d3409987-485c-47c9-9430-01fa2c0e650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the pre-processed data\n",
    "preprocessed_files = [\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed1.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed2.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed3.pkl'),\n",
    "    os.path.join(data_preprocessed, 'preprocessed_speeches_indexed4.pkl')\n",
    "]\n",
    "\n",
    "wordcloud_files = [\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed1.pkl'),\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed2.pkl'),\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed3.pkl'),\n",
    "    os.path.join(data_preprocessed, 'wordcloud_speeches_indexed4.pkl')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "134ddbf7-9b95-4697-9d9c-f9c55b52ec80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\temp\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d006e-6a23-453b-9c91-0e60bd3f8301",
   "metadata": {},
   "source": [
    "## Word-Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf2443e-4c76-41b2-bc16-e72d91c318a8",
   "metadata": {},
   "source": [
    "### Count frequencies of all tokens and display the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "154d978d-cddd-431c-9d53-73c9faf8fdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed1.pkl: removed words with freq < 10\n",
      "Processed C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed2.pkl: removed words with freq < 10\n",
      "Processed C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed3.pkl: removed words with freq < 10\n",
      "Processed C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\\preprocessed\\preprocessed_speeches_indexed4.pkl: removed words with freq < 10\n",
      "\n",
      "[Stemmed] Top 100 most common words:\n",
      "nation: 18105\n",
      "unit: 14689\n",
      "countri: 13158\n",
      "intern: 12274\n",
      "develop: 10826\n",
      "state: 10574\n",
      "peac: 10229\n",
      "world: 10221\n",
      "peopl: 9893\n",
      "secur: 6483\n",
      "govern: 6138\n",
      "general: 6120\n",
      "econom: 5576\n",
      "organ: 5364\n",
      "right: 5129\n",
      "year: 5048\n",
      "assembl: 4905\n",
      "new: 4463\n",
      "human: 4352\n",
      "effort: 4263\n",
      "problem: 4245\n",
      "support: 4189\n",
      "continu: 3948\n",
      "polit: 3879\n",
      "communiti: 3722\n",
      "time: 3650\n",
      "region: 3583\n",
      "member: 3571\n",
      "africa: 3417\n",
      "import: 3253\n",
      "war: 3248\n",
      "council: 3135\n",
      "session: 3114\n",
      "work: 3084\n",
      "achiev: 3056\n",
      "need: 3053\n",
      "power: 2913\n",
      "principl: 2841\n",
      "republ: 2830\n",
      "situat: 2789\n",
      "conflict: 2780\n",
      "hope: 2759\n",
      "nuclear: 2688\n",
      "presid: 2687\n",
      "forc: 2677\n",
      "oper: 2656\n",
      "south: 2644\n",
      "resolut: 2624\n",
      "relat: 2613\n",
      "order: 2606\n",
      "concern: 2604\n",
      "global: 2574\n",
      "great: 2550\n",
      "solut: 2534\n",
      "establish: 2516\n",
      "action: 2493\n",
      "polici: 2413\n",
      "confer: 2389\n",
      "african: 2389\n",
      "independ: 2381\n",
      "social: 2308\n",
      "interest: 2307\n",
      "charter: 2299\n",
      "respect: 2281\n",
      "effect: 2265\n",
      "system: 2248\n",
      "weapon: 2209\n",
      "commit: 2193\n",
      "today: 2154\n",
      "deleg: 2125\n",
      "progress: 2073\n",
      "territori: 2053\n",
      "contribut: 2041\n",
      "believ: 2025\n",
      "chang: 2023\n",
      "repres: 2020\n",
      "question: 2007\n",
      "agreement: 1999\n",
      "negoti: 1973\n",
      "process: 1967\n",
      "implement: 1957\n",
      "way: 1939\n",
      "respons: 1930\n",
      "arm: 1923\n",
      "issu: 1901\n",
      "end: 1895\n",
      "secretari: 1846\n",
      "law: 1844\n",
      "cooper: 1835\n",
      "futur: 1828\n",
      "meet: 1818\n",
      "disarma: 1806\n",
      "live: 1804\n",
      "possibl: 1792\n",
      "east: 1789\n",
      "express: 1786\n",
      "mean: 1753\n",
      "remain: 1741\n",
      "posit: 1734\n",
      "wish: 1732\n",
      "\n",
      "[Stemmed] Top 300 least common words:\n",
      "actionclos: 1\n",
      "hovel: 1\n",
      "bogotÃ¡: 1\n",
      "captainci: 1\n",
      "podul: 1\n",
      "fdi: 1\n",
      "foreword: 1\n",
      "monuc: 1\n",
      "ibrahima: 1\n",
      "passant: 1\n",
      "deceas: 1\n",
      "alhaji: 1\n",
      "fiftysixth: 1\n",
      "seungsoo: 1\n",
      "desk: 1\n",
      "bang: 1\n",
      "tickl: 1\n",
      "afroasian: 1\n",
      "brewer: 1\n",
      "mischiev: 1\n",
      "rider: 1\n",
      "godmoth: 1\n",
      "fairi: 1\n",
      "homework: 1\n",
      "decrepit: 1\n",
      "thraldom: 1\n",
      "lighter: 1\n",
      "yearl: 1\n",
      "lucki: 1\n",
      "terrif: 1\n",
      "buzz: 1\n",
      "therewith: 1\n",
      "lab: 1\n",
      "hifalutin: 1\n",
      "escapad: 1\n",
      "cutback: 1\n",
      "lode: 1\n",
      "balanceof: 1\n",
      "illiquid: 1\n",
      "enfeebl: 1\n",
      "upright: 1\n",
      "nonraci: 1\n",
      "flux: 1\n",
      "mushawarah: 1\n",
      "djakarta: 1\n",
      "kommuni: 1\n",
      "countrywid: 1\n",
      "cobbold: 1\n",
      "fathomless: 1\n",
      "audibl: 1\n",
      "agroindustri: 1\n",
      "corsican: 1\n",
      "criminolog: 1\n",
      "traceca: 1\n",
      "caucasia: 1\n",
      "ingushetia: 1\n",
      "dagestan: 1\n",
      "chechnya: 1\n",
      "selfprotect: 1\n",
      "pop: 1\n",
      "shaw: 1\n",
      "jingoist: 1\n",
      "prognosi: 1\n",
      "fluent: 1\n",
      "predisposit: 1\n",
      "everexpand: 1\n",
      "famish: 1\n",
      "infest: 1\n",
      "inhospit: 1\n",
      "villaini: 1\n",
      "wean: 1\n",
      "rueful: 1\n",
      "guyanavenezuela: 1\n",
      "stiell: 1\n",
      "fiona: 1\n",
      "bate: 1\n",
      "kerryn: 1\n",
      "tether: 1\n",
      "honorari: 1\n",
      "cochairperson: 1\n",
      "reacquaint: 1\n",
      "egregi: 1\n",
      "finder: 1\n",
      "underworld: 1\n",
      "undersid: 1\n",
      "chanceri: 1\n",
      "yudhoyono: 1\n",
      "bambang: 1\n",
      "susilo: 1\n",
      "statebuild: 1\n",
      "pire: 1\n",
      "emilia: 1\n",
      "insignia: 1\n",
      "hillari: 1\n",
      "tractabl: 1\n",
      "beast: 1\n",
      "malan: 1\n",
      "voerwoerd: 1\n",
      "maledict: 1\n",
      "abstentionist: 1\n",
      "apposit: 1\n",
      "widelyheld: 1\n",
      "rag: 1\n",
      "understat: 1\n",
      "unco: 1\n",
      "infrequ: 1\n",
      "aflam: 1\n",
      "ironfist: 1\n",
      "proconsul: 1\n",
      "noxious: 1\n",
      "quiescent: 1\n",
      "procliv: 1\n",
      "psychopath: 1\n",
      "reformatori: 1\n",
      "mortar: 1\n",
      "crackl: 1\n",
      "lybia: 1\n",
      "twentyfour: 1\n",
      "lingual: 1\n",
      "spurt: 1\n",
      "signor: 1\n",
      "newgener: 1\n",
      "valuat: 1\n",
      "minutia: 1\n",
      "bazaar: 1\n",
      "mitterrand: 1\n",
      "coteri: 1\n",
      "handov: 1\n",
      "zealot: 1\n",
      "krugerrand: 1\n",
      "trunk: 1\n",
      "theolog: 1\n",
      "parenthesi: 1\n",
      "begrudg: 1\n",
      "reconvert: 1\n",
      "comecon: 1\n",
      "flinch: 1\n",
      "gendarmeri: 1\n",
      "fondamental: 1\n",
      "loi: 1\n",
      "abstin: 1\n",
      "seventeen: 1\n",
      "rectitud: 1\n",
      "amphibi: 1\n",
      "moat: 1\n",
      "mich: 1\n",
      "mintoff: 1\n",
      "lampedusa: 1\n",
      "palestian: 1\n",
      "frigat: 1\n",
      "diver: 1\n",
      "bonniei: 1\n",
      "mifsud: 1\n",
      "rhythm: 1\n",
      "alfonsin: 1\n",
      "demo: 1\n",
      "cassandra: 1\n",
      "papandreou: 1\n",
      "barri: 1\n",
      "substratum: 1\n",
      "freshen: 1\n",
      "energis: 1\n",
      "premonit: 1\n",
      "rarest: 1\n",
      "nap: 1\n",
      "ahc: 1\n",
      "symbolis: 1\n",
      "obot: 1\n",
      "counsul: 1\n",
      "gonzagu: 1\n",
      "incendiari: 1\n",
      "hack: 1\n",
      "galo: 1\n",
      "laviss: 1\n",
      "pÃ©guy: 1\n",
      "hachett: 1\n",
      "episcop: 1\n",
      "mÃ©moir: 1\n",
      "mirebalai: 1\n",
      "cabrit: 1\n",
      "sondÃ©: 1\n",
      "pont: 1\n",
      "marc: 1\n",
      "limbÃ©: 1\n",
      "plaisanc: 1\n",
      "momanc: 1\n",
      "petionvill: 1\n",
      "cottag: 1\n",
      "sunlight: 1\n",
      "inez: 1\n",
      "cleo: 1\n",
      "aztec: 1\n",
      "ghareb: 1\n",
      "zafarana: 1\n",
      "foundri: 1\n",
      "ciromium: 1\n",
      "toussaint: 1\n",
      "seyn: 1\n",
      "philipp: 1\n",
      "tubman: 1\n",
      "supin: 1\n",
      "rope: 1\n",
      "sarcasm: 1\n",
      "manna: 1\n",
      "blacker: 1\n",
      "walloon: 1\n",
      "autonomist: 1\n",
      "sanctimoni: 1\n",
      "aher: 1\n",
      "tawdri: 1\n",
      "snivel: 1\n",
      "spilt: 1\n",
      "backer: 1\n",
      "jackboot: 1\n",
      "guil: 1\n",
      "tsz: 1\n",
      "indiana: 1\n",
      "pauw: 1\n",
      "flunkey: 1\n",
      "torpor: 1\n",
      "waken: 1\n",
      "gravedigg: 1\n",
      "subthem: 1\n",
      "longerterm: 1\n",
      "albin: 1\n",
      "riven: 1\n",
      "nofli: 1\n",
      "mau: 1\n",
      "husband: 1\n",
      "kilimanjaro: 1\n",
      "vetobear: 1\n",
      "narrowest: 1\n",
      "blip: 1\n",
      "tsunamirel: 1\n",
      "ding: 1\n",
      "alvarez: 1\n",
      "aitimova: 1\n",
      "wasantham: 1\n",
      "waddakin: 1\n",
      "localgovern: 1\n",
      "vanni: 1\n",
      "idolatr: 1\n",
      "scheler: 1\n",
      "max: 1\n",
      "evatt: 1\n",
      "inexpedi: 1\n",
      "noel: 1\n",
      "fairest: 1\n",
      "oas: 1\n",
      "clos: 1\n",
      "tow: 1\n",
      "prepot: 1\n",
      "engel: 1\n",
      "colossus: 1\n",
      "iberian: 1\n",
      "stettin: 1\n",
      "priceless: 1\n",
      "phillip: 1\n",
      "java: 1\n",
      "ontong: 1\n",
      "raquel: 1\n",
      "hom: 1\n",
      "topeopl: 1\n",
      "biocultur: 1\n",
      "drawdown: 1\n",
      "unfathom: 1\n",
      "birendra: 1\n",
      "ptira: 1\n",
      "jee: 1\n",
      "ajc: 1\n",
      "reslowyear: 1\n",
      "nepales: 1\n",
      "deva: 1\n",
      "bikram: 1\n",
      "bir: 1\n",
      "birenda: 1\n",
      "muffl: 1\n",
      "boggl: 1\n",
      "unbeknown: 1\n",
      "plenipotentiari: 1\n",
      "demilitaris: 1\n",
      "kohl: 1\n",
      "solo: 1\n",
      "harmel: 1\n",
      "mainz: 1\n",
      "skubiszewski: 1\n",
      "glebe: 1\n",
      "koni: 1\n",
      "yangouvonda: 1\n",
      "inscript: 1\n",
      "hydrolog: 1\n",
      "craftsmen: 1\n",
      "countermand: 1\n",
      "ieal: 1\n",
      "fetid: 1\n",
      "unprov: 1\n",
      "smitten: 1\n",
      "counterweight: 1\n",
      "sdi: 1\n",
      "mirv: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Wordcloud] Top 100 most common words:\n",
      "nations: 14502\n",
      "united: 14477\n",
      "international: 11749\n",
      "world: 10030\n",
      "countries: 8857\n",
      "peace: 8344\n",
      "states: 7588\n",
      "development: 6381\n",
      "people: 6222\n",
      "security: 5992\n",
      "general: 5985\n",
      "economic: 5555\n",
      "assembly: 4780\n",
      "government: 4623\n",
      "new: 4463\n",
      "country: 4159\n",
      "organization: 3978\n",
      "human: 3828\n",
      "political: 3698\n",
      "peoples: 3667\n",
      "rights: 3523\n",
      "efforts: 3460\n",
      "community: 3404\n",
      "africa: 3294\n",
      "support: 3132\n",
      "council: 3062\n",
      "session: 2994\n",
      "time: 2991\n",
      "war: 2912\n",
      "republic: 2738\n",
      "nuclear: 2686\n",
      "state: 2648\n",
      "south: 2643\n",
      "years: 2575\n",
      "national: 2572\n",
      "problems: 2538\n",
      "great: 2527\n",
      "order: 2507\n",
      "year: 2470\n",
      "situation: 2452\n",
      "work: 2368\n",
      "global: 2326\n",
      "developing: 2291\n",
      "charter: 2277\n",
      "social: 2243\n",
      "african: 2228\n",
      "conference: 2175\n",
      "today: 2100\n",
      "important: 2098\n",
      "president: 2067\n",
      "hope: 2014\n",
      "relations: 1971\n",
      "weapons: 1958\n",
      "continue: 1934\n",
      "principles: 1926\n",
      "region: 1919\n",
      "need: 1905\n",
      "peaceful: 1885\n",
      "progress: 1857\n",
      "member: 1853\n",
      "respect: 1846\n",
      "operation: 1837\n",
      "system: 1836\n",
      "secretary: 1834\n",
      "future: 1825\n",
      "action: 1816\n",
      "policy: 1806\n",
      "delegation: 1803\n",
      "disarmament: 1801\n",
      "east: 1788\n",
      "solution: 1767\n",
      "process: 1763\n",
      "members: 1718\n",
      "problem: 1707\n",
      "independence: 1678\n",
      "conflict: 1657\n",
      "law: 1634\n",
      "cooperation: 1625\n",
      "end: 1610\n",
      "resolution: 1606\n",
      "way: 1602\n",
      "right: 1527\n",
      "believe: 1505\n",
      "military: 1492\n",
      "foreign: 1491\n",
      "powers: 1491\n",
      "present: 1486\n",
      "question: 1466\n",
      "democratic: 1448\n",
      "agreement: 1445\n",
      "role: 1432\n",
      "resources: 1419\n",
      "means: 1418\n",
      "possible: 1410\n",
      "union: 1388\n",
      "measures: 1364\n",
      "fact: 1352\n",
      "interests: 1331\n",
      "freedom: 1321\n",
      "trade: 1305\n",
      "\n",
      "[Wordcloud] Top 300 least common words:\n",
      "actionclosing: 1\n",
      "hovel: 1\n",
      "reds: 1\n",
      "bogotÃ¡: 1\n",
      "listeners: 1\n",
      "captaincy: 1\n",
      "podul: 1\n",
      "fdi: 1\n",
      "foreword: 1\n",
      "monuc: 1\n",
      "ibrahima: 1\n",
      "icing: 1\n",
      "gambians: 1\n",
      "passant: 1\n",
      "deceased: 1\n",
      "alhaji: 1\n",
      "thoughtfulness: 1\n",
      "gambian: 1\n",
      "fiftysixth: 1\n",
      "seungsoo: 1\n",
      "desk: 1\n",
      "bangs: 1\n",
      "tickling: 1\n",
      "afroasian: 1\n",
      "brewer: 1\n",
      "mischievous: 1\n",
      "rider: 1\n",
      "godmothers: 1\n",
      "fairy: 1\n",
      "tumbling: 1\n",
      "savages: 1\n",
      "homework: 1\n",
      "disused: 1\n",
      "decrepit: 1\n",
      "thraldom: 1\n",
      "begged: 1\n",
      "lighter: 1\n",
      "slam: 1\n",
      "yearling: 1\n",
      "lucky: 1\n",
      "sidetrack: 1\n",
      "terrific: 1\n",
      "federate: 1\n",
      "tempers: 1\n",
      "socialistic: 1\n",
      "precinct: 1\n",
      "buzzes: 1\n",
      "therewith: 1\n",
      "labs: 1\n",
      "bastardization: 1\n",
      "bastardized: 1\n",
      "amalgam: 1\n",
      "hinted: 1\n",
      "hifalutin: 1\n",
      "lodging: 1\n",
      "bully: 1\n",
      "bulldoze: 1\n",
      "escapades: 1\n",
      "feathers: 1\n",
      "cutback: 1\n",
      "lode: 1\n",
      "balanceof: 1\n",
      "illiquidity: 1\n",
      "impinging: 1\n",
      "enfeeblement: 1\n",
      "upright: 1\n",
      "nonracial: 1\n",
      "flux: 1\n",
      "mushawarah: 1\n",
      "pillaged: 1\n",
      "palars: 1\n",
      "djakarta: 1\n",
      "malayas: 1\n",
      "kommunis: 1\n",
      "advisement: 1\n",
      "bruneis: 1\n",
      "independents: 1\n",
      "countrywide: 1\n",
      "enquiries: 1\n",
      "cobbold: 1\n",
      "rahmans: 1\n",
      "stephens: 1\n",
      "collating: 1\n",
      "insinuated: 1\n",
      "fathomless: 1\n",
      "blunting: 1\n",
      "audible: 1\n",
      "extractive: 1\n",
      "agroindustrial: 1\n",
      "dysfunction: 1\n",
      "corsican: 1\n",
      "criminology: 1\n",
      "roadway: 1\n",
      "traceca: 1\n",
      "caucasia: 1\n",
      "ingushetia: 1\n",
      "dagestan: 1\n",
      "chechnya: 1\n",
      "selfprotection: 1\n",
      "pops: 1\n",
      "shaw: 1\n",
      "jingoistic: 1\n",
      "mandating: 1\n",
      "formalisms: 1\n",
      "isolates: 1\n",
      "verging: 1\n",
      "exhorted: 1\n",
      "impinged: 1\n",
      "duplicated: 1\n",
      "prognosis: 1\n",
      "fluent: 1\n",
      "nuances: 1\n",
      "predisposition: 1\n",
      "everexpanding: 1\n",
      "fetter: 1\n",
      "famished: 1\n",
      "intellectualism: 1\n",
      "morbid: 1\n",
      "infested: 1\n",
      "inhospitable: 1\n",
      "villainy: 1\n",
      "weaned: 1\n",
      "rueful: 1\n",
      "guyanavenezuela: 1\n",
      "productions: 1\n",
      "stiell: 1\n",
      "fiona: 1\n",
      "bated: 1\n",
      "kerryne: 1\n",
      "echelons: 1\n",
      "tinkered: 1\n",
      "equalizer: 1\n",
      "tethered: 1\n",
      "womans: 1\n",
      "honorary: 1\n",
      "cochairpersons: 1\n",
      "reacquaint: 1\n",
      "egregious: 1\n",
      "rug: 1\n",
      "finders: 1\n",
      "underworld: 1\n",
      "underside: 1\n",
      "rotting: 1\n",
      "chancery: 1\n",
      "yudhoyono: 1\n",
      "bambang: 1\n",
      "susilo: 1\n",
      "statebuilding: 1\n",
      "pires: 1\n",
      "emilia: 1\n",
      "insignia: 1\n",
      "hillary: 1\n",
      "tractable: 1\n",
      "reappears: 1\n",
      "beasts: 1\n",
      "disapproved: 1\n",
      "malans: 1\n",
      "voerwoerds: 1\n",
      "hurls: 1\n",
      "maledictions: 1\n",
      "rub: 1\n",
      "abstentionist: 1\n",
      "verwoerds: 1\n",
      "apposite: 1\n",
      "widelyheld: 1\n",
      "considerateness: 1\n",
      "concordant: 1\n",
      "johnsons: 1\n",
      "rags: 1\n",
      "understatement: 1\n",
      "unco: 1\n",
      "cripples: 1\n",
      "infrequent: 1\n",
      "aflame: 1\n",
      "ironfisted: 1\n",
      "proconsuls: 1\n",
      "noxious: 1\n",
      "pathet: 1\n",
      "quiescent: 1\n",
      "proclivity: 1\n",
      "showdown: 1\n",
      "psychopathic: 1\n",
      "reformatory: 1\n",
      "deriding: 1\n",
      "shudders: 1\n",
      "tempering: 1\n",
      "excerpts: 1\n",
      "scrape: 1\n",
      "mortars: 1\n",
      "cracklings: 1\n",
      "lybia: 1\n",
      "twentyfour: 1\n",
      "lingual: 1\n",
      "spurt: 1\n",
      "adamancy: 1\n",
      "convenes: 1\n",
      "thants: 1\n",
      "signor: 1\n",
      "newgeneration: 1\n",
      "harmonizes: 1\n",
      "venues: 1\n",
      "tiered: 1\n",
      "plaguing: 1\n",
      "valuation: 1\n",
      "minutiae: 1\n",
      "ashed: 1\n",
      "bazaar: 1\n",
      "mitterrand: 1\n",
      "coterie: 1\n",
      "wringing: 1\n",
      "bystander: 1\n",
      "handover: 1\n",
      "zealots: 1\n",
      "coins: 1\n",
      "krugerrands: 1\n",
      "australians: 1\n",
      "borrowers: 1\n",
      "trunk: 1\n",
      "contends: 1\n",
      "careers: 1\n",
      "theology: 1\n",
      "webs: 1\n",
      "worthiness: 1\n",
      "parenthesis: 1\n",
      "begrudge: 1\n",
      "enthroned: 1\n",
      "evaluates: 1\n",
      "reconverting: 1\n",
      "comecon: 1\n",
      "flinched: 1\n",
      "officered: 1\n",
      "gendarmerie: 1\n",
      "fondamentale: 1\n",
      "loi: 1\n",
      "abstinence: 1\n",
      "wavering: 1\n",
      "seventeen: 1\n",
      "divesting: 1\n",
      "courageously: 1\n",
      "rectitude: 1\n",
      "amphibious: 1\n",
      "commonwealths: 1\n",
      "labored: 1\n",
      "moat: 1\n",
      "befalls: 1\n",
      "mich: 1\n",
      "mintoff: 1\n",
      "lampedusa: 1\n",
      "cruising: 1\n",
      "palestian: 1\n",
      "boeing: 1\n",
      "simplifications: 1\n",
      "navies: 1\n",
      "scaled: 1\n",
      "frigate: 1\n",
      "divers: 1\n",
      "wrecks: 1\n",
      "bonniei: 1\n",
      "mifsud: 1\n",
      "mold: 1\n",
      "rhythm: 1\n",
      "alfonsin: 1\n",
      "erection: 1\n",
      "demo: 1\n",
      "perseveres: 1\n",
      "cassandras: 1\n",
      "provenance: 1\n",
      "papandreou: 1\n",
      "barry: 1\n",
      "substratum: 1\n",
      "organisational: 1\n",
      "kuwaits: 1\n",
      "freshening: 1\n",
      "energised: 1\n",
      "pits: 1\n",
      "premonition: 1\n",
      "mortgaging: 1\n",
      "rarest: 1\n",
      "napped: 1\n",
      "ahc: 1\n",
      "symbolises: 1\n",
      "obote: 1\n",
      "consoling: 1\n",
      "counsul: 1\n",
      "sainte: 1\n",
      "gonzague: 1\n",
      "examinations: 1\n",
      "chancellery: 1\n",
      "incendiary: 1\n",
      "tail: 1\n",
      "engined: 1\n",
      "hack: 1\n",
      "galo: 1\n",
      "lavisse: 1\n",
      "pÃ©guy: 1\n",
      "hachette: 1\n",
      "bishops: 1\n",
      "episcopal: 1\n",
      "mÃ©moires: 1\n",
      "mirebalais: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\sarah\\\\OneDrive\\\\Dokumente\\\\Masterarbeit\\\\data\\\\freq\\\\word_counts_wordcloud.pkl']"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#== Count token frequencies ==\n",
    "\n",
    "def count_frequencies(filenames):\n",
    "    total_freq = Counter()\n",
    "    for fname in tqdm(filenames):\n",
    "        data = joblib.load(fname)\n",
    "        tokens = chain.from_iterable(row[1] for row in data if isinstance(row[1], list))\n",
    "        total_freq.update(tokens)\n",
    "    return total_freq\n",
    "\n",
    "def remove_rare_words(filenames, freqs, min_count=10):\n",
    "    for fname in filenames:\n",
    "        data = joblib.load(fname)\n",
    "        filtered_data = []\n",
    "        for doc_id, tokens in data:\n",
    "            filtered_tokens = [w for w in tokens if freqs.get(w, 0) >= min_count]\n",
    "            filtered_data.append([doc_id, filtered_tokens])\n",
    "        joblib.dump(filtered_data, fname)  # overwrite or save as new file\n",
    "        print(f\"Processed {fname}: removed words with freq < {min_count}\")\n",
    "\n",
    "# === Count for preprocessed (stemmed) speeches ===\n",
    "word_counts_stemmed = count_frequencies(preprocessed_files)\n",
    "\n",
    "remove_rare_words(preprocessed_files, word_counts_stemmed, min_count=10)\n",
    "\n",
    "print(\"\\n[Stemmed] Top 100 most common words:\")\n",
    "for word, count in word_counts_stemmed.most_common(100):\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\n[Stemmed] Top 300 least common words:\")\n",
    "for word, count in word_counts_stemmed.most_common()[:-301:-1]:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Save stemmed word counts\n",
    "save_path_stemmed = os.path.join(data_freq, 'word_counts_stemmed.pkl')\n",
    "joblib.dump(word_counts_stemmed, save_path_stemmed)\n",
    "\n",
    "# === Count for wordcloud (unstemmed) speeches ===\n",
    "word_counts_wordcloud = count_frequencies(wordcloud_files)\n",
    "\n",
    "print(\"\\n[Wordcloud] Top 100 most common words:\")\n",
    "for word, count in word_counts_wordcloud.most_common(100):\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\n[Wordcloud] Top 300 least common words:\")\n",
    "for word, count in word_counts_wordcloud.most_common()[:-301:-1]:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Save unstemmed word counts\n",
    "save_path_wordcloud = os.path.join(data_freq, 'word_counts_wordcloud.pkl')\n",
    "joblib.dump(word_counts_wordcloud, save_path_wordcloud)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "932ce36d-d21c-4f1c-aff7-899c321dcfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\OneDrive\\Dokumente\\Masterarbeit\\data\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_c)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39acefa1-c504-478e-98e5-9d465963dc82",
   "metadata": {},
   "source": [
    "### Count the frequency of the dictionary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "9e28f481-7e20-486c-a6b3-8729d6aeaad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of affect dictionary:\n",
      "['forbid', 'unattract', 'cruelti', 'crappi', 'apathi', 'scari', 'unimpress', 'sin', 'dumbest', 'eas', 'agit', 'sob', 'shocker', 'tragedi', 'fabul', 'strongest', 'giver', 'sigh', 'aw', 'witch', 'hurtl', 'fucktard', 'cruel', 'glamor', 'funni', 'smarter', 'brillianc', 'irrate', 'alright', 'honest', 'profit', 'fearless', 'grievous', 'relax', 'isolationist', 'hah', 'shyness', 'poorest', 'cruelest', 'troublemak', 'disagre', 'agon', 'terror', 'fight', 'pleas', 'poor', 'crazi', 'hostil', 'stupid', 'damnat', 'vain', 'jade', 'heartless', 'nag', 'gloomi', 'damn', 'dishearten', 'pleaser', 'credit', 'warmth', 'greatest', 'whine', 'shame', 'angriest', 'envious', 'grin', 'blameless', 'sweeter', 'laidback', 'stupidest', 'unprotect', 'whiner', 'unlov', 'shake', 'boredom', 'fairer', 'weaker', 'wellb', 'bold', 'sucki', 'unsuccess', 'mourner', 'liken', 'defens', 'invigor', 'tedious', 'paranoid', 'cynic', 'dignifi', 'paranoia', 'sweetest', 'contented', 'humili', 'crush', 'terrif', 'alarm', 'asham', 'impati', 'fake', 'flirti', 'laziest', 'libertarian', 'empti', 'panicki', 'rape', 'contemptu', 'incent', 'reward', 'brutal', 'ignoramus', 'bereav', 'wimp', 'laugh', 'nicer', 'warm', 'resent', 'threat', 'hell', 'tortur', 'mad', 'worship', 'uncomfort', 'depress', 'unsaf', 'faultless', 'numb', 'humorless', 'murder', 'prettiest', 'jealous', 'tragic', 'discomfort', 'loveli', 'ungrat', 'divin', 'lamest', 'weaken', 'horribl', 'fuck', 'agoni', 'grievanc', 'ach', 'isolation', 'masoch', 'uglier', 'amaz', 'devilish', 'bitter', 'devil', 'smartest', 'glori', 'trite', 'vicious', 'unsavori', 'grossest', 'sinist', 'cheeri', 'dismay', 'easier', 'flatter', 'virtuous', 'egot', 'generos', 'cherish', 'heartwarm', 'advers', 'tough', 'adorn', 'overwhelm', 'devast', 'egotist', 'sociabl', 'lazier', 'affection', 'sarcasm', 'humor', 'funniest', 'startl', 'innoc', 'vomit', 'shyli', 'kiss', 'burdensom', 'melancholi', 'weakest', 'worthless', 'gentlest', 'proud', 'alarmist', 'pervert', 'demeanor', 'loneli', 'reassur', 'weird', 'passionless', 'loyalti', 'frustrat', 'weak', 'immor', 'hater', 'libertin', 'flirt', 'terribl', 'threaten', 'fatalist', 'melancholia', 'avers', 'prick', 'sucker', 'meanest', 'virtuos', 'benefici', 'neurot', 'disappoint', 'restless', 'romant', 'bastard', 'angri', 'flatteri', 'stun', 'bolder', 'amor', 'forbidden', 'hug', 'demean', 'disgust', 'truer', 'benefic', 'teas', 'stinki', 'condemnatori', 'turmoil', 'poison', 'cunt', 'gorgeous', 'harm', 'lousi', 'glorifi', 'piti', 'wimpish', 'impression', 'freakish', 'happiest', 'apathet', 'pitiabl', 'unpleas', 'suffer', 'miseri', 'toler', 'mess', 'casual', 'anxieti', 'stale', 'fieri', 'grudg', 'arrog', 'outrag', 'thrill', 'faithless', 'popular', 'vaniti', 'palat', 'fantasi', 'decay', 'domin', 'mood', 'proudest', 'scream', 'stress', 'calm', 'fatigu', 'dumber', 'benign', 'rude', 'steal', 'tension', 'enthusiast', 'intimid', 'sweetheart', 'prettier', 'perv', 'pathet', 'flawless', 'contradictori', 'distress', 'moron', 'gratifi', 'frighten', 'anguish', 'snob', 'defenseless', 'fantast', 'sad', 'thief', 'worsen', 'tear', 'handsom', 'attract', 'messi', 'smother', 'scarier', 'afraid', 'insult', 'disreput', 'wept', 'victim', 'traumat', 'prejudic', 'amus', 'decent', 'joyous', 'masochist', 'abandon', 'foe', 'pleasantri', 'shitless', 'strengthen', 'antagonist', 'liar', 'romantic', 'violenc', 'sentimentalist', 'gratif', 'decept', 'witchcraft', 'liveli', 'impressionist', 'guilt', 'griev', 'rotten', 'disgrac', 'insincer', 'pushi', 'enjoy', 'freaki', 'ador', 'grate', 'homesick', 'braver', 'battlefield', 'niceti', 'hooray', 'incompet', 'thiev', 'timid', 'adventur', 'needi', 'joyless', 'grimac', 'gloom', 'dump', 'horrid', 'magnific', 'unlucki', 'killer', 'uncontrol', 'funnier', 'ruin', 'weepi', 'play', 'encourag', 'fantas', 'irrit', 'sincer', 'rage', 'despis', 'unhappi', 'hate', 'bravest', 'harmless', 'hero', 'vigor', 'pervers', 'dumb', 'degrad', 'shameless', 'villaini', 'envi', 'aggress', 'fucker', 'courag', 'craziest', 'delici', 'scare', 'finer', 'bittersweet', 'trivial', 'wealthi', 'heroic', 'ecstat', 'irrat', 'prejudici', 'smug', 'stammer', 'feroci', 'phobic', 'villain', 'anger', 'fond', 'obsession', 'awesom', 'wors', 'compassion', 'weakl', 'calmer', 'inhibit', 'bother', 'rancid', 'hatr', 'beaten', 'opportunist', 'offens', 'haunt', 'appal', 'painless', 'pessimist', 'molest', 'lol', 'wrongdo', 'frantic', 'heroin', 'rejoic', 'woe', 'miser', 'meaner', 'saddest', 'mooch', 'attack', 'enthusiasm', 'joke', 'meritocraci', 'angrier', 'emot', 'weirdest', 'silliest', 'kisser', 'hurrah', 'crueler', 'nervous', 'dishonor', 'gracious', 'sicker', 'hopeless', 'delect', 'compliment', 'petrifi', 'cuter', 'reek', 'loveless', 'daze', 'weirder', 'triumphant', 'impolit', 'offend', 'richer', 'violent', 'crazier', 'comedown', 'heroism', 'stubborn', 'maniac', 'strain', 'broke', 'jerk', 'unkind', 'asshol', 'reveng', 'jealousi', 'damnabl', 'unimport', 'careless', 'gentler', 'bitch', 'shi', 'terrorist', 'rigid', 'harsh', 'passion', 'annoy', 'compass', 'geek', 'weirdo', 'agre', 'whore', 'splendor', 'suck', 'fun', 'chuckl', 'snobberi', 'mock', 'doom', 'reluct', 'mourn', 'boldest', 'helpless', 'trembl', 'pestil', 'curs', 'desper', 'harass', 'complain', 'joker', 'obsess', 'uneasi', 'nasti', 'greedi', 'belov', 'grim', 'upset', 'greed', 'honesti', 'phoni', 'feudal', 'loveliest', 'trauma', 'dwell', 'ecstasi', 'blame', 'legit', 'calmest', 'sickest', 'scariest', 'sarcast', 'cheat', 'fatal', 'stronger', 'energet', 'feroc', 'healthi', 'goddam', 'disast', 'flirtati', 'defeatist', 'glorious', 'terrifi', 'gossip', 'forbad', 'sicken', 'poorer', 'strangest', 'cute', 'struggl', 'woebegon', 'worri', 'gratitud', 'cheer', 'distraught', 'ok', 'vulner', 'blessed', 'fairest', 'lame', 'optim', 'grouch', 'fright', 'pessim', 'sadder', 'worst', 'despair', 'bash', 'intellectu', 'promis', 'prais', 'heartfelt', 'horror', 'crap', 'repress', 'prouder', 'fought', 'discourag', 'heal', 'lazi', 'precious', 'melanchol', 'isol', 'aggressor', 'giggl', 'idiot', 'twitchi', 'glorif', 'kid', 'selfish', 'smile', 'grief', 'hellish', 'shit', 'fume', 'bitchi', 'fool', 'trustworthi', 'optimist', 'dumpi', 'unwelcom', 'neat', 'nicest', 'furi', 'unfriend', 'ridicul', 'arguabl', 'shamefac', 'pervi', 'upbeat', 'wow', 'easygo', 'blameworthi', 'charmer', 'stink', 'phobia', 'adversari', 'sillier', 'distrust', 'dread', 'happier', 'vital', 'dissatisfact', 'insecur', 'assault', 'ineffect', 'panic', 'powerless', 'nightmar', 'wealth', 'enrag', 'splendid', 'foolish', 'ugliest', 'contradict', 'worrier', 'freak']\n",
      "Number of words in affect dictionary: 629\n",
      "Contents of cognition dictionary:\n",
      "['analyt', 'guess', 'suspect', 'coz', 'mysteri', 'unawar', 'proverbi', 'dissimilar', 'consequenti', 'stimul', 'occasion', 'rethink', 'purest', 'entireti', 'abnorm', 'referenti', 'differenti', 'diagnos', 'ambigu', 'concentr', 'undon', 'undecid', 'disclosur', 'fundamentalist', 'blur', 'reaction', 'variabl', 'recogniz', 'meant', 'reconstruct', 'supposit', 'acknowledg', 'choosi', 'accuraci', 'mislead', 'perceiv', 'hypothesi', 'unknow', 'cuz', 'reveal', 'potenti', 'puriti', 'persuad', 'indirect', 'generat', 'presumpt', 'clue', 'justif', 'evidenti', 'realize', 'deduct', 'causal', 'inevit', 'thinker', 'evidentiari', 'factor', 'undeni', 'unclear', 'pick', 'induc', 'accur', 'absolut', 'everytim', 'reconsider', 'diagnost', 'solut', 'diagnosi', 'discoveri', 'ration', 'unambigu', 'explicit', 'decis', 'insight', 'confess', 'fuzzi', 'led', 'lesson', 'theoriz', 'analyz', 'learner', 'split', 'enlighten', 'cos', 'respons', 'dubious', 'react', 'guarante', 'someday', 'blatant', 'undo', 'perceptu', 'infal', 'afterthought', 'inequ', 'feasibl', 'knowabl', 'approxim', 'logic', 'meaning', 'unknown', 'categor', 'trigger', 'incomplet', 'curious', 'attribut', 'pretend', 'reactionari', 'prove', 'meaningless', 'reactiv', 'proof', 'bet', 'purpos', 'suppos', 'discern', 'imagin', 'kinda', 'comprehens', 'sens', 'percept', 'everyday', 'rational', 'unquestion', 'random', 'implicit', 'clarif', 'theori', 'dunno', 'analysi', 'obedi', 'memor', 'genuin', 'evalu', 'explor', 'linkag', 'grasp', 'curios', 'methink', 'somewher', 'deduc', 'unwant', 'induct', 'unrel', 'outstand', 'discov', 'unresolv', 'exploratori', 'choos', 'perspect', 'inferenti', 'disclos', 'factual', 'arbitrari', 'conting', 'vagu', 'persuas', 'manipul', 'impli', 'somehow', 'sorta', 'borderlin', 'reconcil', 'anyhow', 'conscious', 'probabilist', 'precis', 'rearrang', 'referenc', 'obscur']\n",
      "Number of words in cognition dictionary: 169\n"
     ]
    }
   ],
   "source": [
    "# == Count dictionary words\n",
    "\n",
    "# Load dictionaries            ##### How did they come up with this dictionary? Why did they exclude words?\n",
    "\n",
    "affect_path = os.path.join(data_dict, 'dictionary_affect.pkl')\n",
    "cognition_path = os.path.join(data_dict, 'dictionary_cognition.pkl')\n",
    "\n",
    "with open(affect_path, 'rb') as f:\n",
    "    affect_dict = pickle.load(f)\n",
    "print(\"Contents of affect dictionary:\")\n",
    "print(affect_dict)\n",
    "print(\"Number of words in affect dictionary:\", len(affect_dict))\n",
    "\n",
    "with open(cognition_path, 'rb') as f:\n",
    "    cognition_dict = pickle.load(f)\n",
    "print(\"Contents of cognition dictionary:\")\n",
    "print(cognition_dict)\n",
    "print(\"Number of words in cognition dictionary:\", len(cognition_dict))\n",
    "\n",
    "affect = joblib.load(affect_path)\n",
    "cognition = joblib.load(cognition_path)\n",
    "\n",
    "a = [[i, word_counts_stemmed[i]] for i in affect if i in word_counts_stemmed]\n",
    "c = [[i, word_counts_stemmed[i]] for i in cognition if i in word_counts_stemmed]\n",
    "\n",
    "a = sorted(a, key=lambda x: x[1], reverse=True)\n",
    "c = sorted(c, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "a = [[i[0], f\"({i[1]}),\"] for i in a]\n",
    "c = [[i[0], f\"({i[1]}),\"] for i in c]\n",
    "\n",
    "a1 = ' '.join(str(r) for v in a for r in v)\n",
    "c1 = ' '.join(str(r) for v in c for r in v)\n",
    "\n",
    "affect_out_path = os.path.join(data_freq, \"affect_words.txt\")\n",
    "cog_out_path = os.path.join(data_freq, \"cog_words.txt\")\n",
    "\n",
    "os.makedirs(data_freq, exist_ok=True)  # ensure directory exists\n",
    "\n",
    "with open(affect_out_path, \"w\") as output:\n",
    "    output.write(a1)\n",
    "\n",
    "with open(cog_out_path, \"w\") as output:\n",
    "    output.write(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "0dffe87f-d712-4d71-9305-fb288cf06402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 words by weighted frequency:\n",
      "burlesqu: 0.9990900578814182\n",
      "indisposit: 0.9990900578814182\n",
      "breyten: 0.9990900578814182\n",
      "breytenbach: 0.9990900578814182\n",
      "tambo: 0.9990900578814182\n",
      "bewitch: 0.9990900578814182\n",
      "couitri: 0.9990900578814182\n",
      "gladiatori: 0.9990900578814182\n",
      "whet: 0.9990900578814182\n",
      "roryo: 0.9990900578814182\n",
      "eyaderaa: 0.9990900578814182\n",
      "postdisast: 0.9990900578814182\n",
      "drugcontrol: 0.9990900578814182\n",
      "newfound: 0.9990900578814182\n",
      "antirac: 0.9990900578814182\n",
      "triennial: 0.9990900578814182\n",
      "multifold: 0.9990900578814182\n",
      "sodist: 0.9990900578814182\n",
      "sebastian: 0.9990900578814182\n",
      "piÃ±era: 0.9990900578814182\n",
      "echeÃ±iqu: 0.9990900578814182\n",
      "lood: 0.9990900578814182\n",
      "mudslid: 0.9990900578814182\n",
      "plurin: 0.9990900578814182\n",
      "interventionin: 0.9990900578814182\n",
      "maronit: 0.9990900578814182\n",
      "influent: 0.9990900578814182\n",
      "communion: 0.9990900578814182\n",
      "debtserv: 0.9990900578814182\n",
      "ancillari: 0.9990900578814182\n",
      "sovietgerman: 0.9990900578814182\n",
      "nahum: 0.9990900578814182\n",
      "goldmann: 0.9990900578814182\n",
      "judenstaat: 0.9990900578814182\n",
      "yassir: 0.9990900578814182\n",
      "theocraci: 0.9990900578814182\n",
      "ismailia: 0.9990900578814182\n",
      "evermor: 0.9990900578814182\n",
      "prebisch: 0.9990900578814182\n",
      "sardenberg: 0.9990900578814182\n",
      "diplomatist: 0.9990900578814182\n",
      "xhosa: 0.9990900578814182\n",
      "sanguin: 0.9990900578814182\n",
      "sorest: 0.9990900578814182\n",
      "apportion: 0.9990900578814182\n",
      "readili: 0.9990900578814182\n",
      "anteroom: 0.9990900578814182\n",
      "cheerless: 0.9990900578814182\n",
      "bridgeabl: 0.9990900578814182\n",
      "tshomb: 0.9990900578814182\n",
      "snowman: 0.9990900578814182\n",
      "incorrig: 0.9990900578814182\n",
      "fritter: 0.9990900578814182\n",
      "deracin: 0.9990900578814182\n",
      "motley: 0.9990900578814182\n",
      "dimmer: 0.9990900578814182\n",
      "therefrom: 0.9990900578814182\n",
      "chea: 0.9990900578814182\n",
      "Ðµec: 0.9990900578814182\n",
      "bbc: 0.9990900578814182\n",
      "selfassess: 0.9990900578814182\n",
      "romant: 0.9990900578814182\n",
      "gÃ¼l: 0.9990900578814182\n",
      "changer: 0.9990900578814182\n",
      "zonal: 0.9990900578814182\n",
      "communautair: 0.9990900578814182\n",
      "levantin: 0.9990900578814182\n",
      "geni: 0.9990900578814182\n",
      "glamor: 0.9990900578814182\n",
      "ingvar: 0.9990900578814182\n",
      "interlud: 0.9990900578814182\n",
      "compliant: 0.9990900578814182\n",
      "bakili: 0.9990900578814182\n",
      "muluzi: 0.9990900578814182\n",
      "readmiss: 0.9990900578814182\n",
      "srdjan: 0.9990900578814182\n",
      "roma: 0.9990900578814182\n",
      "gorani: 0.9990900578814182\n",
      "redraw: 0.9990900578814182\n",
      "serbianalbanian: 0.9990900578814182\n",
      "cilss: 0.9990900578814182\n",
      "sputnik: 0.9990900578814182\n",
      "astravyet: 0.9990900578814182\n",
      "pedro: 0.9990900578814182\n",
      "pablo: 0.9990900578814182\n",
      "kuczynski: 0.9990900578814182\n",
      "thechart: 0.9990900578814182\n",
      "toughest: 0.9990900578814182\n",
      "intertogoles: 0.9990900578814182\n",
      "bioga: 0.9990900578814182\n",
      "unjustic: 0.9990900578814182\n",
      "traceabl: 0.9990900578814182\n",
      "sebastopol: 0.9990900578814182\n",
      "bayan: 0.9990900578814182\n",
      "nour: 0.9990900578814182\n",
      "multipurpos: 0.9990900578814182\n",
      "dubai: 0.9990900578814182\n",
      "assalam: 0.9990900578814182\n",
      "alaikum: 0.9990900578814182\n",
      "ting: 0.9990900578814182\n"
     ]
    }
   ],
   "source": [
    "# == Calculate weighted frequencies for all words\n",
    "\n",
    " # STEMMED OR NOT?\n",
    "# - downweights very common words by giving more importance to rare ones\n",
    "word_counts_stemmed = joblib.load(os.path.join(data_freq, 'word_counts_stemmed.pkl'))\n",
    "\n",
    "l = sum(word_counts_stemmed.values())\n",
    "\n",
    "a = 0.001\n",
    "word_counts_weighted = {k: a / (a + (v / l)) for k, v in word_counts_stemmed.items()}\n",
    "#for key in word_counts.keys():\n",
    " #   word_counts[key] = a / (a + (word_counts[key] / l))\n",
    "\n",
    "joblib.dump(word_counts_weighted, os.path.join(data_freq, 'word_counts_weighted.pkl'))\n",
    "\n",
    "################################################################################ ISSUE##################\n",
    "# To print top 100 by weighted values, sort the dictionary by value descending:\n",
    "top_100_weighted = sorted(word_counts_weighted.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "\n",
    "print(\"Top 100 words by weighted frequency:\")\n",
    "for word, weight in top_100_weighted:\n",
    "    print(f\"{word}: {weight}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0856a2-44f1-4304-b221-09a9ab72ad17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001e6aaf-101b-4912-a198-3d79f6d6e828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5ba7c0-33c7-4420-9cb0-fe54a94ee649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf1db25-1369-45bc-b6c4-ddddb77a3c52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
